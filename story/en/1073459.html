<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>缓存关联性Cache Associativity</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Cache Associativity<br/>缓存关联性</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-15 15:35:05</div><div class="page_narrow text-break page_content"><p>Consider a  strided incrementing loop over an array of size $N=2^{21}$ with a fixed step size of 256:</p><p>考虑在一个大小为$n＝2 ^ { 21 }的数组上的一个步长递增循环，其固定步长为256：</p><p>    Which one will be faster to finish? There are several considerations that come to mind:</p><p>哪一个更快完成？我想到了几点考虑：</p><p> At first, you think that there shouldn’t be much difference, or maybe that the second loop is $\frac{257}{256}$ times faster or so because it does fewer iterations in total.</p><p>一开始，您认为应该没有太大的差异，或者第二个循环的速度是$\frac{257}{256}$倍左右，因为它总共进行了较少的迭代。</p><p> Then you recall that 256 is a nice round number, which may have something to do with  SIMD or the memory system, so maybe the first one is faster.</p><p>然后你会想起256是一个很好的整数，这可能与SIMD或内存系统有关，所以第一个可能更快。</p><p> But the right answer is very counterintuitive: the second loop is faster — and by a factor of 10.</p><p>但正确的答案是非常违反直觉的：第二个循环更快，而且速度是10倍。</p><p> This isn’t just a single bad step size. The performance degrades for all indices that are multiples of large powers of two:</p><p>这不仅仅是一个糟糕的步长。如果所有指数都是2的大幂的倍数，则性能会下降：</p><p>  There is no vectorization or anything, and the two loops produce the same assembly except for the step size. This effect is due only to the memory system, in particular to a feature called  cache associativity, which is a peculiar artifact of how CPU caches are implemented in hardware.</p><p>没有矢量化或任何东西，两个循环产生相同的集合，除了步长。这种影响只归因于内存系统，尤其是一种称为缓存关联性的功能，它是CPU缓存在硬件中实现方式的一种特殊产物。</p><p>   When we were studying the memory system  theoretically, we discussed different ways one can  implement cache eviction policies in software. One particular strategy we focused on was the  least recently used (LRU) policy, which is simple and effective but still requires some non-trivial data manipulation.</p><p>当我们从理论上研究内存系统时，我们讨论了在软件中实现缓存逐出策略的不同方法。我们关注的一个特定策略是最近最少使用（LRU）策略，它简单有效，但仍然需要一些非平凡的数据操作。</p><p> In the context of hardware, such scheme is called  fully associative cache: we have $M$ cells, each capable of holding a cache line corresponding to any of the $N$ total memory locations, and in case of contention, the one not accessed the longest gets kicked out and replaced with the new one.</p><p>在硬件环境中，这样的方案被称为完全关联缓存：我们有$M$个单元，每个单元都能够容纳一条缓存线，对应于总内存为$N$的任何位置，在发生争用的情况下，最长未被访问的单元将被踢出并替换为新的单元。</p><p>  The problem with fully associative cache is that implementing the “find the oldest cache line among millions” operation is pretty hard to do in software and just unfeasible in hardware. You can make a fully associative cache that has 16 entries or so, but managing hundreds of cache lines already becomes either prohibitively expensive or so slow that it’s not worth it.</p><p>完全关联缓存的问题是，在软件中实现“在数百万个缓存线中查找最古老的缓存线”操作非常困难，在硬件中则不可行。您可以创建一个具有16个条目左右的完全关联缓存，但管理数百条缓存线已经变得非常昂贵，或者速度太慢，不值得这么做。</p><p> We can resort to another, much simpler approach: just map each block of 64 bytes in RAM to a single cache line which it can occupy. Say, if we have 4096 blocks in memory and 64 cache lines for them, then each cache line at any time stores the contents of one of $\frac{4096}{64} = 64$ different blocks.</p><p>我们可以求助于另一种更简单的方法：只需将RAM中64字节的每个块映射到它可以占用的单个缓存线。比方说，如果我们在内存中有4096个块和64个缓存线，那么每个缓存线在任何时候都存储$\frac{4096}{64}=64$不同块中的一个块的内容。</p><p>  A direct-mapped cache is easy to implement doesn’t require storing any additional meta-information associated with a cache line except its tag (the actual memory location of a cached block). The disadvantage is that the entries can be kicked out too quickly — for example, when bouncing between two addresses that map to the same cache line — leading to lower overall cache utilization.</p><p>直接映射缓存很容易实现，除了标记（缓存块的实际内存位置）之外，它不需要存储与缓存线相关的任何附加元信息。缺点是，条目可能被踢出得太快——例如，在映射到同一缓存线的两个地址之间跳转时——导致整体缓存利用率降低。</p><p> For that reason, we settle for something in-between direct-mapped and fully associative caches: the  set-associative cache. It splits the address space into equal groups, which separately act as small fully-associative caches.</p><p>因此，我们满足于直接映射缓存和完全关联缓存之间的某种东西：集合关联缓存。它将地址空间分成相等的组，这些组分别充当小型全关联缓存。</p><p>  Associativity is the size of these sets, or, in other words, how many different cache lines each data block can be mapped to. Higher associativity allows for more efficient utilization of cache but also increases the cost.</p><p>关联性是这些集合的大小，或者换句话说，每个数据块可以映射到多少不同的缓存线。较高的关联性允许更有效地利用缓存，但也会增加成本。</p><p> For example, on  my CPU, the L3 cache is 16-way set-associative, and there are 4MB available to a single core. This means that there are in total $\frac{2^{22}}{2^{6}} = 2^{16}$ cache lines, which are split into $\frac{2^{16}}{16} = 2^{12}$ groups, each acting as a fully associative cache of their own $(\frac{1}{2^{12}})$-th fraction of the RAM.</p><p>例如，在我的CPU上，L3缓存是16路集关联的，单个内核有4MB可用空间。这意味着总共有$\frac{2^{22}}{2^{6}}}=2^{16}$缓存线，这些缓存线被分割成$\frac{2^{16}{16}=2^{12}$组，每个组充当它们自己的第$（\frac{1}{2^{12}}}）部分的完全关联缓存。</p><p> Most other CPU caches are also set-associative, including the non-data ones such as the instruction cache and the TLB. The exceptions are small specialized caches that only house 64 or fewer entries — these are usually fully associative.</p><p>大多数其他CPU缓存也设置为关联，包括非数据缓存，如指令缓存和TLB。例外情况是只有64个或更少条目的小型专用缓存——它们通常是完全关联的。</p><p>    If we implemented set-associative cache in software, we would compute some hash function of the memory block address and then use its value as the cache line index. In hardware, we can’t really do that because it is too slow: for example, for the L1 cache, the latency requirement is 4 or 5 cycles, and even  taking a modulo takes around 10-15 cycles, let alone something more sophisticated.</p><p>如果我们在软件中实现set关联缓存，我们将计算内存块地址的一些哈希函数，然后使用其值作为缓存线索引。在硬件方面，我们不能真正做到这一点，因为它太慢了：例如，对于一级缓存，延迟要求是4或5个周期，甚至取模也需要10-15个周期，更不用说更复杂的东西了。</p><p> Instead, the hardware uses the lazy approach. It takes the memory address that needs to be accessed and splits it into three parts — from lower bits to higher:</p><p>相反，硬件使用惰性方法。它获取需要访问的内存地址，并将其分为三部分——从低位到高位：</p><p> offset — the index of the word within a 64B cache line ($\log_2 64 = 6$ bits);</p><p>偏移量——64B缓存线内的字索引（$\log_2 64=6$位）；</p><p> index — the index of the cache line set (the next $12$ bits as there are $2^{12}$ cache lines in the L3 cache);</p><p>index-缓存线集的索引（接下来的$12$位，因为三级缓存中有$2^{12}$缓存线）；</p><p> tag — the rest of the memory address, which is used to tell the memory blocks stored in the cache lines apart.</p><p>标记-内存地址的其余部分，用于区分存储在缓存线中的内存块。</p><p> In other words, all memory addresses with the same “middle” part map to the same set.</p><p>换句话说，所有具有相同“中间”部分的内存地址都映射到同一个集合。</p><p>  This makes the cache system simpler and cheaper to implement but also susceptible to certain bad access patterns.</p><p>这使得缓存系统实现起来更简单、更便宜，但也容易受到某些不良访问模式的影响。</p><p>   Now, where were we? Oh, yes: the reason why iteration with strides of 256 causes such a terrible slowdown.</p><p>现在，我们在哪里？哦，是的：为什么256步的迭代会导致如此严重的减速。</p><p> When we jump over 256 integers, the pointer always increments by $1024 = 2^{10}$, and the last 10 bits remain the same. Since the cache system uses the lower 6 bits for the offset and the next 12 for the cache line index, we are essentially using just $2^{12 - (10 - 6)} = 2^8$ different sets in the L3 cache instead of $2^{12}$, which has the effect of shrinking our L3 cache by a factor of $2^4 = 16$. The array stops fitting into the L3 cache ($N=2^{21}$) and spills into the order-of-magnitude slower RAM, which causes the performance to decrease.</p><p>当我们跳过256个整数时，指针总是增加$1024=2^{10}$，最后10位保持不变。由于缓存系统使用较低的6位作为偏移量，使用下一个12位作为缓存线索引，因此我们在三级缓存中基本上只使用$2^{12-（10-6）}=2^8$不同的集合，而不是$2^{12}$，这会将三级缓存缩小一倍$2^4=16$。阵列停止装入L3缓存（$N=2^{21}$），并溢出到数量级较慢的RAM中，这会导致性能降低。</p><p> Performance issues caused by cache associativity effects arise with remarkable frequency in algorithms because, for multiple reasons, programmers just love using powers of two when indexing arrays:</p><p>缓存关联性效应导致的性能问题在算法中出现的频率非常高，因为出于多种原因，程序员喜欢在索引数组时使用二的幂：</p><p> It is easier to calculate the address for multi-dimensional array accesses if the last dimension is a power of two, as it only requires a binary shift instead of a multiplication.</p><p>如果最后一个维度是2的幂，则更容易计算多维数组访问的地址，因为它只需要二进制移位而不是乘法。</p><p> It is easier to calculate modulo a power of two, as it can be done with a single bitwise “and”.</p><p>计算二次幂的模更容易，因为它可以用一个按位的“and”来完成。</p><p> It is convenient and often even necessary to use power-of-two problem sizes in divide-and-conquer algorithms.</p><p>在分治算法中，使用两个问题大小的幂是方便的，甚至经常是必要的。</p><p> It is the smallest integer exponent, so using the sequence of increasing powers of two as problem sizes are a popular choice when benchmarking memory-bound algorithms.</p><p>它是最小的整数指数，因此在对内存限制算法进行基准测试时，使用二次幂递增序列作为问题大小是一种流行的选择。</p><p> Also, more natural powers of ten are by transitivity divisible by a slightly lower power of two.</p><p>此外，更自然的十次幂可以通过传递性被稍微低一点的二次幂整除。</p><p> This especially often applies to implicit data structures that use a fixed memory layout. For example, binary searching over arrays of size $2^{20}$ takes about ~360ns per query while searching over arrays of size $(2^{20} + 123)$ takes ~300ns. When the array size is a multiple of a large power of two, then the indices of the “hottest” elements, the ones we likely request on the first dozen or so iterations, will also be divisible by some large powers of two and map to the same cache line — kicking each other out and causing a ~20% performance decrease.</p><p>这尤其适用于使用固定内存布局的隐式数据结构。例如，在大小为$2^{20}$的数组上进行二进制搜索，每次查询大约需要约360ns，而在大小为$（2^{20}+123）$的数组上进行搜索则需要约300ns。当数组大小是2的大幂的倍数时，“最热”元素的索引（我们可能在前十几次迭代中请求的元素）也将被一些大的2幂整除，并映射到同一缓存线——相互踢出，导致性能降低约20%。</p><p> Luckily, such issues are more of an anomaly rather than serious problems. The solution is usually simple: avoid iterating in powers of two, make the last dimensions of multi-dimensional arrays a slightly different size or use any other method to insert “holes” in the memory layout, or create some seemingly random bijection between the array indices and the locations where the data is actually stored.</p><p>幸运的是，这些问题更多的是反常现象，而不是严重问题。解决方案通常很简单：避免以二的幂进行迭代，使多维数组的最后一个维度的大小略有不同，或者使用任何其他方法在内存布局中插入“孔”，或者在数组索引和实际存储数据的位置之间创建一些看似随机的双射。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/缓存/">#缓存</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cache/">#cache</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>