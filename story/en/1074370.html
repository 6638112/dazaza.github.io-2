<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>深度学习中的训练测试Train Test Split in Deep Learning</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Train Test Split in Deep Learning<br/>深度学习中的训练测试</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-21 04:48:03</div><div class="page_narrow text-break page_content"><p>One of the golden rules in machine learning is to split your dataset into train, validation, and test set. Learn how to bypass the most common caveats!</p><p>机器学习的黄金法则之一是将数据集分为训练集、验证集和测试集。了解如何绕过最常见的警告！</p><p> The reason we do that is very simple. If we would not split the data into different sets the model would be evaluated on the same data it has seen during training. We therefore could run into problems such as overfitting without even knowing it.</p><p>我们这样做的原因很简单。如果我们不将数据分成不同的集合，模型将根据训练期间看到的相同数据进行评估。因此，我们可能会遇到一些问题，比如在不知不觉中过度装修。</p><p>      We split the dataset  randomly into three subsets called the  train,  validation, and  test set. Splits could be 60/20/20 or 70/20/10 or any other ratio you desire.</p><p>我们将数据集随机分成三个子集，称为训练集、验证集和测试集。拆分可以是60/20/20或70/20/10或任何其他您想要的比率。</p><p>   If we are not happy with the results we can change the hyperparameters or pick another model and  go again to step 2</p><p>如果我们对结果不满意，我们可以更改超参数或选择另一个模型，然后再次转到步骤2</p><p> Finally, once we’re happy with the results on the  validation set we can evaluate our model on the  test set.</p><p>最后，一旦我们对验证集的结果满意，我们就可以在测试集上评估我们的模型。</p><p> If we’re happy with the results we can now again train our model on the  train and  validation set combined using last the hyperparameters we derived.</p><p>如果我们对结果感到满意，我们现在可以再次在训练集和验证集上训练我们的模型，并使用我们最后导出的超参数。</p><p> We can again evaluate the model accuracy on the  test set and if we’re happy deploy the model.</p><p>我们可以在测试集上再次评估模型的准确性，如果我们愿意，可以部署模型。</p><p> Most ML frameworks provide built-in methods for random train/ test splits of a dataset. The most well-known example is the  train_test_split function of scikit-learn.</p><p>大多数ML框架为数据集的随机训练/测试分割提供了内置方法。最著名的例子是scikit learn的train_test_split函数。</p><p>   Yes, this could be a problem. With very small datasets the test set will be tiny and therefore a single wrong prediction has a strong impact on the test accuracy. Fortunately, there is a way to work around this problem.</p><p>是的，这可能是个问题。对于非常小的数据集，测试集将非常小，因此单个错误预测对测试精度有很大影响。幸运的是，有一种方法可以解决这个问题。</p><p> The solution to this problem is called  cross-validation. We essentially create partitions of our dataset as shown in the image below. We always hold out a set for testing and use all the other data for training. Finally, we gather and average all the results from the testing sets. We essentially trained k models and using this trick managed to get statistics of evaluating the model on the full dataset (as every sample has been part of one of the k test sets).</p><p>这个问题的解决方案称为交叉验证。我们基本上创建了数据集的分区，如下图所示。我们总是拿出一套测试数据，并使用所有其他数据进行培训。最后，我们收集并平均测试集的所有结果。我们基本上训练了k个模型，并使用这个技巧获得了在完整数据集上评估模型的统计数据（因为每个样本都是k个测试集的一部分）。</p><p>  This approach is barely used in recent deep learning methods as it’s super expensive to train a model k times.</p><p>这种方法在最近的深度学习方法中几乎没有使用，因为训练一个k型的时间非常昂贵。</p><p> With the rise of deep learning and the massive increase in dataset sizes, the need for techniques such as cross-validation or having a separate validation set has diminished. One reason for this is that experiments are very expensive and take a long time. Another one is that due to the large datasets and nature of most deep learning methods the models got less affected by overfitting.</p><p>随着深度学习的兴起和数据集规模的大幅增加，对交叉验证或单独验证集等技术的需求已经减少。其中一个原因是实验非常昂贵，而且需要很长时间。另一个原因是，由于大数据集和大多数深度学习方法的性质，模型受过度拟合的影响较小。</p><p> Overfitting is still a problem in deep learning. But overfitting to 50 samples with 10 features happens faster than overfitting to 100k images with millions of pixels</p><p>过度拟合仍然是深度学习中的一个问题。但是，对10个特征的50个样本进行过拟合比对百万像素的100k图像进行过拟合要快</p><p> One could argue that researchers and practitioners got lazy/ sloppy. It would be interesting to see any recent paper investigating such effects again. For example, it could be that researchers in the past years have heavily overfitted their models to the test set of ImageNet as there has been an ongoing struggle to improve it and become state-of-the-art.</p><p>有人可能会说，研究人员和从业者变得懒惰/邋遢。如果能看到最近的任何一篇论文再次研究这种效应，那将是一件有趣的事情。例如，可能是过去几年，研究人员在ImageNet的测试集上过度拟合了他们的模型，因为一直在努力改进它，使之成为最先进的技术。</p><p>   Naively, one could just manually split the dataset into three chunks. The problem with this approach is that we humans are very biased and this bias would get introduced into the three sets.</p><p>天真的是，可以手动将数据集分成三个块。这种方法的问题是，我们人类是非常有偏见的，这种偏见会被引入到三个集合中。</p><p> In academia, we learn that we should pick them randomly. A random split into the three sets guarantees that all three sets follow the same statistical distribution. And that’s what we want since ML is all about statistics.</p><p>在学术界，我们知道我们应该随机挑选它们。随机分成三组可以保证所有三组遵循相同的统计分布。这就是我们想要的，因为ML是关于统计的。</p><p> Deriving the three sets from completely different distributions would yield some unwanted results. There is not much value in training a model on pictures of cats if we want to use it to classify flowers.</p><p>从完全不同的分布中导出这三个集合会产生一些不想要的结果。如果我们想用猫的图片训练一个模型来对花进行分类，那么它没有多大价值。</p><p>  However, the underlying assumption of a random split is that the initial dataset already matches the statistical distribution of the problem we want to solve. That would mean that for problems such as autonomous driving the assumption is that our dataset covers all sorts of cities, weather conditions, vehicles, seasons of the year, special situations, etc.</p><p>然而，随机分割的基本假设是，初始数据集已经与我们想要解决的问题的统计分布相匹配。这意味着，对于自动驾驶等问题，假设我们的数据集涵盖了各种城市、天气条件、车辆、一年四季、特殊情况等。</p><p> As you might think this assumption is actually not valid for most practical deep learning applications. Whenever we collect data using sensors in an uncontrolled environment we might not have the desired data distribution.</p><p>正如你可能认为的那样，这个假设实际上并不适用于大多数实际的深度学习应用。每当我们在一个不受控制的环境中使用传感器收集数据时，我们可能无法获得所需的数据分布。</p><p> But that’s bad. What am I supposed to do if I’m not able to collect a representative dataset of the problem I try to solve?</p><p>但这很糟糕。如果我无法收集我试图解决的问题的代表性数据集，我该怎么办？</p><p> What you’re looking for is the research area around finding and dealing with  domain gaps,  distributional shifts, or  data drift. All these terms have their own specific definition. I’m listing them here so you can search for the relevant problems easily.</p><p>你要寻找的是关于发现和处理领域差距、分布变化或数据漂移的研究领域。所有这些术语都有自己的具体定义。我在这里列出了它们，这样你就可以轻松地搜索相关问题。</p><p> With a  domain, we refer to the data domain, as the source and type of the data we use. There are three ways to move forward:</p><p>对于域，我们指的是数据域，即我们使用的数据的来源和类型。有三种前进的方式：</p><p>    In machine learning, we refer to out-of-distribution whenever our model has to perform well in a situation where the new input data is from a different distribution than the training data. Going back to our autonomous driving example from before, we could say that for a model that has only been trained on sunny California weather, doing predictions in Europe is out of distribution.</p><p>在机器学习中，当我们的模型必须在新输入数据来自与训练数据不同的分布的情况下表现良好时，我们指的是分布外。回到我们之前的自动驾驶示例，我们可以说，对于一个只在加利福尼亚晴朗天气下训练的模型，在欧洲进行预测是不合理的。</p><p>  Since we collected the data using different sensors we also might have additional information about the source for each of the samples (a sample could be an image, lidar frame, video, etc.).</p><p>由于我们使用不同的传感器收集数据，我们还可能有关于每个样本来源的附加信息（样本可以是图像、激光雷达帧、视频等）。</p><p>       You very likely overfitted your model to the validation set or validation and test set are very different. But how?</p><p>您很可能过度拟合了验证集的模型，或者验证集和测试集非常不同。但是怎么做呢？</p><p> You likely did several iterations of tweaking the parameters to squeeze out the last bit of accuracy your model can yield on the validation set. The validation set is no longer fulfilling its purpose. At this point, you should relax some of your hyperparameters or introduce regularization methods.</p><p>您可能多次反复调整参数，以挤出模型在验证集上可以产生的最后一点精度。验证集不再满足其目的。此时，您应该放松一些超参数或引入正则化方法。</p><p> After deriving my final hyperparameters I want to retrain my model on the full dataset (train + validation + test) before shipping</p><p>在导出最终的超参数后，我想在发货前在完整的数据集（训练+验证+测试）上重新训练我的模型</p><p> No, don’t do this. The hyperparameters have been tuned for the train (or maybe the train + validation) set and might yield a different result when used for the full dataset. Furthermore, you won’t be able to answer the question anymore of how good your model really performs as the test set no longer exists.</p><p>不，不要这样做。hyperparameters已经针对train（或者train+验证）集进行了调整，当用于完整数据集时，可能会产生不同的结果。此外，由于测试集不再存在，您将无法再回答模型的实际性能有多好的问题。</p><p> I have a video dataset and want to split the frames randomly into train, validation, and test set</p><p>我有一个视频数据集，想把帧随机分成训练集、验证集和测试集</p><p> Since video frames are very likely highly correlated (e.g. two frames next to each other almost look the same) this is a bad idea. It’s almost the same as if we would evaluate the model on the training data. Instead, you should split the dataset across videos (e.g. videos 1,3,5 are used for training and video 2,4 for validation). You can again use a random train test split but this time on the video level instead of the frame level.</p><p>由于视频帧很可能高度相关（例如，相邻的两帧看起来几乎相同），这是一个坏主意。这与我们根据训练数据评估模型几乎是一样的。相反，您应该将数据集拆分为多个视频（例如，视频1、3、5用于培训，视频2、4用于验证）。您可以再次使用随机列车测试拆分，但这次是在视频级别而不是帧级别。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/深度学习/">#深度学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/深度/">#深度</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/test/">#test</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>