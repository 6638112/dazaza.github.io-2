<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>深度学习模型压缩方法 Deep learning model compression methods</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Deep learning model compression methods<br/>深度学习模型压缩方法 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-05 11:21:50</div><div class="page_narrow text-break page_content"><p>This post covers model inference optimization or compression in breadth and hopefully depth as of March 2021. This includes engineering topics like model quantization and binarization, more research-oriented topics like knowledge distillation, as well as well-known-hacks.</p><p>这篇文章涵盖了模型推理优化或压缩，截至3月2021年3月的宽度和最深度。这包括模型量化和二值化的工程主题，更具研究型主题，如知识蒸馏等，以及知名的，以及众所周知的话题。</p><p> If anyone notices anything incorrect, please let me know. Feel free to contact me at my email (my full name @ outlook.com).</p><p> 如果有人注意到任何不正确的东西，请告诉我。请随时与我联系我的电子邮件（我的全名@ Outlook.com）。</p><p> Each year, larger and larger models are able to find methods for extracting signal from the noise in machine learning. In particular, language models get larger every day. These models are computationally expensive (in both runtime and memory), which can be both costly when served out to customers or too slow or large to function in edge environments like a phone.</p><p> 每年，较大且较大的模型能够找到从机器学习中噪声提取信号的方法。特别是，语言模型每天都会变大。这些模型在计算上昂贵（在运行时和内存中），在向客户提供或过于慢或较大时可以昂贵，以便在Phone等边缘环境中运行。</p><p> Researchers and practitioners have come up with many methods for optimizing neural networks to run faster or with less memory usage. In this post I’m going to cover some of the state-of-the-art methods. If you know of another method you think should be included, I’m happy to add it. This has a slight PyTorch bias (haha) because I’m most familiar with it.</p><p> 研究人员和从业者提出了许多方法，以优化神经网络以更快或更少的内存使用率运行。在这篇文章中，我将涵盖一些最先进的方法。如果您知道应包括其他方法，我很乐意添加它。这有一个轻微的Pytorch偏见（哈哈），因为我最熟悉它。</p><p>  Quantization generally refers to taking a model with parameters trained at high precision (32 or 64 bits) and reducing the number of bits that each weight takes (for example down to 16, 8, or even fewer). In practice, this usually leads to a speedup of 2-4x (highest for nets with convolutions, in my experience).</p><p>  量化通常是指采用具有在高精度（32或64位）的参数的模型，并减少每个重量所采用的比特数（例如，低至16,8，甚至更少）。在实践中，这通常会导致2-4倍的加速（在我的经验中具有互联网的最高网络）。</p><p> Why does this work? It turns out that for deep networks to work, we don’t need highly precise values for the network’s weights. With proper hardware support, processing deep learning kernels (a fancy term for mathematical operations) using fewer bits can be faster and more memory efficient simply because there’s fewer bits to compute ( torch.qint8 is 8 bits, and  torch.float32 is 32 bits, so 4x smaller).  Downsides: Depending on the level of quantization attempted, you might find that an operation you want (for example, a particular convolutional op or even something as simple as transpose) might not be implemented. Of course, as with all methods, you might find that accuracy drops off too much to be useful.</p><p> 为什么这项工作？事实证明，对于深度网络工作，我们不需要高精度的网络权重。通过适当的硬件支持，使用更少位处理深度学习内核（用于数学运算的花哨术语）可以更快，更快的内存效率更快，因为计算的比特较少（torch.qint8是8位，而torch.float32是32位，所以4x较小）。缺点：根据尝试的量化水平，您可能会发现您想要的操作（例如，特定的卷积OP或甚至是转置的特定卷积OP）。当然，与所有方法一样，您可能会发现准确性降低太多才能有用。</p><p>    PyTorch has support for special  quantized tensors, which in their case corresponds to storing data in 8 or 16 bits. It’s important to understand one specific detail about how this works. If your network has a special structure that means that at some point all of the outputs are between 0 and 1 (e.g. from a sigmoid), then you might be able to choose a better, more specific quantization. This means that quantization needs to collect some data about how your network runs on representative inputs. In particular, most quantization happens via a method like  round(x * scalar), where  scalar is a learned parameter (akin to BatchNorm).</p><p>    PyTorch具有支持特殊量化的张量，其在其案例中对应于在8或16位存储数据。了解有关这项工作的具体细节非常重要。如果您的网络具有特殊的结构，这意味着在某个位置，所有输出都在0到1之间（例如，从Sigmoid）之间，那么您可能能够选择更好，更具体的量化。这意味着量化需要收集有关网络如何运行代表性输入的一些数据。特别地，大多数量化通过像圆形（x *标量）这样的方法发生，其中标量是学习参数（类似于Batchnorm）。 </p><p> Support for some of these operations are in libraries that are &#34;external&#34; to PyTorch (but loaded as required). Think of this like BLAS or MKL for quantized operations.  FBGEMM is an implementation for servers, and  QNNPACK is an implementation for mobile devices (now inside PyTorch proper).</p><p>对其中一些操作的支持是＆＃34;外部＆＃34; to pytorch（但根据需要加载）。想象类似于诸如量化操作的Blas或MKL。 FBGEMM是服务器的实现，QNNPACK是移动设备的实现（现在在Pytorch适当的内部）。</p><p> Quantization occasionally has gotchas - accumulating in higher precision data types is often more stable than using lower precision values. Picking the right precision for each operation can be nonobvious, so PyTorch has a   torch.cuda.amp package to help you automatically cast different parts of your network to half precision ( torch.float16) where it’s possible. If you want to do this manually, there’s some helpful tips on that page.</p><p> 量化偶尔会在高精度数据类型中累积累积通常比使用较低的精度值更稳定。为每个操作挑选正确的精度可能是不可吸取的，因此Pytorch有一个火炬.cuda.amp包，可以帮助您自动将网络的不同部分投入到半精度（Torch.float16）中。如果您想手动执行此操作，则该页面上有一些有用的提示。</p><p> One of the very first things you can try is to take your existing model that&#39;s all `torch.float32`, and run it using `torch.cuda.amp` and see if it still runs with accuracy. Half precision support is still relatively sparse in consumer GPUs, but it works on the very common V100/P100.</p><p> 您可以尝试的第一件事之一是携带现有的模型，＆＃39; s所有`torch.float32`，使用`torch.cuda.amp`运行它，并查看它是否仍然以准确性运行。消费者GPU中的半精确支持仍然相对稀疏，但它适用于非常常见的V100 / P100。</p><p> If you want more control or want to deploy to a non-CUDA environment, there are 3 levels of manual quantization (under the label “eager mode quantization”) that you can try, depending on why you’re trying to quantize and how much you’re willing to sweat:</p><p> 如果您想要更多控制或想要部署到非CUDA环境，则有3个级别的手动量化（根据标签“渴望模式量化”）您可以尝试的，具体取决于您试图量化和多少你愿意出汗：</p><p> Dynamic quantization: This is the easiest method. Essentially, we store the weights of the network in the specified quantization, and then at run time, activations are dynamically converted to the quantized format, combined with the (quantized) weights, then written in memory at full precision. Then the next layer quantizes those, combines with the next quantized weights, and so on. Why does this happen? My understanding is that  scalar can be dynamically determined from the data, which means this is a  data-free method.</p><p> 动态量化：这是最简单的方法。基本上，我们将网络的权重存储在指定的量化中，然后在运行时，激活动态地转换为量化格式，与（量化）权重组合，然后以完全精度写入存储器。然后下一层量化那些，与下一个量化权重相结合，等等。为什么会发生这种情况？我的理解是可以从数据动态确定标量，这意味着这是一种无数据的方法。</p><p> How do we do this in PyTorch? It’s short enough that we can write it down here:</p><p> 我们如何在Pytorch中这样做？这足够短，我们可以在这里写下来：</p><p> # quantize the LSTM and Linear parts of our network # and use the torch.qint8 type to quantizequantized_model = torch.quantization.quantize_dynamic( model, {nn.LSTM, nn.Linear}, dtype=torch.qint8)</p><p> ＃量化我们网络的LSTM和线性部分＃，并使用Torch.Qint8类型来量化Quantized_Model = Torch.Quantization.Quantize_Dynamic（Model，{Nn.Lstm，NN.LineAl}，DType = Torch.Qint8） </p><p> There are many more knobs you can turn to make this better for your model. See more details  in this blog post.</p><p>有更多的旋钮可以为您的型号做出更好的方式。查看此博客文章的更多详细信息。</p><p> Static quantization: Runtime conversion to a full precision type and back is expensive. We can remove that if we know what the distribution of activations will be (by recording real data flowing through the network, as mentioned above). When you have access to data flowing through your network, PyTorch can also inspect your model and implement extra optimizations such as quantized operator fusion. Here’s an example of setting up the observers, running it with some data, and then exporting to a new statically quantized model:</p><p> 静态量化：运行时转换为全精密类型和背部是昂贵的。如果我们知道激活的分布是（通过记录通过网络的真实数据，我们可以删除，如上所述）。当您可以访问流过网络的数据时，Pytorch还可以检查您的模型并实现额外的优化，例如量化操作员融合。以下是设置观察者的示例，使用一些数据运行它，然后导出到新的静态量化模型：</p><p> # this is a default quantization config for mobile-based inference (ARM)model.qconfig = torch.quantization.get_default_qconfig( &#39;qnnpack&#39;) # this chain (conv + batchnorm + relu) is one of a few sequences  # that are supported by the model fuser model_fused = torch.quantization.fuse_modules(model, [[ &#39;conv&#39;,  &#39;bn&#39;,  &#39;relu&#39;]]) # insert observersmodel_with_observers = torch.quantization.prepare(model_fused)model_with_observers(example_batch)quantized_model = torch.quantization.convert(model_with_observers)</p><p> ＃这是基于移动的推断（ARM）Model.qconfig = torch.get_default_qconfig（qnnpack＆＃39;）＃本链（conv + batchnorm + relu）是几个序列之一的默认量化＃由模型定位器Model_Fused = torch.quantization.fuse_modules（型号，[＆＃39; conc＆＃39; bn＆＃39;＆＃39; relu＆＃39;]]）＃插入观察员_with_observers = torch.quantization.prepare（model_fused）model_with_observers（example_batch）量化_model = torch.quantization.convert（model_with_observers）</p><p> Quantization-aware Training (QAT): if you’re familiar with neural network training, you know where this is going. If you tell the training method some fact about how the network is used, the network will adapt to this information. How does this work? During the forward and backward passes, the model’s activations are rounded to the picked quantization. This means the model gets gradients based on rounded values, which means it “adjusts” to its limited capacity. Very importantly, however, the actual backprop (i.e. the gradient descent of the weights) happens in full precision.</p><p> 量化感知培训（QAT）：如果您熟悉神经网络培训，您就知道这将在哪里。如果您告诉培训方法有关如何使用网络的事实，网络将适应此信息。这是如何运作的？在前向和后退通过期间，模型的激活是舍入的挑选量化。这意味着模型基于舍入值获得梯度，这意味着它“调整”其有限的容量。然而，非常重要的是，实际的反射（即重量的梯度下降）完全精确地发生。</p><p> I’m leaving out the code example because this is a more involved method, but you can find a  full example here. There are again many knobs.</p><p> 我抛出了代码示例，因为这是一个更涉及的方法，但您可以在此处找到一个完整的示例。还有很多旋钮。</p><p> Note! See the helpful tips under  Model Preparation for Quantization  here before using PyTorch quantization.</p><p> 笔记！在使用PyTorch量化之前，请参阅模型准备下的有用提示进行量化。</p><p>  PyTorch-based quantization might not necessarily work in other production environments. In particular, when converting to Apple’s CoreML format, you need to just use their quantization (which might be limited to just 16-bit quantization). When using edge devices, be careful to check that quantization is possible (in Apple’s case the hardware is already computing everything in fp16 on GPU, so you only save possibly the memory of the network’s weights).</p><p>  基于Pytorch的量化可能不一定在其他生产环境中工作。特别是，当转换为Apple的CoreM1格式时，您需要使用它们的量化（可能限于16位量化）。使用边缘设备时，请小心检查是否可以检查量化（在Apple的情况下，硬件已经在GPU上计算了FP16中的所有内容，因此您只能保存网络权重的内存）。 </p><p> Tensorflow has a similar set of steps as above, though the examples are focused on TFLite. Essentially, static and dynamic quantization are explained in the  Post-training quantization page, and there’s a  QAT page. I think the tradeoffs are very similar, though there’s always some feature mismatch between PyTorch and TF.</p><p>Tensorflow具有类似于上面的类似步骤，尽管这些例子集中在TFLITE上。基本上，在训练后量化页面中解释了静态和动态量化，并且有一个QAT页面。我认为权衡非常相似，但在Pytorch和TF之间总是有些功能不匹配。</p><p>  Apoarently down to 1 bit! There have been several  attempts  over the  years to create binary neural networks if you want the most extreme version of the accuracy vs speed tradeoff. For the most part, these are still research projects rather than usable ideas, though XNOR-Net++ seems to have been implemented in PyTorch.</p><p>  即将到达1位！多年来已经有几个尝试来创建二元神经网络，如果您想要最精确的vs速度权衡。在大多数情况下，这些仍然是研究项目而不是可用的想法，尽管Xnor-Net ++似乎已在Pytorch中实现。</p><p>    Pruning is removing some weights (i.e. connections) or entire neurons from a neural network after or during training. In practice we can often remove  90% of the parameters in large deep neural networks without significantly affecting model performance.</p><p>    在训练之后或期间，修剪从神经网络中除去一些重量（即连接）或整个神经元。在实践中，我们通常可以在大型深度神经网络中删除90％的参数，而不会显着影响模型性能。</p><p> Why does this work?: Let’s imagine that your model is a fully connected neural network with just one hidden layer, such that the input is size 1024, the hidden size is 100, and the output is 20 dimensions. Then the number of parameters (without bias) is 104400. If there’s a neuron in the hidden layer that never fires (or is ignored downstream) then removing it from the network saves 1044 parameters. Why not just train the smaller network right away? The most compelling explanation is something called   the lottery ticket hypothesis:</p><p> 为什么这项工作？：让我们想象你的模型是一个完全连接的神经网络，只有一个隐藏层，使得输入是尺寸1024，隐藏尺寸为100，输出为20尺寸。然后参数（无偏差）的数量是104400.如果隐藏层中有一个神经元，则从从未触发的隐藏层（或在下游忽略）中，则从网络中删除它可以节省1044参数。为什么不立即培训较小的网络？最引人注目的解释是呼气票假设的东西：</p><p> Any large network that trains successfully contains a subnetwork that is initialized such that - when trained in isolation - it can match the accuracy of the original network in at most the same number of training iterations.</p><p> 列车成功包含一个初始化的子网络，初始化的子网 - 当触摸训练时 - 它可以与最多相同数量的培训迭代匹配原始网络的准确性。</p><p> Downside: Removing neurons or choosing a subnetwork is what I (and others) consider  structured pruning. However, a lot of methods (including Tensorflow’s  tensorflow_model_optimization toolkit at this time and PyTorch’s  torch.nn.utils.prune) are focused on sparsifying model weights so that they are more compressible (what some call  unstructured pruning). This means the matrices are the same size, but some values are set to 0. It’s currently unclear to me if this means that larger models can use less GPU memory (I think essentially not), but it can save you disk space. When sparse model support fully lands in the various frameworks (i.e you can multiply a sparse vector and a sparse matrix faster than the dense ones) you might be able to speed up inference as well.</p><p> 缺点：去除神经元或选择子网是我（和他人）考虑结构化修剪的内容。但是，此时和Pytorch的Torch.nn.utils.prune的许多方法（包括Tensorflow的TensorFlow_Model_Optimization Toolkit）专注于稀疏模型权重，以便它们更加压缩（有些呼叫非结构化修剪）。这意味着矩阵尺寸相同，但是一些值设置为0.如果这意味着这意味着更大的模型可以使用较少的GPU内存（我基本上不一致），它目前不清楚，但它可以节省磁盘空间。当稀疏模型支持在各种框架中完全落地时（即，您可以将稀疏向量和稀疏矩阵乘以，而不是密集的速度）您也可以加速推断。</p><p> For that reason, I’m not going to spend much time on unstructured pruning because it doesn’t seem that useful, but essentially you can prune during or after training, and you pick a certain target sparsity (e.g. 80% of the weights of your network will be zeroed out). However, there’s  a lot of confusion in this area which makes it hard to recommend anything. Tensorflow has a  a few guides on pruning both during and after training and PyTorch has  a tutorial on pruning using some set of heuristcs after training.</p><p> 因此，我不会花很多时间在非结构化修剪上花费很多时间，因为它似乎没有有用，但基本上你可以在训练期间或之后修剪，你可以选择一定的目标稀疏性（例如80％的重量您的网络将被归零）。然而，这个领域有很多困惑，这使得它很难推荐任何东西。 TensoRflow在培训期间和之后修剪了一些导游，并且Pytorch在训练后使用某些Heuristcs进行修剪教程。 </p><p> In the space of structured pruning, there’s still active research and no clear API. We can pick a metric to compute a relevance score for each neuron, and then remove the ones that have the least information content. Metrics that might be useful here are the  Shapley value, a Taylor approximation of the loss functions sensitivity to a neuron’s activation, or even a random neuron. The  TorchPruner library implements some of these automatically for  nn.Linear and convolutions ( nn.Conv1D, nn.Conv2D, etc) modules. Another library  Torch-Pruning has support for a few more operations. One of the most well-known  older works in this area prunes filters from a convnet using the L1 norm of the filter’s weights. However, this is still an active area of research.</p><p>在结构修剪的空间中，仍然有活跃的研究，没有明确的API。我们可以选择一个指标来计算每个神经元的相关性分数，然后删除具有最少信息内容的值。这里可能有用的指标是福芙值，泰勒近似对神经元激活的敏感性，甚至是随机神经元的敏感性。 Torchpruner库自动为NN.LineAre和卷积（NN.conv1d，nn.conv2d等）模块实现了一些。另一个图书馆火炬修剪有几个运营。该区域中最着名的旧作品之一，使用过滤器权重的L1标准从GROMNET提出过滤器。然而，这仍然是一个活跃的研究领域。</p><p>  In both cases, it’s standard to retrain the network after applying the pruning. The  best method I know of is basically to reset the learning rate ( learning rate rewinding) and start retraining the network. If you’d like, you can use  weight rewinding, which is resetting the weights for the unpruned parts of the network to their value earlier in training (e.g. 1/3 trained weights). My intuition on this is that it’s essentially training the lottery ticket subnetwork now that we’ve identified it.</p><p>  在两种情况下，在应用修剪后重新培训网络的标准。我知道的最佳方法基本上是重置学习率（学习速率倒带）并开始再培训网络。如果您想要的话，您可以使用体重倒带，这是将网络的未偿还部分的重量重置为其培训前面的值（例如1/3训练权重）。我的直觉就是现在我们已经确定了彩票票据。</p><p> Overall, I think a practitioner who is really interested in trying this should start with TorchPruner or Torch-Pruning and then try fine tuning the resulting network with learning rate rewinding. However, I&#39;d say that for most architectures (including ResNets because of skip connections) it&#39;ll be pretty nonobvious how to trim the rest of the network around this.</p><p> 总的来说，我认为一个真正对尝试这一感兴趣的从业者应该从武器或火炬修剪开始，然后尝试使用学习速率倒带进行精细调整所得到的网络。然而，我为大多数架构（包括跳过连接而包括reasnets），它＆＃39; ll是非常不合作的如何修剪围绕它的其余网络。</p><p>  I’ll cover this in more detail once I have more experience with it, but essentially:  DeepSpeed is a library that helps  train large to extremely large models (e.g. 1bn+ parameters) faster and using less GPU memory. This works by exploiting smart parallelism and better caching. It comes in the form of an extension to PyTorch.</p><p>  一旦我拥有更多的经验这是通过利用智能并行性和更好的缓存来作用。它以扩展到Pytorch来了。</p><p>  Knowledge distillation is a method for creating smaller and more efficient models from large models. In NLP this has also been referred to as  teacher-student methods, because the large model trains the student model. The reference work in this area is  (Hinton et al., 2015).</p><p>  知识蒸馏是一种从大型型号产生更小且更高效的模型的方法。在NLP中，这也被称为教师 - 学生方法，因为大型模型列举了学生模型。该地区的参考工作是（Hinton等，2015）。</p><p> In practice, suppose we have a classification task. Suppose our smaller student model is $f_\theta$, where $\theta$ is the set of parameters. We take either a large model or an ensemble of models (possibly even the same model trained with different initializations), and call it $F$ (we won’t worry about its parameters). Then we train the student network with the following loss:</p><p> 在实践中，假设我们有一个分类任务。假设我们的较小学生模型是$ f_ \ theta $，其中$ \ theta $是该组参数。我们采用大型模型或模型的集合（可能是具有不同初始化培训的相同模型），并调用它$ F $（我们不会担心其参数）。然后我们用以下损失训练学生网络：</p><p>  where $F(x_i)$ is the probability distribution over the labels created by passing example $x_i$ through the network. If you want, you can mix in a little bit of the regular cross entropy loss using the proper labels:</p><p>  其中$ f（x_i）$是通过通过网络传递示例$ x_i $创建的标签上的概率分布。如果您愿意，可以使用正确的标签在常规跨熵丢失中混合： </p><p> $$\mathcal{L} = \sum_{i = 1}^n \left(\operatorname{KL}\left(F(x_i), f_\theta(x_i)\right) - \beta \cdot \sum_{k = 1}^K y_i[k] \log f_\theta(x_i)[k]\right)$$</p><p>$$ \ mathcal {l} = \ sum_ {i = 1} ^ n \ left（\ operatorname {kl} \ left（f（x_i），f_ \ theta（x_i）\右） -  \ beta \ cdot \ sum_ { k = 1} ^ k y_i [k] \ log f_ \ theta（x_i）[k] \右）$$</p><p> Note that this second term is just the KL divergence from the “true” distribution (i.e. the one-hot distribution from the labels) to the student model, since $y_i$ is one-hot.</p><p> 请注意，第二项术语只是“真实”分布（即从标签的单热分配）到学生模型的KL发散，因为$ Y_I $是一种热门。</p><p> Why does this work? There’s no consensus best opinion, as far as I know. The most compelling explanation I’ve read so far is that distillation is a form of rough data augmentation. I can recommend this paper:  Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning, which is focused on the idea of multiple views. At risk of being long-winded, here’s a thought experiment that might explain:</p><p> 为什么这项工作？据我所知，没有达成共识的最佳意见。到目前为止我读过的最引人注目的解释是蒸馏是一种粗糙数据增强的形式。我可以推荐这篇论文：在深入学习中了解集合，知识蒸馏和自我蒸馏，其专注于多种观点的想法。有长啰嗦的风险，这是一个可能解释的思想实验：</p><p> Distillation thought experiment: Let’s say that we have a large teacher model that is trained to classify images (e.g. CIFAR-100). This model implicitly has a bunch of “feature-detectors” built-in, e.g. a set of convolutional filters that fire when pointy ears are seen, which increase the probability of a label like “cat”. Let’s say that there’s a training image of a Batman mask, labeled “mask”. The teacher model’s pointy ears filters might still fire, telling us that the model thinks that this looks 10% like a cat.</p><p> 蒸馏思想实验：让我们说我们拥有一个培训的大师模型，以对图像进行分类（例如Cifar-100）。该模型隐含地具有一堆“特征探测器”内置，例如，一组卷积滤波器，当看到尖尖的耳朵时，这增加了标签的概率，如“猫”。让我们说蝙蝠侠面具的训练形象，标有“面具”。教师模型的尖锐耳朵过滤器可能仍然射击，告诉我们，该模型认为这看起来像猫10％。</p><p> When the student model is trained to match the probability distribution of the teacher, because the distribution is 0.1 cat, it will still get a small signal that this image is catlike, which might help the student model recognize cats better than it could otherwise. If the student model was trained on just the true labels, it would have no idea that this Batman mask looks a bit like a cat.</p><p> 当学生模型接受培训以匹配教师的概率分布时，因为分布是0.1只猫，它仍然可以获得这个图像是猫的小信号，这可能有助于学生模型比否则更好地识别猫。如果学生模型只是在真正的标签上接受培训，则不知道这个蝙蝠侠面具看起来有点像猫。</p><p> A similar, but slightly different idea explains why ensembles of models (even the same architecture) might work well:</p><p> 类似但略有不同的想法解释了为什么模型（甚至相同的架构）的合奏可能很好：</p><p> Ensembling thought experiment: Let’s say there’s 3 pictures of a cat in a dataset we’re using for image classification. Let’s say that image 1 has a cat with feature A (e.g. pointed ears), image 2 has feature B (e.g. whiskers), and image 3 has both A and B.</p><p> 合奏思想实验：假设我们使用用于图像分类的数据集中有3个猫的图片。假设图像1具有具有特征A（例如耳朵）的猫，图像2具有特征B（例如晶须），并且图像3具有A和B。 </p><p> Then, let’s say the neural network learns feature A (e.g. by seeing image 1). When it sees image 3, that set of convolution filters will fire, and so the image will be correctly classified. So, there’ll be no gradient that tunes the net to recognize feature B, even though a good net would learn that.</p><p>然后，假设神经网络学习特征A（例如，通过看图像1）。当它看到图像3时，这组卷积滤波器将射击，因此图像将被正确分类。因此，即使一个好的网会学会，它也不会调整网络以识别特征B的梯度。</p><p> Once a neural network has become good enough, its signal from some data points decreases.</p><p> 一旦神经网络变得足够好，它来自某些数据点的信号就会减少。</p><p>  Knowledge distillation is a very deep and wide research area, touching adversarial attacks, knowledge transfer, and privacy. Unfortunately I can’t cover those in any real detail, so I’ll leave them for a future day.</p><p>  知识蒸馏是一个非常深入的研究区，触及对抗性攻击，知识转移和隐私。不幸的是，我无法涵盖任何真实细节的人，所以我会把它们留在未来的一天。</p><p> In practice, the method I’ve described above is called  response-based distillation. There are also other forms of distillation, including  feature-based and  relation-based knowledge distillation, which are entire subfields based on what parts (or computations from) the student and teacher model we should tie together.</p><p> 在实践中，我上面描述的方法称为基于响应的蒸馏。还有其他形式的蒸馏，包括基于特征和基于关系的知识蒸馏，这是基于学生和教师模型的哪些部分（或计算）的整个子场。</p><p> Furthermore, there’s a division between  offline distillation (i.e. train the student after the teacher),  online distillation (train the student and teacher together), and  self-distillation (where the teacher model has the same architecture as the student). Together this makes it difficult to track distillation in practice; a set of adhoc model-specific techniques might be the best general recommendation.</p><p> 此外，在离线蒸馏（即老师之后的学生）之间存在一个分裂，在线蒸馏（一起培训学生和教师），以及自我蒸馏（教师模型与学生的建筑相同）。这使得这使得难以在实践中追踪;一组特定于adhoc模型的技术可能是最佳的一般性推荐。</p><p> In fact,  (Cho &amp; Hariharan, 2019) found that when the student model’s capacity is too low, using knowledge distillation will actually adversely affect training. They found that knowledge distillation papers rarely use ImageNet and so often don’t work well on difficult problems. Perplexingly, that paper and  (Mirzadeh et al., 2019) found that better teacher models don’t always mean better distillation, and the farther the student and teacher model’s capacities are, the less effective distillation was. You can find a recent investigation in  (Tang et al., 2021).</p><p> 事实上，（Cho＆amp; Hariharan，2019）发现，当学生模型的容量太低时，使用知识蒸馏实际上会对培训产生不利影响。他们发现知识蒸馏纸很少使用ImageNet，因此通常不适用于难题。令人困惑的是，这篇论文和（Mirzadeh等，2019）发现，更好的教师模型并不总是意味着更好的蒸馏，并且学生和教师模型的能力越远，蒸馏越少。您可以找到最近的调查（Tang等，2021）。</p><p> All in all, my understanding so far is that distillation is fairly difficult. You might be able to get some free performance points by training a student with a slightly smaller capacity and then using vanilla response-based offline distillation.</p><p> 总而言之，到目前为止，我的理解是蒸馏相当困难。您可能能够通过培训具有稍微较小的容量，然后使用基于香草响应的离线蒸馏的学生获得一些免费的绩效点。 </p><p>  However, deep learning researchers have spent a lot of time distilling large models using model-specific methods, and if you need to gain some performance, you might be able to find a pre-trained distilled version of the large model you’re currently using. For example, in NLP, HuggingFace makes it easy to access both DistilBert and TinyBert. In computer vision, Facebook Research’s   d2go has a bunch of pretrained mobile-ready models, and they’ve specialized some distillation methods in  DeiT.</p><p>然而，深度学习研究人员花了很多时间使用模型的方法蒸馏大型模型，如果您需要获得一些性能，您可能能够找到您目前正在使用的大型模型的预先培训的蒸馏版本。例如，在NLP中，HuggingFace可以轻松访问Distilbert和Tinybert。在计算机愿景中，Facebook Research的D2Go拥有一堆预用的移动式模型，他们专注于Deit的一些蒸馏方法。</p><p> Well-Read Students Learn Better: On the Importance of Pre-training Compact Models makes a recommendation (with high quality ablation experiments) that for training BERT architectures, the best approach is:</p><p> 良好的学生学习更好：关于预训练专业模型的重要性，提出了一种推荐（具有高质量消融实验），用于训练BERT架构，最好的方法是：</p><p> Pre-train a compact model architecture on the masked language model (MLM) objective developed by the original BERT papers (Devlin et al., 2018).</p><p> 在原始伯格纸开发的蒙面语言模型（MLM）目标上预先列车专门模型架构（Devlin等，2018）。</p><p> Take a large task-specific teacher model (e.g. if the task is NLI, the output is a distribution over the 3 classes (entailment, contradiction, neutral)), and perform basic  response-based offline distillation on the pre-trained compact model from step 1.</p><p> 拍摄一个大的任务特定的教师模型（例如，如果任务是NLI，则输出是通过3类（征集，矛盾，中性））的分布，并在预训练的紧凑模型上执行基于基于响应的离线蒸馏步骤1。</p><p> Finally, if required,  fine-tune the compact model from step 2 on the task-specific data (e.g. if the task is NER, train over the CoNLL 2003 dataset).This method (which they call  Pre-trained Distillation (PD)) is that it’s architecture-agnostic. I think if you are going to use a compact NLP model in practice it’s worth skimming the paper, especially section 6.</p><p> 最后，如果需要，请在特定于任务特定数据的步骤2上微调紧凑型模型（例如，如果任务是ner，则在Conll 2003数据集上培训）。这方法（他们调用预先训练的蒸馏（PD））这是它是架构不可知论者。我认为如果您将在实践中使用紧凑的NLP模型，值得撇去纸张，特别是第6节。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://rachitsingh.com/deep-learning-model-compression/">https://rachitsingh.com/deep-learning-model-compression/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/深度学习/">#深度学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>