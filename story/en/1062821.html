<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>在PostgreSQL调试随机慢写 Debugging random slow writes in PostgreSQL</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Debugging random slow writes in PostgreSQL<br/>在PostgreSQL调试随机慢写 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-15 10:14:33</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/5/cedd17fe0587d757a85bdf49218f296d.webp"><img src="http://img2.diglog.com/img/2021/5/cedd17fe0587d757a85bdf49218f296d.webp" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In web applications it’s not rare to face performance issues that we can’t quite understand. Especiallywhen working with databases, we treat them as this huge “black box” that 99% of the times works amazingly withoutus even caring about it. Heck, we even use stuff like ORMs that essentially “hide” our interaction withthe database, making us think that we don’t need to care about this stuff.</p><p>在Web应用程序中，面对我们无法理解的性能问题并不罕见。特别是与数据库一起使用，我们将它们视为这个巨大的“黑匣子”，99％的时间令人惊讶地毫无疑问地关心它。哎呀，我们甚至使用像orms的东西，基本上“隐藏”我们与数据库的互动，让我们认为我们不需要关心这些东西。</p><p> If you’re developing something small, contained, simple then this is probably the case. Your database willmost likely perform OK no matter how poorly designed or configured it might be. You won’t have any issues using“naive” queries built by ORMs, everything will work just fine.</p><p> 如果你正在开发一些小的，所在的东西，简单，那可能是这种情况。无论设计多么差或配置，您的数据库都可能表现正常。您不会使用ORMS构建的“天真”查询的任何问题，一切都会正常工作。</p><p> However, as your application grows, then the database is something that can make or brake you. It’s one ofthe hardest things to scale (you can’t just spin up multiple instances), it’s hard to re-design or migrateand essentially it’s the core of your application, serving and storing all your data. Hence its performanceis critical.</p><p> 但是，随着应用程序的增长，数据库是可以制作或制动的东西。它是最艰难的事情（你不能只是旋转多个实例），很难重新设计或MIGRATEAND基本上它是您的应用程序的核心，服务和存储所有数据。因此它的表现至关重要。</p><p> In this post I’ll showcase a real-life example of debugging a seemingly weird database performance degradation.While I obviously intend to share the solution and what to avoid, I’d also like to take you through the journeyand show you some tools &amp; processes that can help you dig into SQL performance.</p><p> 在这篇文章中，我将展示调试一个看似奇怪的数据库性能下降的真实例子。我显然打算分享解决方案以及要避免的解决方案，我也想带你去跑步和向你展示一些工具和amp ;可以帮助您挖掘SQL性能的进程。</p><p>   The database we’ll be studying is an AWS Aurora RDS (PostgreSQL 12). It is a clustered database and has two replicas,a reader (read-only replica) and a writer. AWS Aurora is pretty close to an actual PostgreSQL with some zero-lagreplication capabilities on top (and some managed features of course). The whole process discussed here shouldapply to a self-managed RDS PostgreSQL as well.</p><p>   我们将学习的数据库是AWS Aurora RDS（PostgreSQL 12）。它是一个群集数据库，并且有两个副本，读者（只读副本）和写入器。 AWS Aurora非常接近实际的PostgreSQL，顶部有一些零滞后功能（以及某些管理功能当然）。这里讨论的整个过程不应进入自我管理的RDS PostgreSQL。</p><p> The problem we will be studying is the (seemingly) random poor performance of  UPDATE /  INSERT statements. Thiswas observed in a specific table, that had ~20000000 rows and 23 indexes.</p><p> 我们将要学习的问题是（看似）更新/插入语句的随机性能不佳。在一个特定的表中观察到这个问题，它有〜20000000行和23个索引。</p><p> So while, most writes (&gt;99.99%) take &lt;10ms to complete, some statements were taking more than 40 seconds. Some evenended up being killed by the  statement_timeout setting (which was set at 100s!). It was baffling to say the least.</p><p> 因此，大多数写作（＆gt; 99.99％）才能完成，一些陈述需要超过40秒。一些康复的Sented Up and datement_timeout设置（它设置为100岁！）。至少可以说是令人困惑的。 </p><p>    The first assumption was that there was just too much write volume on the database. While this is partially true,our evidence didn’t support that this could be the root cause of the problem since it occured uniformly both inperiods with high write volume but also in periods where write volume was minimal.</p><p>第一个假设是数据库上的写卷太多。虽然这部分是真实的，但我们的证据不支持这可能是问题的根本原因，因为它均匀地发生了高写卷，而且在写体积最小的时段中也是如此。</p><p>   Quite often the application was writing rows in batches. This was also considered a possible cause and batched writes were removed.However, the problem remained (and the performance overall was worse when writing one row at a time).</p><p>   应用程序通常是用批量写作行。这也被认为是可能的原因，并且批量写入被删除。然而，剩余问题（一次在一次写一行时，性能变得更糟）。</p><p>  This table had numerous access patterns, and hence it required numerous indexes to perform well.It had 23 B-Tree indexes, 6 Foreign-key constraints, 3 BRIN indexes and 1 GIN index (for full text search). While it isclear that indexes play a role in write performance (since for every write you need update every index), but this didn’texplain why most updates were really fast and some excruciatigly slow.</p><p>  此表具有许多访问模式，因此需要众多索引执行井。它有23个B树索引，6个外键约束，3个Brin索引和1个GIN索引（用于全文搜索）。虽然尚不清楚索引在写性能中发挥作用（自从每次写作，你需要每一个索引更新），但这是Notexplain为什么大多数更新都非常快，一些泄漏速度。</p><p>  The last assumption was that there maybe were competing locks in the database. Specifically, maybe some long running processesopened big transactions and locked resources for a long time. Then, other writes were waiting to update the locked rowsand couldn’t finish. This seemed like a good assumption and it couldn’t be disprooved with the data at hand. So it was timefor further investigation</p><p>  最后假设是数据库中可能竞争锁。具体而言，可能很长时间运行的大型事务和长期锁定资源。然后，其他写入等待更新锁定的rowsand无法完成。这似乎是一个很好的假设，它不能被手头的数据被解释。所以是时候进一步调查了</p><p>  To help us check our assumptions, PostgreSQL offers some tools. Those can be enabled via its configuration ( postgresql.conf or theRDS parameter groups in AWS). Some interesting options are:</p><p>  为了帮助我们检查我们的假设，PostgreSQL提供了一些工具。可以通过其配置（PostgreSQL.conf或AWS中的参数组）启用。一些有趣的选择是：</p><p> log_lock_waits: Enabling this will instruct the Deadlock Detector to log whenever a statement exceeds  deadlock_timeout.There is no performance overhead enabling this, since the Deadlock Detector should be running anyway, and if it is, then it’s practically free ( source)</p><p> log_lock_waits：启用这将指示Deadlock检测器在陈述超出DeadLock_Timeout时记录日志。没有能够开销启用此操作，因为Deadlock检测器应该运行，如果它是，那么它实际上是免费的（源）</p><p>  auto_explain: This is actually a number of configurations ( auto_explain.log_min_duration,  auto_explain.log_analyze etc).They control when and how PostgreSQL will automatically perform an  EXPLAIN on running queries. Those are usefulas a precaution too, to make sure that poorly performing statements will leave traces &amp; query plans for you to debug.You can read more  here</p><p>  auto_explain：这实际上是许多配置（auto_explain.log_min_duration，auto_explain.log_Analyze等）.They控制后的PostgreSQL何时以及如何在运行查询上自动执行解释。那些也有用的预防措施，以确保表现不佳的陈述将离开痕迹和amp;查询您的计划才能调试。您可以在这里阅读更多 </p><p> log_statement: This is pretty useful. It can enable logging all / most / error statements etc. If you want to findout if something’s wrong with your database, it’s a common practice to set this to  all for some time, gather the outputand analyze it with a tool like  pgBadger. You can see all the logging-related options here</p><p>log_statement：这非常有用。它可以启用记录全部/大多数/错误语句等。如果您想找到数据库的错误，这是一个常见的做法，可以将其设置为一段时间，收集outputand与PGBadger等工具分析它。您可以在此处查看所有与日志记录相关的选项</p><p> So, I went to enable those, reproduce the issue and see what the heck is going on.</p><p> 所以，我开始启用这些，重现问题，看看哎呀正在发生什么。</p><p>   However, while testing something irrelevant on our staging instance, I managed to reproduce the issue predictably. To do that,all I had to do was to update 5000 rows on this table. While doing that, once or twice every 5000 updates, an update would take &gt; 40sec!</p><p>   但是，在测试您的舞台实例上无关的东西时，我设法重现了可预测的问题。为此，我所要做的就是在此表上更新5000行。虽然这样做，但每次或两次每5000个更新，更新都会采取＆gt; 40年代！</p><p> This was a true blessing, because it ruled out both the “too much write volume” hypothesis (our staging DB had zero traffic) and thelong locks as well, since there were no processes locking the rows I was updating.</p><p> 这是一个真正的祝福，因为它排除了“太多写入音量”假设（我们的暂存数据库有零流量）和龙龙锁，因为没有进程锁定我正在更新的行。</p><p> I performed an  EXPLAIN ANALYZE on the problematic query to see what is going on</p><p> 我在有问题的查询上进行了解释分析，看看发生了什么</p><p>    Update on public.my_table (cost=0.43..8.45 rows=1 width=832) (actual time=2.037..2.037 rows=0 loops=1) Buffers: shared hit=152 read=1 I/O Timings: read=1.22 -&gt; Index Scan using my_table_pkey on public.my_table (cost=0.43..8.45 rows=1 width=837) (actual time=0.024..0.026 rows=1 loops=1) Output: (...) Index Cond: (my_table.id = 130561719) Buffers: shared hit=4Planning Time: 1.170 msExecution Time: 2.133 ms</p><p>    Update.my_table（成本= 0.43..8.45行= 1宽= 832）（实际时间= 2.037..2.037行= 0循环= 1）缓冲区：共享命中= 152 Read = 1 I / O定时：Read = 1.22  - ＆gt;索引扫描在public.my_table上使用my_table_pkey扫描（成本= 0.43...45行= 1宽度= 837）（实际时间= 0.024..0.026行= 1循环= 1）输出：（...）索引COND :( my_table。 ID = 130561719）缓冲区：共享命中= 4planning时间：1.170麦克风时间：2.133毫秒</p><p>  Update on public.my_table (cost=0.56..8.58 rows=1 width=832) (actual time=34106.965..34106.966 rows=0 loops=1) Buffers: shared hit=431280 read=27724 &lt;----- THIS IS HUGE!! I/O Timings: read=32469.021 -&gt; Index Scan using my_table_pkey on public.my_table (cost=0.56..8.58 rows=1 width=832) (actual time=0.100..0.105 rows=1 loops=1) Output: (...) Index Cond: (my_table.id = 130561719) Buffers: shared hit=7Planning Time: 23.872 msExecution Time: 34107.047 ms</p><p>  Update.my_table（成本= 0.56..8.58行= 1宽度= 832）（实际时间= 34106.965..34106.966行= 0循环= 1）缓冲区：共享命中= 431280 READ = 27724＆lt; -----这是巨大的!! I / O定时：READ = 32469.021  - ＆gt;索引扫描在public.my_table上使用my_table_pkey扫描（成本= 0.56...58行= 1宽= 832）（实际时间= 0.100..0.105行= 1循环= 1）输出：（...）索引COND :( my_table。 ID = 130561719）缓冲区：共享命中= 7planning时间：23.872麦克风时间：34107.047毫秒 </p><p>  the predicted cost was the same in both cases (although running time clearly wasn’t)</p><p>两种情况下预测成本是相同的（虽然运行时间明显没有）</p><p> The last one was a clear indicator that there was an issue. But I couldn’t figure out what was causing this.I started doing various experiments, hoping to mitigate it. I tried:</p><p> 最后一个是一个明确的指标，有一个问题。但我无法弄清楚导致的事情。我开始做各种实验，希望减轻它。我试过了：</p><p> Doing a  VACCUM FULL ( ref) hoping that maybe this happened because  AUTOVACUUMdidn’t function well. Sadly, no result.</p><p> 做一个全部习惯（参考），希望可能发生这种情况，因为Autovacuumdid不起作用。可悲的是，没有结果。</p><p> Doing an  ANALYZE on the table to force PostgreSQL to update its stats and maybe execute the query more efficiently. Again, no luck.</p><p> 在表上进行分析以强制PostgreSQL更新其统计数据，并更有效地执行查询。再次，没有运气。</p><p>   Before giving up, I decided to reach out to the masters. I wrote the following post on the DBA StackExchange, which is a targetedcommunity for Database Administrators and developers working with databases.</p><p>   在放弃之前，我决定接触大师。我在dba stackexchange上写下了以下帖子，这是数据库管理员和开发人员使用数据库的TargetEdCommunity。</p><p>  What amazed me was that even before I explained the specifics of my case, people were asking in the comments if my table had a GIN index.Actually, they were pretty sure that it had one. Moreover, they suggested that I had something called  fastupdate enabled.</p><p>  对我来说，令人惊讶的是，即使在我解释了我的案子的具体细节之前，如果我的桌子有一个杜松子酒指数，人们就会在评论中询问。他们非常确定它有一个。此外，他们建议我有一个被称为Fastupdate的东西。</p><p> That led me to the  documentation (once again). Let’s quote a bit from it:</p><p> 这让我进入了文件（再次）。让我们从中引出一点： </p><p> Updating a GIN index tends to be slow because of the intrinsic nature of inverted indexes: inserting or updating one heap row can cause many inserts into the index (one for each key extracted from the indexed item). As of PostgreSQL 8.4, GIN is capable of postponing much of this work by inserting new tuples into a temporary, unsorted list of pending entries.</p><p>更新GIN索引往往是缓慢的，因为反向索引的内在性质：插入或更新一个堆行可能导致许多插入索引（从索引项目中提取的每个键）。截至PostgreSQL 8.4，GIN能够通过将新元组插入临时未学列表的待处理条目中来推迟大部分工作。</p><p> This described our case 100%. Most writes, since they didn’t trigger a cleanup of the pending list, were blazing fast. However, when thepending list cleanup was triggered (its size grew more than  gin_pending_list_limit) the process that performed the write blocked untilthe pending list was cleaned up and the index was synchronized.</p><p> 这描述了我们的案例100％。大多数写作，因为他们没有触发待处理列表的清理，很快就会燃烧。但是，触发正常列表清理时（其大小增长超过gin_pending_list_limit），在清理完成待定列表之前执行写入的过程并将索引同步。</p><p>   I went on to check if my index had  fastupdate set. This is an option in the index storage parameters. To check that, you can use \d+ &lt;index_name&gt; in psql. I didn’t see anything there, but reading up on the  CREATE INDEX  commandI noticed that  fastupdate was ON by default. I switched it off to do some tests:</p><p>   我继续检查我的索引是否具有Fastupdate Set。这是索引存储参数中的选项。要检查，您可以使用\ d +＆lt; index_name＆gt;在psql。我没有在那里看到任何东西，而是在Create Index Commandi上读取默认情况下，Fastupdate默认情况下。我将其切换为执行一些测试：</p><p>  Be careful when running statements like the one above. For one, this will trigger a lock until the index storage parameters are changed.Moreover, disabling fastupdate means that you will manually have to cleanup the pending list too (using  SELECT gin_clean_pending_list()) orrebuild the index (using  REINDEX). Both cases will probably cause performance or integrity issues in a production system, so be careful.</p><p>  运行像上面的陈述时要小心。对于一个，这将触发锁定，直到索引存储参数已更改.OROVER，禁用Fastupdate意味着您也将手动清理待定列表（使用SELECT GIN_CLEAN_PENDS_LIST（））ORREBUILD索引（使用REINDEX）。这两种情况都可能导致生产系统中的表现或完整性问题，所以要小心。</p><p> Voila! The problem was gone. Every write took the same, predictable time. However, as expected, it was noticably slower. So I was reluctantto consider disabling  fastupdate altogther.</p><p> 瞧！问题已经消失了。每次写作都采取了相同，可预测的时间。然而，正如所料，它显着慢。所以我不愿意考虑禁用Fastupdate Altogther。</p><p>  At this point, a complete solution had been submitted in my StackExchange post and I saw some other more viable options:</p><p>  此时，在我的Stackexchange帖子中提交了完整的解决方案，我看到了其他一些更多可行的选择：</p><p> I could run  VACUUM more aggressively, hoping that it will cleanup the pending list on the background and my queries will never trigger a cleanup.However, I don’t think this would be 100% reliable again.</p><p> 我可以更积极地运行真空，希望它将在背景上清理待处理的列表，并且我的查询永远不会触发清理。但是，我认为这不会再次可靠。 </p><p> I could set an even higher  gin_pending_list_limit (default: 4MB). This would mean that cleanups would be really rare but it could impact  SELECTstatements (they have to read the pending list too) and if a cleanup occured it would take huge amounts of time.</p><p>我可以设置更高的gin_pending_list_limit（默认值：4MB）。这意味着清理会真的很少见，但它可能会影响选择的级别（他们也必须阅读待定列表），如果发生清理，则会花费大量的时间。</p><p> I could set a background process to perform a  SELECT gin_clean_pending_list() periodically. However, much like option 1 this would not guarantee anything</p><p> 我可以设置一个后台进程，以定期执行SELECT GIN_CLEAN_PENDS_LIST（）。但是，就像选项1一样，这不会保证任何东西</p><p> I could set a smaller  gin_pending_list_limit so that cleanups are more often but take less time.</p><p> 我可以设置一个较小的gin_pending_list_limit，以便清理更频繁但花费更少的时间。</p><p> I decided to go with the last, and ran some experiments to see how this would impact the system. Out of curiosity, I even dropped the index to see how muchit affected write performance. You can see some results below:</p><p> 我决定使用最后一个，并跑一些实验，了解这将如何影响系统。出于好奇心，我甚至丢弃了指数，看看影响性能有多大数量。您可以看到以下结果：</p><p>   Average time of inserting 5000 rows is the same without  fastupdate and with any size of  gin_pending_list_limit, which is expected.</p><p>   插入5000行的平均时间是相同的，而无需Fastupdate，并且具有任何大小的Gin_Pending_List_limit，预期将为预期的。</p><p> Updates that don’t trigger a cleanup take the same time, no matter how big or small  gin_pending_list_limit is (again, expected).</p><p> 更新不触发清理的同时，无论Gin_Pending_List_limit如何（再次，预期）。</p><p> With a value of 128KB, updates that triggered a cleanup took 4sec, which was very tolerable</p><p> 具有128KB的值，触发清除的更新需要4秒，这是非常容忍的 </p><p> When the index was dropped, we saw a huge performance boost ( 3x faster with non-batched updates and  &gt;6x faster with batched!)</p><p>当索引被删除时，我们看到了一个巨大的性能提升（使用非批量更新和gt速度快3倍; 6倍以批量更快！）</p><p>  By experimentation, 128KB seemed like a good value. So I chose to proceed this way.</p><p>  通过实验，128kb似乎是一个很好的价值。所以我选择了这样的方式。</p><p>  Via  postgresql.conf (or DB parameter groups in AWS RDS). This affects all GIN indexes. In AWS RDS it doesn’t require a restart(it’s a dynamic parameter). However, if you’re running a self-managed PostgreSQL you’ll most likely need to restart for the changesin  postgresql.conf to take effect</p><p>  通过Postgresql.conf（或AWS RDS中的DB参数组）。这会影响所有GIN索引。在AWS RDS中，它不需要重启（它是一个动态参数）。但是，如果您正在运行一个自我管理的PostgreSQL，您将很可能需要重新启动更改PostgreSQL.conf以生效</p><p> By altering the index storage parameters ( ALTER INDEX &lt;index_name&gt; SET (gin_pending_list_limit=128)). But this could causea number of issues (see the note above)</p><p> 通过更改索引存储参数（ALTER index＆lt; index_name＆gt; set（gin_pending_list_limit = 128））。但这可能会导致问题的数量（见上文的纸币）</p><p> By altering the  gin_pending_list_limit for the specific user ( ALTER USER &lt;user_name&gt; SET gin_pending_list_limit=128).This would affect all new connections and wouldn’t require a restart.</p><p> 通过改变特定用户的gin_pending_list_limit（更改用户＆lt; user_name＆gt; set gin_pending_list_limit = 128）。这会影响所有新连接，不需要重启。</p><p> Personally I’d choose the first one. In this case I had to go with the latter because of some unrelated issues. But they allwould do the trick.</p><p> 就个人而言，我会选择第一个。在这种情况下，由于一些不相关的问题，我必须与后者一起去。但他们都可以做到这一点。</p><p>  After monitoring for 1 week, there were no random failing writes which was an amazing relief since the issue had been there forever.The whole process took about a week and apart from gaining knowledge on GIN index internals, it also provided some insight on howmuch a GIN index can affect write times and triggered a discussion for reconsidering full text search in PostgreSQL.</p><p>  在监测1周后，没有随机失败的写作，这是一个惊人的救济，因为这个问题永远存在。整个过程大约需要一个星期，除了获得关于GIN指数内部的知识，它还为Howmuch提供了一些洞察力GIN索引可以影响写入时间并触发了在PostgreSQL中重新考虑的全文搜索的讨论。 </p><p>    Have suggestions? I&#39;d love to hear from you! Don&#39;t hesitate to reach out in any of my social channels.</p><p>有建议吗？ 我喜欢收到你的来信！ 不要在我的任何社交渠道中犹豫不决。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://iamsafts.com/posts/postgres-gin-performance/">https://iamsafts.com/posts/postgres-gin-performance/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/random/">#random</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/索引/">#索引</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>