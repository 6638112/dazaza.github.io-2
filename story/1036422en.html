<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>适用于Apache Airflow的AWS托管工作流AWS Managed Workflows for Apache Airflow</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">AWS Managed Workflows for Apache Airflow<br/>适用于Apache Airflow的AWS托管工作流</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-25 05:21:27</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/86e388335e67afbc46ed792a9333914e.png"><img src="http://img2.diglog.com/img/2020/11/86e388335e67afbc46ed792a9333914e.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>As the volume and complexity of your data processing pipelines increase, you can simplify the overall process by decomposing it into a series of smaller tasks and coordinate the execution of these tasks as part of a  workflow. To do so, many developers and data engineers use  Apache Airflow, a platform created by the community to programmatically author, schedule, and monitor workflows. With Airflow you can manage workflows as scripts, monitor them via the user interface (UI), and extend their functionality through a set of powerful plugins. However, manually installing, maintaining, and scaling Airflow, and at the same time handling security, authentication, and authorization for its users takes much of the time you’d rather use to focus on solving actual business problems.</p><p>随着数据处理管道的数量和复杂性的增加，您可以通过将其分解为一系列较小的任务并协调这些任务作为工作流一部分的执行来简化整个过程。为此，许多开发人员和数据工程师使用由社区创建的Apache Airflow这个平台，以编程方式编写，安排和监视工作流。借助Airflow，您可以将工作流程作为脚本进行管理，通过用户界面（UI）对其进行监视，并通过一组功能强大的插件来扩展其功能。但是，手动安装，维护和扩展Airflow并同时为其用户处理安全性，身份验证和授权需要花费大量时间，而您宁愿专注于解决实际的业务问题。</p><p>  For these reasons, I am happy to announce the availability of  Amazon Managed Workflows for Apache Airflow (MWAA), a fully managed service that makes it easy to run open-source versions of Apache Airflow on AWS, and to build workflows to execute your  extract-transform-load (ETL) jobs and data pipelines.</p><p>  由于这些原因，我很高兴宣布适用于Apache Airflow的Amazon Managed Workflows（MWAA），这是一项完全托管的服务，可让您轻松在AWS上运行Apache Airflow的开源版本，并构建工作流程来执行摘录-transform-load（ETL）作业和数据管道。</p><p>  Airflow workflows retrieve input from sources like  Amazon Simple Storage Service (S3) using  Amazon Athena queries, perform transformations on  Amazon EMR clusters, and can use the resulting data to train machine learning models on  Amazon SageMaker. Workflows in Airflow are authored as  Directed Acyclic Graphs (DAGs) using the Python programming language.</p><p>  气流工作流程使用Amazon Athena查询从Amazon Simple Storage Service（S3）等来源检索输入，在Amazon EMR集群上执行转换，并可以使用所得数据在Amazon SageMaker上训练机器学习模型。使用Python编程语言，将Airflow中的工作流编写为有向非循环图（DAG）。</p><p>  A key benefit of Airflow is its open extensibility through  plugins which allows you to create tasks that interact with AWS or on-premise resources required for your workflows including  AWS Batch,  Amazon CloudWatch,  Amazon DynamoDB,  AWS DataSync,  Amazon ECS and  AWS Fargate,  Amazon Elastic Kubernetes Service (EKS),  Amazon Kinesis Firehose,  AWS Glue,  AWS Lambda,  Amazon Redshift,  Amazon Simple Queue Service (SQS), and  Amazon Simple Notification Service (SNS).</p><p>  Airflow的主要优势在于其通过插件的开放可扩展性，使您可以创建与AWS交互的任务或工作流所需的本地资源，包括AWS Batch，Amazon CloudWatch，Amazon DynamoDB，AWS DataSync，Amazon ECS和AWS Fargate，Amazon弹性Kubernetes服务（EKS），Amazon Kinesis Firehose，AWS Glue，AWS Lambda，Amazon Redshift，Amazon Simple Queue Service（SQS）和Amazon Simple Notification Service（SNS）。</p><p>  To improve observability, Airflow  metrics can be published as CloudWatch Metrics, and  logs can be sent to CloudWatch Logs.  Amazon MWAA provides automatic minor version  upgrades and patches by default, with an option to designate a maintenance window in which these upgrades are performed.</p><p>  为了提高可观察性，可以将Airflow指标发布为CloudWatch指标，并将日志发送到CloudWatch Logs。默认情况下，Amazon MWAA提供自动的次要版本升级和补丁程序，并带有一个选项来指定在其中执行这些升级的维护时段。</p><p>    Create an environment – Each environment contains your Airflow cluster, including your scheduler, workers, and web server.</p><p>    创建环境–每个环境都包含Airflow集群，包括调度程序，工作程序和Web服务器。</p><p>  Upload your DAGs and plugins to S3 – Amazon MWAA loads the code into Airflow automatically.</p><p>  将您的DAG和插件上传到S3 – Amazon MWAA将代码自动加载到Airflow中。</p><p>  Run your DAGs in Airflow – Run your DAGs from the Airflow UI or command line interface (CLI) and monitor your environment with CloudWatch.</p><p>在Airflow中运行DAG –从Airflow UI或命令行界面（CLI）运行DAG，并使用CloudWatch监控环境。</p><p>    How to Create an Airflow Environment Using  Amazon MWAA  In the   Amazon MWAA console, I click on  Create environment. I give the environment a name and select the  Airflow version to use.</p><p>    如何使用Amazon MWAA创建气流环境在Amazon MWAA控制台中，单击“创建环境”。我给环境命名，然后选择要使用的Airflow版本。</p><p>    Then, I select the S3 bucket and the folder to load my  DAG code. The bucket name must start with  airflow-.</p><p>    然后，选择S3存储桶和文件夹以加载我的DAG代码。桶名称必须以airflow-开头。</p><p>      For plugins and requirements, I can select the  S3 object version to use. In case the plugins or the requirements I use create a non-recoverable error in my environment,  Amazon MWAA will automatically roll back to the previous working version.</p><p>      对于插件和要求，我可以选择要使用的S3对象版本。如果我使用的插件或要求在我的环境中创建了不可恢复的错误，Amazon MWAA将自动回滚到以前的工作版本。</p><p>  I click  Next to configure the advanced settings, starting with  networking. Each environment runs in a  Amazon Virtual Private Cloud using private subnets in two  availability zones. Web server access to the Airflow UI is always protected by a secure login using  AWS Identity and Access Management (IAM). However, you can choose to have web server access on a public network so that you can login over the Internet, or on a private network in your VPC. For simplicity, I select a  Public network. I let  Amazon MWAA create a new  security group with the correct inbound and outbound rules. Optionally, I can add one or more existing security groups to fine-tune control of inbound and outbound traffic for your environment.</p><p>  我从网络开始单击“下一步”以配置高级设置。每个环境都使用两个可用区中的私有子网在Amazon Virtual Private Cloud中运行。 Web服务器对Airflow UI的访问始终受使用AWS Identity and Access Management（IAM）的安全登录保护。但是，您可以选择在公共网络上访问Web服务器，以便可以通过Internet或VPC中的专用网络登录。为简单起见，我选择一个公共网络。我让Amazon MWAA使用正确的入站和出站规则创建一个新的安全组。 （可选）我可以添加一个或多个现有安全组，以针对您的环境微调对入站和出站流量的控制。</p><p>    Now, I configure my  environment class. Each environment includes a scheduler, a web server, and a worker. Workers automatically scale up and down according to my workload. We provide you a suggestion on which class to use based on the number of DAGs, but you can monitor the load on your environment and modify its class at any time.</p><p>    现在，我配置我的环境类。每个环境都包括一个调度程序，一个Web服务器和一个工作程序。工人们会根据我的工作量自动扩大和缩小。我们会根据DAG的数量为您提供关于使用哪个类的建议，但是您可以随时监视环境的负载并修改其类。</p><p>    Encryption is always enabled for data at rest, and while I can select a customized key managed by  AWS Key Management Service (KMS) I will instead keep the default key that AWS owns and manages on my behalf.</p><p>    始终为静态数据启用加密，尽管我可以选择由AWS Key Management Service（KMS）管理的自定义密钥，但我将保留AWS代表我拥有和管理的默认密钥。</p><p>    For  monitoring, I publish environment performance to CloudWatch Metrics. This is enabled by default, but I can disable CloudWatch Metrics after launch. For the  logs, I can specify the log level and which Airflow components should send their logs to CloudWatch Logs. I leave the default to send only the task logs and use log level  INFO.</p><p>为了进行监控，我将环境性能发布到CloudWatch Metrics。默认情况下启用此功能，但是启动后我可以禁用CloudWatch指标。对于日志，我可以指定日志级别以及哪些Airflow组件应将其日志发送到CloudWatch Logs。我保留默认设置，仅发送任务日志并使用日志级别的INFO。</p><p>    I can modify the default settings for Airflow  configuration options, such as  default_task_retries or  worker_concurrency. For now, I am not changing these values.</p><p>    我可以修改Airflow配置选项的默认设置，例如default_task_retries或worker_concurrency。目前，我不会更改这些值。</p><p>    Finally, but most importantly, I configure the  permissions that will be used by my environment to access my DAGs, write logs, and run DAGs accessing other AWS resources. I select  Create a new role and click on  Create environment. After a few minutes, the new Airflow environment is ready to be used.</p><p>    最后，但最重要的是，我配置了环境将用来访问我的DAG，编写日志以及运行DAG来访问其他AWS资源的权限。我选择“创建新角色”，然后单击“创建环境”。几分钟后，即可使用新的Airflow环境。</p><p>    Using the Airflow UI  In the  Amazon MWAA console, I look for the new environment I just created and click on  Open Airflow UI. A new browser window is created and I am authenticated with a secure login via AWS IAM.</p><p>    使用Airflow UI在Amazon MWAA控制台中，我寻找刚创建的新环境，然后单击Open Airflow UI。创建一个新的浏览器窗口，并通过AWS IAM通过安全登录进行身份验证。</p><p>  There, I look for a DAG that I put on S3 in the  movie_list_dag.py file. The DAG is downloading the  MovieLens dataset, processing the files on S3 using  Amazon Athena, and loading the result to a Redshift cluster, creating the table if missing.</p><p>  在那里，我在movie_list_dag.py文件中寻找放在S3上的DAG。 DAG正在下载MovieLens数据集，使用Amazon Athena在S3上处理文件，并将结果加载到Redshift集群中，如果缺少该表，则创建该表。</p><p>    from airflow import DAGfrom airflow.operators.python_operator import PythonOperatorfrom airflow.operators import HttpSensor, S3KeySensorfrom airflow.contrib.operators.aws_athena_operator import AWSAthenaOperatorfrom airflow.utils.dates import days_agofrom datetime import datetime, timedeltafrom io import StringIOfrom io import BytesIOfrom time import sleepimport csvimport requestsimport jsonimport boto3import zipfileimport ios3_bucket_name = &#39;my-bucket&#39;s3_key=&#39;files/&#39;redshift_cluster=&#39;my-redshift-cluster&#39;redshift_db=&#39;dev&#39;redshift_dbuser=&#39;awsuser&#39;redshift_table_name=&#39;movie_demo&#39;test_http=&#39;https://grouplens.org/datasets/movielens/latest/&#39;download_http=&#39;http://files.grouplens.org/datasets/movielens/ml-latest-small.zip&#39;athena_db=&#39;demo_athena_db&#39;athena_results=&#39;athena-results/&#39;create_athena_movie_table_query=&#34;&#34;&#34;CREATE EXTERNAL TABLE IF NOT EXISTS Demo_Athena_DB.ML_Latest_Small_Movies ( `movieId` int, `title` string, `genres` string )ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;WITH SERDEPROPERTIES ( &#39;serialization.format&#39; = &#39;,&#39;, &#39;field.delim&#39; = &#39;,&#39;) LOCATION &#39;s3://my-bucket/files/ml-latest-small/movies.csv/ml-latest-small/&#39;TBLPROPERTIES ( &#39;has_encrypted_data&#39;=&#39;false&#39;, &#39;skip.header.line.count&#39;=&#39;1&#39;); &#34;&#34;&#34;create_athena_ratings_table_query=&#34;&#34;&#34;CREATE EXTERNAL TABLE IF NOT EXISTS Demo_Athena_DB.ML_Latest_Small_Ratings ( `userId` int, `movieId` int, `rating` int, `timestamp` bigint )ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;WITH SERDEPROPERTIES ( &#39;serialization.format&#39; = &#39;,&#39;, &#39;field.delim&#39; = &#39;,&#39;) LOCATION &#39;s3://my-bucket/files/ml-latest-small/ratings.csv/ml-latest-small/&#39;TBLPROPERTIES ( &#39;has_encrypted_data&#39;=&#39;false&#39;, &#39;skip.header.line.count&#39;=&#39;1&#39;); &#34;&#34;&#34;create_athena_tags_table_query=&#34;&#34;&#34;CREATE EXTERNAL TABLE IF NOT EXISTS Demo_Athena_DB.ML_Latest_Small_Tags ( `userId` int, `movieId` int, `tag` int, `timestamp` bigint )ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#39;WITH SERDEPROPERTIES ( &#39;serialization.format&#39; = &#39;,&#39;, &#39;field.delim&#39; = &#39;,&#39;) LOCATION &#39;s3://my-bucket/files/ml-latest-small/tags.csv/ml-latest-small/&#39;TBLPROPERTIES ( &#39;has_encrypted_data&#39;=&#39;false&#39;, &#39;skip.header.line.count&#39;=&#39;1&#39;); &#34;&#34;&#34;join_tables_athena_query=&#34;&#34;&#34;SELECT REPLACE ( m.title , &#39;&#34;&#39; , &#39;&#39; ) as title, r.ratingFROM demo_athena_db.ML_Latest_Small_Movies mINNER JOIN (SELECT rating, movieId FROM demo_athena_db.ML_Latest_Small_Ratings WHERE rating &gt; 4) r on m.movieId = r.movieId&#34;&#34;&#34;def download_zip(): s3c = boto3.client(&#39;s3&#39;) indata = requests.get(download_http) n=0 with zipfile.ZipFile(io.BytesIO(indata.content)) as z:  zList=z.namelist() print(zList) for i in zList:  print(i)  zfiledata = BytesIO(z.read(i)) n += 1 s3c.put_object(Bucket=s3_bucket_name, Key=s3_key+i+&#39;/&#39;+i, Body=zfiledata)def clean_up_csv_fn(**kwargs):  ti = kwargs[&#39;task_instance&#39;] queryId = ti.xcom_pull(key=&#39;return_value&#39;, task_ids=&#39;join_athena_tables&#39; ) print(queryId) athenaKey=athena_results+&#34;join_athena_tables/&#34;+queryId+&#34;.csv&#34; print(athenaKey) cleanKey=athena_results+&#34;join_athena_tables/&#34;+queryId+&#34;_clean.csv&#34; s3c = boto3.client(&#39;s3&#39;) obj = s3c.get_object(Bucket=s3_bucket_name, Key=athenaKey) infileStr=obj[&#39;Body&#39;].read().decode(&#39;utf-8&#39;) outfileStr=infileStr.replace(&#39;&#34;e&#34;&#39;, &#39;&#39;)  outfile = StringIO(outfileStr) s3c.put_object(Bucket=s3_bucket_name, Key=cleanKey, Body=outfile.getvalue())def s3_to_redshift(**kwargs):  ti = kwargs[&#39;task_instance&#39;] queryId = ti.xcom_pull(key=&#39;return_value&#39;, task_ids=&#39;join_athena_tables&#39; ) print(queryId) athenaKey=&#39;s3://&#39;+s3_bucket_name+&#34;/&#34;+athena_results+&#34;join_athena_tables/&#34;+queryId+&#34;_clean.csv&#34; print(athenaKey) sqlQuery=&#34;copy &#34;+redshift_table_name+&#34; from &#39;&#34;+athenaKey+&#34;&#39; iam_role &#39;arn:aws:iam::163919838948:role/myRedshiftRole&#39; CSV IGNOREHEADER 1;&#34; print(sqlQuery) rsd = boto3.client(&#39;redshift-data&#39;) resp = rsd.execute_statement( ClusterIdentifier=redshift_cluster, Database=redshift_db, DbUser=redshift_dbuser, Sql=sqlQuery ) print(resp) return &#34;OK&#34;def create_redshift_table(): rsd = boto3.client(&#39;redshift-data&#39;) resp = rsd.execute_statement( ClusterIdentifier=redshift_cluster, Database=redshift_db, DbUser=redshift_dbuser, Sql=&#34;CREATE TABLE IF NOT EXISTS &#34;+redshift_table_name+&#34; (title	character varying, rating	int);&#34; ) print(resp) return &#34;OK&#34;DEFAULT_ARGS = { &#39;owner&#39;: &#39;airflow&#39;, &#39;depends_on_past&#39;: False, &#39;email&#39;: [&#39;airflow@example.com&#39;], &#39;email_on_failure&#39;: False, &#39;email_on_retry&#39;: False }with DAG( dag_id=&#39;movie-list-dag&#39;, default_args=DEFAULT_ARGS, dagrun_timeout=timedelta(hours=2), start_date=days_ago(2), schedule_interval=&#39;*/10 * * * *&#39;, tags=[&#39;athena&#39;,&#39;redshift&#39;],) as dag: check_s3_for_key = S3KeySensor( task_id=&#39;check_s3_for_key&#39;, bucket_key=s3_key, wildcard_match=True, bucket_name=s3_bucket_name, s3_conn_id=&#39;aws_default&#39;, timeout=20, poke_interval=5, dag=dag ) files_to_s3 = PythonOperator( task_id=&#34;files_to_s3&#34;, python_callable=download_zip ) create_athena_movie_table = AWSAthenaOperator(task_id=&#34;create_athena_movie_table&#34;,query=create_athena_movie_table_query, database=athena_db, output_location=&#39;s3://&#39;+s3_bucket_name+&#34;/&#34;+athena_results+&#39;create_athena_movie_table&#39;) create_athena_ratings_table = AWSAthenaOperator(task_id=&#34;create_athena_ratings_table&#34;,query=create_athena_ratings_table_query, database=athena_db, output_location=&#39;s3://&#39;+s3_bucket_name+&#34;/&#34;+athena_results+&#39;create_athena_ratings_table&#39;) create_athena_tags_table = AWSAthenaOperator(task_id=&#34;create_athena_tags_table&#34;,query=create_athena_tags_table_query, database=athena_db, output_location=&#39;s3://&#39;+s3_bucket_name+&#34;/&#34;+athena_results+&#39;create_athena_tags_table&#39;) join_athena_tables = AWSAthenaOperator(task_id=&#34;join_athena_tables&#34;,query=join_tables_athena_query, database=athena_db, output_location=&#39;s3://&#39;+s3_bucket_name+&#34;/&#34;+athena_results+&#39;join_athena_tables&#39;) create_redshift_table_if_not_exists = PythonOperator( task_id=&#34;create_redshift_table_if_not_exists&#34;, python_callable=create_redshift_table ) clean_up_csv = PythonOperator( task_id=&#34;clean_up_csv&#34;, python_callable=clean_up_csv_fn, provide_context=True  ) transfer_to_redshift = PythonOperator( task_id=&#34;transfer_to_redshift&#34;, python_callable=s3_to_redshift, provide_context=True  ) check_s3_for_key &gt;&gt; files_to_s3 &gt;&gt; create_athena_movie_table &gt;&gt; join_athena_tables &gt;&gt; clean_up_csv &gt;&gt; transfer_to_redshift files_to_s3 &gt;&gt; create_athena_ratings_table &gt;&gt; join_athena_tables files_to_s3 &gt;&gt; create_athena_tags_table &gt;&gt; join_athena_tables files_to_s3 &gt;&gt; create_redshift_table_if_not_exists &gt;&gt; transfer_to_redshift</p><p>    从气流进口DAG从airflow.operators.python_operator导入PythonOperator从airflow.operators导入HttpSensor，S3KeySensor从airflow.contrib.operators.aws_athena_operator导入AWSAthenaOperator从airflow.utils.dates导入days_ago从datetime导入datetime，timedelta从io导入StringIO从io导入BytesIO从进口睡眠开始导入csv汇入要求导入json导入boto3导入压缩文件进口ios3_bucket_name ='我的桶's3_key ='文件/'redshift_cluster ='my-redshift-cluster'redshift_db ='dev'redshift_dbuser ='awsuser'redshift_table_name ='电影_演示'test_http ='https：//grouplens.org/datasets/movielens/latest/'download_http ='http：//files.grouplens.org/datasets/movielens/ml-latest-small.zip'athena_db ='demo_athena_db'athena_results ='雅典娜结果/'create_athena_movie_table_query =“”“如果不存在，则创建外部表Demo_Athena_DB.ML_Latest_Small_Movies（ `movieId` int， `title`字符串， 类型字符串）行格式SERDE'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'与SERDEPROPERTIES（ 'serialization.format'='，'， 'field.delim'='，'）位置's3：//my-bucket/files/ml-latest-small/movies.csv/ml-latest-small/'TBLPROPERTIES（ 'has_encrypted_data'='false'， 'skip.header.line.count'='1'）;“”create_athena_ratings_table_query =“”“如果不存在，则创建外部表Demo_Athena_DB.ML_Latest_Small_Ratings（ `userId` int， `movieId` int， `rating` int， 时间戳记bigint）行格式SERDE'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'与SERDEPROPERTIES（ 'serialization.format'='，'， 'field.delim'='，'）位置's3：//my-bucket/files/ml-latest-small/ratings.csv/ml-latest-small/'TBLPROPERTIES（ 'has_encrypted_data'='false'， 'skip.header.line.count'='1'）;“”create_athena_tags_table_query =“”“如果不存在，则创建外部表Demo_Athena_DB.ML_Latest_Small_Tags（ `userId` int， `movieId` int， `tag` int， 时间戳记bigint）行格式SERDE'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'与SERDEPROPERTIES（ 'serialization.format'='，'， 'field.delim'='，'）位置's3：//my-bucket/files/ml-latest-small/tags.csv/ml-latest-small/'TBLPROPERTIES（ 'has_encrypted_data'='false'， 'skip.header.line.count'='1'）;“”join_tables_athena_query =“”“选择替换（m.title，'“'，''）作为标题，等级从demo_athena_db.ML_Latest_Small_Movies m内联接（SELECT等级，来自demo_athena_db.ML_Latest_Small_Ratings WHERE等级> 4的movieId）r on m.movi​​eId = r.movi​​eId“”def download_zip（）： s3c = boto3.client（'s3'） indata = requests.get（download_http） n = 0 zipfile.ZipFile（io.BytesIO（indata.content））为z： zList = z.namelist（） 打印（zList） 对于zList中的i： 打印（i） zfiledata = BytesIO（z.read（i）） n + = 1 s3c.put_object（Bucket = s3_bucket_name，Key = s3_key + i +'/'+ i，Body = zfiledata）def clean_up_csv_fn（** kwargs）： ti = kwargs ['task_instance'] queryId = ti.xcom_pull（key ='return_value'，task_ids ='join_athena_tables'） 打印（查询ID） athenaKey = athena_results +“ join_athena_tables /” + queryId +“。csv” 打印（athenaKey） cleanKey = athena_results +“ join_athena_tables /” + queryId +“ _ clean.csv” s3c = boto3.client（'s3'） obj = s3c.get_object（Bucket = s3_bucket_name，Key = athenaKey） infileStr = obj ['Body']。read（）。decode（'utf-8'） outfileStr = infileStr.replace（'“ e”'，''） outfile = StringIO（outfileStr） s3c.put_object（Bucket = s3_bucket_name，Key = cleanKey，Body = outfile.getvalue（））def s3_to_redshift（** kwargs）： ti = kwargs ['task_instance'] queryId = ti.xcom_pull（key ='return_value'，task_ids ='join_athena_tables'） 打印（查询ID） athenaKey ='s3：//'+ s3_bucket_name +“ /” + athena_results +“ join_athena_tables /” + queryId +“ _clean.csv” 打印（athenaKey） sqlQuery =“ copy” + redshift_table_name +“ from'” + athenaKey +“'iam_role'arn：aws：iam :: 163919838948：role / myRedshiftRole'CSV IGNOREHEADER 1;” 打印（sqlQuery） rsd = boto3.client（'redshift-data'） resp = rsd.execute_statement（ ClusterIdentifier = redshift_cluster， 数据库= redshift_db， DbUser = redshift_dbuser， Sql = sqlQuery ） 打印（resp） 返回“确定”def create_redshift_table（）： rsd = boto3.client（'redshift-data'） resp = rsd.execute_statement（ ClusterIdentifier = redshift_cluster， 数据库= redshift_db， DbUser = redshift_dbuser， Sql =“如果不存在则创建表” + redshift_table_name +“（标题字符不同，等级为int）；” ） 打印（resp） 返回“确定”DEFAULT_ARGS = { '所有者'：'气流'， 'depends_on_past'：错误， '电子邮件'：['airflow@example.com']， 'email_on_failure'：错误， 'email_on_retry'：错误}与DAG（ dag_id ='电影列表-dag'， default_args = DEFAULT_ARGS， dagrun_timeout = timedelta（小时= 2）， start_date = days_ago（2）， schedule_interval ='* / 10 * * * *'， 标签= ['雅典娜'，'红移']，）作为dag： check_s3_for_key = S3KeySensor（ task_id ='check_s3_for_key'， bucket_key = s3_key， wildcard_match =真， bucket_name = s3_bucket_name， s3_conn_id ='aws_default'， 超时= 20， poke_interval = 5， dag = dag ） files_to_s3 = PythonOperator（ task_id =“ files_to_s3”， python_callable = download_zip ） create_athena_movie_table = AWSAthenaOperator（task_id =“ create_athena_movie_table”，query = create_athena_movie_table_query，database = athena_db，output_location ='s3：//'+ s3_bucket_name +“ /” +“ athena_results +'create_athena_movie_table'） create_athena_ratings_table = AWSAthenaOperator（task_id =“ create_athena_ratings_table”，query = create_athena_ratings_table_query，database = athena_db，output_location ='s3：//'+ s3_bucket_name +“ /” + athena_results +'create_athena_ratings_table'） create_athena_tags_table = AWSAthenaOperator（task_id =“ create_athena_tags_table”，query = create_athena_tags_table_query，database = athena_db，output_location ='s3：//'+ s3_bucket_name +“ /” + athena_results +'create_athena_tags_table'） join_athena_tables = AWSAthenaOperator（task_id =“ join_athena_tables”，query = join_tables_athena_query，database = athena_db，output_location ='s3：//'+ s3_bucket_name +“ /” + athena_results +'join_athena_tables'） create_redshift_table_if_not_exists = PythonOperator（ task_id =“ create_redshift_table_if_not_exists”， python_callable = create_redshift_table ） clean_up_csv = PythonOperator（ task_id =“ clean_up_csv”， python_callable = clean_up_csv_fn， Provide_context =真 ） transfer_to_redshift = PythonOperator（ task_id =“ transfer_to_redshift”， python_callable = s3_to_redshift， Provide_context =真 ） check_s3_for_key >> files_to_s3 >> create_athena_movie_table >> join_athena_tables >> clean_up_csv >> transfer_to_redshift files_to_s3 >> create_athena_ratings_table >> join_athena_tables files_to_s3 >> create_athena_tags_table >> join_athena_tables files_to_s3 >> create_redshift_table_if_not_exists >> transfer_to_redshift</p><p>  In the code, different tasks are created using operators like  PythonOperator, for generic Python code, or  AWSAthenaOperator, to use the integration with  Amazon Athena. To see how those tasks are connected in the workflow, you can see the latest few lines, that I repeat here (without indentation) for simplicity:</p><p>  在代码中，使用诸如PythonOperator之类的运算符（针对通用Python代码）或AWSAthenaOperator来创建不同的任务，以使用与Amazon Athena的集成。要查看这些任务如何在工作流中进行连接，您可以看到最新的几行，为简单起见，在此重复（不缩进）：</p><p>  check_s3_for_key &gt;&gt; files_to_s3 &gt;&gt; create_athena_movie_table &gt;&gt; join_athena_tables &gt;&gt; clean_up_csv &gt;&gt; transfer_to_redshiftfiles_to_s3 &gt;&gt; create_athena_ratings_table &gt;&gt; join_athena_tablesfiles_to_s3 &gt;&gt; create_athena_tags_table &gt;&gt; join_athena_tablesfiles_to_s3 &gt;&gt; create_redshift_table_if_not_exists &gt;&gt; transfer_to_redshift</p><p>check_s3_for_key >> files_to_s3 >> create_athena_movie_table >> join_athena_tables >> clean_up_csv >> transfer_to_redshiftfiles_to_s3 >> create_athena_ratings_table >> join_athena_tablesfiles_to_s3 >> create_athena_tags_table >> join_athena_tablesfiles_to_s3 >> create_redshift_table_if_not_exists >> transfer_to_redshift</p><p>  The Airflow code is  overloading the right shift  &gt;&gt; operator in Python to create a dependency, meaning that the task on the left should be executed first, and the output passed to the task on the right. Looking at the code, this is quite easy to read. Each of the four lines above is adding dependencies, and they are all evaluated together to execute the tasks in the right order.</p><p>  Airflow代码在Python中重载了右移>>运算符以创建依赖关系，这意味着应首先执行左侧的任务，并将输出传递到右侧的任务。查看代码，这很容易阅读。上面的四行中的每一行都添加了依赖关系，并且将它们一起评估以按正确的顺序执行任务。</p><p>  In the Airflow console, I can see a  graph view of the DAG to have a clear representation of how tasks are executed:</p><p>  在Airflow控制台中，我可以看到DAG的图形视图，以清楚地表示任务的执行方式：</p><p>    Available Now   Amazon Managed Workflows for Apache Airflow (MWAA) is available today in US East (Northern Virginia), US West (Oregon), US East (Ohio), Asia Pacific (Singapore), Asia Pacific (Toyko), Asia Pacific (Sydney), Europe (Ireland), Europe (Frankfurt), and Europe (Stockholm). You can launch a new  Amazon MWAA environment from the console,  AWS Command Line Interface (CLI), or  AWS SDKs. Then, you can develop workflows in Python using Airflow’s ecosystem of integrations.</p><p>    现在可用美国东部（北弗吉尼亚州），美国西部（俄勒冈州），美国东部（俄亥俄州），亚太地区（新加坡），亚太地区（Toyko），亚太地区（悉尼）提供适用于Apache Airflow（MWAA）的Amazon托管工作流。 ），欧洲（爱尔兰），欧洲（法兰克福）和欧洲（斯德哥尔摩）。您可以从控制台，AWS命令行界面（CLI）或AWS开发工具包启动新的Amazon MWAA环境。然后，您可以使用Airflow的集成生态系统在Python中开发工作流程。</p><p>  With  Amazon MWAA, you pay based on the environment class and the workers you use. For more information, see the  pricing page.</p><p>  使用Amazon MWAA，您可以根据环境等级和所使用的工人付费。有关更多信息，请参见定价页面。</p><p>  Upstream compatibility is a core tenet of  Amazon MWAA. Our code changes to the AirFlow platform are released back to open source.</p><p>  上游兼容性是Amazon MWAA的核心宗旨。我们对AirFlow平台的代码更改已发布回开源。</p><p>  With  Amazon MWAA you can spend more time building workflows for your engineering and data science tasks, and less time managing and scaling the infrastructure of your Airflow platform.</p><p>  借助Amazon MWAA，您可以将更多的时间花费在为工程和数据科学任务构建工作流上，而无需花费更多时间来管理和扩展Airflow平台的基础架构。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/">https://aws.amazon.com/blogs/aws/introducing-amazon-managed-workflows-for-apache-airflow-mwaa/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/apache/">#apache</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/managed/">#managed</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/airflow/">#airflow</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>