<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>对PostgreSQL的异步和“直接”IO支持 Asynchronous and “direct” IO support for PostgreSQL</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Asynchronous and “direct” IO support for PostgreSQL<br/>对PostgreSQL的异步和“直接”IO支持 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-30 03:05:24</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/c07b25dff739d90d25cb05efd821308a.png"><img src="http://img2.diglog.com/img/2021/3/c07b25dff739d90d25cb05efd821308a.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>over the last ~year I spent a lot of time trying to figure out how we could add AIO (asynchronous IO) and DIO (direct IO) support to postgres. While there&#39;s still a *lot* of open questions, I think I now have a decent handle on most of the bigger architectural questions. Thus this long email.</p><p>在过去的一年中，我花了很多时间试图弄清楚我们如何为Postgres添加AIO（异步IO）和DIO（直接IO）支持。虽然在那里＆＃39;仍然是一个*很多打开的问题，我认为我现在在大多数更大的架构问题上都有一个体面的处理。因此这段长电子邮件。</p><p> Just to be clear: I don&#39;t expect the current to design to survive as-is. If there&#39;s a few sentences below that sound a bit like describing the new world, that&#39;s because they&#39;re from the README.md in the patch series...</p><p> 只是为了清楚：我不希望目前设计以幸存下来。如果有一些句子下面那么听起来有点像描述新世界，那个＆＃39;因为它们来自README.MD的补丁系列......</p><p>   - Lower CPU usage / higher throughput. Particularly on modern storage  buffered writes are bottlenecked by the operating system having to  copy data from the kernel&#39;s page cache to postgres buffer pool using  the CPU. Whereas direct IO can often move the data directly between  the storage devices and postgres&#39; buffer cache, using DMA. While  that transfer is ongoing, the CPU is free to perform other work,  with little impact. - Avoiding double buffering between operating system cache and  postgres&#39; shared_buffers. - Better control over the timing and pace of dirty data writeback. - Potential for concurrent WAL writes (via O_DIRECT | O_DSYNC writes)</p><p>    - 降低CPU使用率/较高吞吐量。特别是现代存储缓冲写入是由操作系统的瓶颈，必须使用CPU将数据从内核和＃39; S页面缓存复制到Postgres缓冲池。而Direct IO通常可以直接在存储设备和Postgres之间移动数据;缓冲区缓存，使用DMA。虽然该转移正在进行中，CPU可以自由地执行其他工作，影响很小。 - 避免在操作系统缓存和postgres之间进行双重缓冲; shared_buffers。 - 更好地控制脏数据写回的时序和步伐。 - 并发WAL写入的潜力（通过O_DIRECT | O_DSYNC写道）</p><p>  - Without AIO, Direct IO is unusably slow for most purposes. - Even with AIO, many parts of postgres need to be modified to perform  explicit prefetching. - In situations where shared_buffers cannot be set appropriately  large, e.g. because there are many different postgres instances  hosted on shared hardware, performance will often be worse then when  using buffered IO.</p><p>   - 没有aio，直接IO对于大多数目的而言是不可用的。 - 即使使用AIO，需要修改许多Postgres的部分以执行显式预取。 - 在Shared_Buffers无法适当地设置的情况下，例如，如此由于共享硬件上托管了许多不同的Postgres实例，因此使用缓冲IO时，性能通常会更差。</p><p>   - Without asynchronous IO (AIO) PG has to rely on the operating system  to hide the cost of synchronous IO from Postgres. While this works  surprisingly well in a lot of workloads, it does not do as good a job  on prefetching and controlled writeback as we would like. - There are important expensive operations like fdatasync() where the  operating system cannot hide the storage latency. This is particularly  important for WAL writes, where the ability to asynchronously issue  fdatasync() or O_DSYNC writes can yield significantly higher  throughput. - Fetching data into shared buffers asynchronously and concurrently with query  execution means there is more CPU time for query execution.</p><p>    - 没有异步IO（AIO）PG必须依靠操作系统来隐藏来自Postgres同步IO的成本。虽然这在很多工作负载中令人惊讶地令人惊讶的是，但我们希望在预取和受控写回的工作。 - 有重要的昂贵操作，如fdatasync（），操作系统无法隐藏存储延迟。这对WAL写入尤其重要，其中异步发出FDATASYNC（）或O_DSYNC写入的能力可以产生显着更高的吞吐量。 - 将数据与查询执行异步和同时将数据置于共享缓冲区，意味着有更多的查询执行CPU时间。</p><p>   - Platform dependency: The common AIO APIs are typically specific to one  platform (linux AIO, linux io_uring, windows IOCP, windows overlapped IO) or  a few platforms (posix AIO, but there&#39;s many differences).</p><p>    - 平台依赖项：公共AIO API通常特定于一个平台（Linux Aio，Linux IO_调节，Windows IoCP，Windows重叠的IO）或几个平台（Posix Aio，但在那里的许多差异）。</p><p> - There are a lot of separate places doing IO in PG. Moving all of these to  use efficiently use AIO is an, um, large undertaking.</p><p>  -  PG中有很多单独的位置。将所有这些用于有效使用AIO是一个，UM，大型企业。 </p><p> - Nothing in the buffer management APIs expects there to be more than one IO  to be in progress at the same time - which is required to do AIO.</p><p>- 缓冲区管理API中的任何内容都希望在同一时间内有多个IO  - 这是必需的。</p><p>  To avoid the issue of needing non-AIO codepaths to support platforms without native AIO support a worker process based AIO implementation exists (and is currently the default). This also is convenient to check if a problem is related to the native IO implementation or not.</p><p>  为避免在没有本机AIO的没有本机AIO支持平台的问题上，支持基于工人的AIO实现（并且当前是默认值）。这也方便地检查问题是否与本机IO实现相关。</p><p> Thanks to Thomas Munro for helping a *lot* around this area. He wrote the worker mode, the posix aio mode, added CI, did a lot of other testing, listened to me...</p><p> 谢谢托马斯芒罗在这个地区帮助一个*很多*。他写了工人模式，posix aio模式，添加了ci，做了很多其他测试，听到我...</p><p>  Using AIO in a naive way can easily lead to deadlocks in an environment where the source/target of AIO are shared resources, like pages in postgres&#39; shared_buffers.</p><p>  使用AIO以天真的方式可以很容易地导致AIO的源/目标是共享资源的环境中的死锁，如Postgres＆＃39中的页面; shared_buffers。</p><p> Consider one backend performing readahead on a table, initiating IO for a number of buffers ahead of the current &#34;scan position&#34;. If that backend then performs some operation that blocks, or even just is slow, the IO completion for the asynchronously initiated read may not be processed.</p><p> 考虑一个后端在表上执行readAhead，在当前＆＃34之前启动io的许多缓冲区;扫描位置＆＃34;如果该后端执行块或甚至慢慢执行某些操作，则可能不会处理异步启动读取的IO完成。</p><p> This AIO implementation solves this problem by requiring that AIO methods either allow AIO completions to be processed by any backend in the system (e.g. io_uring, and indirectly posix, via signal handlers), or to guarantee that AIO processing will happen even when the issuing backend is blocked (e.g. worker mode, which offloads completion processing to the AIO workers).</p><p> 这种AIO实现通过要求AIO方法允许通过系统中的任何后端（例如，IO_URE，以及间接POSIX，通过信号处理程序）来处理AIO完成来解决此问题，或者即使在发布后端，也可以保证将发生AIO处理被阻止（例如，工人模式，将完成处理卸载到AIO工人）。</p><p>     The (read|write|fsync|flush_range) indicates the operation, whereas  (smgr|sb|raw|wal) determines how IO completions, errors, ... are handled.</p><p>     （读写| fsync | flush_range）表示操作，而（SMGR | SB | WAL）确定如何完成IO完成，错误。 </p><p> (see below for more details about this design choice - it might or not be  right)</p><p>（有关此设计选择的更多详细信息，请参阅下文 - 它可能是不是正确的）</p><p>  4) 2) alone does *not* cause the IO to be submitted to the kernel, but to be  put on a per-backend list of pending IOs. The pending IOs can be explicitly  be flushed pgaio_submit_pending(), but will also be submitted if the  pending list gets to be too large, or if the current backend waits for the  IO.</p><p>  4）2）单独使用*不是*导致IO提交给内核，但要将其挂起IOS的每个后端列表。挂起的iOS可以明确地刷新pgaio_submit_pending（），但如果挂起的列表变为太大，或者当前后端等待IO，也将提交。</p><p> The are two main reasons not to submit the IO immediately:  - If adjacent, we can merge several IOs into one &#34;kernel level&#34; IO during  submission. Larger IOs are considerably more efficient.  - Several AIO APIs allow to submit a batch of IOs in one system call.</p><p> 这些主要原因是立即提交IO的两个主要原因： - 如果邻近，我们可以将多个iO合并为一个＆＃34;内核级别＆＃34; Io在提交期间。较大的iOS比较有效。 - 几个AIO API允许在一个系统调用中提交一批iOS。</p><p> 5) wait for the IO: pgaio_io_wait() waits for an IO &#34;owned&#34; by the current  backend. When other backends may need to wait for an IO to finish,  pgaio_io_ref() can put a reference to that AIO in shared memory (e.g. a  BufferDesc), which can be waited for using pgaio_io_wait_ref().</p><p> 5）等待IO：pgaio_io_wait（）等待IO＆＃34;拥有的＆＃34;通过当前的后端。当其他后端可能需要等待IO完成时，pgaio_io_ref（）可以对共享内存（例如a bufferdesc）的那个aio引用，可以使用pgaio_io_wait_ref（）。</p><p> 6) Process the results of the request. If a callback was registered in 3),  this isn&#39;t always necessary. The results of AIO can be accessed using  pgaio_io_result() which returns an integer where negative numbers are  -errno, and positive numbers are the [partial] success conditions  (e.g. potentially indicating a short read).</p><p> 6）处理请求的结果。如果在3）中注册了回调，则此ISN＆＃39; t始终是必要的。可以使用pgaio_io_result（）访问aio的结果，该pgaio_io_result（）返回一个整数，其中负数是-errno，正数是[部分]成功条件（例如，可能指示短读数）。</p><p> 7) release ownership of the io (pgaio_io_release()) or reuse the IO for  another operation (pgaio_io_recycle())</p><p> 7）版本IO的所有权（PGAIO_IO_RELEASE（））或重复使用IO的另一个操作（pgaio_io_recycle（））</p><p> Most places that want to use AIO shouldn&#39;t themselves need to care about managing the number of writes in flight, or the readahead distance. To help with that there are two helper utilities, a &#34;streaming read&#34; and a &#34;streaming write&#34;.</p><p> 大多数想要使用aio的地方＆＃39; t本身需要关心管理飞行中的写入数量，或readahead距离。帮助有两个辅助公用事业，A＆＃34;流读＆＃34;和一个＆＃34;流媒体写作＆＃34; </p><p> The &#34;streaming read&#34; helper uses a callback to determine which blocks to prefetch - that allows to do readahead in a sequential fashion but importantly also allows to asynchronously &#34;read ahead&#34; non-sequential blocks.</p><p>＆＃34;流读＆＃34;帮助者使用回调来确定要预取的块 - 这允许以顺序方式进行readahead，但重要的是还允许异步地＆＃34;阅读前方＆＃34;非顺序块。</p><p> E.g. for vacuum, lazy_scan_heap() has a callback that uses the visibility map to figure out which block needs to be read next. Similarly lazy_vacuum_heap() uses the tids in LVDeadTuples to figure out which blocks are going to be needed. Here&#39;s the latter as an example:  https://github.com/anarazel/postgres/commit/a244baa36bfb252d451a017a273a6da1c09f15a3#diff-3198152613d9a28963266427b380e3d4fbbfabe96a221039c6b1f37bc575b965R1906</p><p> 例如。对于真空，Lazy_scan_heap（）有一个回调，它使用可见性映射来弄清楚接下来需要读取哪个块。类似地，Lazy_Vacuum_heap（）使用LVDeadTuples中的TID来弄清楚需要哪些块。在这里作为一个例子</p><p>  One difficulty I had in this process was how to initialize IOs in light of the layering (from bufmgr.c over smgr.c and md.c to fd.c and back, but also e.g. xlog.c). Sometimes AIO needs to be initialized on the bufmgr.c level, sometimes on the md.c level, sometimes on the level of fd.c. But to be able to react to the completion of any such IO metadata about the operation is needed.</p><p>  我在这个过程中我有一个难度是如何根据分层的方式初始化iOS（从bufmgr.c上的smgr.c和md.co to fd.cd.c向后，但也是xlog.c）。有时需要在Bufmgr.c级上初始化aio，有时在md.c等级上初始化。有时在FD.C的水平上。但是能够对完成任何此类IO元数据进行反应，但需要对操作进行任何此类IO元数据。</p><p> Early on fd.c initialized IOs, and the context information was just passed through to fd.c. But that seems quite wrong - fd.c shouldn&#39;t have to know about which Buffer an IO is about. But higher levels shouldn&#39;t know about which files an operation resides in either, so they can&#39;t do all the work either...</p><p> 在FD.C初始化的iOS上提前，并且上下文信息刚刚传递给FD.C.但这似乎是错的 -  fd.cn＆＃39; t必须知道哪个缓冲区是关于哪个缓冲区。但更高的级别应该知道操作所在的文件所在的文件，所以他们可以＆＃39; t做所有的工作......</p><p> To avoid that, I ended up splitting the &#34;start an AIO&#34; operation into a higher level part, e.g. pgaio_io_start_read_smgr() - which doesn&#39;t know about which smgr implementation is in use and thus also not what file/offset we&#39;re dealing with, which calls into smgr-&gt;md-&gt;fd to actually &#34;prepare&#34; the IO (i.e. figure out file / offset). This currently looks like:</p><p> 避免这种情况，我最终拆分了＆＃34;开始一个aio＆＃34;操作进入更高级别的部分，例如， pgaio_io_start_read_smgr（） - 知道哪个SMGR实现在使用中，因此也不是什么文件/偏移WE＆＃39;重新处理SMGR和GT; MD-＆GT; FD实际上＆＃ 34;准备＆＃34; IO（即找出文件/偏移量）。这目前如下所示：</p><p>     Once this reaches the fd.c layer the new FileStartRead() function calls pgaio_io_prep_read() on the IO - but doesn&#39;t need to know anything about weird higher level stuff like relfilenodes.</p><p>     一旦这达到了FD.c层，新的filestartread（）函数调用IO上的pgaio_io_prep_read（） - 但不需要了解奇怪的更高级别的东西，如Relfilenodes。</p><p>  I&#39;m not sure this is the right design - but it seems a lot better than what I had earlier...</p><p>  我不确定这是正确的设计 - 但似乎比我早些时候更好...... </p><p>   Shared callbacks, which can be invoked by any backend (normally the issuing backend / the AIO workers, but can be other backends if they are waiting for the IO to complete). For operations on shared resources (e.g. shared buffer reads/writes, or WAL writes) these shared callback needs to transition the state of the object the IO is being done for to completion. E.g. for a shared buffer read that means setting BM_VALID / unsetting BM_IO_IN_PROGRESS.</p><p>共享回调，可以通过任何后端调用（通常是发布后端/ aio工人，但如果他们正在等待IO完成），则可以是其他后端）。对于共享资源的操作（例如，共享缓冲区读/写入或WAL WRITE）这些共享回调需要转换IO正在完成的对象的状态。例如。对于共享缓冲区读取，这意味着设置bm_valid /未设置bm_io_in_progress。</p><p> The main reason these callbacks exist is that they make it safe for a backend to issue non-blocking IO on buffers (see the deadlock section above). As any blocked backend can cause the IO to complete, the deadlock danger is gone.</p><p> 这些回调存在的主要原因是，它们使其安全的后端在缓冲区上发出非阻塞IO（请参阅上面的死锁部分）。由于任何阻塞后端可能导致IO完成，死锁危险消失了。</p><p> Local callbacks, one of which the issuer of an IO can associate with the IO. These can be used to issue further readahead. I initially did not have these, but I found it hard to have a controllable numbers of IO in flight. They are currently mainly used for error handling (e.g. erroring out when XLogFileInit() cannot create the file due to ENOSPC), and to issue more IO (e.g. readahead for heapam).</p><p> 本地回调，其中IO的发行者可以与IO关联。这些可用于发出进一步的readahead。我最初没有这些，但我发现很难在飞行中拥有可控的IO数量。它们目前主要用于错误处理（例如，当XLogfileInit（）由于ENSPC而创建文件时出错），并发出更多IO（例如，ReadAhead for Heapam）。</p><p>   Currently the patch series converts a number of subsystems to AIO. They are of very varying quality. I mainly did the conversions that I considered either be of interest architecturally, or that caused a fair bit of pain due to slowness (e.g. VACUUMing without AIO is no fun at all when using DIO). Some also for fun ;)</p><p>   目前，补丁系列将许多子系统转换为AIO。它们具有非常不同的质量。我主要是在建筑上被认为是兴趣的转换，或者由于缓慢而导致痛苦的公平疼痛（例如，使用DIO时，没有AIO没有乐趣）。有些人也有趣;）</p><p> Most conversions are fairly simple. E.g. heap scans, checkpointer, bgwriter, VACUUM are all not too complicated.</p><p> 大多数转换都很简单。例如。堆扫描，检查点，BGWriter，真空是不是太复杂。</p><p>  1) Asynchronous, concurrent, WAL writes. This is important because we right  now are very bottlenecked by IO latency, because there effectively only  ever is one WAL IO in flight at the same time. Even though in many  situations it is possible to issue a WAL write, have one [set of] backends  wait for that write as its completions satisfies their XLogFlush() needs,  but concurrently already issue the next WAL write(s) that other backends  need.</p><p>  1）异步，并发，WAL写入。这很重要，因为我们现在的IO延迟是非常典型的，因为同时有效地只有一个沃尔IO。即使在许多情况下，有可能发出WAL写入，有一个[SET]后端等待在其完成满足其XLogflush（）需要时，但同时已经发出了其他后端的下一个WAL WRITE 。</p><p>  2) Asynchronous buffer replacement. Even with buffered IO we experience a lot  of pain when ringbuffers need to write out data (VACUUM!). But with DIO the  issue gets a lot worse - the kernel can&#39;t hide the write latency from us  anymore. This change makes each backend asynchronously clean out buffers  that it will need soon. When a ringbuffer is is use this means cleaning out  buffers in the ringbuffer, when not, performing the clock sweep and cleaning  out victim buffers. Due to 1) the XLogFlush() can also be done  asynchronously.</p><p>  2）异步缓冲区更换。即使使用缓冲IO，我们会在戒指软骨写出数据（真空！）时经历了很多痛苦。但随着DIO这个问题变得更糟 - 内核可以＆＃39; t隐藏了我们的写入延迟了。此更改使每个后端异步清除它需要即将需要的缓冲区。当铃声被使用时，这意味着在没有，执行时钟扫描和清理受害者缓冲区时清理铃声中的缓冲器。由于1）XLogFlush（）也可以异步完成。 </p><p>   There are two new views: pg_stat_aios showing AIOs that are currently in-progress, pg_stat_aio_backends showing per-backend statistics about AIO.</p><p>有两个新视图：PG_STAT_AIOS显示当前正在进行的AIO，PG_STAT_AIO_BACKENS显示关于AIO的每个后端统计信息。</p><p>   I was not planning to attach all the patches on the way to AIO - it&#39;s too many right now... I hope I can reduce the size of the series iteratively into easier to look at chunks.</p><p>   我不打算将所有补丁附加到Aio  - 它＆＃39;现在太多了......我希望我能够迭代地减少系列的大小，更容易看块。</p><p>  This is worth an email on its own, and it&#39;s pretty late here already and I want to rerun benchmarks before posting more numbers. So here are just a few that I could run before falling asleep.</p><p>  这是一个值得一封电子邮件，它已经很晚了，我已经很晚了，我想在发布更多数字之前重新运行基准。所以这里只是我可以在睡着之前跑的几个。</p><p> 1) 60s of parallel COPY BINARY of a 89MB into separate tables (s_b = 96GB):</p><p> 1）60S并将89MB的并联复制二进制单独的表（S_B = 96GB）：</p><p> slow NVMe SSD branch	 dio clients tps/stddev	checkpoint write time master	 n 8		3.0/2296 ms	4.1s / 379647 buffers = 723MiB/s aio	 n 8		3.8/1985 ms	11.5s / 1028669 buffers = 698MiB/ aio	 y 8		4.7/204 ms	10.0s / 1164933 buffers = 910MiB/s</p><p> 慢性NVME SSD分支DIO客户端TPS / STDDEV检查点写入时间MAST N 8 3.0 / 2296 MS 4.1S / 379647缓冲器= 723MIB / S AIO N 8 3.8 / 1985 MS 11.5S / 1028669缓冲区= 698MIB / AIO Y 8 4.7 / 204 MS 10.0s / 1164933缓冲率= 910mib / s</p><p> raid of 2 fast NVMe SSDs (on pcie3): branch	 dio clients tps/stddev	checkpoint write time master	 n 8		9.7/62 ms	7.6s / 1206376 buffers = 1240MiB/s aio	 n 8		11.4/82 ms	14.3s / 2838129 buffers = 1550MiB/s aio	 y 8		18.1/56 ms	8.9s / 4486170 buffers = 3938MiB/s</p><p> 突击胜点2个快速的NVME SSD（PCIE3）：分支机构客户端TPS / STDDEV检查点写入时间Master N 8 9.7 / 62 MS 7.6s / 1206376缓冲器= 1240mib / s aio n 8 11.4 / 82 ms 14.3s / 2838129缓冲液= 1550mib / s aio y 8 18.1 / 56 ms 8.9s / 4486170缓冲液= 3938mib / s</p><p>   pg_prewarm(62GB, read) branch	 dio	time		bw master	 n	17.4s		3626MiB/s aio	 n	10.3s		6126MiB/s (higher cpu usage) aio	 y	9.8s		6438MiB/s</p><p>   PG_PREWARM（62GB，读取）分支DIO时间BW Master N 17.4s 3626MIB / S AIO N 10.3S 6126MIB / S（更高CPU使用）Aio Y 9.8s 6438Mib / s </p><p> pg_prewarm(62GB, buffer) branch	 dio	time		bw master	 n	38.3s		1647MiB/s aio	 n	13.6s		4639MiB/s (higher cpu usage) aio	 y	10.7s		5897MiB/s</p><p>PG_PREWARM（62GB，缓冲区）分支DIO时间BW Master N 38.3s 1647MIB / S AIO N 13.6S 4639MIB / S（更高CPU使用率）Aio Y 10.7s 5897Mib / s</p><p>   branch	 dio	 max_parallel	time master	 n	 0			40.5s master	 n	 1			22.6s master	 n	 2			16.4s master	 n	 4			10.9s master	 n	 8			9.3s</p><p>   Branch DIO MAX_POLLEAL时间MASTER N 0 40.5S MASTER N 1 22.6S MASTER N 2 16.4S Master N 4 10.9s Master N 8 9.3s</p><p> aio	 y	 0			33.1s aio	 y	 1			17.2s aio	 y	 2			11.8s aio	 y	 4			9.0s aio	 y	 8			9.2s</p><p> aio y 0 33.1s aio y 1 17.2s aio y 2 11.8s aio y 4 9.0s aio y 8 9.2s</p><p> On local SSDs there&#39;s some, but not a huge performance advantage in most transactional r/w workloads. But on cloud storage - which has a lot higher latency - AIO can yield huge advantages. I&#39;ve seen over 4x.</p><p> 在本地SSD上有一些，但在大多数交易R / W工作负载中，但不是巨大的性能优势。但是在云存储 - 延迟高度延迟 -  AIO可以产生巨大的优势。我看到了4倍。</p><p> There&#39;s definitely also cases where AIO currently hurts - most of those I just didn&#39;t get aroung to address.</p><p> 在那里＆＃39;也肯定是Aio目前伤害的情况 - 大多数我刚刚没有的人和＃39; T获得aroung到地址。</p><p> There&#39;s a lot more cases in which DIO currently hurts - mostly because the necessary smarts haven&#39;t yet been added.</p><p> 在那里有很多案例，其中dio目前伤害 - 主要是因为所需的智慧哈登＆＃39; t尚未添加。</p><p>  I plan to send separate emails about smaller chunks of this seperately - the whole topic is just too big. In particular I plan to send something around buffer locking / state management - it&#39;s a one of the core issues around this imo.</p><p>  我计划在这个单独的电子邮件中发送关于较小块的单独电子邮件 - 整个主题太大了。特别是我计划在缓冲区锁定/状态管理周围发送某些东西 - 它＆＃39;是这个imo周围的核心问题之一。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.postgresql.org/message-id/20210223100344.llw5an2aklengrmn%40alap3.anarazel.de">https://www.postgresql.org/message-id/20210223100344.llw5an2aklengrmn%40alap3.anarazel.de</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/io/">#io</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/direct/">#direct</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>