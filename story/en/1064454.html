<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>通过数据压缩进行文本分类 Text Classification by Data Compression</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Text Classification by Data Compression<br/>通过数据压缩进行文本分类 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-09 08:35:46</div><div class="page_narrow text-break page_content"><p>Last night I felt like reading   Artificial Intelligence: A Modern Approach. I stumbled on something fun in the natural language processing chapter. The section I was reading dealt with classifying text. The idea of the particular subsection I was reading was to classify documents by using a  compression algorithm. This is such a left field idea, and yet it does make sense when you think about it. To quote the book:</p><p>昨晚我觉得像阅读人工智能：一种现代方法。我偶然发现了自然语言处理章节中的一些乐趣。我正在阅读分类文本的部分。我正在阅读的特定小节的想法是通过使用压缩算法对文档进行分类。这是一个如此左上的领域的想法，但是当你想到它时它确实有道理。引用这本书：</p><p> In effect, compression algorithms are creating a language model. The  LZW algorithm in particular directly models a maximum-entropy probability distribution.</p><p> 实际上，压缩算法正在创建语言模型。 LZW算法特别直接模拟最大熵概率分布。</p><p> In other words, a compression algorithm has some knowledge of the distribution of words in a corpus. It can thus be used to classify documents. The learning algorithm is quite straightforward:</p><p> 换句话说，压缩算法在语料库中有一些了解单词的分布。因此，它可以用于对文档进行分类。学习算法非常简单：</p><p> Build a single document per label by concatenating the texts that belong to that label.</p><p> 通过连接属于该标签的文本来构建每个标签的单个文档。</p><p>   The idea is that if a document is similar to the training texts associated with a particular label, they will share patterns that will get exploited by the compression algorithm. Ideally, the size increase of compressing the training texts with the new text should be correlated with the similarity between the text and the training texts. The smaller the increase, the more likely the label should be assigned.</p><p>   这个想法是，如果文档类似于与特定标签关联的培训文本类似，则它们将共享将被压缩算法利用的模式。理想情况下，使用新文本压缩训练文本的大小增加应与文本和培训文本之间的相似性相关。增加越小，应将标签越可能。</p><p> I think this is an elegant idea. It’s not sophisticated, and I don’t expect it to perform better than a plain and simple logistic regression. Moreover, it’s expensive because the training texts of each label have to be recompressed for each test document. Still, I found the idea intriguing and decided to implement it in Python.</p><p> 我认为这是一个优雅的想法。它不是复杂的，我不希望它比平原和简单的逻辑回归更好。此外，它很昂贵，因为必须为每个测试文件重新压下每个标签的训练文本。尽管如此，我发现了这个想法有趣并决定在Python中实现它。</p><p> Here I’ll use the  newsgroup20 dataset from scikit-learn. I’m using the same four categories they use in their  user guide to have something to compare against.</p><p> 在这里，我将使用scikit-rement的新闻组20个数据集。我正在使用与他们在其用户指南中使用的相同的四个类别来进行比较。 </p><p>  The first step is to concatenate the texts that belong to each of the four categories. I’m adding a space before each text so that the last word isn’t glued with the first word of the next text.</p><p>第一步是连接属于四个类别中的每一个的文本。我在每个文本之前添加一个空格，以便最后一个单词与下一个文本的第一个单词粘合。</p><p> from  collections  import  defaultdict label_texts  =  defaultdict ( str ) for  i ,  text  in  enumerate ( train [ &#39;data&#39; ]):  label  =  train [ &#39;target_names&#39; ][ train [ &#39;target&#39; ][ i ]]  label_texts [ label ]  +=  &#39; &#39;  +  text . lower ()</p><p> 从集合导入defaultdict label_texts = defaultdict（str），枚举中的文本（火车[＆＃39; data＆＃39;]）：label =火车[＆＃39; target_names＆＃39; ] [训练[＆＃39;目标＆＃39; ] [i]] label_texts [标签] + =＆＃39; ＆＃39; +文本。降低 （）</p><p> The next step is to compress each of these big texts and measure the size of the compressed result. It’s quite easy to do this in Python. Indeed, Python provides high-level functions that compress a sequence of bytes into a smaller sequence of bytes. The  len method gives us the number of bytes in the sequence. I picked   gzip at random from the compression methods listed  here. Each of these provides an easy-to-use   compress method.</p><p> 下一步是压缩这些大文本中的每一个并测量压缩结果的大小。在Python中这样做是很容易的。实际上，Python提供了将一系列字节序列压缩成较小的字节序列。 LEN方法为我们提供序列中的字节数。我从这里列出的压缩方法随机挑选GZIP。这些中的每一个都提供了一种易于使用的压缩方法。</p><p>   That’s all there is to the training phase. The training texts have to be kept in memory because they have to be used for the prediction phase. Let’s say we’re given a list of unlabeled texts. The idea for each text is to concatenate it with each training text, compress the result, and then measure the size of the compressed result.</p><p>   这就是培训阶段的全部。培训文本必须保持在记忆中，因为它们必须用于预测阶段。让我们说我们有一个未标记的文本列表。每个文本的想法是使用每个训练文本连接它，压缩结果，然后测量压缩结果的大小。</p><p> When that’s done, we just need to compare the obtained sizes with the original and return the label for which the size increased the least. Note that there is no notion of probability. There might be some weird way to cook up some probabilities, but they wouldn’t be  calibrated.</p><p> 完成后，我们只需要使用原件比较所获得的大小并返回最少尺寸的标签。请注意，没有概率的概念。烹饪一些概率可能会有一些奇怪的方法，但它们不会被校准。</p><p> test  =  fetch_20newsgroups ( subset = &#39;test&#39; ,  categories = categories ) predictions  =  [] for  text  in  test [ &#39;data&#39; ]:  sizes  =  {  label :  len ( METHOD . compress ( f &#39;{label_text} {text.lower()}&#39; . encode ()))  for  label ,  label_text  in  label_texts . items ()  }  predicted_label  =  min (  sizes ,  key = lambda  label :  sizes [ label ]  -  original_sizes [ label ]  )  predictions . append ( predicted_label )</p><p> test = fetch_20newsgroups（子集=＆＃39;测试＆＃39;，类别=类别）预测= []用于测试中的文本[＆＃39;数据＆＃39; ]：sizes = {label：len（方法。压缩（f＆＃39; {label_text} {text.lower（）＆＃39;。编码（））标签，label_text中的label_text。项目（）} predicted_label = min（大小，key = lambda标签：大小[标签]  -  Oricalind_size [标签]）预测。附加（预测_Label）</p><p> So how well does this fair? Well, it takes over 5 minutes to run on my laptop for only 1,353 test cases. That’s 0.2 seconds per document, which is rather slow! Here’s the classification report:</p><p> 这展会有多好？好吧，只需要5分钟的时间才能在我的笔记本电脑上运行只有1,353个测试用例。每份文件是0.2秒，这相当慢！以下是分类报告： </p><p>  precision recall f1-score support alt.atheism 0.678 0.680 0.679 319 comp.graphics 0.784 0.897 0.837 389 sci.space 0.878 0.746 0.807 394talk.religion.misc 0.609 0.614 0.611 251 accuracy 0.749 1353 macro avg 0.737 0.734 0.733 1353 weighted avg 0.754 0.749 0.749 1353</p><p>精密召回F1-Score支持Alt.atheism 0.678 0.680 0.679 319 Comp.Graphics 0.784 0.897 0.837 389 SCI.Space 0.878 0.746 0.807 394Talk.religion.misc 0.609 0.614 0.611 251精度0.74910733 1353加权平均值0.754 0.749 0.749 1353</p><p> Is that good? No, not really. A multinomial Naive Bayes classifier achieves a macro F1-score of 0.88 with the same categories and train/test split. It’s also dramatically faster. Is there anything we can do? We can try other compression algorithms. Here are the results for   zlib, which was a bit faster and took just over 4 minutes to run:</p><p> 那是好吗？不，不是真的。多项式朴素贝叶斯分类器达到0.88的宏F1分，具有相同的类别和火车/测试分裂。它也很快更快。我们可以做些什么吗？我们可以尝试其他压缩算法。以下是ZLIB的结果，这有点速度迅速，只需4分钟即可运行：</p><p> precision recall f1-score support alt.atheism 0.656 0.687 0.671 319 comp.graphics 0.782 0.902 0.838 389 sci.space 0.880 0.744 0.806 394talk.religion.misc 0.612 0.578 0.594 251 accuracy 0.745 1353 macro avg 0.732 0.728 0.727 1353 weighted avg 0.749 0.745 0.744 1353</p><p> 精密召回F1  - 分数支持Alt.atheism 0.656 0.687 0.671 319 Comp.Graphics 0.782 0.902 0.838 389 SCI.Space 0.880 0.744 0.806 394Talk.religion.misc 0.612 0.578 0.594 251精度0.745 1353宏AVG 0.732 0.728 0.727 1353加权公平0.744 0.744 1353</p><p>  precision recall f1-score support alt.atheism 0.648 0.110 0.188 319 comp.graphics 0.732 0.584 0.649 389 sci.space 0.923 0.305 0.458 394talk.religion.misc 0.271 0.928 0.420 251 accuracy 0.455 1353 macro avg 0.644 0.482 0.429 1353 weighted avg 0.682 0.455 0.442 1353</p><p>  精密召回F1-Score支持Alt.atheism 0.648 0.111139 Comp.Graphics 0.732 0.584 0.649 389 SCI.Space 0.923 0.305 0.458 394Talk.religion.misc 0.271 0.928 0.420 251精度0.455 1353宏AVG 0.644 0.482 0.429 1353加权AVG 0.682 0.455 0.455 0.455 0.455 0.442 1353</p><p> The performance is awful in comparison. Also, it took more than 7 minutes to run. Now how about   lzma?</p><p> 相比之下，表现很糟糕。此外，跑步超过7分钟。现在LZMA怎么样？</p><p> precision recall f1-score support alt.atheism 0.851 0.875 0.862 319 comp.graphics 0.923 0.959 0.941 389 sci.space 0.927 0.934 0.930 394talk.religion.misc 0.866 0.773 0.817 251 accuracy 0.897 1353 macro avg 0.892 0.885 0.888 1353 weighted avg 0.897 0.897 0.896 1353</p><p> 精密召回F1-Score支持Alt.atheism 0.851 0.875 0.862 319 Comp.Graphics 0.923 0.959 0.941 389 SCI.Space 0.927 0.934 0.930 394Talk.religion.misc 0.866 0.773 0.817 251精度0.897 1353宏观501353宏观50.892 0.885 0.8881353加权AVG 0.897 0.896 1353</p><p> The performance is surprisingly good! But this comes at a cost: it took 32 minutes to complete, which is ~1.4 seconds per document. Still, it’s very interesting to see this kind of result for an algorithm that wasn’t at all designed to classify documents. Well, the  lzma in fact implements the LZW algorithm that is mentioned in the  Artificial Intelligence: A Modern Approach book, so it’s less surprising that it does well!</p><p> 表现令人惊讶的是好！但这是成本的：完成32分钟才能完成，每份文件〜1.4秒。尽管如此，可以看到这种结果不符合所有旨在对文档进行分类的算法非常有趣。嗯，Lzma实际上实现了人工智能中提到的LZW算法：一种现代化的书籍，所以它不太令人惊讶地做得很好！ </p><p> There’s probably some fine-tuning that could be done to improve this kind of approach. However, it would most probably not be worth using such an approach because of the prohibitive computational cost. This should simply be seen as an interesting example of thinking outside the box. It also goes to show that statistical modelling and information theory are very much intertwined.</p><p>可能会有一些微调可以改善这种方法。 但是，由于禁止的计算成本，它可能是最值得的使用这种方法。 这应该只是被视为盒子外面思考的一个有趣的例子。 它还表明统计建模和信息理论非常交织。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://maxhalford.github.io/blog/text-classification-by-compression/">https://maxhalford.github.io/blog/text-classification-by-compression/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/标签/">#标签</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>