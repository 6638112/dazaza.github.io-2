<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>使HTAP数据库成为现实：我从PingCAP的VLDB文件中学到的东西 Making an HTAP Database a Reality: What I Learned from PingCAP's VLDB Paper</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Making an HTAP Database a Reality: What I Learned from PingCAP's VLDB Paper<br/>使HTAP数据库成为现实：我从PingCAP的VLDB文件中学到的东西 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-13 04:14:54</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/f97582974e0631ffe847e14d02e3eba1.jpg"><img src="http://img2.diglog.com/img/2020/12/f97582974e0631ffe847e14d02e3eba1.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Recently,  VLDB 2020 published  PingCAP&#39;s paper,  TiDB: A Raft-based HTAP Database. This is the first paper in the industry to describe the implementation of a distributed  Hybrid Transactional/Analytical Processing (HTAP) database. As a DBA who benefits greatly from  TiDB, an open-source, distributed SQL database, I&#39;m happy that VLDB recognized TiDB, and I&#39;m inspired by the PingCAP engineering team&#39;s novel ideas.</p><p>最近，VLDB 2020发布了PingCAP的论文TiDB：基于Raft的HTAP数据库。这是业界第一篇描述分布式混合事务/分析处理（HTAP）数据库的实现的论文。作为一个从开放式分布式SQL数据库TiDB中受益匪浅的DBA，我很高兴VLDB认可TiDB，并受到PingCAP工程团队的新颖思想的启发。</p><p> PingCAP&#39;s paper is not the typical theoretical research paper, proposing an idea that may never be implemented. Instead, it proves, clearly and pragmatically, that a distributed HTAP database is achievable. Database researchers can use this information to head more confidently in the right direction.</p><p> PingCAP的论文不是典型的理论研究论文，而是提出了一个永远无法实现的想法。相反，它清楚而实用地证明了可以实现分布式HTAP数据库。数据库研究人员可以使用此信息更自信地朝正确的方向前进。</p><p> In this article, I&#39;ll share with you my thoughts about TiDB&#39;s implementation of an HTAP database that provides strong data consistency and resource isolation, as well as my expectations of TiDB in the future.</p><p> 在本文中，我将与大家分享我对TiDB实现HTAP数据库的想法，该数据库提供了强大的数据一致性和资源隔离性，以及我对TiDB的期望。</p><p>  As you probably know, databases are divided into two types:  Online Transactional Processing (OLTP) and  Online Analytical Processing (OLAP). But have you ever wondered why?</p><p>  您可能知道，数据库分为两种类型：在线事务处理（OLTP）和在线分析处理（OLAP）。但是您是否想过为什么？</p><p> OLTP and OLAP describe two very different data processing methods, and, therefore, they have different database requirements.</p><p> OLTP和OLAP描述了两种非常不同的数据处理方法，因此它们具有不同的数据库要求。</p><p>  Years ago, databases made little distinction between OLTP and OLAP. Instead, one database processed both types of requests. However, as the data volume grew, it became difficult to process two types of workloads in a single database. Most significantly, the different workload types interfered with each other.</p><p>  几年前，数据库在OLTP和OLAP之间几乎没有区别。相反，一个数据库处理两种类型的请求。但是，随着数据量的增长，在单个数据库中处理两种类型的工作负载变得很困难。最重要的是，不同的工作负载类型会相互干扰。</p><p> Thus, to meet the special needs of OLAP workloads, people designed a separate database that only processed OLAP workloads. They exported data from OLTP databases to OLAP databases, and processed the OLAP workloads there. Separating the OLTP and OLAP workloads resolved the conflicts between the two workloads, but it also introduced external data replication. During the replication, it was hard to ensure that data was consistent and in real time.</p><p> 因此，为了满足OLAP工作负载的特殊需求，人们设计了一个单独的数据库，该数据库仅处理OLAP工作负载。他们将数据从OLTP数据库导出到OLAP数据库，并在那里处理OLAP工作负载。分离OLTP和OLAP工作负载可以解决两个工作负载之间的冲突，但同时也引入了外部数据复制。在复制期间，很难确保数据的一致性和实时性。 </p><p> PingCAP&#39;s paper proposes a new way to solve this problem:  the replication should take place inside the database, rather than outside of it.</p><p>PingCAP的论文提出了一种解决此问题的新方法：复制应在数据库内部而不是数据库外部进行。</p><p>  Because OLTP and OLAP are very different workloads, it is difficult to do both kinds of jobs in a single database. There are two general schemas:</p><p>  由于OLTP和OLAP是非常不同的工作负载，因此很难在单个数据库中完成两种工作。有两种通用模式：</p><p> Design a storage engine suitable for both OLTP and OLAP. In the storage engine, data is consistent and real time, but it&#39;s hard to make sure that two workloads do not get in the way of each other.</p><p> 设计适用于OLTP和OLAP的存储引擎。在存储引擎中，数据是一致且实时的，但是很难确保两个工作负载不会互相干扰。</p><p> Build two sets of storage engines in one database. Each storage engine would handle one type of workload, so OLTP and OLAP don&#39;t affect each other. However, since data is replicated between two engines, it might be challenging to achieve strong data consistency and resource isolation at the same time.</p><p> 在一个数据库中构建两组存储引擎。每个存储引擎将处理一种类型的工作负载，因此OLTP和OLAP不会相互影响。但是，由于数据是在两个引擎之间复制的，因此同时实现强大的数据一致性和资源隔离可能很困难。</p><p> TiDB chose the second method.  TiKV, the row-based storage engine, handles OLTP workloads, while  TiFlash, the columnar storage engine, handles OLAP workloads. But how do they provide strong consistency and resource isolation?</p><p> TiDB选择了第二种方法。 TiKV（基于行的存储引擎）处理OLTP工作负载，而TiFlash（列式存储引擎）处理OLAP工作负载。但是它们如何提供强大的一致性和资源隔离性？</p><p> For most distributed storage systems, strong consistency and resource isolation is an either-or question. You can&#39;t have both. But TiDB has an answer:  extend the  Raft consensus algorithm by adding a Learner role.</p><p> 对于大多数分布式存储系统而言，强一致性和资源隔离是一个任择的问题。您不能同时拥有两者。但是TiDB有一个答案：通过添加学习者角色来扩展Raft共识算法。</p><p>  In TiKV, the basic unit of data storage is the  Region, which represents a range of data (96 MB by default). Each Region has three replicas by default. Replicas of the same Region replicate data from Leader to Follower via the Raft consensus algorithm. This is synchronous replication.</p><p>  在TiKV中，数据存储的基本单位是Region，它表示数据范围（默认为96 MB）。默认情况下，每个区域都有三个副本。相同区域的副本通过Raft共识算法将数据从Leader复制到Follower。这是同步复制。 </p><p> Assuming that TiFlash is a Follower in the Raft group, the replication between TiKV and TiFlash is synchronous. When TiFlash&#39;s replication is slow, or when a TiFlash node goes down, a majority of nodes are less likely to successfully replicate data, thus affecting TiKV&#39;s availability. So TiFlash can&#39;t be a Follower, because it interferes with TiKV, and the resources are not isolated.</p><p>假设TiFlash是Raft组中的关注者，则TiKV和TiFlash之间的复制是同步的。如果TiFlash的复制速度很慢，或者TiFlash节点出现故障，则大多数节点不太可能成功复制数据，从而影响TiKV的可用性。因此，TiFlash不能成为关注者，因为它会干扰TiKV，并且资源不是隔离的。</p><p> To solve this problem, TiDB extends the Raft algorithm by adding a Learner role to it. The Learner replica only receives Raft logs asynchronously. It doesn&#39;t commit logs or participate in the Leader election. When the Learner replica is replicating data, the Followers&#39; and Leader&#39;s performance overhead is very low. When TiFLash&#39;s Learner replica receives data, TiFlash converts the row format tuples to column format data and stores it in the column store. In this way, data is both in row store and column store format at the same time.</p><p> 为了解决此问题，TiDB通过向其添加学习者角色来扩展Raft算法。学习者副本仅异步接收Raft日志。它不会提交日志或参加“领导者”选举。当学习者副本正在复制数据时，关注者＆＃39;并且Leader的性能开销非常低。当TiFLash的Learner副本接收数据时，TiFlash会将行格式元组转换为列格式数据，并将其存储在列存储中。这样，数据同时处于行存储和列存储格式。</p><p> However, if TiFlash&#39;s Learner replica asynchronously receives logs from the Raft group, how does it ensure the data is strongly consistent?</p><p> 但是，如果TiFlash的Learner副本异步地从Raft组接收日志，那么如何确保数据高度一致？</p><p> TiFlash guarantees strong consistency when the application reads data from TiFlash. Similar to Raft&#39;s follower read mechanism, the Learner replica provides snapshot isolation, so we can read data from TiFlash using a specified timestamp. When TiFlash gets a read request, the Learner replica sends a  ReadIndex request to its Leader. According to the received  ReadIndex, the Leader holds back the request until the corresponding Raft log is replicated to the Learner, and then filters the required data by the specified timestamp. This is how TiFlash can provide strongly consistent data.</p><p> 当应用程序从TiFlash读取数据时，TiFlash保证了强大的一致性。与Raft的追随者读取机制类似，Learner副本提供快照隔离，因此我们可以使用指定的时间戳从TiFlash读取数据。当TiFlash收到读取请求时，学习者副本将ReadIndex请求发送到其领导者。根据收到的ReadIndex，Leader保留请求，直到将相应的Raft日志复制到学习者，然后按指定的时间戳过滤所需的数据。这就是TiFlash可以提供高度一致的数据的方式。</p><p> When the application reads data from TiFlash, TiFlash&#39;s Learner replica only needs to perform a  ReadIndex operation with TiKV&#39;s Leader replica. This operation imposes very little burden on TiKV. According to the paper, when TiDB processed both OLTP and OLAP workloads, OLAP throughput decreased by less than 5%, and OLTP throughput decreased by only 10%.</p><p> 当应用程序从TiFlash读取数据时，TiFlash的Learner副本只需要对TiKV的Leader副本执行ReadIndex操作。此操作对TiKV的负担很小。根据该论文，当TiDB处理OLTP和OLAP工作负载时，OLAP吞吐量降低了不到5％，而OLTP吞吐量仅降低了10％。</p><p> Moreover, the paper documents an experiment in which the asynchronous data replication from TiKV to TiFlash produced a very low latency. In the case of a data volume of 10 warehouses, the latency was mostly within 100 ms, with the largest latency smaller than 300 ms. In the case of 100 warehouses, the latency was mostly within 500 ms, with the largest latency smaller than 1500 ms. Most importantly, the latency didn&#39;t affect data consistency. It only made TiFlash process requests a little slower.</p><p> 而且，该论文记录了一个实验，其中从TiKV到TiFlash的异步数据复制产生了非常低的延迟。对于10个仓库的数据量，延迟通常在100毫秒以内，最大延迟小于300毫秒。对于100个仓库，延迟主要在500毫秒以内，最大延迟小于1500毫秒。最重要的是，延迟不会影响数据的一致性。它只会使TiFlash处理请求的速度变慢。</p><p>  TiDB has two storage engines, TiKV for OLTP and TiFlash for OLAP, and both support strongly consistent data replication and provide the same snapshot isolation.</p><p>  TiDB有两个存储引擎，用于OLTP的TiKV和用于OLAP的TiFlash，它们都支持强一致性数据复制并提供相同的快照隔离。 </p><p> This is a huge bonus for optimizing queries at the computing layer. When the optimizer processes a request, it has three options: row scan ( TiKV), index scan ( TiKV), or column scan ( TiFlash). For a single request, the optimizer can apply different scans for different parts of data, which provides a lot of flexibility for optimization. The PingCAP paper also proved that  an OLAP request that uses both storage engines works better than a request that uses either of the two engines.</p><p>这对于在计算层优化查询是一个巨大的好处。当优化程序处理请求时，它具有三个选项：行扫描（TiKV），索引扫描（TiKV）或列扫描（TiFlash）。对于单个请求，优化器可以对数据的不同部分应用不同的扫描，这为优化提供了很大的灵活性。 PingCAP论文还证明，使用两个存储引擎的OLAP请求比使用两个引擎中的任何一个的请求都更好。</p><p> While older databases required users to divide their requests and send them to the appropriate database,  TiDB offers a different workflow: there&#39;s only one database for the user. The database analyzes the request and determines which storage engine to use.  It takes the trouble of dividing databases and requests out of the user&#39;s mind, which is a higher level of abstraction.</p><p> 尽管较旧的数据库要求用户划分请求并将其发送到适当的数据库，但TiDB提供了不同的工作流程：该用户只有一个数据库。数据库分析请求并确定要使用哪个存储引擎。它会麻烦地划分数据库和请求，而这是更高层次的抽象。</p><p>  Different architectural designs coexist in the current distributed database industry. One prominent design is decentralized architecture, like Cassandra or CockroachDB. But TiDB is not strictly decentralized, because it has the  Placement Driver (PD), a central scheduling manager.</p><p>  在当前的分布式数据库行业中，不同的体系结构设计共存。一种杰出的设计是去中心化架构，例如Cassandra或CockroachDB。但是TiDB并不是严格分散的，因为它具有中央调度管理器Placement Driver（PD）。</p><p> Decentralized architecture excels at fault tolerance, attack resistance, and collusion resistance. But since databases are deployed in reliable, internal networks, resisting attacks or collusion is not an issue. Fault tolerance, on the other hand, can also be handled in a centralized architecture. Therefore, decentralization is not required.</p><p> 分散式架构在容错性，抗攻击性和抗共谋性方面表现出色。但是，由于数据库部署在可靠的内部网络中，因此抵抗攻击或串通并不是问题。另一方面，容错也可以在集中式体系结构中进行处理。因此，不需要分散。</p><p> Compared to decentralized architecture,  centralized architecture is better at scheduling. HTAP databases are made for huge volumes of data. When the number of nodes and data volumes grow to an unprecedented extent, the database must be able to elastically scale. Whether a database has intelligent scheduling capabilities will become the key ingredient that determines its performance and stability. However, in the decentralized architecture, scheduling is difficult, because the scheduler can&#39;t detect everything happening in the cluster, nor can it easily coordinate the decision-making process between multiple nodes.</p><p> 与分散式架构相比，集中式架构更易于调度。 HTAP数据库用于处理大量数据。当节点数量和数据量增长到前所未有的程度时，数据库必须能够弹性伸缩。数据库是否具有智能调度功能将成为决定其性能和稳定性的关键因素。但是，在分散式架构中，调度很困难，因为调度程序无法检测到集群中发生的所有事情，也无法轻松地协调多个节点之间的决策过程。</p><p> So you see, a centralized scheduler—in this case, PD—has a unique strength. It has a global view and multi-node coordination for better scheduling. Therefore, for a distributed database, as long as the centralized scheduler doesn&#39;t become the system bottleneck, the benefits greatly outweigh the costs. In this paper, PingCAP also conducted strict performance testing to prove that PD, as a single point, will not limit the horizontal scalability of the whole system.</p><p> 如此看来，集中式调度程序（在本例中为PD）具有独特的优势。它具有全局视图和多节点协调功能，以实现更好的调度。因此，对于分布式数据库，只要集中式调度程序不会成为系统瓶颈，那么收益将大大超过成本。在本文中，PingCAP还进行了严格的性能测试，以证明PD作为单一点不会限制整个系统的水平可扩展性。</p><p>  In TiDB, the storage is completely detached from the computing. The storage layer has two engines, TiKV and TiFlash, and the computing layer also has two engines, SQL engine and  TiSpark (a thin layer for running Apache Spark). In the future, both layers can easily extend into other ecosystems. From a user&#39;s perspective, I hope PingCAP will expand TiDB from a database into a distributed storage ecosystem.</p><p>  在TiDB中，存储与计算完全分离。存储层具有两个引擎TiKV和TiFlash，计算层还具有两个引擎SQL引擎和TiSpark（用于运行Apache Spark的薄层）。将来，这两个层次都可以轻松扩展到其他生态系统。从用户的角度来看，我希望PingCAP将TiDB从数据库扩展到分布式存储生态系统。 </p><p> HTAP is highly efficient in a single TiDB cluster. It offers strong consistency and resource isolation and eliminates the process of replicating data between different databases. Engineers can write code without worrying about importing and exporting data.</p><p>HTAP在单个TiDB集群中非常高效。它提供了强大的一致性和资源隔离性，并消除了在不同数据库之间复制数据的过程。工程师可以编写代码而不必担心导入和导出数据。</p><p> However, when you have multiple TiDB clusters, things get a bit tricky. Though TiDB provides horizontal scalability, it doesn&#39;t support  multitenancy. Currently, due to application and maintenance requirements (such as backup and restore), a company is unlikely to put all its data in a single TiDB cluster. Thus, if the data required for OLAP requests is stored across multiple clusters, users still have to load data from multiple clusters into a database that processes OLAP requests—again, the troublesome process of importing and exporting data.</p><p> 但是，当您有多个TiDB集群时，情况会有些棘手。尽管TiDB提供了水平可伸缩性，但它不支持多租户。当前，由于应用程序和维护要求（例如备份和还原），公司不太可能将所有数据放在单个TiDB集群中。因此，如果OLAP请求所需的数据跨多个群集存储，则用户仍然必须将多个群集中的数据加载到处理OLAP请求的数据库中，这同样是导入和导出数据的麻烦过程。</p><p> One solution is to add a layer similar to  Google F1 on top of TiDB clusters. Under a unified F1 layer there are multiple TiDB clusters. Each cluster is a tenant completely isolated from the others. The F1 layer manages metadata, routes read and write requests, and processes OLAP requests across clusters.</p><p> 一种解决方案是在TiDB群集之上添加类似于Google F1的层。在统一的F1层下，有多个TiDB集群。每个群集都是一个与其他群集完全隔离的租户。 F1层管理元数据，路由读取和写入请求，并跨集群处理OLAP请求。</p><p> Another formula is adding tenant management in the storage layer. Each tenant corresponds to a group of storage nodes. In this architecture, the tenants are isolated in the storage layer, and the computing layer can process cross-tenant OLAP requests.</p><p> 另一个公式是在存储层中添加租户管理。每个租户对应于一组存储节点。在此体系结构中，租户隔离在存储层中，并且计算层可以处理跨租户OLAP请求。</p><p> In a word, this is a question about isolating resources in the storage layer and providing a unified view in the computing layer. I look forward to seeing how TiDB will follow up on this.</p><p> 简而言之，这是一个关于在存储层中隔离资源并在计算层中提供统一视图的问题。我期待看到TiDB将如何跟进此事。</p><p>  PingCAP&#39;s implementation of an HTAP database has elegantly solved a decades-long conflict: how to process two types of queries in a single database.  Their paper does more than lay out a theoretical case; it shows exactly how they implemented the database, and it backs up their claims with solid testing.</p><p>  HTAP数据库的PingCAP实现很好地解决了长达数十年的冲突：如何在单个数据库中处理两种类型的查询。他们的论文不仅提供了理论上的依据。它准确地显示了他们如何实现数据库，并通过可靠的测试来备份他们的声明。</p><p> As the first paper in the industry to describe the implementation of a distributed HTAP database, TiDB&#39;s paper proved that a distributed, Raft-based HTAP database is achievable. It may speed up the development and adoption of distributed HTAP databases. In this regard, it marks a milestone.</p><p> 作为行业中描述分布式HTAP数据库实现的第一篇论文，TiDB的论文证明了基于Raft的分布式HTAP数据库是可以实现的。它可以加快分布式HTAP数据库的开发和采用。在这方面，它是一个里程碑。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://pingcap.com/blog/making-htap-database-reality-what-i-learned-from-pingcap-vldb-paper">https://pingcap.com/blog/making-htap-database-reality-what-i-learned-from-pingcap-vldb-paper</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/database/">#database</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/htap/">#htap</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>