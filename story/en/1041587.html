<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>MuZero：无规则掌握围棋，象棋，将棋和Atari MuZero: Mastering Go, chess, shogi and Atari without rules</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">MuZero: Mastering Go, chess, shogi and Atari without rules<br/>MuZero：无规则掌握围棋，象棋，将棋和Atari </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-24 21:40:38</div><div class="page_narrow text-break page_content"><p>In 2016, we introduced  AlphaGo, the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor -  AlphaZero - learned from scratch to master Go, chess and shogi. Now, in  a paper in the journal Nature, we describe MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.</p><p>2016年，我们推出了AlphaGo，这是第一个在古代Go游戏中击败人类的人工智能（AI）程序。两年后，它的继任者-AlphaZero-从零开始学习了Go，国际象棋和将棋的掌握。现在，在《自然》杂志的一篇论文中，我们描述了MuZero，这是在追求通用算法方面迈出的重要一步。由于MuZero能够在未知环境中计划获胜策略，因此无需掌握规则即可掌握Go，棋，shogi和Atari的知识。</p><p> For many years, researchers have sought methods that can both learn a model that explains their environment, and can then use that model to plan the best course of action. Until now, most approaches have struggled to plan effectively in domains, such as Atari, where the rules or dynamics are typically unknown and complex.</p><p> 多年来，研究人员一直在寻找既可以学习解释其环境的模型，又可以使用该模型来计划最佳行动方案的方法。到目前为止，大多数方法都难以在有效地进行规划的领域（例如Atari），在这些领域中规则或动态特性通常是未知且复杂的。</p><p> MuZero, first introduced in a  preliminary paper in 2019, solves this problem by learning a model that focuses only on the most important aspects of the environment for planning. By combining this model with AlphaZero’s powerful lookahead tree search, MuZero set a new state of the art result on the Atari benchmark, while simultaneously matching the performance of AlphaZero in the classic planning challenges of Go, chess and shogi. In doing so, MuZero demonstrates a significant leap forward in the capabilities of reinforcement learning algorithms.</p><p> MuZero于2019年在初步论文中首次引入，通过学习仅关注计划环境中最重要方面的模型来解决此问题。通过将此模型与AlphaZero强大的前瞻性树搜索功能相结合，MuZero在Atari基准上树立了最新的技术水平，同时在Go，国际象棋和将棋的经典规划挑战中将AlphaZero的性能相匹配。这样，MuZero展示了强化学习算法功能的重大飞跃。</p><p>      The ability to plan is an important part of human intelligence, allowing us to solve problems and make decisions about the future. For example, if we see dark clouds forming, we might predict it will rain and decide to take an umbrella with us before we venture out. Humans learn this ability quickly and can generalise to new scenarios, a trait we would also like our algorithms to have.</p><p>      计划能力是人类智力的重要组成部分，它使我们能够解决问题并为未来做出决策。例如，如果我们看到乌云形成，我们可以预测会下雨，然后决定冒险出门。人类可以快速学习这种能力，并且可以将其推广到新的场景中，这也是我们希望算法具有的一个特征。</p><p> Researchers have tried to tackle this major challenge in AI by using two main approaches: lookahead search or model-based planning.</p><p> 研究人员已尝试通过两种主要方法来应对AI中的这一重大挑战：超前搜索或基于模型的计划。</p><p> Systems that use lookahead search, such as AlphaZero, have achieved remarkable success in classic games such as checkers, chess and poker, but rely on being given knowledge of their environment’s dynamics, such as the rules of the game or an accurate simulator. This makes it difficult to apply them to messy real world problems, which are typically complex and hard to distill into simple rules.</p><p> 使用先行搜索的系统（例如AlphaZero）在经典游戏（例如，跳棋，国际象棋和扑克）中取得了显著成功，但依赖于其环境动态知识（例如游戏规则或精确的模拟器）的掌握。这使得很难将它们应用于混乱的现实世界中的问题，这些问题通常很复杂并且很难提炼成简单的规则。</p><p> Model-based systems aim to address this issue by learning an accurate model of an environment’s dynamics, and then using it to plan. However, the complexity of modelling every aspect of an environment has meant these algorithms are unable to compete in visually rich domains, such as Atari.  Until now, the best results on Atari are from model-free systems, such as  DQN,  R2D2 and  Agent57. As the name suggests, model-free algorithms do not use a learned model and instead estimate what is the best action to take next.</p><p> 基于模型的系统旨在通过学习环境动态的精确模型，然后使用其进行规划来解决此问题。但是，对环境的各个方面进行建模的复杂性意味着这些算法无法在视觉丰富的领域（如Atari）中竞争。到目前为止，在Atari上最好的结果是来自无模型系统，例如DQN，R2D2和Agent57。顾名思义，无模型算法不使用学习的模型，而是估计下一步将采取的最佳措施。 </p><p> MuZero uses a different approach to overcome the limitations of previous approaches. Instead of trying to model the entire environment, MuZero just models aspects that are important to the agent’s decision-making process. After all, knowing an umbrella will keep you dry is more useful to know than modelling the pattern of raindrops in the air.</p><p>MuZero使用另一种方法来克服以前方法的局限性。 MuZero并没有尝试对整个环境建模，而只是对对代理商的决策过程至关重要的方面进行建模。毕竟，了解雨伞会使您保持干燥比建模空气中的雨滴模式更有用。</p><p>   These are all learned using a deep neural network and are all that is needed for MuZero to understand what happens when it takes a certain action and to plan accordingly.</p><p>   这些都是使用深度神经网络学习的，是MuZero理解采取特定行动时会发生什么并进行相应计划所需的全部。</p><p>            This approach comes with another major benefit: MuZero can repeatedly use its learned model to improve its planning, rather than collecting new data from the environment. For example, in tests on the Atari suite, this variant - known as MuZero Reanalyze - used the learned model 90% of the time to re-plan what should have been done in past episodes.</p><p>            这种方法的另一个主要优点是：MuZero可以反复使用其学习的模型来改进其计划，而不必从环境中收集新数据。例如，在Atari套件的测试中，这种变体-称为MuZero Reanalyze-90％的时间使用学习的模型来重新计划过去情节中应该做的事情。</p><p>   We chose four different domains to test MuZeros capabilities. Go, chess and shogi were used to assess its performance on challenging planning problems, while we used the Atari suite as a benchmark for more visually complex problems. In all cases, MuZero set a new state of the art for reinforcement learning algorithms, outperforming all prior algorithms on the Atari suite and matching the superhuman performance of AlphaZero on Go, chess and shogi.</p><p>   我们选择了四个不同的域来测试MuZeros的功能。 Go，国际象棋和将棋被用来评估其在挑战性计划问题上的表现，而我们使用Atari套件作为视觉上更为复杂的问题的基准。在任何情况下，MuZero都为强化学习算法设定了新的技术水平，其性能优于Atari套件上的所有先前算法，并与AlphaZero在围棋，象棋和将棋上的超人性能相匹配。</p><p>      We also tested how well MuZero can plan with its learned model in more detail. We started with the classic precision planning challenge in Go, where a single move can mean the difference between winning and losing. To confirm the intuition that planning more should lead to better results, we measured how much stronger a fully trained version of MuZero can become when given more time to plan for each move (see left hand graph below). The results showed that playing strength increases by more than 1000 Elo (a measure of a player&#39;s relative skill) as we increase the time per move from one-tenth of a second to 50 seconds. This is similar to the difference between a strong amateur player and the strongest professional player.</p><p>      我们还详细测试了MuZero可以利用其学习的模型进行计划的能力。我们从Go中的经典精度计划挑战开始，在此挑战中，单步行动可能意味着成功与失败之间的区别。为了证实直觉，更多的计划应该会带来更好的结果，我们测量了如果有更多的时间来计划每个举动，那么经过全面训练的MuZero版本可以变得更强大（请参见下面的左图）。结果表明，随着我们将每步动作的时间从十分之一秒增加到50秒，游戏强度会增加1000 Elo（衡量玩家的相对技能）。这类似于强大的业余玩家和最强的职业玩家之间的区别。</p><p>      To test whether planning also brings benefits throughout training, we ran a set of experiments on the Atari game Ms Pac-Man (right hand graph above) using separate trained instances of MuZero. Each one was allowed to consider a different number of planning simulations per move, ranging from five to 50. The results confirmed that increasing the amount of planning for each move allows MuZero to both learn faster and achieve better final performance.</p><p>      为了测试计划是否还会在整个培训过程中带来好处，我们在Atari游戏Ms Pac-Man上进行了一组实验（上面的右图），使用的是单独训练的MuZero实例。允许每个人考虑每个动作的不同数量的计划模拟，范围从5到50。结果证实，增加每个动作的计划数量可以使MuZero更快地学习并获得更好的最终性能。</p><p> Interestingly, when MuZero was only allowed to consider six or seven simulations per move - a number too small to cover all the available actions in Ms Pac-Man - it still achieved good performance. This suggests MuZero is able to generalise between actions and situations, and does not need to exhaustively search all possibilities to learn effectively.</p><p> 有趣的是，当MuZero仅允许每步进行6或7次模拟时-这个数字太小而无法覆盖Pac-Man女士的所有可用动作-它仍然取得了不错的性能。这表明MuZero能够在行动和情况之间进行概括，而无需详尽搜索所有可能的内容以有效学习。 </p><p>  MuZero’s ability to both learn a model of its environment and use it to successfully plan demonstrates a significant advance in reinforcement learning and the pursuit of general purpose algorithms. Its predecessor, AlphaZero, has already been applied to a range of complex problems in  chemistry,  quantum physics and beyond. The ideas behind MuZero&#39;s powerful learning and planning algorithms may pave the way towards tackling new challenges in robotics, industrial systems and other messy real-world environments where the “rules of the game” are not known.</p><p>MuZero学习环境模型并成功进行规划的能力证明了强化学习和追求通用算法方面的重大进步。 它的前身AlphaZero已被应用于化学，量子物理学等领域的一系列复杂问题。 MuZero强大的学习和计划算法背后的思想可能为应对机器人，工业系统和其他杂乱无章的现实世界环境下的新挑战铺平道路，而这些现实世界对“游戏规则”一无所知。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules">https://deepmind.com/blog/article/muzero-mastering-go-chess-shogi-and-atari-without-rules</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/围棋/">#围棋</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/mastering/">#mastering</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/muzero/">#muzero</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>