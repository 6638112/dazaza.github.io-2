<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>从头开始更好的KubernetesA better Kubernetes from the ground up</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">A better Kubernetes from the ground up<br/>从头开始更好的Kubernetes</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-29 17:37:52</div><div class="page_narrow text-break page_content"><p>Recently I had a chat with the excellent  Vallery Lancey, about Kubernetes. Specifically, what we would do differently if we built something new, from the ground up, with no regard for compatibility with Kubernetes. I found that conversation so stimulating that I feel the need to write things down, so here we are.</p><p>最近，我与优秀的Vallery Lancey进行了有关Kubernetes的聊天。具体来说，如果我们从头开始构建新的东西，而又不考虑与Kubernetes的兼容性，我们将采取不同的做法。我发现对话如此刺激，以至于我觉得有必要写下来，所以就在这里。</p><p>  This is not a fully formed design. Some of these things may not work at all, or require significant redesign. Each section is one random piece of the entire puzzle.</p><p>  这不是完整的设计。其中某些功能可能根本无法使用，或者需要进行大量的重新设计。每个部分都是整个拼图的一个随机部分。</p><p> These are not solely my ideas. Some I  think are original, but like many things in the Kubernetes community it’s the product of collective thinking. I know at least  Vallery and  Maisem Ali have influenced my thinking at one time or another, and I’m forgetting many more. If you like an idea, it was a group effort. If you hate it, it’s entirely mine.</p><p> 这些不仅仅是我的想法。我认为有些是原创的，但就像Kubernetes社区中的许多事情一样，这是集体思维的产物。我知道至少Vallery和Maisem Ali一次或一次地影响了我的思维，而我忘记了更多。如果您喜欢一个主意，那就是集体努力。如果您讨厌它，那完全是我的。</p><p> Some of these things are polarizing. I’m designing something that makes  me happy.</p><p> 其中一些事情是两极分化的。我正在设计让我开心的东西。</p><p>  My experience of Kubernetes comes from two very different places: authoring  MetalLB for bare metal clusters, and operating a large fleet of clusters-as-a-service in  GKE SRE. Both of these taught me that Kubernetes is extremely complex, and that most people who are trying to use it are not prepared for the sheer amount of work that lies between the marketing brochure and the system those brochures promise.</p><p>  我对Kubernetes的经验来自两个截然不同的地方：为裸机集群编写MetalLB，以及在GKE SRE中运行大型集群即服务。这两件事都告诉我，Kubernetes非常复杂，并且大多数尝试使用它的人都没有为市场宣传册和这些宣传册所承诺的系统之间的大量工作做好准备。</p><p> MetalLB taught me that it’s not possible to build robust software that integrates with Kubernetes. I think MetalLB makes a damn good go of it, but Kubernetes still makes it far too easy to construct broken configurations, and far too hard to debug them. GKE SRE taught me that even the foremost Kubernetes experts cannot safely operate Kubernetes at scale. (Although GKE SRE does a spectacular job with the tools they’re given.)</p><p> MetalLB告诉我，不可能构建与Kubernetes集成的强大软件。我认为MetalLB做得很好，但是Kubernetes仍然很容易构造损坏的配置，而调试它们也太困难了。 GKE SRE告诉我，即使是最重要的Kubernetes专家也无法安全地大规模操作Kubernetes。 （尽管GKE SRE使用提供的工具做得非常出色。）</p><p> Kubernetes is the C++ of orchestration software. Immensely powerful, includes all the features, looks deceptively simple, and  will hurt you repeatedly until you join its priesthood and devote your life to its mysteries. And even then, the matrix of possible ways to configure and deploy it is so large that you’re never on firm footing.</p><p> Kubernetes是编排软件的C ++。功能强大，具有所有功能，看上去看似简单，并且会一再伤害您，直到您加入其神职人员并将生命奉献给它的奥秘为止。即便如此，配置和部署它的可能方法的矩阵仍然很大，以至于您永远都不会站稳脚跟。</p><p> Continuing that analogy, my guide star is Go. If Kubernetes is C++, what would the Go of orchestration systems look like? Aggressively simple, opinionated, grown slowly and warily, and you can learn it in under a week and get on with what you were actually trying to accomplish.</p><p>继续类推，我的导游明星是Go。如果Kubernetes是C ++，那么编排系统Go会是什么样？进取心简单，固执己见，成长缓慢而谨慎，您可以在不到一周的时间里学到它，然后继续进行实际要完成的工作。</p><p> With that, let’s get going. Starting with Kubernetes, and with a license to completely and utterly break compatibility, what would I do?</p><p> 这样，我们开始吧。从Kubernetes开始，并获得完全彻底打破兼容性的许可，我该怎么办？</p><p>  In Kubernetes, pods are mostly (but not entirely) immutable after creation. If you want to change a pod, you don’t. Make a new one and delete the old one. This is unlike most other things in Kubernetes, which are mostly mutable and gracefully reconcile towards the new spec.</p><p>  在Kubernetes中，豆荚在创建后大部分（但不是全部）是不可变的。如果您要更改广告连播，则不需要。制作一个新的并删除旧的。这与Kubernetes中的大多数其他事物不同，Kubernetes中的大多数事物都是可变的，可以与新规范优雅地协调一致。</p><p> So, I’m going to make pods be not special. Make them entirely read-write, and reconcile them like you would any other object.</p><p> 因此，我将使豆荚不特别。使它们完全可读写，并像对待其他任何对象一样协调它们。</p><p> The immediately useful thing I get from that is in-place restarts. If scheduling constraints and resource allocations haven’t changed, guess what? SIGTERM runc, restart runc with different parameters, and you’re done. Now pods look like regular old systemd services, that can move between machines  if necessary.</p><p> 我从中得到的直接有用的东西是就地重启。如果计划约束和资源分配没有改变，您猜怎么着？ SIGTERM runc，使用不同的参数重新启动runc，您已完成。现在，pod看起来像常规的旧systemd服务，必要时可以在机器之间移动。</p><p> Note that this doesn’t require doing mutability at the runtime layer. If you change a pod definition, it’s still mostly fine to terminate the container and restart it with a new configuration. The pod is still holding onto the resource reservation that got it scheduled onto this machine, so conceptually it’s equivalent to  systemctl restart blah.service. You could try to be fancy and make some operations actually update in place at the runtime level as well, but don’t have to. The main benefit is decoupling scheduling, pod lifetime, and lifetime at the runtime layer.</p><p> 请注意，这不需要在运行时层进行可变性。如果您更改Pod定义，终止容器并使用新配置重新启动容器仍然可以。 Pod仍会保留将其安排在此计算机上的资源预留，因此从概念上讲，它等效于systemctl restart blah.service。您可以尝试花哨的方法，并在运行时级别上实际进行一些操作的更新，但不必这样做。主要好处是解耦调度，pod生存期和运行时层的生存期。</p><p>  Sticking at the pod layer for a bit longer: now that they’re mutable, the next obvious thing I want is rollbacks. For that, let’s keep old versions of pod definitions around, and make it trivial to “go back to version N”.</p><p>  在Pod层停留更长的时间：既然它们是可变的，那么我接下来想要做的很明显的事情就是回滚。为此，让我们保留Pod定义的旧版本，并使其“回到N版本”变得微不足道。</p><p> Now, a pod update looks like: write an updated definition of the pod, and it updates to match. Update broken? Write back version N-1, and you’re done.</p><p>现在，pod更新如下所示：编写pod的更新定义，并进行更新以匹配。更新坏了吗？回写版本N-1，您就完成了。</p><p> Bonus things you get from this: a diffable history of what happened to your cluster, without needing GitOps nonsense. By all means keep the GitOps nonsense if you want, it has benefits, but you can answer a basic “what changed?” question using only data in the cluster.</p><p> 您从中获得的好处是：无需担心GitOps，即可轻松了解集群发生的事情。如果需要的话，请一定不要说GitOps，它有好处，但是您可以回答一个基本的“什么变化？”问题仅使用群集中的数据。</p><p> This needs a bit more design. In particular, I want to separate out external changes (human submits a new pod) from mechanical changes (some internals of k8s alter a pod definition). I haven’t thought through how to encode both those histories and make both accessible to operators and automation. Maybe it could also be completely generic, wherein a “changer” identifies itself when submitting a new version, and you can then query for changes by or excluding particular changers (think similar to how label queries work at the minute). Again, more design needed there, I just know that I want versioned objects with an accessible history.</p><p> 这需要更多的设计。特别是，我想将外部更改（人类提交新的Pod）与机械更改（k8的某些内部更改Pod定义）分开。我还没有考虑过如何对这两种历史进行编码，并使操作员和自动化都可以访问它们。也许它也可能是完全通用的，其中“更改者”在提交新版本时会标识自己，然后您可以查询特定更改者（或排除特定更改者）以查找更改（类似于标签查询的工作原理）。同样，在那里需要更多的设计，我只知道我想要具有可访问历史记录的版本化对象。</p><p> We’ll need garbage collection eventually. That said, changes to single pods should delta-compress really well, so my default would be to just keep everything until it becomes a truly dumb amount of data, and figure something out at that point. Keeping everything also acts as a useful mild pressure to avoid “death by a thousand changes” in the rest of the system. Prefer to have fewer, more meaningful changes over a flurry of control loops each changing one field in pursuit of convergence.</p><p> 最终我们将需要垃圾回收。就是说，对单个Pod的更改应该很好地进行delta压缩，因此我的默认设置是仅保留所有内容，直到它变成真正愚蠢的数据量为止，然后在此基础上解决问题。保留所有内容也是一种有用的适度压力，可避免系统其余部分“因一千次更改而死亡”。最好在一系列控制回路中进行更少，更有意义的更改，每个控制回路都在追求收敛的同时改变一个领域。</p><p> Once we have this history, we can do some neat minor things too. For example, the node software could keep container images for the last N versions pinned to the machine, so that rollbacks are as fast as they can possibly be. With an accessible history, you can do this more precisely than “GC older than 30 days and hope”. Generalizing, all the orchestration software can use older versions as GC roots for various resources, to make rollbacks faster. Rollbacks being the primary way of ending outages, this is a very valuable thing to have.</p><p> 一旦有了这段历史，我们也可以做一些整洁的小事情。例如，节点软件可以将最新的N个版本的容器映像固定在机器上，以使回滚尽可能快。有了可访问的历史记录，您可以比“ GC已超过30天并希望”更精确地做到这一点。概括而言，所有编排软件都可以将旧版本用作各种资源的GC根目录，以加快回滚速度。回滚是结束中断的主要方式，这是非常有价值的事情。</p><p>  This is a short section to basically say that  Vallery knocked it out of the park with her  PinnedDeployment resource, which lets operators explicitly control a rollout by tracking 2 versions of the deployment state. It’s a deployment object designed by an SRE, with a crisp understanding of what SREs want in a deployment. I love it.</p><p>  这是一个简短的部分，基本上可以说Vallery用她的PinnedDeployment资源将其从公园中淘汰了，该资源使操作员可以通过跟踪两个部署状态版本来明确控制部署。这是由SRE设计的部署对象，对SRE在部署中需要什么有清晰的了解。我喜欢它。</p><p> This combines super well with the versioned, in-place pod updates above, and I really don’t have anything to add. It’s clearly how multi-pod things should work. There’s probably some tweaking required to adapt from the Kubernetes-constrained world to this new wonderful unconstrained universe, but the general design is perfect.</p><p> 这与上面的版本化就地Pod更新结合得非常好，我真的没有什么可添加的。显然，多脚架应该如何工作。要从Kubernetes受限的世界适应这一新的，不受限制的奇妙宇宙，可能需要进行一些调整，但是总体设计是完美的。</p><p>  The biggest issue I have with the “API machinery” bits of Kubernetes is the idea of orchestration as a loose choreography of independent control loops. On the surface, this seems like a nice idea: you have dozens of little control loops, each focused on doing one small thing. When combined in a cluster, they indirectly cooperate with each other to push the state forward and converge on the desired end state. So, what’s the problem?</p><p>我对Kubernetes的“ API机器”位所遇到的最大问题是编排的思想，即独立控制循环的松散编排。从表面上看，这似乎是一个好主意：您有数十个小控制循环，每个控制循环专注于做一件小事情。当组合成一个集群时，它们彼此间接协作以推动状态前进并收敛于所需的最终状态。所以有什么问题？</p><p> The problem is that it’s entirely impossible to debug when it goes wrong. A typical failure mode in Kubernetes is that you submit a change to the cluster, then repeatedly refresh waiting for stuff to converge. When it doesn’t… Well, you’re screwed. Kubernetes doesn’t know the difference between “the system has converged successfully” and “a control loop is wedged and is blocking everything else.” You can hope that the offending control loop posted some events to the object to help you, but by and large they don’t.</p><p> 问题是出现错误时完全不可能进行调试。 Kubernetes中典型的故障模式是您将更改提交给集群，然后重复刷新以等待聚合。如果还没有...那么，您就被搞砸了。 Kubernetes不知道“系统已成功收敛”和“控制环被束缚并阻止了其他所有事物”之间的区别。您可以希望有问题的控制循环向该对象发布一些事件以对您有所帮助，但总的来说它们并没有帮助。</p><p> At which point your only option is to cat the logs of every control loop that might be involved, looking for the one that was wedged. You can make this a bit faster if you have intimate knowledge of all the control loops and what each one does, because that lets you infer from the object’s current state which loop might be trying to run right now.</p><p> 此时，您唯一的选择是收集可能涉及的每个控制循环的日志，寻找被楔入的循环。如果您对所有控制循环以及每个控制循环都有深入的了解，则可以使其速度更快一些，因为这可以让您从对象的当前状态推断出哪个循环可能正在尝试立即运行。</p><p> The key thing to notice here is that the complexity has been shifted from the designer of the control loop to the cluster operator. It’s easy (though not trivial) to make a control loop that does a dinky little thing in isolation. But to operate a cluster with dozens of these control loops requires the operator to assimilate the behavior of all of them, their interactions with each other, and try to reason about an extremely loosely coupled system. This is a problem because you have to write and test the control loop once, but work with it and its bugs many more times. And yet, the bias is to simplify the thing you only do once.</p><p> 这里要注意的关键是，复杂度已经从控制循环的设计者转移到了集群运算符。制作一个可以独立执行小动作的控制循环很容易（虽然不容易）。但是，要使用具有许多这样的控制回路的集群来操作，就需要操作员吸收所有这些回路的行为，它们之间的相互作用，并尝试推断出一个松散耦合的系统。这是一个问题，因为您必须编写并测试一次控制循环，但是要处理它及其错误多次。但是，偏向是简化只执行一次的操作。</p><p> To fix this, I would look to systemd. It solves for a similar lifecycle problem: given a current state and a target, how do you get from A to B? The difference is that in systemd, the steps and their dependencies are made explicit. You  tell systemd that your unit is a required part of  multi-user.target (aka “normally-booted happy system”), that it must run after filesystems have been mounted, but before networking it brought up, and so forth. You can also depend on other concrete parts of the system, for example to say that your thing needs to run whenever sshd is running (sounds like a sidecar, right?).</p><p> 为了解决这个问题，我希望使用systemd。它解决了类似的生命周期问题：给定当前状态和目标，您如何从A变为B？区别在于在systemd中，步骤及其相关性是明确的。您告诉systemd，您的单元是multi-user.target（又名“正常启动的快乐系统”）的必需部分，它必须在挂载文件系统之后但在联网之前运行，依此类推。您还可以依赖系统的其他具体部分，例如说只要sshd运行，您的东西就需要运行（听起来像边车，对吗？）。</p><p> The net result of this is that systemd can tell you precisely what piece of the system malfunctioned, or is still working on its thing, or failed a precondition. It can also print you a graph of the system’s boot process, and analyze it for things like “what’s the long pole of bootup?”</p><p> 这样做的最终结果是，systemd可以准确地告诉您系统的哪一部分发生故障，或者仍在运行，或者失败了前提条件。它还可以为您打印系统启动过程的图形，并对其进行分析，例如“什么是启动的长杆？”</p><p> I want to steal all this wholesale, and plop it into my cluster orchestration system. It does need some adjusting to this new world, but roughly: control loops must declare their dependencies on other control loops, must produce structured logs such that I can trivially search for “all control loop activity regarding pod X”, and the orchestration system handles lifecycle events like systemd handles switching to a new target unit.</p><p> 我想偷走所有这些批发产品，然后将其放入我的集群编排系统中。它确实需要对这个新世界进行一些调整，但是粗略地说：控制循环必须声明它们对其他控制循环的依赖性，必须生成结构化日志，以便我可以轻松搜索“有关Pod X的所有控制循环活动”，并且编排系统可以处理诸如systemd之类的生命周期事件会处理向新目标单元的切换。</p><p> What does that look like in practice? Let’s focus on pod lifecycle. Probably we’ll define an abstract “running” target, which is where we want to end up - the pod has started and is happy. Working backwards, the container runtime will add a task that happens before “running”, to start the containers. But it should probably not run until storage systems have had a chance to set up networked mounts, so it’ll order itself after a “storage” target. Similarly for networking, container startup wants to happen after the “networking” target.</p><p>在实践中看起来像什么？让我们集中讨论pod的生命周期。可能我们将定义一个抽象的“运行中”目标，这是我们要结束的目标-吊舱已经开始并且很高兴。向后工作，容器运行时将添加一个在“运行”之前发生的任务，以启动容器。但它可能要到存储系统有机会设置网络安装后才能运行，因此它将在“存储”目标之后自行订购。同样对于网络，容器启动希望在“网络”目标之后发生。</p><p> Now your Ceph control loop schedules itself to run before the “storage” target, since it’s responsible for bringing up the storage. Other storage control loops do the same (local bind mount, NFS, …). Note this means their setups can run concurrently, because they all declare that they want to run before storage is considered ready, but don’t care if they run before or after other stuff. Or maybe they do care! Maybe you wrote a cool storage addon that does something amazing, but NFS mounting has to happen before you can execute. Cool, that’s fine, add a dependency on the  nfs-mounts step, and you’re done. Like systemd, we would have both ordering requirements and hard “I need this other thing to function at all” requirements, so you can have graceful optional ordering of steps.</p><p> 现在，您的Ceph控制循环将自己安排为在“存储”目标之前运行，因为它负责启动存储。其他存储控制循环也执行相同的操作（本地绑定安装，NFS等）。请注意，这意味着它们的设置可以同时运行，因为它们都声明要在存储准备就绪之前运行，但是不在乎它们在其他运行之前还是之后运行。也许他们确实在乎！也许您编写了一个很棒的存储插件，它的功能令人赞叹，但是必须先进行NFS挂载，然后才能执行。很好，很好，在nfs-mounts步骤中添加一个依赖项，就可以完成了。像systemd一样，我们既有订购要求，又有严格的“我还需要其他东西才能完全发挥作用”的要求，因此您可以轻松地进行可选的步骤订购。</p><p> (I’m simplifying a little here and assuming the steps aren’t too intertwined. This generalizes to a more complex flow if needed - but see sections further down about working hard to avoid the need for very complex flows in the first place.)</p><p> （我在这里稍微简化一下，并假设步骤不是很纠结。如果需要的话，这可以概括为更复杂的流程-但请参阅下文进一步探讨如何努力避免首先需要非常复杂的流程。）</p><p> With this in place, your orchestrator can help you answer “why isn’t my pod starting?” You can simply dump the work graph for the pod, and see which steps have completed, which have failed, which are still running. NFS mounting has been going for 5 minutes? I’m guessing the server’s down and the control loop is missing a timeout. Going back to the observation about the matrix of possible configurations and states being immense: that can be okay,  if you provide the tools to debug it. Systemd allows you to add arbitrary anythings to the boot process, in any order, with any constraints. But I can still troubleshoot it when it goes wrong, because the constraints it does have combined with the tooling it offers let me quickly make sense of a particular machine from first principles.</p><p> 完成此操作后，协调器可以帮助您回答“为什么我的吊舱没有启动？”您可以简单地转储pod的工作图，并查看哪些步骤已完成，哪些步骤失败，哪些仍在运行。 NFS安装已经进行了5分钟？我猜测服务器已关闭，并且控制循环缺少超时。回到关于可能的配置和状态矩阵庞大的观察：如果您提供调试它的工具，那可以。 Systemd允许您以任何顺序，任何约束向引导过程添加任意内容。但是，即使出现错误，我仍然可以对其进行故障排除，因为它的限制条件与它提供的工具相结合，使我可以从最初的原理快速理解特定的机器。</p><p> Similar to the benefit systemd brings to system startup, this also lets you parallelize lifecycle operations as aggressively as they can be, but no more. And because the workflow graph is explicit, it’s extensible. Does your cluster have some company-specific step that happens for every pod, and must happen at a specific place in the lifecycle? Define a new intermediate target for that, make it depend on the right pre- and post-requisites, and hook your control loop on there. The orchestration system will ensure that your control loop always gets involved at exactly the right point in the lifecycle.</p><p> 与systemd给系统启动带来的好处类似，这还使您可以尽可能积极地并行化生命周期操作，但仅此而已。而且由于工作流程图是显式的，因此可以扩展。您的集群是否在每个Pod上都有特定于公司的步骤，并且必须在生命周期的特定位置进行？为此定义一个新的中间目标，使其取决于正确的前提条件和前提条件，然后将控制循环挂接到那里。编排系统将确保您的控制循环始终处于生命周期中的正确位置。</p><p> Note this also fixes the weirdness with things like Istio, where they have to hackily inject themselves into the human-provided definition in order to function. There’s no need for that! Insert the appropriate control loops into the lifecycle graph, and have it adjust things as needed on the inside. No need to muck with operator-provided objects, as long as you can express to the system where in the lifecycle you need to do stuff.</p><p> 请注意，这还解决了诸如Istio之类的怪异问题，在Istio中，它们必须将自己笨拙地注入到人类提供的定义中才能起作用。没必要！将适当的控制循环插入生命周期图中，并根据需要在内部进行调整。只要您可以向系统表示在生命周期中需要执行操作的位置，就无需考虑操作员提供的对象。</p><p> This section is both very long, and way too short. This is a very large departure from k8s’s API machinery, and so would require a  lot of new design work to flesh out. In particular, the major change is that control loops no longer simply observe all cluster state and race to do whatever, but have to wait to be called upon by the orchestrator for specific objects, when those objects reach the right point in their lifecycle. You  can bolt this onto k8s as it is now using annotations and programming conventions, but the crisp observability and debuggability benefits don’t fully materialize until you burn the existing thing to the ground.</p><p> 本节既长又短。这与k8s的API机制大相径庭，因此需要大量新的设计工作才能充实。特别是，主要的变化是控制循环不再简单地观察所有集群状态并竞相做任何事情，而是必须等待协调器为特定对象调用，当这些对象到达其生命周期中的正确点时。您现在可以使用注释和编程约定将其固定在k8s上，但是在将现有的东西烧成灰烬之前，清晰的可观察性和可调试性的好处并没有完全实现。</p><p> Interestingly, Kubernetes sort-of already has a prototypical implementation of some of these ideas: initializers and finalizers are effectively happens-before hooks for two lifecycle steps. It lets you hook your control loop onto two hardcoded “targets”. They split control loops into three buckets: initialization, “default,” and finalization. It’s the hardcoded beginnings of an explicit workflow graph. I’m arguing to push that to its logical conclusion.</p><p>有趣的是，Kubernetes排序已经实现了其中一些想法的原型实现：初始化器和终结器实际上是发生在两个生命周期步骤的钩子之前。它使您可以将控制循环挂接到两个硬编码的“目标”上。他们将控制循环分为三个部分：初始化，“默认”和终结。这是显式工作流程图的硬编码开始。我正在争论把它推到逻辑上的结论。</p><p>  A modest expansion of the previous section: make each field of an object owned explicitly by a particular control loop. That loop is the only one allowed to write to that field. If no owner is defined, the field is writable by the cluster operator, and nothing else. This is enforced by API machinery, not convention.</p><p>  上一部分的适度扩展：使对象的每个字段都由特定的控制循环显式拥有。该循环是唯一允许写入该字段的循环。如果未定义所有者，则该字段可被集群操作员写入，而不能写其他任何内容。这是由API机制（而非约定）强制执行的。</p><p> This is already  mostly the case, but the ownerships are implicit. This leads to two problems: if a field is wrong, it’s hard to figure out who’s responsible; and it’s easy to accidentally run “duelling controllers” that fight indefinitely over a field.</p><p> 这已经是大多数情况，但是所有权是隐式的。这导致两个问题：如果字段错误，则很难弄清谁负责。而且很容易意外地运行无限期在战场上作战的“决斗控制器”。</p><p> The latter is the bane of MetalLB’s existence, wherein it gets into fights with other load-balancer implementations. That should never happen. The orchestrator should have rejected MetalLB’s addition to the cluster, because LB-related fields would have two owners.</p><p> 后者是MetalLB存在的祸根，在其中它与其他负载平衡器实现方式发生了冲突。那永远不会发生。协调员应该拒绝MetalLB添加到集群中，因为与LB相关的字段将有两个所有者。</p><p> There probably needs to be an escape valve that lets you explicitly permit multiple ownership of fields, but I would start without it and see how far we get. Shared ownership is a code smell and/or a bug until proven otherwise.</p><p> 可能需要一个泄压阀，让您明确允许多个字段的拥有权，但是我将首先没有它，然后看看我们能走多远。共享所有权是代码气味和/或错误，除非另行证明。</p><p> If you also require explicit registration of what fields a control loop  reads (and strip out those it doesn’t - no cheating) this also lets you do exciting things like prove that the system converges (no loops of read-&gt;write-&gt;read), or at least reason about the long pole in your orchestration.</p><p> 如果您还需要显式注册控制循环读取的字段（并剔除那些不读取的字段-不作弊），这还使您可以做一些令人兴奋的事情，例如证明系统收敛（没有read-> write-> read的循环） ），或至少要考虑编排中的长杆。</p><p>  I’m intimately familiar with Kubernetes networking, and as such it’s the piece I most want to rip out wholesale. There are reasons for why it looks the way it does, and I don’t mean to say there’s not a place for k8s networking. But that place is not in  my orchestration system. This is going to be a long section, so strap in.</p><p>  我对Kubernetes联网非常熟悉，因此，这是我最想批发的部分。它看起来像是有原因的，我并不是要说没有k8s联网的地方。但是那个地方不在我的业务流程系统中。这将是一个很长的部分，所以请扎紧。</p><p> So, for starters, let’s rip out all k8s networking. Overlay networks, gone. Services, gone. CNI, gone. kube-proxy, gone. Network addons, gone.</p><p>因此，对于初学者来说，让我们淘汰所有k8s网络。覆盖网络不见了。服务不见了。 CNI，走了。 kube-proxy，走了。网络插件不见了。</p><p> (That last one is why this could never happen in k8s proper, by the way. By now there’s an ecosystem of companies selling network addons, and you’d better believe they’re not going to stand by and let me get rid of their reason for existing. The first priority of all ecosystems, in nature and software, is to ensure their continued existence. You can’t ask an ecosystem to evolve itself into extinction, you have to trigger extinction from the outside.)</p><p> （顺便说一句，这就是为什么这在k8s中永远不可能发生的原因。到目前为止，有一个由生态系统组成的公司在出售网络插件，您最好相信他们不会袖手旁观，让我摆脱他们的存在的理由。在性质和软件上，所有生态系统的首要任务是确保它们的持续存在。您不能要求生态系统自我灭绝，而必须从外部引发灭绝。）</p><p> Right, clean slate. We have containers, and they probably do need to talk to each other and the world. What do?</p><p> 对，干净的板岩。我们有容器，它们可能确实需要与彼此和世界进行对话。做什么？</p><p> Let’s give every pod an IPv6 address. Yes, only an IPv6 address for now. Where do they come from? Your LAN has a /64 already (if it doesn’t, get with the program, I’m not designing for the past here), so pluck IPs from there. You barely even need to do duplicate address detection, 2^64 is large enough that rolling random numbers will mostly just work. We’ll need a teensy bit of machinery on each node to make neighbour discovery work, so that machines can find where other pods are hosted, but that’s very easy to do and reason about: to the rest of the LAN, a pod appears to be on the machine that’s running it.</p><p> 让我们为每个吊舱提供一个IPv6地址。是的，目前只有一个IPv6地址。他们来自哪里？您的LAN已经有一个/ 64（如果没有，请与该程序一起使用，我不是在这里为过去设计的），因此请从那里获取IP。您甚至都不需要执行重复的地址检测，2 ^ 64足够大，以至于滚动随机数几乎可以正常工作。我们将需要在每个节点上使用少量机器来使邻居发现工作正常进行，以便机器可以找到其他Pod的托管位置，但这很容易做到，原因如下：在LAN的其余部分，pod似乎可以在运行它的机器上。</p><p> Or maybe we just make up a ULA, and do the routing on each node manually. It’s really easy to implement, and the addressing plan remains mostly “pick a random number and you’re done”. Maybe a tiny bit of subsetting so that node-to-node routes are more efficient, but this is all easy stuff.</p><p> 或者，也许我们只是组成一个ULA，然后手动在每个节点上进行路由。实施起来确实很容易，而且寻址方案基本上仍然是“选择一个随机数，您就完成了”。也许只需要进行一点点设置，即可使节点到节点的路由更有效，但这一切都很简单。</p><p> An annoyance is that clouds love to break basic networking primitives, so the IPAM portion will likely have to remain pluggable (within the workflow model from above), so that we can do things like explain to AWS how the traffic is meant to flow. And of course, using IPv6 will make it impossible to run this on GCP. Hahahaha.</p><p> 令人烦恼的是，云喜欢打破基本的网络原语，因此IPAM部分可能必须保持可插拔性（在上面的工作流模型中），以便我们可以做一些事情，例如向AWS解释流量的含义。当然，使用IPv6将使其无法在GCP上运行。哈哈哈哈</p><p> Anyway, there’s a couple of ways to skin this cat, but fundamentally, we’re going to use IPv6 and basic, boring routing between nodes. That takes care of pod&lt;&gt;pod connectivity in as close to zero config as we can get, because IPv6 is a large enough space that we can throw random numbers at the wall and come out on top.</p><p> 无论如何，有几种方法可以使这只猫变皮，但从根本上讲，我们将在节点之间使用IPv6和基本的无聊路由。由于IPv6足够大的空间，我们可以将随机数扔在墙上然后放在顶部，因此可以在尽可能接近零的配置中处理pod <> pod连接。</p><p> If you have more elaborate connectivity needs, you bolt those on as additional network interfaces and boring, predictable IPv6 routes. Need to secure node-to-node comms? Bring up wireguard tunnels, add routes to push node IPs through the wireguard tunnel, and you’re done. The orchestration system doesn’t need to know about any of this, other than probably adding a small control loop to node lifecycle, such that it doesn’t become ready until the tunnels are up.</p><p>如果您有更详尽的连接需求，则可以将其作为其他网络接口和无聊的可预测IPv6路由。需要保护节点到节点的通信吗？搭建Wireguard隧道，添加路由以通过Wireguard隧道推送节点IP，即可完成。编排系统不需要了解任何这些内容，除了可能在节点生命周期中添加一个小的控制环之外，这样直到隧道建立起来，它才准备就绪。</p><p> Okay, so we have pod&lt;&gt;pod connectivity, and pod&lt;&gt;internet connectivity, albeit IPv6-only. How do we get IPv4 into this?</p><p> 好的，所以我们有pod <> pod连接性和pod <>互联网连接性，尽管仅IPv6。我们如何将IPv4纳入其中？</p><p> First off, we decree that IPv4 is only for pod&lt;&gt;internet. Thou shalt use IPv6 within the cluster.</p><p> 首先，我们决定IPv4仅用于pod <> internet。您必须在群集中使用IPv6。</p><p> Given that constraint, we can do this a couple of ways. Trivially, we can have each node masquerade IPv4 traffic, and allocate out of some small rfc1918 space (the same space on all nodes) for pods. That lets them reach the IPv4 internet, but it’s all static per-node configuration that doesn’t need to be visible to the cluster at all. You could even entirely hide the IPv4 stuff from the control plane, it’s just an implementation detail of the per-machine runtime.</p><p> 在这种限制下，我们可以通过两种方法来做到这一点。简单地说，我们可以让每个节点伪装IPv4流量，并为Pod分配一些小的rfc1918空间（所有节点上的相同空间）。这样一来，他们就可以连接到IPv4互联网，但这完全是静态的每个节点的配置，根本不需要集群可见。您甚至可以从控制平面完全隐藏IPv4内容，这只是每台机器运行时的实现细节。</p><p> We could also have some fun with NAT64 and CLAT: make the entire network IPv6-only, but use CLAT to trick pods into thinking they have v4 connectivity. Within the pod, do 4-to-6 translation and send the traffic onwards to a NAT64 gateway. </p><p> 我们还可以对NAT64和CLAT感到一些乐趣：将整个网络设置为仅IPv6，但使用CLAT诱使pod认为它们具有v4连接性。在Pod中，进行4到6转换，然后将流量发送到NAT64网关。</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.dave.tf/post/new-kubernetes/">https://blog.dave.tf/post/new-kubernetes/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/从头开始/">#从头开始</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/pod/">#pod</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>