<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>分支专业化 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">分支专业化 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-07 00:42:26</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/789488a5eac805345893f13db5701867.jpg"><img src="http://img2.diglog.com/img/2021/4/789488a5eac805345893f13db5701867.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>如果我们认为作为一种“神经网络的解剖学”的可解释性，大多数电路线程都涉及研究微小的小静脉 - 在单个神经元和它们的连接方式上看小规模。但是，小规模方法没有解决许多自然问题。</p><p>  相比之下，生物解剖学中最突出的抽象涉及更大规模的结构：像心脏这样的单个器官，或像呼吸系统一样的整个器官系统。所以我们想知道：有一个人工神经网络的“呼吸系统”或“心脏”或“大脑区域”？神经网络是否有任何紧急结构，我们可以研究比电路更大级别？</p><p>  本文介绍了分支专业化，三种更大的“结构现象”之一我们已经在神经网络中观察。 （另外两个，标准和重量束缚，具有单独的专用文章。）当神经网络层分成分支时，发生分支专业化。神经元和电路倾向于自组织，将相关的功能丛生在每个分支中并形成较大的功能单元 - 一种“神经网络脑区域”。我们发现证据表明，这些结构隐含地存在于没有分支的神经网络中，并且该分支是简单地重新装入否则存在的结构。</p><p>  分支专业化的最早示例是我们意识到的来自AlexNet。 Alexnet作为计算机愿景的跳跃，可以说是开始深入学习的革命，但埋藏在论文中是一个令人着迷的，很少讨论的细节。前两层alexNet被分成两个分支，直到它们在第二层之后重新加入。这种结构用于最大化培训模型在两个GPU上的效率，但作者注意到了结果发生了非常好奇的事情。第一层中的神经元组织成两组：形成在一个分支和在另一个分支上的一个分支和低频颜色探测器上形成的黑白gabor过滤器。</p><p>    虽然第一层AlexNet是分支专业化的唯一例子，但我们意识到在文献中讨论，似乎是一种常见的现象。我们发现分支专业化在后面的隐藏层中发生，而不仅仅是第一层。它发生在低级和高级功能。它发生在各种模型中，包括您可能不期望的地方 - 例如，Resnet中的残余块可以在功能上是分支和专业化。最后，分支专业化似乎是普通卷积网的结构现象，即使没有任何特定的结构导致它。</p><p>  有没有大规模的结构来如何运作？如何在模型中组织的功能和电路？网络架构是否会影响表格的功能和电路？分支专业化提示在令人兴奋的故事与所有这些问题相关的故事。</p><p>    许多神经网络架构具有分支机构，暂时无法访问仍然传递给稍后的层的“并行”信息的层的分支机构。 </p><p>Inceptionv1有九套称为“inception块”的四路分支。有几个双向分支机构。 AlexNet残差网络通常不被认为是具有分支，但残留块可以被视为一种分支。</p><p>  过去，具有明确标记的分支的模型是流行的（例如AlexNet和Invepion网络的网络）。在近年来，这些已经变得不那么常见，但残留网络 - 可以被视为隐含地在其剩余块中具有分支 - 已经变得非常普遍。我们有时会在神经架构搜索中看到分支架构自动开发，这是一种学习网络架构的方法。</p><p>  残余网络的隐式分支具有一些重要的细微差别。乍一看，每层都是双向分支。但是由于分支机构组合在一起，我们实际上可以重写模型以揭示残差块可以被理解为分支：</p><p>  我们通常将残余块视为顺序层，彼此顶部构建。 + + + + ......但我们也可以将它们概念化为一定程度，由于跳过连接导致的平行分支。这意味着剩余块可能专注于此。 +</p><p>  我们通常会看到剩余块专注于非常深的剩余网络（例如Resnet-152）。一个假设为什么，在这些模型中，层的精确深度无关紧要，分支方面比顺序方面更重要。</p><p>  正常分支模型的概念弱点之一是，虽然分支可以节省参数，但仍需要大量参数来混合分支之间的值。但是，如果您购买残差网络的分支解释，您可以将它们视为索引的策略：剩余网络中的intermix分支（例如阻塞稀疏权重），具有低秩连接（将所有块投射到相同的总和中，然后返回向上）。这似乎是一种非常优雅的方式来处理分支。更实际上，它表明，可以通过支付块中的单位来密切关注剩余网络的分析，并且我们可能期望残余流是异常的。</p><p>    分支专业化由分支机构之间组织的功能定义。在正常层中，随机组织特征：给定的特征与层中的任何神经元一样可能。但在分支层中，我们经常将给定类型群集的功能视为一个分支。该分支专门用于该类型的功能。 </p><p>这是怎么发生的？我们的直觉是培训期间存在正面反馈循环。</p><p>  A1 B1 C1 D1 A2 B2 D2 D2分支的第一部分被激活以形成与下半部分相关的特征。分支的下半部分喜欢上半场提供基元的特征。</p><p>  另一种方式考虑这一点是，如果您需要将神经网络切割成彼此通信能力有限的部分，则组织类似的特征在一起进行了意义，因为他们可能需要共享更多信息。</p><p>    到目前为止，我们所展示的分支专业化的唯一具体例子是第一层亚历纳特。稍后的层怎么样？亚历克网还将其后面的层分成分支。这似乎是未探索的，因为第一层更加困难后的学习功能。对于第一层，可以可视化RGB重量;对于以后的图层，需要使用功能可视化。</p><p>  不幸的是，亚历克网后来层面的分支专业化也非常微妙。而不是一个整体分裂，更像是有几十个小簇的神经元，每个集群被分配给一个分支。很难相信一个人不仅仅是看到噪音的模式。</p><p>  但其他型号在以后的层层中具有非常清晰的分支专业化。当分支构成一个层的层数时，这倾向于发生，因为有许多分支或者因为一个比其他分支小得多。在这些情况下，该分支可以专注于层中存在的特征的非常小的子集并揭示清晰的模式。</p><p>  例如，大多数Incepionv1的层具有分支结构。该分支具有不同数量的单位，并且变化卷积尺寸。 5x5分支是最小的分支，也具有最大的卷积大小。它经常非常专注： </p><p>混合3A，相对早期的层的5x5分支专门用于颜色检测，尤其是黑白与颜色检测。混合3A_5x5：该分支似乎专门用于复杂的形状和3D几何探测器。我们没有本层的全分类，以允许定量评估。混合4A_5x5：3D几何/复杂形状BW VS颜色其他边界检测器其他其他亮度其他颜色对比度</p><p>  这不太可能发生偶然发生。例如，混合和白色的所有9个黑白与彩色探测器在混合的3A_5X5中，尽管它在层中的256个神经元中仅为32。偶然发生的可能性小于1/10 8.对于更优点的例子，混合3B中的所有30个与曲线相关的特征都在混合3b_5x5中，尽管层中的480个神经元中只有96个。偶然发生的可能性小于1/10 20。</p><p> 值得注意的是一个可能会影响专业化的一个混杂因素。 5x5分支是最小的分支，而且还具有比其邻居更大的卷曲（5x5而不是3x3或1x1）。有一些东西表明分支发挥着重要作用：混合3a和混合3b是含有相对相似的特征并且具有相同规模的相邻层。如果它只是关于卷积大小，我们为什么不在混合的3B_5x5分支中看到混合的3a_5x5分支或颜色中的任何曲线？</p><p>    也许关于分支专业化的最令人惊讶的是，同一分支机构似乎又一次地发生了不同的架构和任务。</p><p>  例如，我们在AlexNet中观察到的分支专业化 - 第一层专门从事黑白Gabor分支与低频颜色分支 - 是一个令人惊讶的强大现象。如果你重新夺回亚历纳网，它会一致地发生。如果您将其他架构与前几个层分成两个分支，则会出现。如果您在其他自然图像数据集上训练那些模型，如地方而不是想象成，它甚至会出现。轶事，我们似乎也看到了其他类型的分支专业化。例如，发现似乎专注于曲线检测的分支似乎是非常常见的（尽管登陆器的混合3b_5x5是我们仔细描述的唯一一个）。</p><p>    一个假设似乎非常诱人。请注意，在正常的非分支模型中形成的许多相同功能也似乎在分支模型中形成。例如，分支和非分支模型的第一层包含Gabor滤波器和颜色特征。如果存在相同的功能，则可能在它们之间存在相同的权重。</p><p>  可以是分支只是浮出浮出的结构吗？也许在正常模型中的第一和第二DIRM层的权重之间存在两个不同的子图，它们之间具有相对较小的权重，当您培训分支模型时，这两个子图锁定到分支上。 （这将定向地类似于在神经网络中找到模块化子结构的工作。） </p><p>为了测试这一点，让我们来看看具有非分支第一和第二卷积层的模型。让我们掌握它们之间的重量，并在权重的绝对值上执行奇异值分解（SVD）。这将向我们展示各种变异的主要因素，其中神经元在下一层中连接到不同神经元（无论这些连接是否是兴奋性的或抑制性）。</p><p>  当然，第一两个卷积层的重物之间的奇异矢量（变化因数）的Inceptionv1之间的重量是颜色。</p><p>  第一卷积层神经元在组织中由左字载体| W |。 Inceptionv1（TF-Slim版本）在想象中培训。第一奇异矢量分离颜色和黑色和白色，这意味着这是最大的变体的尺寸，其中神经元连接到下一层。 Gabor过滤器和颜色特征相距甚远，这意味着它们倾向于连接到下一层中的不同功能。奇异的载体1（频率？）奇异矢量0（颜色？）Inceptionv1在Places365训练的情况下，第一奇异矢量分离颜色和黑色和白色，这意味着神经元在下一层中连接的变化的最大尺寸。奇异的载体1（频率？）奇异载体0（颜色？）奇异载体1（频率？）奇异载体0（颜色？）奇异载体1（频率？）奇异的载体0（颜色？）第二卷积层神经元组织中W |的正确奇异载体。</p><p>  我们还看到第二个因素似乎是频率。这表明了一个有趣的预测：也许如果我们将该层拆分为两个以上的分支机构，我们也会在颜色外观察频率的专业化。</p><p>  这似乎可能是真的。例如，这里我们看到一个高频黑白分支，中频多为黑白分支，中频色分支和低频颜色分支。</p><p>      我们已经表明，分支专业化是结构现象的一个示例 - 神经网络中的大规模结构。它发生在各种情况和神经网络架构中，并且它发生在一致性 - 某些专业图案，例如颜色，频率和曲线，遍布不同的架构和任务。</p><p>  返回与解剖学的比较，尽管我们犹豫不决向神经科学声称明确的平方，但它很诱人地绘制分支专业化与大脑区域的存在，专注于特定任务。 Visual Cortex，听觉皮层，Broca的地区和Wernicke的区域在灵长类动物视觉皮层的V2区域内的亚专业化是神经科学的另一个强大示例。 V2内的一种条纹对取向或亮度敏感，而另一种类型的条纹含有颜色选择性神经元。我们感谢Patrick MineAult进行这种类比，并且进一步指出，高频功能与灵长类动物V2区域中的高级功能的一些已知表示一致。 - 这些都是大脑领域的例子，在宽阔的人群中，神经科学家和心理学家都能够表征具有显着一致的功能的群体。作为没有神经科学的专业知识的研究人员，我们不确定这种联系的有用程度，但考虑到分支专业化是否可以是在生物神经网络中出现的专业化的有用模型。 </p><p>Matthew Nolan是爱丁堡大学发现大脑的探索大脑科学和西蒙斯倡议中心的神经电路和西蒙斯倡议的资金教授。伊恩Hawes是爱丁堡大学翻译神经科学的Wellcome Trust计划的博士学位。</p><p>  由于神经科学家，我们通过这项工作令人兴奋，因为它为关于如何组织脑的长期问题提供了新的理论观点以及如何发展。在整个大脑中发现了分支和专业化。良好的研究示例是与空间和非空间视觉处理相关联的背部和腹侧视觉流。在每种途径中的微电路水平神经元中是相似的。然而，神经活动的录音表明了卓越的专业化; 20世纪70年代和80年代的经典实验建立了腹侧流能够识别物体的想法，而背部流代表其位置。从那时起，很多关于这些途径中的信号处理，而是基本的问题，例如为什么有多个流以及它们的建立方式是未答复的。</p><p>  从神经科学家的角度来看，由Voss和她的同事调查分支机构的调查是，在没有任何复杂的分支特定设计规则的情况下出现了强大的分支专业化。他们的分析表明，专业化在架构中和跨越不同的培训任务。这里的含义是分支专业化不需要特定的说明。实际上，他们的分析表明它甚至在没有预定的分支的情况下出现。相比之下，许多神经科学家的直觉将是Neocortex的不同领域的专业化需要特定于每个区域的发育机制。对于旨在了解大脑的感知和认知功能如何出现的神经科学家，这里的重要想法是驱动皮质途径的分离，例如背部和腹侧视觉流的发展机制可能是绝对关键的。</p><p>  虽然在大脑中的人工神经网络和神经电路之间的分支专业化之间的平方醒目，但显然存在显着的差异和许多未突出的问题。从建设人工神经网络的角度来看，我们想知道是否特定各个单位的调整及其连接规则将提高性能？在大脑中，有良好的证据表明各个神经元的激活功能在不同的神经电路之间进行微调。如果这种微调赋予大脑的益处，那么我们可能会期望人造网络中的类似益处。从理解大脑的角度来看，我们想知道分公司专业化是否有助于进行实验可测试的预测？如果人造网络可以设计有具有与大脑中的分支途径类似的组织的分支，则可以将对这些网络的操纵进行比较，以与致敏和化学策略实现的实验性操纵。鉴于许多大脑疾病涉及对特定神经群体的变化，类似的策略可以对这些病态变化改变大脑功能的洞察。例如，在阿尔茨海默病的早期阶段中断的神经元的非常特异性群体。通过中断神经网络模型中的相应单元，可以探索所产生的计算缺陷和恢复认知功能的可能策略。</p><p>       与许多科学合作一样，贡献难以分开，因为这是我们一起写作的合作努力。</p><p>  研究。克里斯奥拉最初观察到分支专业化现象。克里斯还开发了重量PCA实验，表明它隐含地发生在非分支模型中。在Nick Cammarata，Gabe Go，Chelsea Voss，Ludwig Schubert和Chris的情况下，通过协同研究在和通知的背景下进行了这项调查。切尔西和尼克有助于构建这项工作，这在电路顶部的大规模结构的重要性中。</p><p>  基础设施。分支专业化仅被发现，因为Ludwig Schubert的早期版本的显微镜使得易于浏览某些层处存在的神经元。 Michael Petrov，Ludwig和Nick建立了各种基础设施，使我们的研究成为可能。 </p><p>写作和图表。 切尔西写了这篇文章，基于克里斯和克里斯的帮助。 切尔西和克里斯展示了图表。  我们感谢BriceMénard推动我们来调查我们是否能找到更大的结构，例如在此调查的结构。 我们感谢#Circuits的参与者在蒸馏池中的参与者，以便他们参与本文，特别是对AlexBäuerle，Ben Egan，Patrick Mineault，Matt Nolan和Vincent Tjeng的讲话。 我们感谢Patrick MineAult，以指出与亚专业化的神经科学比较  ...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://distill.pub/2020/circuits/branch-specialization/">https://distill.pub/2020/circuits/branch-specialization/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/分支/">#分支</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/branch/">#branch</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1047738.html"><img src="http://img2.diglog.com/img/2021/2/thumb_10351cd2dd020f6e869f9eb9ee66ba43.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1047738.html">这是分行吗？ </a></div><span class="my_story_list_date">2021-2-17 18:14</span></div><div class="col-sm"><div><a target="_blank" href="/story/1031388.html"><img src="http://img2.diglog.com/img/2020/10/thumb_77a729e8f0c189fc29b3f597aa9e59c7.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031388.html">发布流程：我们如何在VSTS团队中进行分支(2018)</a></div><span class="my_story_list_date">2020-10-27 10:26</span></div><div class="col-sm"><div><a target="_blank" href="/story/1027478.html"><img src="http://img2.diglog.com/img/2020/10/thumb_fb43c76fdbb28247d20cc64bbbd18cc4.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1027478.html">什么是Git Rebase？它与合并有什么不同？</a></div><span class="my_story_list_date">2020-10-7 8:24</span></div><div class="col-sm"><div><a target="_blank" href="/story/1023755.html"><img src="http://img2.diglog.com/img/2020/9/thumb_e875843de005aa54b182fd0d343e1d2b.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1023755.html">树枝：莱昂纳多·达·芬奇的法则与生物力学模型(2014)</a></div><span class="my_story_list_date">2020-9-14 5:8</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>