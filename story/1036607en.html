<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>使用Linkerd在Kubernetes上进行拓扑感知的服务路由Topology-Aware Service Routing on Kubernetes with Linkerd</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Topology-Aware Service Routing on Kubernetes with Linkerd<br/>使用Linkerd在Kubernetes上进行拓扑感知的服务路由</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-26 04:40:57</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/0f73de82299639e44436731741eb7101.jpg"><img src="http://img2.diglog.com/img/2020/11/0f73de82299639e44436731741eb7101.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>A few months ago, I was browsing through issues and feature requests in theLinkerd2 repository in search of a cool topic to submit to the LFX Mentorship program. To participate,students were asked to propose an RFC for a feature that, if accepted, they&#39;dimplement. My choice — Service Topology — which, at the time, was a newlyreleased Kubernetes feature.</p><p>几个月前，我在Linkerd2存储库中浏览问题和功能请求，以寻找一个很酷的主题提交给LFX Mentorship程序。作为参与，要求学生提出一项针对该功能的RFC，如果该功能被接受，则可以实施。我的选择-服务拓扑-当时是新发布的Kubernetes功能。</p><p> Truth be told, I initially struggled to wrap my head around Service Topology.Partially, due to the perceived complexity, but mostly because there weren&#39;t alot of resources on the topic. As a newly released feature, all I had on ServiceTopology was the proposal and its implementation.</p><p> 实话实说，我起初只是想将服务拓扑学全神贯注。部分是由于感知到的复杂性，但主要是因为该主题上没有很多资源。作为一项新发布的功能，我在ServiceTopology上所拥有的只是提案及其实现。</p><p> Fast forward to the present day. Thanks to the cumulative effort of maintainers,contributors, and myself (yep, my RFC was accepted), Linkerd now supportsService Topology out-of-the-box.</p><p> 快进到今天。由于维护者，贡献者和我本人的不懈努力（是的，我的RFC被接受了），Linkerd现在可以立即支持服务拓扑。</p><p> In this article, I’ll discuss what Service Topology is, how Linkerd supports it(no config needed), and some of the challenges I encountered as a newbie opensource contributor.</p><p> 在本文中，我将讨论什么是服务拓扑，Linkerd如何支持它（无需配置）以及作为新手开源贡献者遇到的一些挑战。</p><p>  Unlike the term “topology” may suggest, Service Topology doesn&#39;t refer to thearrangement of services within a cluster. Instead, it refers to the ability toroute traffic for specific services based on a predetermined node topology. Thiscapability was introduced in Kubernetes v1.17 as an alpha feature.</p><p>  与术语“拓扑”可能暗示的不同，服务拓扑不指群集内服务的安排。相反，它指的是基于预定节点拓扑为特定服务路由流量的能力。此功能在Kubernetes v1.17中作为Alpha功能引入。</p><p> The need for topology-aware service routing originated from the higher costassociated with cross-zonal network traffic for multi-zone cluster deployments.The standard Kubernetes proxy (kube-proxy) balances load randomly, meaningservice traffic could be routed to any pod. But some Kubernetes users wouldrather target endpoints that are either co-located or at least closer to theservice. A randomly selected pod by the kube-proxy could be far away, increasinglatency and cost. This is exactly what Service Topology addresses. It allowsservice owners to prioritize topology domains to be routed to.</p><p> 对拓扑感知服务路由的需求源于多区域集群部署中跨域网络流量的较高成本。标准的Kubernetes代理（kube-proxy）会随机平衡负载，这意味着服务流量可以路由到任何Pod。但是，一些Kubernetes用户宁愿将目标定位在同一地点或至少靠近该服务的端点。 kube-proxy可能会随机选择一个吊舱，从而增加了等待时间和成本。这正是服务拓扑所要解决的问题。它允许服务所有者确定要路由到的拓扑域的优先级。</p><p> Figure 1.1 shows what topology-aware service routing looks like. Suppose we havea cluster with two nodes (node-1 and node-2). Let&#39;s say service A wants to talkto service B. Without Service Topology any pod from service B’s pod pool (B-1,B-2, or B-3) could be targeted*.* In the bottom half of the diagram, you&#39;ll seethe same scenario but this time with Service Topology enabled. Service A is onlyallowed to target co-located pods. Now all traffic between service A and B willexclusively route via B-1.</p><p> 图1.1显示了拓扑感知的服务路由。假设我们有一个包含两个节点（节点1和节点2）的集群。假设服务A想与服务B通话。如果没有服务拓扑，则可以将服务B的Pod池（B-1，B-2或B-3）中的任何Pod作为目标*。*在图的下半部分，您将会看到相同的场景，但是这次启用了服务拓扑。服务A仅允许定位到同一位置的Pod。现在，服务A和服务B之间的所有流量都将专门通过B-1路由。</p><p>   While writing the Service Topology RFC, I often asked myself this very question.Initially, the feature seemed complex but there are two Kubernetes resourcesthat facilitate topology-aware service routing:  topology  labels and EndpointSlices.</p><p>在编写服务拓扑RFC时，我经常问自己这个问题。最初，该功能似乎很复杂，但是有两个Kubernetes资源可以促进拓扑感知的服务路由：拓扑标签和EndpointSlice。</p><p> Topology labels are typical Kubernetes k/v pairs used to represent topologicalcluster domains. Topology labels are commonly associated with nodes. Whenprovisioning a cluster, every node should get a list of associated topologylabels (although not always, think of the kubeadm). Here are the labels usedwith service topology:</p><p> 拓扑标签是典型的Kubernetes k / v对，用于表示拓扑集群域。拓扑标签通常与节点关联。设置群集时，每个节点都应获取关联的拓扑标签列表（尽管并非总是如此，请考虑kubeadm）。以下是与服务拓扑一起使用的标签：</p><p>   In the official documentationyou&#39;ll see how different combinations of these three labels (four if you countthe wildcard *) can be used to route traffic based on your topologypreferences.</p><p>   在官方文档中，您将看到如何使用这三个标签的不同组合（如果计算通配符*，则为四个）可以根据您的拓扑首选项来路由流量。</p><p> EndpointSlices is an exciting yet underrated feature that was released a versionbefore Service Topology became available. It optimizes Endpoints, the resourceobject that holds the subsets (IP address of a pod x port combination) of aservice together. As it turns out, holding all subsets of a service in the sameobject means the object will endlessly grow. And the more it grows, the harder(and more costly, performance-wise) to process.</p><p> EndpointSlices是一项令人兴奋却被低估的功能，该功能是在服务拓扑可用之前发布的版本。它优化了端点，端点是将服务的子集（吊舱的IP地址x端口组合）保持在一起的资源对象。事实证明，将服务的所有子集保留在同一对象中意味着该对象将不断增长。而且，它增长得越多，就越难处理（并且在性能方面更昂贵）。</p><p> EndpointSlices break Endpoints up into multiple objects. This is fundamental toService Topology because, as part of the change, these subsets aren&#39;t justseparated, they also contain more identifying information, such as the topologydomain they belong to. As a bonus, EndpointSlices also include support fordual-stack addresses, paving the way for more advanced networking logic that canbe developed in the Kubernetes ecosystem.</p><p> EndpointSlices将端点分解为多个对象。这是服务拓扑的基础，因为作为更改的一部分，这些子集不只是分开的，它们还包含更多标识信息，例如它们所属的拓扑域。另外，EndpointSlices还包括对双栈地址的支持，为可以在Kubernetes生态系统中开发的更高级的网络逻辑铺平了道路。</p><p>  So how does Linkerd fit into our Service Topology discussion? You guessed it: itenables topology-aware service routing! Linkerd supports Service Topology ratherthan extending it or doing something special to it. It enables locality-basedrouting by leveraging the same resources that the kube-proxy uses. Most of thechanges we made were to support Service Topology on the Destination serviceside, which I’ll cover next.</p><p>  那么Linkerd如何适应我们的服务拓扑讨论？您猜对了：它启用了拓扑感知的服务路由！ Linkerd支持服务拓扑，而不是扩展它或对其进行特殊处理。它通过利用kube-proxy使用的相同资源来实现基于位置的路由。我们所做的大多数更改都是为了支持“目标”服务端的服务拓扑，我将在下面介绍。</p><p>  Figure 2.2 depicts a high-level overview of what I call “the four steps toservice discovery”. The destination service accepts connection requests from thesidecar proxies. Each request contains the FQDN or IP address of the servicethat the proxy wants to resolve, as well as metadata such as contextualinformation, i.e on which namespace the pod is. The destination service takesthat FQDN or IP address, translates it internally, and streams it back with alist of pods that the proxy can use — we say stream because the service issending updates back to the proxy, it’s not just a one-time thing. There are twomain components that help us do this:</p><p>  图2.2概述了我所说的“服务发现的四个步骤”。目标服务接受来自sidecar代理的连接请求。每个请求都包含代理想要解析的服务的FQDN或IP地址，以及诸如上下文信息之类的元数据，即Pod位于哪个名称空间上。目标服务采用该FQDN或IP地址，在内部进行转换，然后将其与代理可以使用的Pod列表一起流回。我们之所以说流，是因为该服务将更新发送回代理，这不是一次性的事情。有两个主要组件可以帮助我们完成此任务：</p><p> EndpointWatcher is essentially a caching mechanism. It watches events andholds services and their endpoints in memory.</p><p>EndpointWatcher本质上是一种缓存机制。它监视事件并在内存中保留服务及其端点。</p><p> EndpointTranslator is a listener. It subscribes to a service in theEndpointsWatcher and translates any create, delete, or update operations onthe cached resources into updates for our proxies.</p><p> EndpointTranslator是一个侦听器。它在EndpointsWatcher中订阅服务，并将对缓存资源的所有创建，删除或更新操作转换为代理的更新。</p><p> This differs widely from how the kube-proxy handles routing. For one, we don’tlet the system randomly pick a pod. Instead, we send updates to the proxy whichin turn routes requests based on response latency. That&#39;s ten times cooler thanan iptable rewrite or a randomly chosen pod. Based on how the Destinationservice works, we had to do two things to support Service Topology. First,introduce support for EndpointSlices and then introduce support for the actualService Topology.</p><p> 这与kube-proxy处理路由的方式大不相同。首先，我们不会让系统随机选择一个吊舱。相反，我们将更新发送到代理，代理又根据响应延迟来路由请求。这比iptable重写或随机选择的pod凉十倍。根据Destinationservice的工作方式，我们必须做两件事来支持Service Topology。首先，引入对EndpointSlices的支持，然后引入对实际服务拓扑的支持。</p><p> EndpointSlice support was quite straightforward. Only a few changes wereneeded in the watcher. The first challenge came when dealing with feature gates.Kubernetes’ feature gates can be annoying to deal with, and, as it happens, theEndpointSlices and Service Topology are both behind feature gates. We had tochoose when to use EndpointSlices over Endpoints, or use both and reconcilethem. Since the kube-proxy doesn&#39;t do any reconciliation between the tworesources we decided to go ahead and either use one or the other. After sometrial and error we came up with a CLI flag to enable the support of slices for aLinkerd installation or upgrade. That, coupled with some other checks, gave usconfidence that adopters would be able to opt-in at any point provided they meetthe native Kubernetes service topology prerequisites: active feature flags and arecent Kubernetes version. After resolving the flag, everything else was smoothsailing. We simply added new structs and functions to process the EndpointSlicesas resources.</p><p> EndpointSlice支持非常简单。观察者只需要进行一些更改。第一个挑战是处理要素门时。Kubernetes的要素门可能会令人讨厌，并且碰巧，EndpointSlice和Service Topology都在要素门后面。我们不得不选择何时在Endpoints上使用EndpointSlices，或者同时使用reconcilethem和。由于kube-proxy在这两种资源之间不做任何调和，因此我们决定继续使用两者之一。经过一番尝试和错误后，我们想到了一个CLI标志来启用对Linkerd安装或升级的片的支持。加上其他一些检查，使我们相信，采用者只要满足本地Kubernetes服务拓扑先决条件：活动功能标志和最新的Kubernetes版本，就可以随时选择加入。解决了国旗后，其他一切都顺利进行了。我们仅添加了新的结构和功能来处理EndpointSlicesas资源。</p><p> Service Topology support was easier said than done. First, to identify thetraffic source we had to add additional contextual data to requests sent fromthe proxy to the service. After all, to calculate topology we must look at both— the source and destination nodes. That was a dedicated effort in itself. Itrequired a solid way to grab the name of the source node without messing up thecontrol plane internals. We also needed to identify the best way to addcontextual data moving forward. We decided to turn the contextual informationrepresented as a string (denoting the namespace of the pod) into JSON. Thisensured the value would continue to be opaque to most of the API while onlyrequiring a code change to unmarshal the information on the destination serviceside. The biggest challenge was keeping track of the different topologypreferences and the available endpoints. As I was about to learn, filteringimplies state. A lot of edge cases that I didn&#39;t account for in the RFC startedto surface: how do we do fallbacks? What happens when the service object ismodified after we picked a preference? And so on. The answer to all of it wasstate**.** We decided to change the EndpointTranslator component to be stateful,keeping track of the total amount of endpoints, as well as a snapshot of thepreviously filtered set. This alleviated most of the concerns around thesubstantiated edge cases, and made coming up with filtering logic a breeze.</p><p> 服务拓扑支持说起来容易做起来难。首先，为了识别流量来源，我们必须向从代理发送到服务的请求中添加其他上下文数据。毕竟，要计算拓扑，我们必须同时查看源节点和目标节点。这本身就是一个专门的工作。它需要一种可靠的方法来获取源节点的名称，而又不会弄乱控制平面的内部结构。我们还需要确定向前添加上下文数据的最佳方法。我们决定将表示为字符串（表示pod的名称空间）的上下文信息转换为JSON。这样可以确保该值对于大多数API仍然是不透明的，而仅需要更改代码即可在目标服务端取消封送信息。最大的挑战是跟踪不同的拓扑首选项和可用的端点。在我即将学习的过程中，过滤意味着状态。我未在RFC中说明的许多边缘情况开始浮出水面：我们如何进行后备？在选择首选项后修改服务对象时会发生什么？等等。所有这些的答案都是状态**。**我们决定将EndpointTranslator组件更改为有状态的，以跟踪端点的总数以及以前过滤的集合的快照。这减轻了围绕充实边缘情况的大多数担忧，并使过滤逻辑的提出变得轻而易举。</p><p>  I certainly didn&#39;t expect this feature to take the turns it had. For one, as afinal year undergraduate student, the prospect of coming up with a solution withno help whatsoever seemed far off — until it wasn’t. Knowing that my work willmake other people&#39;s lives easier is incredibly gratifying. What’s more, I get totell people how to deal with feature gates and contextual data!</p><p>  我当然没想到此功能会发生变化。作为一个即将毕业的大四学生，想出一个毫无帮助的解决方案的前景似乎遥不可及，直到没有实现。知道我的工作会使别人的生活更轻松，这真令人难以置信。此外，我还告诉人们如何处理要素门和上下文数据！</p><p> But above all, I am excited to have implemented (what I consider to be) a prettycool feature that the rest of the Linkerd community can build on and use.Service Topology allows you to route traffic based on the location of yourendpoints by default. Enable those feature gates in your cluster and slice downon your costs.</p><p> 但最重要的是，我很高兴能够实现（我认为是）Linkerd社区的其他成员可以构建和使用的相当酷的功能。服务拓扑允许您默认情况下根据端点的位置路由流量。在集群中启用这些功能门，并降低成本。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://linkerd.io/2020/11/23/topology-aware-service-routing-on-kubernetes-with-linkerd/">https://linkerd.io/2020/11/23/topology-aware-service-routing-on-kubernetes-with-linkerd/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/linkerd/">#linkerd</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/aware/">#aware</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/服务/">#服务</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>