<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>登录TwitterLogging at Twitter</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Logging at Twitter<br/>登录Twitter</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-20 09:34:21</div><div class="page_narrow text-break page_content"><p>While centralized logging previously existed at Twitter, it was limited by low ingestion capacity, and limited query capabilities which resulted in poor adoption and failed to deliver the value we hoped for. To address this, we adopted Splunk Enterprise and migrated centralized logging to it. Now we ingest 4 times more logging data and have a better query engine and better user adoption.Our migration to Splunk Enterprise has given us a much stronger logging platform overall, but the process was not without its challenges and learnings, which we’ll share in greater detail in this blog.</p><p>虽然之前Twitter上存在集中式日志记录，但它受到低摄取能力和有限查询能力的限制，这导致采用率低下，未能实现我们希望的价值。为了解决这个问题，我们采用了Splunk Enterprise并将集中式日志记录迁移到它。现在我们接收到的日志数据是原来的4倍，并且有了更好的查询引擎和更好的用户接受度。我们向Splunk Enterprise的迁移总体上为我们提供了一个更强大的日志平台，但这一过程并非没有挑战和经验教训，我们将在本博客中更详细地分享这些挑战和经验教训。</p><p>    Before we adopted Splunk Enterprise at Twitter, our central logging platform was a home-grown system called Loglens. This initial system addressed frustrations around the ephemerality of logs living inside of containers on our compute platform and the difficulty in browsing through different log files from different instances of different services while investigating an ongoing incident. This logging system was designed with the following goals:</p><p>在我们在Twitter上采用Splunk Enterprise之前，我们的中心日志平台是一个名为Loglens的自制系统。这个最初的系统解决了我们的计算平台上容器中的日志短暂性带来的问题，以及在调查正在进行的事件时浏览不同服务的不同实例中的不同日志文件的困难。该记录系统的设计目标如下：</p><p>    Loglens was focused on ingesting logs from services that make up Twitter. In terms of addressing the problems set before it, with the cost and time constraints limiting it, Loglens was a fairly successful solution.</p><p>Loglens专注于从构成Twitter的服务中获取日志。在解决摆在它面前的问题方面，由于成本和时间限制，Loglens是一个相当成功的解决方案。</p><p>  Unsurprisingly, a logging system with such limited resource investment ultimately fell short of users’ expectations, and the system saw suboptimal adoption by developers at Twitter. More information about Loglens can be found on  our previous post. Below is a simplified diagram of the Loglens ingestion system. Logs were written to local  Scribe daemons, forwarded onto Kafka, and then ingested into the Loglens indexing system.</p><p>毫不奇怪，一个资源投入如此有限的日志系统最终没有达到用户的预期，而且该系统在Twitter上被开发者采用的情况也不理想。关于Loglens的更多信息可以在我们之前的帖子中找到。下面是Loglens摄食系统的简图。日志被写入本地Scribe守护进程，转发到Kafka，然后被摄入Loglens索引系统。</p><p>  While running Loglens as our logging backend, we ingested around 600K events per second per datacenter. However, only around 10% of the logs submitted, with the remaining 90% discarded by the rate limiter.</p><p>在运行Loglens作为我们的日志后端时，我们在每个数据中心每秒接收大约6000K个事件。然而，只有大约10%的日志被提交，剩下的90%被速率限制器丢弃。</p><p>    Given the title of this blog post, it’s not exactly a spoiler to say we replaced Loglens with Splunk Enterprise. Our adoption of Splunk Enterprise grew out of an effort to find a central logging system that could handle a wider set of use cases, including system logs from the hundreds of thousands of servers in the Twitter fleet and device logs from networking equipment. We ultimately chose Splunk because it can scale to meet the capacity requirements and offers flexible tooling that can satisfy the majority of our users’ log analysis needs.</p><p>鉴于这篇博文的标题，说我们用Splunk Enterprise取代Loglens并不是一个颠覆。我们之所以采用Splunk Enterprise，是因为我们努力寻找一个中央日志系统，该系统可以处理更广泛的用例集，包括Twitter车队中数十万台服务器的系统日志和网络设备的设备日志。我们最终选择了Splunk，因为它可以扩展以满足容量需求，并提供灵活的工具，可以满足大多数用户的日志分析需求。</p><p>  Our transition to Splunk Enterprise was simple and straightforward. With system logs and network device logs being new categories for us to ingest, we were able to start fresh. We installed the Splunk Universal Forwarder on each server in the fleet, including on some new dedicated servers running rsyslog to relay logs from network equipment to the universal forwarder.</p><p>我们向Splunk Enterprise的过渡简单而直接。随着系统日志和网络设备日志成为我们可以接受的新类别，我们能够重新开始。我们在车队中的每台服务器上安装了Splunk Universal Forwarder，包括一些运行rsyslog的新专用服务器，以将日志从网络设备中继到Universal Forwarder。</p><p>  The most complicated part of the transition was migrating the existing application logs from our Loglens to Splunk Enterprise. Luckily, with the loosely coupled design of the Loglens pipeline, the majority of our team’s effort was in the simple task of creating a new service to subscribe to the Kafka topic that was already in use for Loglens, and then forward those logs on to Splunk Enterprise.</p><p>转换过程中最复杂的部分是将现有的应用程序日志从Loglens迁移到Splunk Enterprise。幸运的是，由于Loglens管道的松散耦合设计，我们团队的大部分工作都是在简单的任务中创建一个新服务，订阅Loglens已经使用的Kafka主题，然后将这些日志转发到Splunk Enterprise。</p><p>  Perhaps a little unimaginatively, we named the new service the Application Log Forwarder (ALF). Our main goals when designing ALF were as follows:</p><p>我们将这项新服务命名为应用程序日志转发器（ALF），这可能有点缺乏想象力。我们在设计ALF时的主要目标如下：</p><p>  ALF should be resilient. That is, event submission should be resilient when faced with intermittent errors.</p><p>阿尔夫应该有韧性。也就是说，当遇到间歇性错误时，事件提交应该具有弹性。</p><p>  ALF should be controllable. In the case of a misbehaving service generating excessive log volume, we should be able to drop events to preserve our daily quota and the stability of the indexer clusters.</p><p>阿尔夫应该是可控的。如果一个行为不端的服务产生过多的日志量，我们应该能够删除事件，以保持每日配额和索引器集群的稳定性。</p><p>  ALF is a pretty simple service. It reads events from Kafka and submits them to Splunk Enterprise using the HTTP Event Collector, with some rudimentary rate limiting based on service name and log level. Despite its simplicity, ALF accounts for over 85% of all logs ingested. System logs forwarded by the Universal Forwarders or network device logs ingested through rsyslog+Universal Forwarder account for less than 15%.</p><p>ALF是一项非常简单的服务。它从Kafka读取事件，并使用HTTP事件收集器将它们提交给Splunk Enterprise，并根据服务名称和日志级别进行一些基本的速率限制。尽管它很简单，但ALF占所有摄入日志的85%以上。通用转发器转发的系统日志或通过rsyslog+通用转发器接收的网络设备日志不到15%。</p><p>  Our new logging pipeline for application logs looks very similar, as seen in the following figure. The only changes are below the blue dashed line.</p><p>我们新的应用程序日志记录管道看起来非常相似，如下图所示。唯一的变化是在蓝色虚线下方。</p><p>    Our current topology involves stamping a mostly independent cluster of indexers, search heads, deployer, and cluster manager in each of our primary data centers to ingest and index the logs from servers and services in that datacenter. The only interactions between the per-datacenter clusters is between the indexers and the license manager running on one of the deployers and the search head clusters configured to search all indexer clusters. We do not make use of multi-site index replication or multi-site search head clusters.</p><p>我们当前的拓扑结构涉及在每个主要数据中心的索引器、搜索头、部署器和集群管理器组成的一个基本独立的集群，以接收和索引来自该数据中心的服务器和服务的日志。每个数据中心集群之间的唯一交互是在其中一个部署器上运行的索引器和许可证管理器之间，以及配置为搜索所有索引器集群的搜索头集群之间。我们不使用多站点索引复制或多站点搜索头群集。</p><p>  Currently we collect around 42 terabytes of data per datacenter each day. This excludes Splunk Enterprise’s internal logging and some logs from the noisiest services, which are rate limited. More than two-thirds of this traffic comes through the Splunk Enterprise HTTP Event Collector, going from log statement to on-disk storage in less than 10 seconds during normal operations. We ingest around 5M events per second per datacenter. This is more than 4 times greater than the events per second ingested when we used Loglens as our backend, and we still have plenty of disk space, disk IO, and CPU headroom for ingesting more logs.</p><p>目前，我们每天在每个数据中心收集大约42 TB的数据。这不包括Splunk Enterprise的内部日志记录，以及来自最嘈杂的服务的一些日志，这些服务的速率是有限的。超过三分之二的流量来自Splunk Enterprise HTTP事件收集器，在正常操作期间，在不到10秒内从日志语句传输到磁盘存储。我们在每个数据中心每秒接收大约500万个事件。这比我们使用Loglens作为后端时每秒接收的事件数高出4倍多，而且我们仍然有足够的磁盘空间、磁盘IO和CPU空间来接收更多日志。</p><p>        While the transition to Splunk Enterprise has significantly improved our logging service here at Twitter, it was not without challenges.</p><p>虽然向Splunk Enterprise的过渡极大地改善了我们在Twitter上的日志服务，但这并非没有挑战。</p><p>    One basic challenge we’ve run into is managing the configuration of our Splunk Enterprise servers. While the standard solution of using a configuration management tool like Puppet or Chef covers the majority of the configuration we need to automate and manage, this approach still fell short of what we wanted for managing indexes and access control.</p><p>我们遇到的一个基本挑战是管理Splunk企业服务器的配置。虽然使用Puppet或Chef等配置管理工具的标准解决方案涵盖了我们需要自动化和管理的大部分配置，但这种方法仍然不能满足我们管理索引和访问控制的需要。</p><p>  When we managed indexes and their access policies with Puppet, the whole process was ultimately limited by the following:</p><p>当我们使用Puppet管理索引及其访问策略时，整个过程最终受到以下限制：</p><p>    Not only did this approach require an engineer’s involvement and limit how quickly we can create an index, but it also did not integrate well into other provisioning tools at Twitter.</p><p>这种方法不仅需要工程师的参与，并且限制了我们创建索引的速度，而且还没有很好地集成到Twitter上的其他资源调配工具中。</p><p>  The solution we landed on was to create a new service that generates the  indexes.conf and  authentication.conf files and deploys them to the correct servers. This service presents an API with role-based access control to the rest of the network, letting us integrate index creation with existing provisioning tools.</p><p>我们找到的解决方案是创建一个生成索引的新服务。确认和认证。conf文件并将其部署到正确的服务器。这项服务提供了一个API，可以对网络的其余部分进行基于角色的访问控制，使我们能够将索引创建与现有的资源调配工具集成在一起。</p><p>  This solution was hindered by some limitations in the Splunk Enterprise API. While there is an API endpoint that can create indexes, it works only for standalone indexers. There are also other limitations around remote management of configuration files. As a result, we copy  files out of band to distribute configuration files. Additionally, the documentation for reloading configuration files is limited, and we are relying on an undocumented endpoint to reload authentication and authorization files without restarting the search heads.</p><p>该解决方案受到Splunk Enterprise API中某些限制的阻碍。虽然有一个API端点可以创建索引，但它只适用于独立的索引器。远程管理配置文件也有其他限制。因此，我们将文件复制到带外以分发配置文件。此外，重新加载配置文件的文档是有限的，我们依靠一个未记录的端点来重新加载身份验证和授权文件，而无需重新启动搜索头。</p><p>  Our approach gives us timely index creation with absolutely no involvement from an engineer on the Logging team under normal operation. We expect to see upwards of 600 indexes created dynamically by this process in the coming year.</p><p>我们的方法为我们提供了及时的索引创建，在正常操作下，日志团队的工程师完全不参与。我们预计，在未来一年，这一过程将动态创建600多个指数。</p><p>    Splunkbase contains an impressive array of add-ons. Many of these define modular inputs to collect data from the API of third-party applications, for example: Github, Slack, or Jenkins. These add-ons are usually simple to set up (though often a clustered environment complicates this), and within minutes you can be collecting data that otherwise is unavailable or difficult to collect.</p><p>Splunkbase包含一系列令人印象深刻的附加组件。其中许多定义了模块化输入，以从第三方应用程序（例如Github、Slack或Jenkins）的API收集数据。这些附加组件通常很容易设置（尽管集群环境通常会使设置复杂化），几分钟内您就可以收集到不可用或难以收集的数据。</p><p>    Distributed resources, like the Splunk Enterprise Key-Value Store, without locking or semaphores, making it vulnerable to race conditions</p><p>分布式资源，如Splunk Enterprise Key-Value Store，没有锁定或信号量，容易受到竞争条件的影响</p><p>  In either case, configuring the modular input on multiple nodes results in duplicate querying of the upstream API and duplication of data submitted to Splunk Enterprise. This means most modular inputs must run on a single node in your Splunk Enterprise infrastructure, creating a single point of failure. Should that node fail, it requires either manual intervention or additional complexity in your configuration management to allow for automatic selection of a new node. Ultimately most Splunkbase add-ons are not designed to run in a way that is resilient to unexpected server failures.</p><p>在任何一种情况下，在多个节点上配置模块化输入都会导致重复查询上游API，并重复提交给Splunk Enterprise的数据。这意味着大多数模块化输入必须在Splunk企业基础设施中的单个节点上运行，从而造成单点故障。如果该节点出现故障，则需要手动干预或配置管理中的额外复杂性，以允许自动选择新节点。归根结底，大多数Splunkbase附加组件的设计并不是为了能够适应意外的服务器故障。</p><p>  Instead, we are implementing plugins for a service that is scheduled by and runs on our compute infrastructure. Our plugins periodically load data from these third-party applications and submit it to our indexer clusters using ALF. The largest disadvantage of this approach is that we must re-implement each plugin ourselves. This is required because the existing modular inputs would require modifications to run under this new service and, given the variations of how different modular inputs are written and store their cursors, these modifications would end up being unique per modular input. It seems it will be easier to write new plugins that behave consistently.</p><p>相反，我们正在为一项服务实施插件，该服务由我们的计算基础设施计划并在其上运行。我们的插件定期从这些第三方应用程序加载数据，并使用ALF将其提交给我们的索引器集群。这种方法最大的缺点是我们必须自己重新实现每个插件。这是必需的，因为现有的模块化输入需要修改才能在这个新服务下运行，而且，考虑到不同模块化输入的写入方式和存储游标的方式不同，这些修改最终将是每个模块化输入唯一的。似乎编写行为一致的新插件会更容易。</p><p>    Most of our data comes through ALF. If a service misbehaves and floods the system with enough logging events that the stability of our Splunk Enterprise clusters is threatened, we can discard events by log level or originating service. This lets us prioritize more important services and the overall health of our indexer clusters over the delivery of every message from every service of every severity. This in turn gives us time to investigate a sudden increase in log volume without causing a larger incident for all customers.</p><p>我们的大部分数据来自ALF。如果某个服务行为不当，并向系统中注入足够多的日志事件，从而威胁到我们Splunk企业集群的稳定性，我们可以按日志级别或原始服务丢弃事件。这使我们能够优先考虑更重要的服务和索引器集群的整体健康状况，而不是来自每个严重性的服务的每条消息的交付。这反过来让我们有时间调查原木量的突然增加，而不会对所有客户造成更大的事故。</p><p>  While it is less common in our environment, the Universal Forwarders running across the fleet sometimes also begin forwarding enough log volume to threaten the stability of the cluster. However, the Universal Forwarder lacks the flexibility to throttle or discard events. Admittedly, this is likely because of design decisions in the Universal Forwarder centered around assumptions that apply to most logs.  However, such assumptions do not fit for many of our application-logs.</p><p>虽然这在我们的环境中不太常见，但跨车队运行的通用转发器有时也会开始转发足够的日志量，从而威胁集群的稳定性。然而，环球货运代理缺乏限制或丢弃事件的灵活性。诚然，这可能是因为环球货运公司的设计决策以适用于大多数原木的假设为中心。然而，这些假设并不适用于我们的许多应用程序日志。</p><p>    Server maintenance, like regular reboots, presents a significant challenge in our environment. While OS updates are applied live, firmware updates or kernel patches require a reboot. This presents a problem primarily for the indexers, as the search heads are less stateful and the cluster manager can be brought down briefly without impacting the indexer clusters or the search head clusters. This is complicated by the size of our clusters, with there being a few hundred indexers in each of our three indexer clusters.    Rebooting too many servers at once or rebooting them too rapidly in succession causes interruptions for searches and poor ingestion rates of new data. We have also seen unusual stability problems until we perform a clean rolling restart. Our uptime and reliability goals do not allow us room for such degraded service that lasts for a few hours for regular maintenance or for a full downtime.</p><p>服务器维护，就像定期重启一样，在我们的环境中是一个巨大的挑战。在实时应用操作系统更新时，固件更新或内核补丁需要重新启动。这主要给索引器带来了一个问题，因为搜索头的状态不太好，集群管理器可以短暂关闭，而不会影响索引器集群或搜索头集群。这一点因集群的大小而变得复杂，三个索引器集群中的每个集群都有几百个索引器。一次重新启动过多服务器或连续快速重新启动服务器会导致搜索中断，新数据的接收率也很低。我们也看到了不寻常的稳定性问题，直到我们执行了一个干净的滚动重启。我们的正常运行时间和可靠性目标不允许我们为这种持续数小时进行定期维护或完全停机的降级服务留出空间。</p><p>    Wait for the missing buckets and pending bucket fixup metrics on the cluster manager to stabilize before moving onto the next batch</p><p>等待cluster manager上丢失的存储桶和挂起的存储桶修复指标稳定下来，然后再进入下一批</p><p>  This process can take up to a week, as it is very engineer driven and so progress occurs only during the work day. While it does not require constant attention from the engineer, it does present a decent sized interruption and burden on our team members.</p><p>这一过程可能需要长达一周的时间，因为它是由工程师驱动的，因此只有在工作日才会取得进展。虽然它不需要工程师的持续关注，但它确实给我们的团队成员带来了相当大的干扰和负担。</p><p>  Future work includes using existing work scheduling and automation services at Twitter to drive these tasks in an intelligent way.</p><p>未来的工作包括使用Twitter上现有的工作调度和自动化服务，以智能的方式驱动这些任务。</p><p>    Migrating our centralized logging service to Splunk Enterprise has enabled our engineers to retain more of their logs, ingest more types of log data, and perform more sophisticated querying and analysis of their logs. Installation and migration were fairly straightforward, resulting in a 400% increase in log volume while keeping the typical ingestion time, going from log statement to on disk, under 10 seconds. It has also introduced new challenges. We were able to work around the challenges we encountered running Splunk Enterprise with large clusters (200+ nodes) and high data volume (5 millions events per second per cluster), at times needing to write custom software to do so. Ultimately, this migration has resulted in increased adoption of centralized logging, including among core application teams and operations and</p><p>将我们的集中式日志服务迁移到Splunk Enterprise使我们的工程师能够保留更多的日志，接收更多类型的日志数据，并对日志执行更复杂的查询和分析。安装和迁移相当简单，导致日志量增加了400%，同时将从日志语句到磁盘上的典型接收时间保持在10秒以下。它也带来了新的挑战。我们能够解决运行Splunk Enterprise时遇到的挑战，该企业具有大型集群（200多个节点）和高数据量（每个集群每秒500万个事件），有时需要编写定制软件来实现这一点。最终，这种迁移导致了集中式日志记录的采用增加，包括在核心应用程序团队和运营部门之间</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/登录/">#登录</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/logging/">#logging</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/日志/">#日志</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>