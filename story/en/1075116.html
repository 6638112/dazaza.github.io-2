<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NumPy中更快的矩阵乘法Faster Matrix Multiplications in NumPy</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Faster Matrix Multiplications in NumPy<br/>NumPy中更快的矩阵乘法</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-26 07:23:39</div><div class="page_narrow text-break page_content"><p>Matrix multiplications in NumPy are reasonably fast without the need for optimization.However, if every second counts, it is possible to significantly improve performance (even without a GPU).</p><p>NumPy中的矩阵乘法速度相当快，无需优化。然而，如果每秒钟都很重要，就有可能显著提高性能（即使没有GPU）。</p><p> Below are a collection of small tricks that can help with large (~4000x4000) matrix multiplications.I have used them to reduce inference time in a deep neural network from 24 seconds to less than one second.In fact, in one case, my optimized code on a CPU turned out to run faster than Tensorflow using a GPU (1 second vs 7 seconds).</p><p>下面是一些小技巧的集合，可以帮助进行大型（~4000x4000）矩阵乘法。我用它们将深层神经网络的推理时间从24秒缩短到不到一秒。事实上，在一个例子中，我在CPU上优化的代码比使用GPU的Tensorflow运行得更快（1秒vs 7秒）。</p><p>  The first step is to measure everything.On Linux, Python’s  time.time() provides a high resolution timer.Timing information will help direct your efforts.</p><p>第一步是衡量一切。在Linux上，是Python的时代。time（）提供高分辨率计时器。时间信息将有助于指导你的工作。</p><p> In a neural network, most of the time will be spent with large matrix multiplications.</p><p>在神经网络中，大部分时间都花在大型矩阵乘法上。</p><p> Extremely complex element-wise operations (such as chains of sigmoids) may have neglible performance impact when compared to a slow matrix multiplication.Until you measure the performance of each step in your algorithm, you don’t know what is affecting performance.</p><p>与缓慢的矩阵乘法相比，极其复杂的元素操作（如Sigmoid链）可能会对性能产生不可忽视的影响。在测量算法中每一步的性能之前，你不知道什么会影响性能。</p><p>  Ensure your arrays have a  dtype of  numpy.float32, rather than the default  numpy.float64. I’ve seen this have a four-fold improvement or more.</p><p>确保阵列的数据类型为numpy。float32，而不是默认的numpy。64。我已经看到这有四倍或更多的改善。</p><p> (It may be tempting to try further reductions to  numpy.float16, but this backfires because the CPU and BLAS libraries do not work natively at this precision.)</p><p>（尝试进一步缩减到numpy.float16可能很有诱惑力，但这会适得其反，因为CPU和BLAS库本机无法以这种精度工作。）</p><p>  BLAS is a high-performance matrix library.Even though NumPy uses BLAS, I&#39;ve noticed performance can be improved by calling BLAS directly.Perhaps this is simply because using direct calls to BLAS forces you to shape your data ready for use with BLAS.</p><p>BLAS是一个高性能矩阵库。尽管NumPy使用BLAS，但我&#39；我注意到直接调用BLAS可以提高性能。也许这仅仅是因为直接调用BLAS会迫使您调整数据，以便与BLAS一起使用。</p><p>    An easy way to check is to look at your CPU usage (e.g., with  top).If your matrix multiplications are using a single core, then you may be using a slow BLAS.You can get over 2x performance improvements by using a multi-core BLAS library such as OpenBLAS or Intel MKL.</p><p>一个简单的检查方法是查看您的CPU使用情况（例如，使用top）。如果你的矩阵乘法使用的是单核，那么你可能使用的是慢BLAS。通过使用多核BLAS库（如OpenBLAS或英特尔MKL），您可以获得2倍以上的性能改进。</p><p>  Check whether values in your matrices are stored in column major ( order = &#39;F&#39;) or row major order ( order = &#39;C&#39;).</p><p>检查矩阵中的值是否存储在主列中（顺序=&#39；F&#39；）或行主顺序（顺序=&#39；C&#39；）。</p><p> You can check the order by examining the  .flags property of your matrix (pay attention to the  C_CONTIGUOUS and  F_CONTIGUOUS flags).</p><p>你可以通过检查菜单来检查订单。矩阵的flags属性（注意C_连续和F_连续标志）。</p><p>  &gt; a = np.array([[1,2],[3,4]])&gt; a.flagsC_CONTIGUOUS : TrueF_CONTIGUOUS : FalseOWNDATA : TrueWRITEABLE : TrueALIGNED : TrueWRITEBACKIFCOPY : FalseUPDATEIFCOPY : False</p><p>&gt；a=np。数组（[[1,2]，[3,4]]）&gt；a、 flagsC_contracting:TrueF_contracting:FalseOWNDATA:TrueWRITEABLE:TrueALIGNED:TrueWRITEBACKIFCOPY:FalseUPDATEIFCOPY:False</p><p> You can change the order by copying your matrix ( numpy.copy(..., order=&#39;F&#39;)) or by a transpose.</p><p>您可以通过复制矩阵来更改顺序（numpy.copy（…，order=&#39；F&#39；）或者通过转置。</p><p> Changing the order of your matrices can improve performance (BLAS typically works better with column major order).</p><p>更改矩阵的顺序可以提高性能（BLAS通常更适合列主顺序）。</p><p> Hint: C and F stand for the orders used in the C and Fortran programming languages. BLAS prefers F because BLAS is written in Fortran.</p><p>提示：C和F代表C和Fortran编程语言中使用的顺序。BLAS更喜欢F，因为BLAS是用Fortran编写的。</p><p>   Each iteration in a recurrent neural networks including LSTMs combines input with the output state of the previous iteration.</p><p>在包含LSTMs的递归神经网络中，每次迭代都将前一次迭代的输入和输出状态结合起来。</p><p>  i.e., rather than  matmul(M, concatenate(x[i], h)), the matrix  M can be factored out into  M_x and  M_h.</p><p>i、 例如，矩阵M可以分解为M_x和M_h，而不是matmul（M，concatenate（x[i]，h））。</p><p> This means that  temp = matmul(M_x, x[:]) can be precomputed in single large batch, then the iterative computation becomes  temp[i] + matmul(M_h, h).</p><p>这意味着temp=matmul（M_x，x[：]）可以在单个大批量中预计算，然后迭代计算变成temp[i]+matmul（M_h，h）。</p><p>    This can be exploited, for example by using the sparse vector  v as a filter for the columns of a matrix:  M[:,v &gt; 0.001].</p><p>例如，可以利用稀疏向量v作为矩阵M[：，v&gt；0.001]列的过滤器。</p><p>   A large matrix can be approximated by computing the Singular Value Decomposition (SVD).Computing an SVD is too slow to be done online.However, if one of your matrices is constant, then ‘precomputation’ can pay off.</p><p>大型矩阵可以通过计算奇异值分解（SVD）来近似。计算SVD的速度太慢，无法在线完成。然而，如果你的一个矩阵是常数，那么“预计算”可以得到回报。</p><p>   U, s, V = numpy.linalg.svd(A) # Very slow, so precompute!rank = len(s) / 3 # Compression by a factor of 3y = matmul(V[:rank,:],x)y *= s[:rank]y = matmul(U[:,:rank], y)</p><p>U、 s，V=numpy。利纳格。svd（A）#非常慢，所以要预计算！秩=len（s）/3#压缩系数为3y=matmul（V[：秩，：]，x）y*=s[：秩]y=matmul（U[：，：秩]，y）</p><p> Hint: precompute and copy the views  V[:rank,:],  s[:rank], and  U[:,:rank] and don&#39;t forget to cast the SVD down to  float32.</p><p>提示：预先计算并复制视图V[：rank，：]，s[：rank]，以及U[：，：rank]和don&#39；别忘了把SVD降到32。</p><p> Reducing a single 2000x2000 matrix multiplication to a 100x2000 followed by a 2000x100 multiplication (for example) can make a big difference!</p><p>将单个2000x2000矩阵乘法减少到100x2000，然后再进行2000x100乘法（例如），可以产生很大的不同！</p><p> I&#39;ve found that reducing the rank of a matrix by a third or more can have negligible impact on the accuracy of a RNN yet produce dramatic improvements in performance.</p><p>我&#39；我们发现，将矩阵的秩减少三分之一或更多，对RNN的精度影响可以忽略不计，但会显著提高性能。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/矩阵/">#矩阵</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/matrix/">#matrix</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>