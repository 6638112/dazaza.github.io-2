<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>#神经网络</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1>#神经网络</h1><div class="row"><div class="col-lg-8 col-12"><div id="list"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1066862.html">通过第一原理对深神经网络的理解推进AI理论 </a></div><div class="item_title_en"><a target="_blank" href="https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/">Advancing AI theory with first-principles understanding of deep neural networks</a><span>(ai.facebook.com)</span></div><span class="my_story_list_date">2021-6-19 19:1</span><div class="my_story_list_item_desc">此请求存在问题。 我们＆＃39;重新努力尽快修复它。 </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ai/">#ai</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/存在/">#存在</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1065853.html">神经网络：LOVE：AI驱动的高清肖像发生器 </a></div><div class="item_title_en"><a target="_blank" href="https://neural.love/portraits">Neural.love: An AI-powered HD portrait generator</a><span>(neural.love)</span></div><span class="my_story_list_date">2021-6-15 17:4</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1065853.html"><img src="http://img2.diglog.com/img/2021/6/thumb_6d9bff3baf32c5763542c5e6ccaffda1.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">我们的自动化机器学习杰作可以从任何时代增强和恢复数字媒体。 上传自己的视频，并为自己看。 </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/love/">#love</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ai/">#ai</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/视频/">#视频</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1064813.html">Hernandez的法律：神经网络如何超越摩尔定律 </a></div><div class="item_title_en"><a target="_blank" href="https://arxiv.org/abs/2005.04305">Hernandez' Law: how neural networks have outpaced Moore's Law</a><span>(arxiv.org)</span></div><span class="my_story_list_date">2021-6-10 19:54</span><div class="my_story_list_item_desc">下载PDF摘要：三个因素驱动AI的进展：算法创新，数据和培训的计算。算法的进度比计算和数据量更难以量化。在这方面，我们认为算法进步有一个方面是既既衡量和有趣的方面，可以减少计算，以达到过去的能力。我们表明，培训分类器所需的浮点数为亚历克莱级性能OnimAgeNet的数量在2012年和2019年之间的44倍下降了44倍......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/法律/">#法律</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/law/">#law</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1064699.html">什么是变形金刚神经网络？ </a></div><div class="item_title_en"><a target="_blank" href="https://www.youtube.com/watch?v=XSSTuhyAmnI">What Are Transformer Neural Networks?</a><span>(www.youtube.com)</span></div><span class="my_story_list_date">2021-6-10 5:50</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1064699.html"><img src="http://img2.diglog.com/img/2021/6/thumb_453789a1ae0f9ac5f275652cd4cd2d85.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">这条短教程涵盖了变压器的基础知识，这是一种用于在机器学习中处理顺序数据的神经网络架构.Timestamps：... </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/变形金刚/">#变形金刚</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/机器/">#机器</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1063380.html">预测编码可以在任何神经网络上进行精确的备份 </a></div><div class="item_title_en"><a target="_blank" href="https://arxiv.org/abs/2103.04689">Predictive Coding Can Do Exact Backpropagation on Any Neural Network</a><span>(arxiv.org)</span></div><span class="my_story_list_date">2021-6-4 5:44</span><div class="my_story_list_item_desc">下载PDF摘要：与神经科学和深度学习相交，为两个领域带来了福利和发展，几十年来，这有助于了解大脑中的学习作品，并在不同的AI基准中实现最先进的艺术形象。 Backpropagation（BP）是人工神经网络训练的最完整采用的方法，但是，由于其生物言言，通常批评（例如，缺乏参数的缺乏局部更新规则）。因此，依赖于预测编码......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/编码/">#编码</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/coding/">#coding</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/bp/">#bp</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1063046.html">尖峰神经网络中的精确渐变基于事件的俯视验证 </a></div><div class="item_title_en"><a target="_blank" href="https://arxiv.org/abs/2009.08378">Event-based backpropagation for exact gradients in spiking neural networks</a><span>(arxiv.org)</span></div><span class="my_story_list_date">2021-6-3 23:14</span><div class="my_story_list_item_desc">下载PDF摘要：尖峰神经网络将模拟计算与基于事件的基于Comcomication使用离散尖峰相结合。虽然通过使用Backpradogation算法训练非尖峰人工神经网络的令人印象深刻的防范的进步，但将该算法应用于尖峰网络，因此是通过离散的尖峰事件和分解的不存在性的存在。首次，这项工作通过将伴随方法与适当的部分移动性跳......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/俯视/">#俯视</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/based/">#based</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1061411.html">研究人员使用了基于神经网络的ML算法来升级低至超级分辨率的宇宙学习，有助于加速他们的研究 </a></div><div class="item_title_en"><a target="_blank" href="https://www.cmu.edu/ai-physics-institute/news/2021-05-05_supersims.html">Researchers used an ML algorithm based on neural networks to upgrade a cosmological simulation from low to super resolution, helping to accelerate their studies</a><span>(www.cmu.edu)</span></div><span class="my_story_list_date">2021-5-8 23:13</span><div class="my_story_list_item_desc">宇宙在数十亿年内发展了数十亿，但研究人员已经开发了一种在不到一天内创建复杂的模拟宇宙的方法。本周发表的技术与国家科学院讨论，汇集了机器学习，高性能计算和天体物理学，并有助于迎来新的高分辨率宇宙学模拟时代。
 宇宙模拟是挑选宇宙的许多奥秘的重要组成部分，包括暗物质和黑暗能量。但到目前为止，研究人员面临着无法实现的常见难......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ml/">#ml</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模拟/">#模拟</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1061201.html">研究人员详细介绍了一种新型攻击，可以通过向网络输入增加少量噪声来提高神经网络的能量消耗 </a></div><div class="item_title_en"><a target="_blank" href="https://www.technologyreview.com/2021/05/06/1024654/ai-energy-hack-adversarial-attack/">Researchers detail a new type of attack that could increase the energy consumption of neural networks by adding small amounts of noise to a network's inputs</a><span>(www.technologyreview.com)</span></div><span class="my_story_list_date">2021-5-7 7:9</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1061201.html"><img src="http://img2.diglog.com/img/2021/5/thumb_8b8e94ef936e768e8fbc14fdeba5d4f8.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">新闻：一种新型的攻击可以提高AI系统的能量消耗。以同样的方式对互联网的拒绝服务攻击寻求堵塞网络并使它无法使用，新攻击强制了深度神经网络，以符合必要的计算资源，并减慢其“思维”过程。
  目标：近年来，对大型AI型号的昂贵能源消耗的越来越关注导致研究人员设计更高效的神经网络。一个类别，称为自适应多退出架构，通过根据它们......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/网络/">#网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/detail/">#detail</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1057190.html">基于CPU的算法将深度神经网络培训高达15倍，而不是顶部GPU </a></div><div class="item_title_en"><a target="_blank" href="https://techxplore.com/news/2021-04-rice-intel-optimize-ai-commodity.html">CPU-based algorithm trains deep neural nets up to 15 times faster than top GPU</a><span>(techxplore.com)</span></div><span class="my_story_list_date">2021-4-9 9:1</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1057190.html"><img src="http://img2.diglog.com/img/2021/4/thumb_ad52d6079c2dfb26104f0a41cb08ed9b.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">赖斯大学计算机科学家展示了在商品处理器上运行的人工智能（AI）软件，并在基于图形处理器的平台上速度快15次培训深度的神经网络。 ＆＃34;培训成本是AI，＆＃34的实际瓶颈; Anshumali Shrivastava是米饭＆＃39;布朗工程学院的计算机科学助理教授。 ＆＃34;公司每周花费数百万美元才能训练和微调他......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/faster/">#faster</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ai/">#ai</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1057068.html">AXON：在Elixir中创建神经网络的库 </a></div><div class="item_title_en"><a target="_blank" href="https://github.com/elixir-nx/axon">Axon: A library for creating neural networks in Elixir</a><span>(github.com)</span></div><span class="my_story_list_date">2021-4-9 0:7</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1057068.html"><img src="http://img2.diglog.com/img/2021/4/thumb_6a6b1eec0de5a63aba2179183e85ac25.png" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">功能API  - 所有其他API构建的数字定义的低级API（DEFN）。
 模型创建API  - 管理模型初始化和应用程序的高级模型创建API。
 Axon提供抽象，可轻松集成，同时保持每个组件之间的分离级别。您应该能够在没有依赖性的情况下使用任何API。通过解耦API，Axon可以完全控制创建和培训神经网络的每个方......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/elixir/">#elixir</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/library/">#library</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/axon/">#axon</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1056427.html">UCLA研究人员创造了全光衍射深神经网络 </a></div><div class="item_title_en"><a target="_blank" href="https://www.photonics.com/Articles/UCLA_Researchers_Create_All-Optical_Diffractive/a63751">UCLA Researchers Create All-Optical Diffractive Deep Neural Network</a><span>(www.photonics.com)</span></div><span class="my_story_list_date">2021-4-6 12:34</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1056427.html"><img src="http://img2.diglog.com/img/2021/4/thumb_a6ab9048c296a4e0a48511485635493f.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">使用3D打印机，UCLA Samueli工程学院的一支研究团队创造了一个人工神经网络，可以分析大量数据并以光速识别对象。该技术使用来自物体的光散射来识别它来识别衍射深度神经网络（D2NN）。该技术基于基于深度学习的被动衍射层的设计，其共同工作。该团队创建了一种计算机模拟的设计，然后使用3D打印机创建薄，8厘米-SQ聚......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/研究/">#研究</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1056229.html">深度神经网络是显着的过度收回吗？ </a></div><div class="item_title_en"><a target="_blank" href="https://lilianweng.github.io/lil-log/2019/03/14/are-deep-neural-networks-dramatically-overfitted.html">Are Deep Neural Networks Dramatically Overfitted?</a><span>(lilianweng.github.io)</span></div><span class="my_story_list_date">2021-4-5 15:31</span><div class="my_story_list_item_desc">如果您就像我一样，困惑的是为什么深神经网络可以概括到样本数据点而无剧烈过度装备，请继续阅读。
  如果你就像我一样，在传统机器学习的经验中进入深度学习的领域，你可能经常在这个问题上思考：由于典型的深神经网络有这么多参数和训练错误，可以很容易地完美，它肯定会受到影响从实质上的过度装备。如何概括为样本数据点？
 理解为什......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1053558.html">Freewire：一种自由有线神经网络的实验 </a></div><div class="item_title_en"><a target="_blank" href="https://github.com/noahtren/Freewire">Freewire: An Experiment with Freely Wired Neural Networks</a><span>(github.com)</span></div><span class="my_story_list_date">2021-3-20 9:19</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1053558.html"><img src="http://img2.diglog.com/img/2021/3/thumb_dbe7cc0a39972eceb42b4a4e6e7a6934.jpeg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">Freewire是一种类似KERAS的API，用于创建优化的可自由有线的神经网络，到史隆CUDA。自由有线的神经网络在各个节点（或神经元）的水平上定义，而不是在均匀层的水平。Freewire的目标是使其成为人工神经元的任何任意DAG定义为首先定义可以在CUDA上运行RunTimeand编译优化的操作集。
 该存储库是......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/节点/">#节点</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1052311.html">地球物理学家的神经网络与地震数据解释的应用 </a></div><div class="item_title_en"><a target="_blank" href="https://arxiv.org/abs/1903.11215">Neural networks for geophysicists and application to seismic data interpretation</a><span>(arxiv.org)</span></div><span class="my_story_list_date">2021-3-14 18:37</span><div class="my_story_list_item_desc">下载PDF摘要：在过去几年中，神经网络已经对抗震诠释的兴趣激增。基于网络的学习方法难以快速准确的自动解释，提供了许多训练标签。我们在熟悉前向模型和反转框架的地球物理学家介绍了Fieldaimed。我们解释了深度网络与其他地球物理逆问题之间的相似性和差异，并展示了他们的井层，地震图像之间岩性插值等岩性插值。我们的方法对......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/物理学家/">#物理学家</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/networks/">#networks</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/arxivlabs/">#arxivlabs</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1051430.html">通过稀疏推理加速在移动设备和网络上的神经网络 </a></div><div class="item_title_en"><a target="_blank" href="https://ai.googleblog.com/2021/03/accelerating-neural-networks-on-mobile.html">Accelerating Neural Networks on Mobile and Web with Sparse Inference</a><span>(ai.googleblog.com)</span></div><span class="my_story_list_date">2021-3-11 0:36</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1051430.html"><img src="http://img2.diglog.com/img/2021/3/thumb_52b03383f830f67c707d01d620371549.png" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">通过稀疏推理加速在移动设备和网络上的神经网络 </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/加速/">#加速</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1049603.html">Geoff Hinton 2021 –如何在神经网络中表示部分整体层次结构 </a></div><div class="item_title_en"><a target="_blank" href="https://arxiv.org/abs/2102.12627">Geoff Hinton 2021 – How to represent part-whole hierarchies in a neural network</a><span>(arxiv.org)</span></div><span class="my_story_list_date">2021-2-26 17:6</span><div class="my_story_list_item_desc">下载PDF摘要：本文未描述工作系统。取而代之的是，它提出了一个关于表示的想法，该想法使几个不同的群体所取得的进步可以组合成一个称为GLOM的假想系统。这些进步包括变压器，神经场，对比表示学习，蒸馏和胶囊。 GLOM回答了这个问题：具有固定体系结构的神经网络如何将图像解析为部分整体的层次结构，而每个部分的结构都具有不同......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/部分/">#部分</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/part/">#part</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/表示/">#表示</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1048666.html">人工神经网络最终为大脑学习提供了线索 </a></div><div class="item_title_en"><a target="_blank" href="https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/">Artificial Neural Nets Finally Yield Clues to How Brains Learn</a><span>(www.quantamagazine.org)</span></div><span class="my_story_list_date">2021-2-21 7:51</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1048666.html"><img src="http://img2.diglog.com/img/2021/2/thumb_e2c55484d9e80631288d70c54a7b46a8.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">在2007年，深层神经网络背后的一些主要思想家在著名的人工智能年度会议的间隙组织了一次非正式的“卫星”会议。会议拒绝了他们举行正式讲习班的要求；深度神经网络距离接管人工智能还需要几年的时间。盗版会议的最终发言人是多伦多大学的杰弗里·欣顿（Geoffrey Hinton），他是认知心理学家和计算机科学家，负责深网领域的......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经元/">#神经元</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1046408.html">麻省理工学院的研究人员开发了一种新的“液态”神经网络，该网络更能适应新信息 </a></div><div class="item_title_en"><a target="_blank" href="https://techcrunch.com/2021/01/28/mit-researchers-develop-a-new-liquid-neural-network-thats-better-at-adapting-to-new-info/">
				MIT researchers develop a new ‘liquid’ neural network that’s better at adapting to new info			</a><span>(techcrunch.com)</span></div><span class="my_story_list_date">2021-1-28 23:46</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1046408.html"><img src="http://img2.diglog.com/img/2021/1/thumb_317f246d11fd5d36288c9d819adaf92d.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">通常，在训练阶段之后，在此阶段中，向神经网络算法提供了大量相关的目标数据以磨练其推理能力，并奖励正确的响应以优化性能，因此它们基本上是固定的。但是，哈萨尼（Hasani）的团队开发了一种方法，通过这种方法，他的“液体”神经网络可以随着时间的流逝而适应新信息，以适应“成功”的参数，这意味着，如果要在自动驾驶汽车上感知的......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/开发/">#开发</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1046237.html">如果忆阻器的行为像神经元，则将它们放入神经网络 </a></div><div class="item_title_en"><a target="_blank" href="https://spectrum.ieee.org/tech-talk/robotics/artificial-intelligence/memristor-random">If memristors act like neurons, put them in neural networks</a><span>(spectrum.ieee.org)</span></div><span class="my_story_list_date">2021-1-28 22:30</span><div class="my_story_list_item_desc">创造  帐户 </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/阻器/">#阻器</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/act/">#act</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/创造/">#创造</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1046058.html">为什么使用Android的神经网络API有时2乘3有时等于7？ </a></div><div class="item_title_en"><a target="_blank" href="http://alexanderganderson.github.io/engineering/2021/01/23/integer_indeterminism.html">Why can 2 times 3 sometimes equal 7 with Android's Neural Network API?</a><span>(alexanderganderson.github.io)</span></div><span class="my_story_list_date">2021-1-24 4:18</span><div class="my_story_list_item_desc">三分之二等于六，或者至少是我天真的期望。
 众所周知，浮点矩阵乘法可能会导致各种意外情况，但您是否知道量化神经网络也不是确定性的？量化神经网络使用整数将神经网络中的权重和激活值相乘。幼稚的逻辑说，两个和三个表示并乘以整数的乘积始终应得出完全相同的结果，即六个。
 但是，根据Android NNAPI驱动程序验证文档，......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/android/">#android</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/equal/">#equal</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1046027.html">为神经网络通用理论构建的基础（2019） </a></div><div class="item_title_en"><a target="_blank" href="https://www.quantamagazine.org/foundations-built-for-a-general-theory-of-neural-networks-20190131/">Foundations Built for a General Theory of Neural Networks (2019)</a><span>(www.quantamagazine.org)</span></div><span class="my_story_list_date">2021-1-24 3:38</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1046027.html"><img src="http://img2.diglog.com/img/2021/1/thumb_cf8c135f0462753e65a7d5bf04d5322e.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">当我们设计摩天大楼时，我们希望它会达到规格：该塔将承受如此之大的重量，并能够承受一定强度的地震。
 但是，借助现代世界上最重要的技术之一，我们正在有效地建立盲目地位。我们采用不同的设计，采用不同的设置，但是直到我们将其取出进行测试运行之前，我们才真正知道它可以做什么或将在哪里失败。
 这项技术就是神经网络，它是当今最......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/通用/">#通用</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/built/">#built</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1044723.html">在水手游戏上训练的类人神经网络国际象棋引擎 </a></div><div class="item_title_en"><a target="_blank" href="https://maiachess.com/">Human-like neural network chess engine trained on lichess games</a><span>(maiachess.com)</span></div><span class="my_story_list_date">2021-1-17 22:6</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1044723.html"><img src="http://img2.diglog.com/img/2021/1/thumb_8a999583a5d03c50c33d2464bc592ddb.png" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">阅读有关Maia的完整研究论文，该论文发表在2020 ACM SIGKDD知识发现和数据挖掘国际会议（KDD 2020）上。
  您可以从计算社会科学实验室或Microsoft Research阅读有关Maia的博客文章。
  我们将发布基于Maia（游戏分析，个性化拼图，图灵测试等）的学习工具，教具和实验的Beta......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/类人/">#类人</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/人类/">#人类</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1043403.html">OpenAI多模式研究 </a></div><div class="item_title_en"><a target="_blank" href="https://openai.com/blog/tags/multimodal/">OpenAI Multimodal Research</a><span>(openai.com)</span></div><span class="my_story_list_date">2021-1-6 21:18</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1043403.html"><img src="http://img2.diglog.com/img/2021/1/thumb_9391322688e1b978cbf8e2bd9254eb7c.png" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">人工智能的长期目标是建立“多模式”神经网络，即AI系统，该系统学习几种模式（主要是文本和视觉域）中的概念，以便更好地了解世界。在我们最新的研究公告中，我们提出了两个神经网络，使我们更接近这一目标。
 第一个神经网络DALL·E可以成功地将文本转换为适合自然语言表达的各种概念的适当图像。 DALL·E使用与GPT-3相......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/模式/">#模式</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1043215.html">为什么我对图神经网络不满意 </a></div><div class="item_title_en"><a target="_blank" href="https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/">Why I'm Lukewarm on Graph Neural Networks</a><span>(www.singlelunch.com)</span></div><span class="my_story_list_date">2021-1-5 0:10</span><div class="my_story_list_item_desc">请通过webmaster@singlelunch.com与服务器管理员联系，以告知他们该错误发生的时间以及您在该错误发生之前执行的操作。  此外，尝试使用ErrorDocument处理请求时遇到500内部服务器错误错误。 </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/graph/">#graph</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/错误/">#错误</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1041810.html">将Quantum BlockChain用于第六代神经网络 </a></div><div class="item_title_en"><a target="_blank" href="https://shkspr.mobi/blog/2020/12/utilizing-quantum-blockchain-for-6th-generation-neural-networks/">Utilizing Quantum BlockChain for 6th Generation Neural Networks</a><span>(shkspr.mobi)</span></div><span class="my_story_list_date">2020-12-26 1:55</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1041810.html"><img src="http://img2.diglog.com/img/2020/12/thumb_5734ff41ff50719b42e7e8a693380fb2.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">Terence Eden的定期博客。 </div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/quantum/">#quantum</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/eden/">#eden</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1040878.html">用Java构建简单的神经网络 </a></div><div class="item_title_en"><a target="_blank" href="https://smalldata.tech/blog/2016/05/03/building-a-simple-neural-net-in-java">Building a simple neural net in Java</a><span>(smalldata.tech)</span></div><span class="my_story_list_date">2020-12-20 21:19</span><div class="my_story_list_item_desc">在这篇文章中，我们将通过一些简单的步骤来解决人工智能问题，并尝试在Java中构建一个非常简单的神经网络。
    神经网络是大脑工作方式的软件表示。不幸的是，我们还不知道大脑到底是如何工作的，但是我们确实知道该过程背后的生物学原理：人脑由1000亿个称为神经元的细胞组成，它们通过突触连接在一起。如果有足够的突触与神经......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/java/">#java</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/simple/">#simple</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/输入/">#输入</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1038556.html">神经网络从其执行轨迹的图像中模拟排序算法 </a></div><div class="item_title_en"><a target="_blank" href="https://www.sciencedirect.com/science/article/abs/pii/S0950584920301178">Neural nets emulate sorting algorithms from images of their execution traces</a><span>(www.sciencedirect.com)</span></div><span class="my_story_list_date">2020-12-9 19:42</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1038556.html"><img src="http://img2.diglog.com/img/2020/12/thumb_97ec77a7f52e135ed9999f163e387ed6.gif" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">神经网络在诸如计算机视觉，自然语言处理等各个领域的适用性方面的最新进展，引起了人们对程序归纳方法的兴趣。 （Kitzelman [1]，Gulwani等人[2]或Kant [3]。）
 执行程序归纳任务时，在所有可能将输入映射到输出的可能程序中搜索是不可行的，因为可能的指令组合或指令序列的数量过多：至少基于生成的程序......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/排序/">#排序</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/nets/">#nets</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1038202.html">了解各个单元在深度神经网络中的作用 </a></div><div class="item_title_en"><a target="_blank" href="https://www.pnas.org/content/117/48/30071">Understanding the role of individual units in a deep neural network</a><span>(www.pnas.org)</span></div><span class="my_story_list_date">2020-12-7 17:17</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1038202.html"><img src="http://img2.diglog.com/img/2020/12/thumb_0cf69263c50045a3bf3c1bfe74444c8f.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">由加利福尼亚州斯坦福市斯坦福大学的David L.Donoho编辑，并于2020年7月7日批准（已于2019年8月31日接受审查）
    深度神经网络擅长寻找解决大型数据集上复杂任务的分层表示形式。我们人类如何理解这些学习的表征？在这项工作中，我们介绍了网络解剖，这是一个分析框架，可以系统地识别图像分类和图像生成网......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/单元/">#单元</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/role/">#role</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/网络/">#网络</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1036269.html">您可能不需要机器学习</a></div><div class="item_title_en"><a target="_blank" href="https://nullprogram.com/blog/2020/11/24/">You might not need machine learning</a><span>(nullprogram.com)</span></div><span class="my_story_list_date">2020-11-24 17:20</span><div class="my_story_list_item_desc">机器学习是一个热门话题，因此自然而然地将它用于不合适的目的，在这些目的中，更简单，更有效，更可靠的解决方案就足够了。前几天，我看到了一个说明性和有趣的示例：神经网络汽车和遗传算法。该视频演示了由神经网络驱动的2D汽车，其权重由通用算法确定。但是，整个方案可以用一阶多项式代替，而不会损失任何功能。机器学习部分过于严格。......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/机器/">#机器</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/machine/">#machine</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button></div></div><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded clearfix"><div class="item_title"><a target="_blank" href="/story/1035609.html">神经网络学习何时不应该信任它</a></div><div class="item_title_en"><a target="_blank" href="https://news.mit.edu/2020/neural-network-uncertainty-1120">A neural network learns when it should not be trusted</a><span>(news.mit.edu)</span></div><span class="my_story_list_date">2020-11-21 19:10</span><div class="float-md-right my_story_img_thumb"><a target="_blank" href="/story/1035609.html"><img src="http://img2.diglog.com/img/2020/11/thumb_296279d15b07d5930f984313b788802f.jpg" class="img-fluid my_story_img_thumb" onerror="this.style.display='none'"></a></div><div class="my_story_list_item_desc">越来越多地，被称为深度学习神经网络的人工智能系统用于告知对人类健康和安全至关重要的决策，例如在自动驾驶或医疗诊断中。这些网络擅长识别大型复杂数据集中的模式以帮助决策。但是我们怎么知道它们是正确的？亚历山大·阿米尼（Alexander Amini）和他在麻省理工学院和哈佛大学的同事希望找出答案。
他们为神经网络开发了一......</div><div class="my_item_tag_container"><button type="button" class="btn btn-light my_tag"><a href="/tag/rust/">#rust</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/network/">#network</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/网络/">#网络</a></button></div></div></div><div id="my_pager"><ul class="pagination justify-content-center"><li class="page-item active"><a class="page-link" href="/tag/神经网络/">0</a></li><li class="page-item"><a class="page-link" href="/tag/神经网络/page1.html">1</a></li><li class="page-item"><a class="page-link" href="/tag/神经网络/page2.html">2</a></li></ul></div></div><div class="col-lg-4 col-0"><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/rust/">#rust</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/数据/">#数据</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/warning/">#warning</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/代码/">#代码</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/ai/">#ai</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/学习/">#学习</a></button></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>