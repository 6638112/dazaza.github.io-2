<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Memento Time Travel：用Wayback机器刮擦存档数据 Memento Time Travel: Scraping Archived Data with the Wayback Machine</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Memento Time Travel: Scraping Archived Data with the Wayback Machine<br/>Memento Time Travel：用Wayback机器刮擦存档数据 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-15 23:55:40</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/138669c596734b6bd937fc8ee336aac0.png"><img src="http://img2.diglog.com/img/2021/6/138669c596734b6bd937fc8ee336aac0.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In a  previous article, I wrote about the possibilities of the  Wayback Machine for scientific writing. I argued that archiving web pages are essential for references as they prevent link rots when cited web resources are not available anymore. With this blog entry, I am looking into the reverse option: Finding and retrieving archived web pages for research reasons.</p><p>在上一篇文章中，我写了关于科学写作的首场机的可能性。我认为归档网页对于引用是必不可少的，因为它们阻止在引用的Web资源时不再使用链接rots。使用此博客条目，我正在研究反向选项：寻找和检索存档的网页以获取研究原因。</p><p> Archived web pages as permanently stored data are indispensable for reproducibility issues. But they are also valuable research resources as they are data for historical and comparative research.</p><p> 归档的网页作为永久存储的数据对于可重复性问题是必不可少的。但他们也是历史和比较研究的数据的宝贵研究资源。</p><p> This article rewrites my blog entry from 2019 on  Scraping Archived Data with the Wayback Machine in three aspects:</p><p> 本文从2019年重写我的博客条目，在三个方面用Wayback机器刮掉归档数据：</p><p> I am going into more detail to explain the Memento Protocol that stands behind the archiving procedures of the Internet Archive with the Wayback Machine.</p><p> 我将详细介绍，解释纪念协议，掌握互联网档案的归档程序与Wayback机器。</p><p> I will demonstrate the research significance of archived data with another — more simple — example. Instead, to analyze yearly differences in the ranking of the popularity of static site generators, I will focus on the number of statistical packages developed by the R community over time. The website structure for static site generators is complex, has changed several times, and contradicts the demonstration purpose.  1</p><p> 我将展示归档数据与另一个 - 更简单的研究的研究意义 - 更简单。相反，分析静态站点生成器普及排名中的年度差异，我将专注于R社区随着时间的推移开发的统计包的数量。静态站点发生器的网站结构复杂多次，并与演示目的相矛盾。 1</p><p> To argue the importance of archived data convincingly, it is necessary to extract the URLs of archived data and use these data for research issues. Therefore, I will show how R can scrape web data and display the data for further analysis.</p><p> 为了令人信服地争论存档数据的重要性，有必要提取存档数据的URL并使用这些数据进行研究问题。因此，我将展示R可以刮擦Web数据并显示数据以进行进一步分析。</p><p> Mementos are prior versions of web pages cached by web crawlers and stored in web archives. The HTTP-based Memento framework is a  description for Time-Based Access to Resource States.</p><p> MEMEMES是由Web爬网站缓存并存储在Web档案中的网页的先前版本。基于HTTP的MEMEMENO框架是对资源状态的基于时间访问的描述。 </p><p> With Mementos, you can access a version of a Web resource as it existed at some date in the past. The complete information about the  Memento Project is specified in RFC 7089 as  HTTP Framework for Time-Based Access to Resource States – Memento. I will explain the essential idea with the gentle  non-technical introduction on the Memento website.</p><p>使用MEMEMES，您可以访问过去某些日期存在的Web资源版本。关于Memento项目的完整信息在RFC 7089中指定为基于时间的资源状态的基于时间访问的HTTP框架 -  Memento。我将在Memento网站上使用温和的非技术介绍来解释必要的想法。</p><p>  Original Resource (URI-R): A Web resource that exists or used to exist on the live Web for which we want to find a prior version.</p><p>  原始资源（URI-R）：存在或用于存在于实时网络上的Web资源，我们要找到先前版本。</p><p> Memento (URI-M): A Web resource that is a prior version of the Original Resource. Prior versions are Web resources encapsulated in what the Original Resource was like at some time in the past.</p><p> Memento（Uri-M）：一个Web资源，它是原始资源的先前版本。先前版本是封装在过去的原始资源的Web资源中。</p><p> TimeGate (URI-G): A Web resource that “decides” on the basis of a given datetime, which Memento best matches what the Original Resource was like around that given datetime.</p><p> TimeGate（Uri-G）：“基于给定的DateTime”决定“的Web资源，Memento最佳匹配原始资源在给定的DateTime的情况下。</p><p> TimeMap (URI-T): A list of URIs of Mementos of the Original Resource that is archived, e.g., available online.</p><p> TimeMap（Uri-T）：归档的原始资源的纪据课题列表，例如，在线提供。</p><p> The central component is the TimeMap Resource. It is a machine-readable document that lists the Original Resource itself, its TimeGate, and its Mementos, as well as associated metadata such as archival DateTime for Mementos.</p><p> 中央组件是TimeMAP资源。它是一种机器可读文档，列出了原始资源本身，它的时光及其纪据，以及相关的元数据，如纪据的档案数据项。</p><p> The HTTP-based Memento framework bridges the present and past Web. It facilitates obtaining representations of prior states of a given resource by introducing datetime negotiation and TimeMaps. Datetime negotiation is a variation on content negotiation that leverages the given resource’s URI and a user agent’s preferred datetime. TimeMaps are lists that enumerate URIs of resources that encapsulate prior states of the given resource. (Quoted from  RFC 7089)</p><p> 基于HTTP的Memento框架桥接当前和过去的网络。它促进通过介绍DateTime谈判和TimeMaps来获得给定资源的先前国家的代表。 DateTime Engotiation是内容协商的变化，它利用给定的资源的URI和用户代理的首选DATETIME。 TimeMaps列出了列出枚举封装给定资源的先前状态的资源URI。 （从RFC 7089引用） </p><p>  The question now arises if these pages are also available in the past. And if so: Have they the same structure to use the identical CSS selector in all archived instances?</p><p>如果过去也提供了这些页面，现在出现了问题。如果是这样：它们是否具有相同的结构在所有归档的实例中使用相同的CSS选择器？</p><p> The question now arises if this page is available also in the past. And if so: Has it the same structure to use the identical CSS selector in all prior archived instances?</p><p> 如果过去也可以使用此页面提供的问题。如果是这样：在所有先前的归档实例中使用相同的CSS选择器具有相同的结构？</p><p> It turns out that the first line after the subheading could be scraped with the CSS selector  #pkgs + p.</p><p> 事实证明，子标题之后的第一行可以用CSS选择器#PKGS + P刮擦。</p><p>   To check if the website structure for the paragraph after the header with id = “pkgs” remains constant, we can use the browser plugin for the Wayback Machine (available for  Google Chrome and  Firefox.).</p><p>   要检查段落的段落的网站结构是否具有ID =“PKGS”之后保持不变，我们可以使用WATBBACK计算机的浏览器插件（可用于Google Chrome和Firefox。）。</p><p>  If you click on the Wayback Machine plugin, you can either go directly to the first or last (recent) snapshot of this web page. I choose “Overview” to display the calendar to inspect different instances of the archived page.</p><p>  如果单击“中行机插件”，则可以直接转到此网页的第一个或最后一个（最近）快照。我选择“概述”以显示日历以检查已归档页面的不同实例。</p><p>  The calendar shows that the page was between May 14, 2008, and April 13, 2021, 145 times archived. From this overview page, one can select different instances and content and design of the ‘Contributed Packages’ page. To get an idea of possible changes in the structure, I start with the first instance of the archived page.</p><p>  该页面显示该页面是2008年5月14日之间，和4月13日，4月13日，145次归档。从此概述页面中，可以选择“贡献包”页面的不同实例和内容和设计。要了解结构中可能的更改，请从归档页面的第一个实例开始。</p><p> The number of times the page was crawled by the Wayback Machine (in my example: 145) has nothing to do with how often the page was updated.</p><p> 页面由Wayback Machine爬出的次数（在我的例子：145中）与页面更新的频率无关。 </p><p>  The archived page displays a different layout. The number of packages is not immediately after the first heading but after several paragraphs later (see number 2 in the image). Also, the name of the heading has changed from “Available Packages” to “Available Bundles and Packages”. But more importantly: The paragraph following immediately of this header mentions the amounts of different kinds of packages and bundles. It would be challenging to detect the desired number (in this case, 1425 packages) programmatically.</p><p>存档页面显示不同的布局。第一个标题后的包数不是在第一个标题之后立即，但在几个段落之后（见图像中的2号）。此外，标题的名称已从“可用软件包”更改为“可用捆绑包和包”。但更重要的是：本标题立即提出了段落的段落提出了不同种类的包和捆绑包的数量。检测以编程方式检测所需的数量（在本例，1425包）是挑战性的。</p><p> A further inspection shows that the id still remains “pkgs”. But because of the different text of the first paragraph, I want to look at another page to grab my desired information easier.</p><p> 进一步的检查表明，ID仍然是“PKGS”。但由于第一段的文本不同，我想看看另一个页面来抓住我想要的信息更容易。</p><p> At first, we have to install and load the  wayback package, a wonderful and very practical library of Memento API wrappers, written by  Bob Rudis.  2</p><p> 首先，我们必须安装和加载Wayback包，由Bob Rudis编写的Memento API包装器的精彩而非常实用的图书馆。 2</p><p> In the meanwhile there exists  another package with the same name on CRAN (Comprehensive R Archive Network). It is dedicated to the installation for legacy R versions and has nothing to do with the wayback package on GitHub for the Wayback Machine of the Internet Archive.</p><p> 同时，在CRAN上存在另一个具有相同名称的包（全面的R存档网络）。它专用于安装遗留R版本，与Internet存档的WATBBACK计算机上的WATSBACK包无关。</p><p> The next step is to use the  get_mementos() function. With  get_mementos(url, timestamp = format(Sys.Date(), &#34;%Y&#34;)) we will receive a shortlist of relevant links to the archived content. Only the first parameter,  url, is mandatory. If no timestamp is provided, then the actual year is taken, and the most recent archived page will be the endpoint, which in our case is ok. The function will return the 4 link relation types as in the  Request for Comment for the Memento framework described and  outlined above.</p><p> 下一步是使用get_mementos（）函数。使用get_mementos（URL，timestamp = format（sys.date（），＆＃34;％y＆＃34;））我们将收到归档内容的相关链接的候选名单。只有第一个参数URL是强制性的。如果没有提供时间戳，则采取实际年份，最新的归档页面将是端点，在我们的情况下是可以的。该函数将返回4个链接关系类型，如在上面描述和概述的Memento框架的评论请求中。</p><p>     Besides these 4 main types of link relations, the function also provides the first, previous, next, and last available memento. When no particular date is given, then the last memento is identical with the next (= nearest) memento. In addition to the two columns,  link and  rel, there is a third one,  ts, containing the timestamps (empty for the first 3 link relation types). The return value in total is a tibble with eight observations (rows) and three columns.  3</p><p>     除了这4个主要类型的链路关系之外，该函数还提供了第一个，上一个，下一个和最后一个可用的纪念品。当没有给出特定日期时，最后一个MEMEMENO与下一个（=最近的）纪念品相同。除了两个列，链路和Rel，还有第三，TS，包含时间戳（对于前3个链路关系类型为空）。总共返回值是具有八个观察（行）和三列的突然间。 3.</p><p> Providing an URL in the search field of the Wayback Machine results in the interactive browser version to the  calendar view as shown by  Figure 12 and  Figure 13. The dates with archived content are blue or green (= redirected URL) circles. The bigger the circles, the more snapshots were archived on these dates.</p><p> 在Warback计算机的搜索字段中提供一个URL，将在交互式浏览器版本到图12和图13所示的日历视图中。具有归档内容的日期是蓝色或绿色（=重定向的URL）圆圈。圆圈越大，这些日期存档了更多的快照。 </p><p> We get these dated crawl lists with the  get_timemap() function using the second observation of the result of the  get_mementos function. This is in our case  cran_link_types$link[2].</p><p>使用Met_MementOS函数结果的第二个观察，我们将这些日期爬网列表与Get_timemap（）函数进行了解。这是我们的案例cran_link_types $ link [2]。</p><p> The execution of the following code chunk can take some time, depending on how many pages of the URL are archived. Be aware that the Wayback server is strained by this query, so do not repeat this operation. I store the result on my hard disk and will use the saved data for further processing.</p><p> 执行以下代码块的执行可能需要一段时间，具体取决于归档的URL页数有多少。请注意，此查询中断WATBBACK SERVER，因此请勿重复此操作。我将结果存储在我的硬盘上，并将使用保存的数据进行进一步处理。</p><p>  cran_crawl_list &lt;- readRDS(&#34;data/cran_crawl_list.rds&#34;)reactable::reactable( cran_crawl_list, pagination = FALSE, highlight = TRUE, height = 500, compact = TRUE, bordered = TRUE, striped = TRUE, wrap = FALSE, resizable = TRUE )</p><p>  cran_crawl_list＆lt;  -  readrds（＆＃34; data / cran_crawl_list.rds＆＃34;）可反应的:: actable（cran_crawl_list，pagination = false，突出显示= true，height = 500，compact = true，tourded = true， Wrap = false，可调整大小= true）</p><p>  We get a table with 85 rows, where the first three rows are not relevant for our purpose, and the last row is empty. So we get 85 - 3 - 1 = 81 mementos, which conforms to the number of  Figure 13. The URLs to the mementos are pretty long. You can widen/narrow the columns to inspect the structure of the URLs. Typically they start with “ http://web.archive.org/web/” followed by the date-time string of the memento and the original URL.</p><p>  我们收到一个带有85行的表，其中前三行与我们的目的无关，最后一行为空。所以我们得到85  -  3  -  1 = 81纪念品，符合图13的数量。纪念品的URL很长。您可以扩展/缩小列以检查URL的结构。通常，它们从“http://web.archive.org/web/”开始，后跟Memento的日期时间字符串和原始URL。</p><p> We have now collected all the URLs for the mementos. The next task is now getting our data from these pages and display them in a graphic. For this last step, we will use the packages  rvest and  ggplot2. It is a “standard” task of manipulating HTML and has only insofar with the memento framework to do, as we are using the URLs of the archived web pages.</p><p> 我们现在收集了纪念品的所有网址。下一个任务现在正在从这些页面获取数据并在图形中显示它们。对于最后一步，我们将使用包已验证和ggplot2。它是操纵HTML的“标准”任务，只有我们使用归档的网页的URL，只有与Memento Framework的框架只有误解。</p><p> We only need the memento links, date, and new column for our number of available packages.</p><p> 我们只需要Memento链接，日期和新列以获取我们的可用包号码。</p><p>  ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──</p><p>  ##──附加包──────────────────────────────────── ── </p><p> ## ✓ ggplot2 3.3.3 ✓ purrr 0.3.4## ✓ tibble 3.1.2 ✓ dplyr 1.0.6## ✓ tidyr 1.1.3 ✓ stringr 1.4.0## ✓ readr 1.4.0 ✓ forcats 0.5.1</p><p>##✓GGPLOT23.3.3✓PURRR0.3.4 ##✓TIBBLE3.1.2✓✓DYR1.1.3✓STRING1.4.0 ##✓READR1.4.0✓FORCATS 0.5.1</p><p> ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──## x dplyr::filter() masks stats::filter()## x dplyr::lag() masks stats::lag()</p><p> ##──冲突─────────────────────────────────────────── ──## x DOLETR :: Filter（）掩码统计:: filter（）## x dplyr :: lag（）掩码stats :: lag（）</p><p> cran_tidy_data &lt;- readRDS(&#34;data/cran_crawl_list.rds&#34;) %&gt;% filter(stringr::str_detect(rel, &#34;memento&#34;)) %&gt;% mutate(date = anytime::anydate(datetime)) %&gt;% add_column(pkgs = 0) %&gt;% select(link, date, pkgs)saveRDS(cran_tidy_data, &#34;data/cran_tidy_data.rds&#34;)glimpse(cran_tidy_data)</p><p> cran_tidy_data＆lt;  -  readrds（＆＃34; data / cran_crawl_list.rds＆＃34;）％＆gt;％过滤器（stringr :: str_detect（rel，＆＃34; memento＆＃34;）％＆gt;％变异（日期= Anytime :: AnyDate（DateTime））％＆gt;％add_column（pkgs = 0）％＆gt;％select（链接，日期，pkgs）saverds（cran_tidy_data，＆＃34; data / cran_tidy_data.rds＆＃34;）glimpse（cran_tidy_data ）</p><p> ## Rows: 81## Columns: 3## $ link &lt;chr&gt; &#34;http://web.archive.org/web/20110926172444/http://cran.r-project.…## $ date &lt;date&gt; 2011-09-26, 2011-10-11, 2011-10-29, 2011-11-29, 2011-12-29, 2012…## $ pkgs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…</p><p> ##行：81 ##列：3 ## $ link＆lt; chr＆gt; ＆＃34; http：//web.archive.org/web/20110926172444/http：//cran.r-project ......## $ date＆lt; date＆gt; 2011-09-26,2011-10-11，2011-10-29，2011-11-29，2011-12-29，2012 ... ## $ PKGS＆lt; dbl＆gt; 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0，0，...</p><p> For the purpose of this demonstration, I do not want to scrape every one of the 81 links but only one link per year. As the archiving dates do not have a systematic regularity, we cannot provide time series, say January 1st every year. But this does not matter for this demo.</p><p> 出于本演示的目的，我不想刮掉81个链接中的每一个，但每年只有一个链接。由于归档日期没有系统规则，我们不能每年1月1日发布时间序列。但这对这个演示并不重要。</p><p> cran_yearly_links &lt;- readRDS(&#34;data/cran_tidy_data.rds&#34;) %&gt;% dplyr::mutate(year = lubridate::year(date), .after = &#34;date&#34;) cran_yearly_links &lt;- cran_yearly_links[!duplicated(cran_yearly_links[ , &#34;year&#34;]), ]saveRDS(cran_yearly_links, &#34;data/cran_yearly_links.rds&#34;)cran_yearly_links</p><p> cran_yearly_links＆lt;  -  readrds（＆＃34; data / cran_tidy_data.rds＆＃34;）％＆gt;％dplyr :: mutate（年= lubrite ::年（日期），。处于=＃34;日期＆＃34;） cran_yearly_links＆lt;  -  cran_yearly_links [！复制（cran_yearly_links [，＆＃34;年＆＃34;]），] saverds（cran_yearly_links，＆＃34; data / cran_yearly_links.rds＆＃34;）cran_yearly_links</p><p> ## # A tibble: 11 x 4## link date year pkgs## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt;## 1 http://web.archive.org/web/20110926172444/http://cran… 2011-09-26 2011 0## 2 http://web.archive.org/web/20120127155954/http://cran… 2012-01-27 2012 0## 3 http://web.archive.org/web/20130121072548/http://cran… 2013-01-21 2013 0## 4 http://web.archive.org/web/20140402061423/http://cran… 2014-04-02 2014 0## 5 http://web.archive.org/web/20150124092856/http://cran… 2015-01-24 2015 0## 6 http://web.archive.org/web/20160104062049/https://cra… 2016-01-04 2016 0## 7 http://web.archive.org/web/20170502002538/http://cran… 2017-05-02 2017 0## 8 http://web.archive.org/web/20180822083144/https://cra… 2018-08-22 2018 0## 9 http://web.archive.org/web/20190103070122/http://cran… 2019-01-03 2019 0## 10 http://web.archive.org/web/20200103222850/https://cra… 2020-01-03 2020 0## 11 http://web.archive.org/web/20210128201734/https://cra… 2021-01-28 2021 0</p><p> ### A Tibble：11 x 4 ##链接日期年份PKGS ##＆lt; chr＆gt; ＆lt; date＆gt; ＆lt; dbl＆gt; ＆lt; dbl＆gt; ## 1 http://web.archive.org/web/20110926172444/http://cran ... 2011-09-26 2011 0 ## 2 http://web.archive.org/web/2012012715595954 / http：// cran ... 2012-01-27 2012 0 ## 3 http://web.archive.org/web/20130121072548/http://cran.. 2013-01-21 2013 0 ## 4 http：/ / web.archive.org/web/20140402061423/http://cran ... 2014-04-02 2014 0 ## 5 http://web.archive.org/web/20150124092856/http://cran.. 2015-01 -24 2015 0 ## 6 http://web.archive.org/web/20160104062049/https：//cra.. 2016-01-04 2016 0 ## 7 http://web.archive.org/web/20170502002538 / http：// cran ... 2017-05-02 2017 0 ## 8 http://web.archive.org/web/20180822083144/htps://cra.. 2018-08-22 2018 0 ## 9 http：/ / web.archive.org/web/20190103070122/http://cran ... 2019-01-03 2019 0 ## 10 http://web.archive.org/web/20200103222850/cra... 2020-01 -03 2020 0 ## 11 http://web.archive.org/web/20210128201734/https://cra.. 2021-01-28 2021 0 </p><p> Although there are only 10 web pages to scrape, this will still take some time. Therefore I provide a progress indicator to monitor how much time the procedure will still last.</p><p>虽然只有10个网页来刮擦，但这仍然需要一些时间。因此，我提供了一个进度指标，以监视程序仍然需要多长时间。</p><p> library(rvest)cran_pkgs &lt;- readRDS(&#34;data/cran_yearly_links.rds&#34;)max = nrow(cran_pkgs)pb &lt;- txtProgressBar(min = 0, max = max, style = 3) for(i in 1:max) { html &lt;- read_html(cran_pkgs$link[i]) links &lt;- html_nodes(html, &#34;a&#34;) cran_pkgs$pkgs[i] &lt;- length(links) setTxtProgressBar(pb, i) }close(pb)saveRDS(cran_pkgs, &#34;data/cran_pkgs.rds&#34;)</p><p> 库（累积）cran_pkgs＆lt;  -  readrds（＆＃34;数据/ cran_yearly_links.rds＆＃34;）max = nrow（cran_pkgs）pb＆lt;  -  txtprogressbar（min = 0，max = max，style = 3）for（i在1：max）{html＆lt;  -  read_html（cran_pkgs $ link [i]）链接＆lt;  -  html_nodes（html，＆＃34; a＆＃34;）cran_pkgs $ pkgs [i]＆lt; length（links）settxtprogressbar （Pb，i）}关闭（pb）saverds（cran_pkgs，＆＃34;数据/ cran_pkgs.rds＆＃34;）</p><p>  ## # A tibble: 11 x 4## link date year pkgs## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt;## 1 http://web.archive.org/web/20110926172444/http://cran… 2011-09-26 2011 3307## 2 http://web.archive.org/web/20120127155954/http://cran… 2012-01-27 2012 3563## 3 http://web.archive.org/web/20130121072548/http://cran… 2013-01-21 2013 4262## 4 http://web.archive.org/web/20140402061423/http://cran… 2014-04-02 2014 5374## 5 http://web.archive.org/web/20150124092856/http://cran… 2015-01-24 2015 6221## 6 http://web.archive.org/web/20160104062049/https://cra… 2016-01-04 2016 7722## 7 http://web.archive.org/web/20170502002538/http://cran… 2017-05-02 2017 10513## 8 http://web.archive.org/web/20180822083144/https://cra… 2018-08-22 2018 12938## 9 http://web.archive.org/web/20190103070122/http://cran… 2019-01-03 2019 13645## 10 http://web.archive.org/web/20200103222850/https://cra… 2020-01-03 2020 15348## 11 http://web.archive.org/web/20210128201734/https://cra… 2021-01-28 2021 17038</p><p>  ### A Tibble：11 x 4 ##链接日期年份PKGS ##＆lt; chr＆gt; ＆lt; date＆gt; ＆lt; dbl＆gt; ＆lt; dbl＆gt; ## 1 http://web.archive.org/web/20110926172444/http://cran ... 2011-09-26 2011 3307 ## 2 http://web.archive.org/web/20120127155954 / http：// cran ... 2012-01-27 2012 3563 ## 3 http://web.archive.org/web/20130121072548/http://cran ... 2013-01-21 2013 4262 ## 4 http：/ / web.archive.org/web/20140402061423/http://cran.. 2014-04-02 2014 5374 ## 5 http://web.archive.org/web/20150124092856/http://cran.. 2015-01 -24 2015 6221 ## http://web.archive.org/web/20160104062049/htps://cra ... 2016-01-04 2016 7722 ## http：//web.archive.org/web/2017050200253/20170502002538 / http：// cran ... 2017-05-02 2017 10513＃8 http://web.archive.org/web/20180822083144/htps://cra.. 2018-08-22 2018 12938 ## 9 http：/ / web.archive.org/web/20190103070122/http://cra ... 2019-01-03 2019 13645＃10 http://web.archive.org/web/20200103222850/htps：/克拉... 2020-01 -03 2020 15348 ## 11 http://web.archive.org/web/20210128201734/htps://cra.. 2021-01-28 2021 17038</p><p> I am not going to produce a sophisticated graphic. A simple line graph to see how the number of packages is increasing has to be enough.</p><p> 我不会产生复杂的图形。一个简单的线条图，看看包数如何增加，必须足够。</p><p>  I have forked his GitHub repo and looked into his R scripts. I have to say that I became somewhat depressed as I noticed how much knowledge I am still lacking. Despite working now for four years with R, there are so many think I still have to learn! ↩︎</p><p>  我已经刻了他的github回购，并看着他的脚本。我不得不说我感到沮丧，因为我注意到我仍然缺乏多少知识。尽管现在工作了四年，但有很多人认为我还必须学习！ ↩︎</p><p> In many cases, the last memento is identical with the memento link relation type. Then the tibble has only seven rows. ↩︎</p><p> 在许多情况下，最后一个Memento与Memento链路关系类型相同。然后撕裂只有七行。 ↩︎ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://notes.peter-baumgartner.net/2021/05/25/memento-time-travel/">https://notes.peter-baumgartner.net/2021/05/25/memento-time-travel/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/归档/">#归档</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>