<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Gradient-Free-Optimizers Python中现代优化方法的集合 Gradient-Free-Optimizers A collection of modern optimization methods in Python</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Gradient-Free-Optimizers A collection of modern optimization methods in Python<br/>Gradient-Free-Optimizers Python中现代优化方法的集合 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-28 22:30:21</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/e0febb08bc47285eea945919fd64119c.png"><img src="http://img2.diglog.com/img/2021/2/e0febb08bc47285eea945919fd64119c.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Gradient-Free-Optimizers provides a collection of easy to use optimization techniques,whose objective function only requires an arbitrary score that gets maximized.This makes gradient-free methods capable of solving various optimization problems, including:</p><p>无梯度优化器提供了一组易于使用的优化技术，其目标函数只需要任意分数即可最大化。这使得无梯度方法能够解决各种优化问题，其中包括：</p><p>  Gradient-Free-Optimizers is the optimization backend of  Hyperactive (in v3.0.0 and higher) but it can also be used by itself as a leaner and simpler optimization toolkit.</p><p>  Gradient-Free-Optimizers是Hyperactive的优化后端（在v3.0.0和更高版本中），但它本身也可以用作更精简的优化工具包。</p><p>       You can optimize anything that can be defined in a python function. For example a simple parabola function:</p><p>       您可以优化可以在python函数中定义的任何内容。例如一个简单的抛物线函数：</p><p>    That`s all the information the algorithm needs to search for the maximum in the objective function:</p><p>    这就是算法在目标函数中搜索最大值所需的所有信息：</p><p> Gradient-Free-Optimizers provides not just meta-heuristic optimization methods but also sequential model based optimizers like bayesian optimization, which delivers good results for expensive objetive functions like deep-learning models.</p><p> 无梯度优化器不仅提供元启发式优化方法，而且还提供基于顺序模型的优化器（如贝叶斯优化），从而为昂贵的对象功能（如深度学习模型）提供了良好的效果。</p><p>   Even for the very simple parabola function the optimization time is about 60% of the entire iteration time when optimizing with random search. This shows, that (despite all its features) Gradient-Free-Optimizers has an efficient optimization backend without any unnecessary slowdown.</p><p>   即使对于非常简单的抛物线函数，使用随机搜索进行优化时，优化时间也大约是整个迭代时间的60％。这表明（尽管具有所有功能）无梯度优化器具有高效的优化后端，而没有任何不必要的速度降低。</p><p>   Per default Gradient-Free-Optimizers will look for the current position in a memory dictionary before evaluating the objective function.</p><p>   默认情况下，Gradient-Free-Optimizers将在评估目标函数之前在内存字典中查找当前位置。 </p><p> If the position is not in the dictionary the objective function will be evaluated and the position and score is saved in the dictionary.</p><p>如果位置不在字典中，则将评估目标函数，并将位置和分数保存在字典中。</p><p> If a position is already saved in the dictionary Gradient-Free-Optimizers will just extract the score from it instead of evaluating the objective function. This avoids reevaluating computationally expensive objective functions (machine- or deep-learning) and therefore saves time.</p><p> 如果某个位置已经保存在字典中，则Gradient-Free-Optimizers只会从中提取分数，而不是评估目标函数。这避免了重新评估计算上昂贵的目标函数（机器学习或深度学习），因此节省了时间。</p><p> Gradient-Free-Optimizers is extensivly tested with more than 400 tests in 2500 lines of test code. This includes the testing of:</p><p> Gradient-Free-Optimizers经过2500行测试代码中的400多次测试，得到了广泛的测试。这包括对以下各项的测试：</p><p>    Each optimization algorithm must perform above a certain threshold (for selected objetive functions) to be included. Poorly performing algorithms are reworked or scraped.</p><p>    每个优化算法必须执行高于特定阈值（对于选定的目标功能）才能包括在内。表现不佳的算法会被重做或取消。</p><p>   Gradient-Free-Optimizers supports a variety of optimization algorithms, which can make choosing the right algorithm a tedious endeavor. The gifs in this section give a visual representation how the different optimization algorithms explore the search space and exploit the collected information about the search space for a convex and non-convex objective function.</p><p>   Gradient-Free-Optimizers支持多种优化算法，这会使选择正确的算法成为一项繁琐的工作。本节中的gif图像直观地表示了不同的优化算法如何探索搜索空间以及如何利用收集到的有关搜索空间的信息来获取凸和非凸目标函数。</p><p> Hill Climbing    Evaluates the score of n neighbours in an epsilon environment and moves to the best one.</p><p> 爬山评估在epsilon环境中n个邻居的得分，并向最佳邻居前进。</p><p>  Repulsing Hill Climbing    Hill climbing iteration + increases epsilon by a factor if no better neighbour was found.</p><p>  排斥爬坡爬坡迭代+如果找不到更好的邻居，则将ε增加一倍。 </p><p>  Simulated Annealing    Hill climbing iteration + accepts moving to worse positions with decreasing probability over time (transition probability).</p><p>模拟的退火山爬坡迭代+接受随着时间的流逝而逐渐降低的概率（过渡概率），移至较差的位置。</p><p>    Random Restart Hill Climbing    Hill climbing + moves to a random position after n iterations.</p><p>    随机重新开始爬山n次迭代后，爬山+移动到随机位置。</p><p>      Particle Swarm Optimization    Population of n particles attracting each other and moving towards the best particle.</p><p>      粒子群优化n个粒子的粒子相互吸引并朝着最佳粒子移动。</p><p>      Tree of Parzen Estimators    Kernel density estimators fitting to good and bad explored positions and predicting promising new positions.</p><p>      Parzen估计量树内核密度估计量适合于好的和坏的探索位置，并预测有前途的新位置。</p><p>  Decision Tree Optimizer    Ensemble of decision trees fitting to explored positions and predicting promising new positions.</p><p>  决策树优化程序决策树的组合适合搜索的职位并预测有希望的新职位。</p><p>          import  numpy  as  np from  gradient_free_optimizers  import  RandomSearchOptimizer def  parabola_function( para):  loss  =  para[ &#34;x&#34;]  *  para[ &#34;x&#34;]  return  - loss search_space  = { &#34;x&#34;:  np. arange( - 10,  10,  0.1)} opt  =  RandomSearchOptimizer( search_space) opt. search( parabola_function,  n_iter = 100000)</p><p>          从gradient_free_optimizers中将numpy导入为np导入RandomSearchOptimizer def parabola_function（para）：loss = para [＆＃34; x＆＃34;] * para [＆＃34; x＆＃34;] return-loss search_space = {＆＃34; x＆ ＃34;：np。 arange（-10，10，0.1）} opt = RandomSearchOptimizer（search_space）opt。搜索（parabola_function，n_iter = 100000）</p><p>   import  numpy  as  np from  gradient_free_optimizers  import  RandomSearchOptimizer def  ackley_function( pos_new):  x  =  pos_new[ &#34;x1&#34;]  y  =  pos_new[ &#34;x2&#34;]  a1  =  - 20  *  np. exp( - 0.2  *  np. sqrt( 0.5  * ( x  *  x  +  y  *  y)))  a2  =  - np. exp( 0.5  * ( np. cos( 2  *  np. pi  *  x)  +  np. cos( 2  *  np. pi  *  y)))  score  =  a1  +  a2  +  20  return  - score search_space  = {  &#34;x1&#34;:  np. arange( - 100,  101,  0.1),  &#34;x2&#34;:  np. arange( - 100,  101,  0.1),} opt  =  RandomSearchOptimizer( search_space) opt. search( ackley_function,  n_iter = 30000)</p><p>   从gradient_free_optimizers中将numpy导入为np导入RandomSearchOptimizer def ackley_function（pos_new）：x = pos_new [＆＃34; x1＆＃34;] y = pos_new [＆＃34; x2＆＃34;] a1 =-20 * np。 exp（-0.2 * np。sqrt（0.5 *（x * x + y * y）））a2 =-np。 exp（0.5 *（np。cos（2 * np。pi * x）+ np。cos（2 * np。pi * y）））score = a1 + a2 + 20 return-score search_space = {＆＃34; x1＆ ＃34;：np。 arange（-100，101，0.1），＆＃34; x2＆＃34 ;: np。 opt（arange（-100，101，0.1），} opt = RandomSearchOptimizer（search_space）opt。搜索（ackley_function，n_iter = 30000） </p><p>   import  numpy  as  np from  sklearn. model_selection  import  cross_val_score from  sklearn. ensemble  import  GradientBoostingClassifier from  sklearn. datasets  import  load_wine from  gradient_free_optimizers  import  HillClimbingOptimizer data  =  load_wine() X,  y  =  data. data,  data. target def  model( para):  gbc  =  GradientBoostingClassifier(  n_estimators = para[ &#34;n_estimators&#34;],  max_depth = para[ &#34;max_depth&#34;],  min_samples_split = para[ &#34;min_samples_split&#34;],  min_samples_leaf = para[ &#34;min_samples_leaf&#34;], )  scores  =  cross_val_score( gbc,  X,  y,  cv = 3)  return  scores. mean() search_space  = {  &#34;n_estimators&#34;:  np. arange( 20,  120,  1),  &#34;max_depth&#34;:  np. arange( 2,  12,  1),  &#34;min_samples_split&#34;:  np. arange( 2,  12,  1),  &#34;min_samples_leaf&#34;:  np. arange( 1,  12,  1),} opt  =  HillClimbingOptimizer( search_space) opt. search( model,  n_iter = 50)</p><p>从sklearn将numpy导入为np。 model_selection从sklearn导入cross_val_score。集成从sklearn导入GradientBoostingClassifier。数据集从gradient_free_optimizers导入load_wine导入HillClimbingOptimizer data = load_wine（）X，y = data。数据，数据。目标def模型（para）：gbc = GradientBoostingClassifier（n_estimators = para [＆＃34; n_estimators＆＃34;]，max_depth = para [＆＃34; max_depth＆＃34;]，min_samples_split = para [＆＃34; min_samples_split＆＃34 ;]，min_samples_leaf = para [＆＃34; min_samples_leaf＆＃34;]，）得分= cross_val_score（gbc，X，y，cv = 3）返回得分。 mean（）search_space = {＆＃34; n_estimators＆＃34;：np。 arange（20，120，1），＆＃34; max_depth＆＃34 ;: np。 arange（2，12，1），＆＃34; min_samples_split＆＃34 ;: np。 arange（2，12，1），＆＃34; min_samples_leaf＆＃34 ;: np。 arange（1，12，1），} opt = HillClimbingOptimizer（search_space）opt。搜索（模型，n_iter = 50）</p><p>     Pass the search_space to the optimizer class to define the space were the optimization algorithm can search for the best parameters for the given objective function.</p><p>     将search_space传递给优化器类以定义空间，优化算法可以为给定目标函数搜索最佳参数。</p><p> The initialization dictionary automatically determines a number of parameters that will be evaluated in the first n iterations (n is the sum of the values in initialize). The initialize keywords are the following:</p><p> 初始化字典自动确定将在前n次迭代中评估的参数数量（n是Initialize中值的总和）。 initialize关键字如下：</p><p> Initializes positions in a grid like pattern. Positions that cannot be put into a grid are randomly positioned.</p><p> 初始化网格状图案中的位置。无法放置到网格中的位置是随机放置的。</p><p> Initializes positions at the vertices of the search space. Positions that cannot be put into a vertices are randomly positioned.</p><p> 初始化搜索空间顶点处的位置。无法放置到顶点的位置是随机放置的。</p><p>  Each optimization class needs the &#34;search_space&#34; as an input argument. Optionally &#34;initialize&#34; and optimizer-specific parameters can be passed as well. You can read more about each optimization-strategy and its parameters in the  Optimization Tutorial.</p><p>  每个优化类都需要＆＃34; search_space＆＃34;作为输入参数。 （可选）＆＃34;初始化＆＃34;并且还可以传递优化程序特定的参数。您可以在《优化教程》中阅读有关每个优化策略及其参数的更多信息。</p><p>     The objective function defines the optimization problem. The optimization algorithm will try to maximize the numerical value that is returned by the objective function by trying out different parameters from the search space.</p><p>     目标函数定义了优化问题。优化算法将通过从搜索空间中尝试不同的参数来尝试最大化目标函数返回的数值。 </p><p> The number of iterations that will be performed during the optimiation run. The entire iteration consists of the optimization-step, which decides the next parameter that will be evaluated and the evaluation-step, which will run the objective function with the chosen parameter and return the score.</p><p>优化运行期间将执行的迭代次数。整个迭代包括优化步骤和评估步骤，优化步骤确定要评估的下一个参数，评估步骤将使用所选参数运行目标函数并返回得分。</p><p> Maximum number of seconds until the optimization stops. The time will be checked after each completed iteration.</p><p> 直到优化停止的最大秒数。在每次完成迭代后将检查时间。</p><p> Maximum score until the optimization stops. The score will be checked after each completed iteration.</p><p> 在优化停止之前的最高分数。每次完成迭代后将检查分数。</p><p> Whether or not to use the &#34;memory&#34;-feature. The memory is a dictionary, which gets filled with parameters and scores during the optimization run. If the optimizer encounters a parameter that is already in the dictionary it just extracts the score instead of reevaluating the objective function (which can take a long time).</p><p> 是否使用记忆功能。内存是一个字典，在优化运行期间会充满参数和分数。如果优化器遇到字典中已经存在的参数，则仅提取分数而不是重新评估目标函数（这可能需要很长时间）。</p><p> Pandas dataframe that contains score and paramter information that will be automatically loaded into the memory-dictionary.</p><p> 包含得分和参数信息的熊猫数据框将自动加载到内存字典中。</p><p> The verbosity list determines what part of the optimization information will be printed in the command line.</p><p> 详细列表确定将在命令行中打印优化信息的哪一部分。</p><p>    Dataframe, that contains information about the score, the value of each parameter and the evaluation and iteration time. Each row shows the information of one optimization iteration.</p><p>    数据框，其中包含有关得分，每个参数的值以及评估和迭代时间的信息。每行显示一次优化迭代的信息。 </p><p>     improve access to parameters of optimizers within population-based-optimizers (e.g. annealing rate of simulated annealing population in parallel tempering)</p><p>在基于种群的优化器中改善对优化器参数的访问（例如，平行回火中模拟退火种群的退火速率）</p><p>   Gradient-Free-Optimizers was created as the optimization backend of the Hyperactive package. Therefore the algorithms are exactly the same in both packages and deliver the same results.However you can still use Gradient-Free-Optimizers as a standalone package.The separation of Gradient-Free-Optimizers from Hyperactive enables multiple advantages:</p><p>   Gradient-Free-Optimizers被创建为Hyperactive软件包的优化后端。因此，两个软件包中的算法完全相同，并且提供相同的结果，但是您仍然可以将Gradient-Free-Optimizers作为独立软件包使用。将Gradient-Free-Optimizers与Hyperactive分开具有多个优势：</p><p> Better isolation from the complex information flow in Hyperactive. GFOs only uses positions and scores in a N-dimensional search-space. It returns only the new position after each iteration.</p><p> 更好地隔离Hyperactive中复杂的信息流。 GFO仅使用N维搜索空间中的位置和分数。每次迭代后，它仅返回新位置。</p><p> a smaller and cleaner code base, if you want to explore my implementation of these optimization techniques.</p><p> 如果要探索我对这些优化技术的实现，请使用更小巧，更简洁的代码库。</p><p>    @Misc{gfo2020, author = {{Simon Blanke}}, title = {{Gradient-Free-Optimizers}: Simple and reliable optimization with local, global, population-based and sequential techniques in numerical search spaces.}, howpublished = {\url{https://github.com/SimonBlanke}}, year = {since 2020}}</p><p>    @Misc {gfo2020，作者= {{Simon Blanke}}，标题= {{Gradient-Free-Optimizers}：在数字搜索空间中使用局部，全局，基于种群和顺序技术进行简单可靠的优化。}，出版方式= { \ url {https://github.com/SimonBlanke}}，年份= {自2020年}} </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/SimonBlanke/Gradient-Free-Optimizers">https://github.com/SimonBlanke/Gradient-Free-Optimizers</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/python/">#python</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/free/">#free</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/优化/">#优化</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>