<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>意外的幂态行为 Accidentally Exponential Behavior in Spark</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Accidentally Exponential Behavior in Spark<br/>意外的幂态行为 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-22 04:09:03</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/1bdd0419bc9746cad6b18131c00d3d21.png"><img src="http://img2.diglog.com/img/2021/6/1bdd0419bc9746cad6b18131c00d3d21.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>At Heap, we use Apache Spark to power our  Heap Connect product. This part of our system is tasked with moving large amounts of data quickly to our customers&#39; warehouses. Using Spark allows us to easily scale up our data processing to support our largest customers, and we also get out-of-the-box support for column-based file formats like ORC.</p><p>在堆中，我们使用Apache Spark来为我们的堆连接产品供电。我们的系统的这一部分是任务的，迅速移动大量数据和我们的客户＆＃39;仓库。使用Spark使我们能够轻松扩展我们的数据处理以支持我们最大的客户，并且我们还会出于over基于列的文件格式的开箱支持。</p><p> This blog post covers one of the first performance hurdles we had to solve. Our data processing job was seemingly hanging for some of our customers. Our root cause analysis identified a slowness in the Spark implementation of the ORC file reader.</p><p> 这篇博文界面涵盖了我们必须解决的第一个性能障碍之一。我们的数据处理工作似乎挂在我们的一些客户。我们的根本原因分析确定了兽人文件读取器的Spark实现中的慢速。</p><p> An unexpected algorithmic gotcha caused a tree-processing algorithm to perform an exponential number of operations. This meant that even a tiny tree with 50 nodes could cause the algorithm to effectively go on indefinitely.</p><p> 意外的算法GOTCHA导致树处理算法执行指数运行。这意味着即使具有50个节点的微小树也可能导致算法无限期地实现。</p><p> Luckily, we managed to work around the problem on our side without depending on Spark for a fix. After fixing our immediate production issue, we also contributed to a fix for Spark.</p><p> 幸运的是，我们设法在我们身边的问题上工作，而不根据火花来解决修复。在修复我们的直接生产问题后，我们也有助于解决火花。</p><p>  Our customers care about users, events (the actions users take on their website or product), and sessions. They also care about filtering these entities. For example, they might be interested in the “count of all events which happened in the United Kingdom.”</p><p>  我们的客户关心用户，活动（操作用户在网站或产品上采取）和会话。他们还关心过滤这些实体。例如，他们可能对“联合王国发生的所有事件的计数”感兴趣。</p><p>   It turns out that a very natural way to represent this set of predicates is with a binary tree, like this one:</p><p>   事实证明，表示这组谓词的非常自然的方式是用二叉树，如此：</p><p>  The leaf nodes represent individual filter conditions, like “country = UK”. Non-leaf nodes represent an operation performed on their subtrees. An AND node says “only match data that conforms to the conditions in my left subtree and in my right subtree”. An OR node would be similar, but matching either the left or right subtree without requiring both subtrees to match at the same time.</p><p>  叶节点表示单个过滤条件，如“Country = UK”。非叶节点表示在其子树上执行的操作。 AND节点表示“只匹配符合我左子子树中的条件的数据以及在我的右子树中”。一个或节点会类似，但匹配左或右子树，而不需要同时匹配两个子树。 </p><p> I mentioned “trees with 50 nodes” earlier. Does anyone really want to apply that many filters to their data? A standard situation that causes this is if the customer wants to filter for events which:</p><p>我提到了早些时候的“有50个节点的树木”。有没有人真正想要将这么多的过滤器应用于他们的数据？导致此原因的标准情况是客户想要过滤事件：</p><p>  Expressing this in a straightforward way, without introducing array-based filters, requires a tree with 50 leaf nodes.</p><p>  在不引入基于阵列的过滤器的情况下，以直接的方式表达这一点，需要一个带有50个叶节点的树。</p><p>  Now that we’ve covered how trees come into the picture, let’s discuss why processing them can require so much time.</p><p>  现在我们已经涵盖了树木如何进入图片，让我们讨论为什么处理它们需要这么多时间。</p><p> Although it’s a standard practice to represent filters as trees, there’s no single, widely accepted implementation. Thus, a large project like Spark might need to support and integrate with multiple such tree-based representations. In our case, some debugging pointed us towards an algorithm for converting from Spark’s internal tree-based filter into an ORC-specific SearchArgument.</p><p> 虽然代表过滤器作为树木是一个标准的做法，但没有单一，广泛接受的实现。因此，像Spark这样的大型项目可能需要支持和集成多个基于树的表示。在我们的情况下，一些调试指向朝向从Spark的内部树木滤波器转换为orc特定的算法 searchargument。</p><p> Since the two formats are not 100% equivalent, the conversion had to check whether the input tree could be converted before actually attempting to do so. This resulted in code like this:</p><p> 由于两种格式不等效100％，因此转换必须检查输入树是否可以在实际尝试之前转换。这导致了这样的代码：</p><p>  At first, this looked pretty reasonable. However, the methods  canTransform and  transform contain almost identical logic. Thus, in the spirit of code reuse, the implementation did this instead:</p><p>  起初，这看起来很合理。但是，Cantransform和变换的方法含有几乎相同的逻辑。因此，在代码重用的精神中，实现了这一点：</p><p>  Still looking pretty reasonable. However, notice the minor, yet very important difference: we’re calling the  transform method twice.</p><p>  仍然看起来很合理。但是，注意到次要，但非常重要的区别：我们正调用两次转换方法。 </p><p> Why would that be very important? Doing something twice instead of only once sounds sub-optimal, but shouldn’t really be that bad. Right...?</p><p>为什么这会非常重要？做某事两次而不是只有一次听起来是次优，但不应该真的那么糟糕。正确的...？</p><p>  A more careful look shows that we’re not simply “doing something twice”. Instead, it’s closer to “doing something twice; then twice again, in each of these two invocations; then twice again, ...” and so on. That is, we’re “doing something twice”, raised to the power of H (where H is the height of the tree).</p><p>  更加谨慎地表明我们不仅仅是“两次做某事”。相反，它更接近“两次做某事;然后再次，在这两个调用中的每一个中;然后再次，......“等等。也就是说，我们“两次做某事”，提升到H的力量（其中H是树的高度）。</p><p> Let’s take a slightly more formal look. Let’s call the number of operations required to transform a tree of height H, f(H). Then:</p><p> 让我们稍微正式看看。让我们召集转换高度H，F（H）树所需的操作数。然后：</p><p>  Or, translated back into words again, the complexity of the transform function is exponential in the height of the tree being transformed.</p><p>  或者，再次翻译成单词，转换函数的复杂性在被转换的树的高度中是指数的。</p><p> For a tree with height 45, we get f(45) = 2 ^ 44 * f(1) = 17,592,186,044,416 * f(1).</p><p> 对于高度45的树，我们得到F（45）= 2 ^ 44 * F（1）= 17,592,186,044,416 * F（1）。</p><p> For simplicity, let’s assume that f(1) = 1. Assuming a modern CPU can run ~3 billion operations per second, this innocent-looking tree transformation would take ~ 5800 seconds, or about an hour and a half. Each additional layer in the tree makes the operation take 2x longer.</p><p> 为简单起见，假设F（1）= 1.假设现代CPU每秒可以运行〜3亿次操作，这一无辜的树木转换将需要〜5800秒，或大约一个半小时。树中的每个附加层使得操作需要2倍。</p><p> Also, notice that 1.5 hours is the time it takes us to simply transform a filter expression from one tree format into another. Once. No customer data was analyzed during that 1.5-hour period. That part is yet to begin.</p><p> 此外，请注意，1.5小时只需将过滤器表达式从一个树格式转换为另一个树格式所需的时间。一次。在1.5小时内没有分析客户数据。那部分尚未开始。 </p><p> It’s also interesting to note where this time is being spent. When running a Spark job, all the actual data processing is done by Spark Executors. Thus, most of the standard monitoring and optimization approaches are largely focused on Executor performance. But in our case, nothing was showing up anywhere in the Executor performance metrics. It seemed like the Executors were just idling all the time.</p><p>值得注意的是，在花费这个时间。运行火花作业时，所有实际数据处理都由Spark Executors完成。因此，大多数标准监测和优化方法都在很大程度上专注于执行者性能。但在我们的情况下，在执行者绩效指标中的任何地方都没有任何东西出现。看起来官员一直只是闲置。</p><p> While Spark Executors are responsible for the data processing, the Spark Driver is the component responsible for query planning. The exponential algorithm above was part of query planning. Thus, its slowness meant that the Spark Driver never managed to successfully plan the query. Because of this, there was no work for the Executors to pick up — and, thus, they were idling.</p><p> 虽然火花执行器负责数据处理，但是Spark驱动程序是负责查询规划的组件。上面的指数算法是查询规划的一部分。因此，它的缓慢意味着火花司机从未成功地计划查询。因此，求助者没有工作才能拾取 - 因此，他们是怠速。</p><p>  When introducing the problem, I said the algorithm was “exponential in the size of the tree”. However, our more careful analysis reveals that it’s not in the  size of the tree, but rather, it’s in the  height of the tree that it’s exponential. We can use that detail to build a workaround.</p><p>  在引入问题时，我表示该算法是“树的大小的指数”。然而，我们更加仔细的分析表明它不是树的大小，而是，它在树的高度是它是指数的。我们可以使用该细节来构建一个解决方法。</p><p> Before we do that, let’s see how we ended up with trees that are so deep. It’s actually very easy to do so if you’re not actively trying to avoid it. In our case, we had code that looked like this:</p><p> 在我们这样做之前，让我们看看我们如何用这么深的树木结束。如果你没有积极地试图避免它，那实际上很容易。在我们的情况下，我们有代码如下所示：</p><p>  This code was very easy to read and write. However, given a long list of  andPredicates, it resulted in heavily skewed trees.</p><p>  此代码非常容易读写。然而，给定长期的和奇特列表，它导致了严重倾斜的树木。</p><p> Given a list of 45 &#34;country = X&#34;, this straightforward implementation results in a tree with depth 45 that immediately triggers the exponential behavior. This causes jobs to get stuck on the predicate conversion step.</p><p> 给定45＆＃34;国家= X＆＃34;，这种直接实现的结果导致一个深度45的树，它立即触发指数行为。这会导致作业卡在谓词转换步骤上。</p><p>  Luckily, in our specific use-case, very deep trees always had the same operation. For example, we could have a tree with 45 OR nodes like the above, or with 45 AND nodes. However, we never created 45-layer-deep trees that had a mix of ANDs and ORs. This consistency allowed us to manipulate the tree freely, without worrying about accidentally changing the semantics.</p><p>  幸运的是，在我们的具体用例中，非常深的树木总是具有相同的操作。例如，我们可以拥有一个具有上述45个或节点的树，或者使用45个和节点。但是，我们从未创建过45层深的树木，含有和或ors的混合。这种一致性使我们可以自由操纵树，而不担心意外地改变语义。 </p><p> This is because, due to associativity, we can combine the AND sub-clauses however we want. We can create a subtree composed of a subset of the AND clauses, and then merge them together, thus preserving the result.</p><p>这是因为，由于关联性，我们可以组合和子条款，但是我们想要的。我们可以创建由和条款子集组成的子树，然后将它们合并在一起，从而保留结果。</p><p> This allowed us to create an equivalent predicate tree that had a different shape. Building a balanced tree from an array of elements is a relatively standard tree algorithm. Using that instead of the heavily right-skewed tree resulted in enormous performance gains, with the performance going from multiple hours (or never finishing, for larger tree) to taking seconds.</p><p> 这使我们可以创建具有不同形状的等效谓词树。从一系列元素构建平衡树是一个相对标准的树算法。使用它而不是重右歪斜的树，导致巨大的性能增益，具有多小时的性能（或从未完成更大的树）以取出秒。</p><p>   Deploying this change to our production jobs made all the performance issues related to this code go away. We didn&#39;t need to wait for a Spark release or fork the codebase in order to change Spark internals, which was great for our velocity in getting a fix out.</p><p>   将此更改部署到我们的生产作业使与此代码相关的所有性能问题消失。我们不需要等待Spark释放或叉码Base才能改变Spark内部，这对于我们速度进行修复而言。</p><p> However, we knew that the right thing to do was to remove the exponential algorithm from Spark altogether, so we tackled that next.</p><p> 但是，我们知道正确的事情是从火花中删除来自火花的指数算法，因此我们接下来解决了这一点。</p><p>  We contributed a fix for the exponential complexity to Spark in  [SPARK-27105][SQL] Optimize away exponential complexity in ORC predicate conversion by IvanVergiliev · Pull Request #24068 · apache/spark. It was included in the Spark 3.0 release  ( Spark Release 3.0.0 | Apache Spark ).</p><p>  我们贡献了在[Spark-27105] [SQL]中的指数复杂性进行了修复，以通过Ivanvergiliev·拉请求的兽人谓词转换中的兽人谓词转换＃24068·apache /火花。它包含在Spark 3.0释放（Spark Release 3.0.0 | Apache Spark）中。</p><p> The high-level idea for avoiding the exponential behavior completely was more or less clear —  don&#39;t call into the same method for &#34;checking for transformability&#34; and &#34;performing the transformation&#34;. Making the two different actions go through different code paths avoids the &#34;doing something twice, recursively&#34; problem we covered above.</p><p> 避免指数行为的高级别思想或多或少地清除 -  Don＆＃39; T呼叫与＆＃34的相同方法;检查变形性和＃34;和＃34;执行转型＆＃34;通过不同的代码路径使两种不同的行动避免了＆＃34;两次做某事，递归和＃34;我们上面覆盖的问题。</p><p> Getting to a code structure that is readable and maintainable and everyone is happy with took a bit longer though. After a number of different iterations, and incorporating ideas from multiple people, we ended up with code that is both readable and performant.</p><p> 获取可读和可维护的代码结构，每个人都很满意，但花了一点时间。在许多不同的迭代之后，并将思想与多个人合并，我们最终用了可读和表演的代码。 </p><p> If you like to solve production performance issues with algorithmic optimizations, and to improve the underlying tools you&#39;re using,  we are hiring! Learn more about the awesome people on  our Engineering team.</p><p>如果您喜欢用算法优化解决生产性能问题，并改善您＆＃39的底层工具;重新使用，我们正在招聘！ 了解有关我们工程团队的令人敬畏的人的更多信息。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://heap.io/blog/accidentally-exponential-behavior-in-spark">https://heap.io/blog/accidentally-exponential-behavior-in-spark</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/幂态/">#幂态</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/spark/">#spark</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>