<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>解释变压器语言模型的接口 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">解释变压器语言模型的接口 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-23 04:21:15</div><div class="page_narrow text-break page_content"><p>Transformer体系结构为NLP的最新进展提供了动力。这里提供了此体系结构的详细信息。基于架构的预训练语言模型，包括自动回归模型（使用自己的输出作为下一个时间步长的输入以及从左到右处理令牌的模型，例如GPT2）和降噪（模型训练后的模型）破坏/屏蔽输入并双向处理令牌，例如BERT变种，在NLP和最近的计算机视觉的各种任务中继续推动着发展。但是，我们对这些模型为何如此有效的理解仍然落后于这些发展。</p><p> 该博览会系列继续致力于解释和可视化基于转换器的语言模型的内部工作。我们说明了一些关键的可解释性方法如何应用于基于转换器的语言模型。本文重点介绍自动回归模型，但是这些方法也适用于其他体系结构和任务。</p><p> 这是该系列的第一篇文章。在其中，我们提供了可探索性和可视化效果，有助于直观地了解：</p><p> 神经元激活以及模型神经元的个体和组如何响应输入并产生输出而突增。</p><p> 下一篇文章介绍了模型各层之间的隐藏状态演化，以及它可能告诉我们有关每一层角色的信息。</p><p> 在可解释机器学习（IML）的语言中，例如Molnar等人的文献。 ，输入显着性是一种解释单个预测的方法。后两种方法属于“分析更复杂的模型的组件”的范畴，并且可以更好地描述为增加变压器模型的透明度。</p><p> 此外，本文还附带了可复制的笔记本和Ecco（一个开放源代码库），可直接在Jupyter笔记本中为来自HuggingFace变压器库的基于GPT的模型创建类似的交互界面。 </p><p>如果我们要强加三个要检查的组件以探索变压器的体系结构，则它看起来如下图所示。</p><p>    当计算机视觉模型将图片分类为包含沙哑时，显着度图可以告诉我们分类是由于动物本身的视觉特性还是由于背景中的雪。这是一种归因方法，用于解释模型的输出和输入之间的关系-帮助我们检测错误和偏差，并更好地了解系统的行为。</p><p>  存在多种用于将重要性得分分配给NLP模型的输入的方法。文献最常与分类任务而不是自然语言生成相关。本文重点介绍语言生成。我们的第一个接口在生成每个令牌后计算功能重要性，并通过悬停或点击输出令牌将显着性图强加于负责生成令牌的令牌。</p><p> 该界面的第一个示例向GPT2-XL询问威廉·莎士比亚的出生日期。该模型能够正确生成日期（1564，但分为两个标记：＆＃34; 15＆＃34;和＆＃34; 64＆＃34 ;，因为该模型的词汇表不包含＆＃34 ; 1564＆＃34;作为单个令牌）。该界面显示了生成每个输出令牌时每个输入令牌的重要性：</p><p>  我们的第二个示例试图探究模型的世界知识，并查看模型是否重复文本中的模式（简单的模式，如数字后的句点和换行，以及涉及更多的模式，如完成编号列表）。这里使用的模型是DistilGPT2。</p><p> 此探索图显示了一个更详细的视图，该视图显示了每个令牌的归因百分比，以防您需要这种精度。</p><p>  在本文的其余部分中，我们将使用另一个示例进行说明，其中一个示例是要求模型完成一个简单的模式： </p><p>也可以使用该接口来分析基于转换器的会话代理的响应。在下面的示例中，我们向DiabloGPT提出了一个存在的问题：</p><p>    上面展示的是基于Gradients X Inputs评分功能的重要性-Atanasova等人展示的基于梯度的显着性方法。在各种数据集中表现出色，以在变压器模型中进行文本分类。</p><p> 为了说明它是如何工作的，我们首先回顾一下模型在每个时间步中如何生成输出令牌。在下图中，我们将看到①语言模型的最终隐藏状态如何投射到模型词汇表中，从而为模型词汇表中的每个标记生成一个数字分数。通过softmax运算将得分矢量传递给每个令牌会得出概率得分。 ②我们根据该向量选择令牌（例如，选择最高概率评分令牌，或从最高评分令牌中抽样）。</p><p>  ③通过将所选logit一直反向传播回输入令牌来计算所选logit（在softmax之前）相对于输入的梯度，我们得到一个信号，表明每个令牌在计算中的重要性，导致生成令牌。该假设基于这样的思想，即输入令牌中具有最高特征重要性值的最小变化会对模型的最终输出产生较大的变化。</p><p>  然后将每个令牌的所得梯度向量乘以相应令牌的输入嵌入。采用所得向量的L2范数会得出令牌的特征重要性得分。然后，我们将这些分数除以这些分数的总和来对其进行归一化。</p><p>  if X ifc（X 1：n）X i∥2 \ lVert \ nabla _ {X_i} f_c（X_ {1：n}）X_i \ lVert_2∇X i�fc（X 1：n）X我∥2</p><p> 其中，输入令牌在时间步i处的嵌入向量在哪里，是所选令牌得分的反向传播梯度，解压缩如下： </p><p>是输入序列（长度为）中输入令牌嵌入向量的列表</p><p>  是前向通过模型（通过包括贪婪/ argmax解码，采样或波束搜索在内的多种方法中的任意一种选择）之后所选令牌的分数。用c代表＆＃34; class＆＃34;鉴于此通常在分类上下文中进行描述。即使在我们的案例中，我们仍保留该符号＆＃34; token＆＃34;更合适。</p><p>  这种形式化是Bastings等人所说的。除了梯度和输入向量是逐元素相乘的。然后，通过计算L2范数，将所得向量汇总为一个分数，如Atanasova等人的经验所示。比其他方法（例如求平均值）要好。</p><p>   前馈神经网络（FFNN）子层是变压器模块内的两个主要组件之一（除了自我关注之外）。它占变压器块参数的66％，因此提供了模型表示能力的很大一部分。先前的工作已经检查了NLP和计算机视觉领域中深层神经网络内部的神经元激发。在本节中，我们将该检查应用于基于转换器的语言模型。</p><p>  为了指导我们的神经元检查，让我们用输入的＆＃34; 1、2、3＆＃34;希望它会回覆逗号/数字的更改，同时还要继续增加数字。</p><p>   通过使用我们将在第2条中讨论的方法（遵循怀旧吉他手的方法），我们可以生成一个图形，该图形在模型的每一层之后显示输出令牌的概率。这将查看每层之后的隐藏状态，并显示该层中最终产生的输出令牌的排名。</p><p> 例如，在第一步中，模型产生了令牌＆＃34;。 4＆＃34;。第一列介绍了该过程。该列最底部的单元格显示令牌＆＃34; 4＆＃34;在最后一层之后的概率排名第一。这意味着最后一层（以及模型）给予它最高的概率分数。上方的单元格表示令牌的排名＆＃34; 4＆＃34;每层之后。 </p><p>通过查看隐藏状态，我们观察到该模型收集了有关不同层上输出序列的两种模式（逗号和升序数字）的置信度。</p><p>  在第4层会发生什么，使模型将数字（4、5、6）提升到概率分布的顶部？</p><p> 我们可以绘制第4层中神经元的激活图，以获得对神经元活动的感觉。这就是下面三个图中的第一个显示的内容。</p><p> 但是，很难通过对模型进行前向传递期间的激活来获得任何解释。</p><p> 下图显示了神经元的激活，同时生成了五个令牌（＆＃39; 4，5，6＆＃39;）。为了避开射击的稀疏性，我们不妨对射击进行聚类，下图显示了这一点。</p><p>  如果可视化并正确检查，神经元放电可以揭示单个神经元和神经元组可以发挥的互补和组成作用。</p><p> 即使是在聚类之后，直接查看激活也是一个粗略而嘈杂的事情。如Olah等人所述。 ，我们最好使用矩阵分解方法降低维数。我们跟随作者建议使用非负矩阵分解（NMF）作为自然的选择，以将维数减少为可能单独解释的组。我们的第一个实验是使用主成分分析（PCA），但是NMF是一种更好的方法，因为它难以解释神经元激发的PCA成分中的负值。 </p><p>通过首先捕获模型FFNN层中神经元的激活，然后使用NMF将它们分解为更易于控制的因素（使用），我们可以阐明各种神经元如何对每个生成的令牌做出贡献。</p><p> 最简单的方法是将激活分为两个因素。在我们的下一个界面中，我们让模型生成三十个令牌，将激活分解为两个因子，并在生成该令牌时以激活程度最高的因子突出显示每个令牌：</p><p>  此接口能够压缩大量数据，这些数据展示了由神经元组组成的因子的兴奋程度。左侧的迷你图提供了整个序列中每个因素的兴奋程度的快照。与迷你图进行交互（通过将鼠标悬停或点击触摸屏）会按右侧序列显示标记上因子的激活。</p><p> 我们可以看到，将激活分解为两个因素会导致与我们正在分析的交替模式（逗号和递增数字）相对应的因素。我们可以通过增加因子数量来提高因子分析的分辨率。下图将相同的激活分解为五个因素。</p><p>  我们可以开始将此扩展到具有更多内容的输入序列，例如欧盟国家/地区列表：</p><p>  另一个有关DistilGPT2对XML的反应方式的示例清楚地表明了影响语法不同组成部分的因素。这次我们将激活分为十个部分：</p><p>   该界面是隐藏状态检查的良好伴侣，它可以突出显示特定的关注层，使用该界面，我们可以将分析重点放在该关注层上。将这种方法直接应用于感兴趣的特定层很简单。例如，隐藏状态演化图表明第0层做了很多繁重的工作，因为它通常倾向于将使它排在概率分布顶部的令牌入围。下图展示了响应Fyodor Dostoyevsky的通过而应用于第0层激活的十个因素： </p><p>我们可以通过增加因素数量来提高分辨率。将其增加到18个因素开始揭示响应副词而亮起的因素，以及响应于部分标记而亮起的其他因素。进一步增加因素的数量，您将开始识别响应于特定单词而亮起的因素（“没什么”和“人”似乎对该层特别挑衅）。</p><p>  上面的可探究结果显示了使用非负矩阵分解分解包含FFNN神经元激活值的矩阵所导致的因素。下图阐明了如何完成此操作：</p><p>  除了降维以外，非负矩阵分解还可以揭示神经元组的潜在常见行为。它可用于分析整个网络，单个层或一组层。</p><p>   本系列的第一篇文章到此结束。请务必单击笔记本并与Ecco一起玩！我希望您对本系列文章和Ecco的反馈。如果您发现有趣的因素或神经元，也可以将它们张贴在这里。欢迎所有反馈！</p><p>  感谢Abdullah Almaatouq，Ahmad Alwosheel，Anfal Alatawi，Christopher Olah，Fahd Alhazmi，Hadeel Al-Negheimish，Isabelle Augenstein，Jasmijn Bastings，Najla Alariefy，Najwa Alghamdi，盖尔曼。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://jalammar.github.io/explaining-transformers/">https://jalammar.github.io/explaining-transformers/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/变压器/">#变压器</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>