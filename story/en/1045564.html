<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>通过机器学习预测硬盘故障 Predicting Hard Drive Failure with Machine Learning</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Predicting Hard Drive Failure with Machine Learning<br/>通过机器学习预测硬盘故障 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-21 23:06:36</div><div class="page_narrow text-break page_content"><p>We’ve all had a hard drive fail on us, and often it’s as sudden as booting your machine and realizing you can’t access a bunch of your files. It’s not a fun experience. It’s especially not fun when you have an entire data center full of drives that are all important to keeping your business running. What if we could predict when one of those drives would fail, and get ahead of it by preemptively replacing the hardware before the data is lost? This is where the history of predictive drive failure at Datto begins.</p><p>我们所有人的硬盘驱动器都发生故障，而且常常是突然启动计算机并意识到您无法访问大量文件。这不是一个有趣的经历。当整个数据中心的驱动器对保持业务正常运行至关重要时，这尤其不好玩。如果我们可以预测这些驱动器之一何时会发生故障，并通过在数据丢失之前抢先更换硬件来领先于它，该怎么办？这是Datto发生预测性驱动器故障的历史的起点。</p><p> First and foremost, to make a prediction you need data. Hard drives have a built-in utility called SMART  (  S elf-  M onitoring,   A nalysis and   R eporting   T echnology) that reports an array of statistics about how the drive is functioning. Here’s an abbreviated view of what that looks like:</p><p> 首先，要做出预测，您需要数据。硬盘驱动器具有一个内置的实用程序，称为SMART（自我监测，分析和报告技术），可报告有关驱动器工作方式的一系列统计信息。这是看起来的简略视图：</p><p>  Datto collects a report like this from each hard drive in its storage servers once per day. Each attribute in the report has three important numbers associated with it: value, thresh, and worst. Each attribute also has a feature named raw_value, but this is discarded due to inconsistent reporting standards between drive manufacturers.</p><p>  Datto每天从其存储服务器中的每个硬盘收集这样的报告。报告中的每个属性都有三个与之相关的重要数字：值，阈值和最差值。每个属性还具有一个名为raw_value的功能，但是由于驱动器制造商之间的报告标准不一致，因此将其丢弃。</p><p> Value: A number between 1 and 253, inclusive. The value reflects how well the drive is operating with respect to the attribute, with 1 being the worst and 253 being the best. The initial value is arbitrarily determined by the manufacturer, and can vary by drive model.</p><p> 值：1到253之间的数字（包括1和253）。该值反映驱动器相对于该属性的运行状况，其中1为最差，253为最佳。初始值由制造商任意确定，并且可以随驱动器型号而变化。</p><p> Thresh: A threshold below which the value should not fall in normal operation. If the value falls below the threshold, there is likely something wrong with the drive.</p><p> 阈值：一个阈值，在正常操作中该值不应低于该阈值。如果该值低于阈值，则驱动器可能存在问题。</p><p>  A quick approach to using these values to make useful predictions is to pick a couple of attributes that seem important and make an alert if any of their values pass below the associated threshold. This is how the first iteration of predictive drive failure at Datto worked. It wasn’t perfect, but it was definitely better than nothing!</p><p>  使用这些值进行有用预测的一种快速方法是选择一些看起来很重要的属性，并在它们的任何一个值超过关联的阈值时发出警报。这就是Datto的预测驱动器故障的第一次迭代的工作方式。这不是完美的，但绝对比没有强！</p><p>  The next iteration of drive failure prediction was assigning a weighted health score to each drive. This score was defined by assigning weight to several different attributes based on how severe they appeared to be, then adding them together. This prediction method was better than its predecessor, but could potentially be improved even further.</p><p>  驱动器故障预测的下一个迭代是为每个驱动器分配加权的健康评分。通过根据权重的严重程度为几个不同的属性分配权重，然后将它们加在一起，来定义该分数。这种预测方法比其前身要好，但有可能进一步改进。 </p><p>  This brings us to the most recent iteration of drive failure prediction at Datto and the topic of this article:  smarterCTL, a machine learning model using most attributes reported by SMART to make the most informed prediction possible.  Smartctl is the command line utility used to collect SMART reports. SmarterCTL is a machine learning model making use of  smartctl, so it’s the smart-ER version of it. Yeah, it’s a bad pun, but I’m an engineer not a comedian.</p><p>这使我们了解了Datto的驱动器故障预测的最新迭代，以及本文的主题：smarterCTL，这是一种使用SMART报告的大多数属性的机器学习模型，可以进行最明智的预测。 Smartctl是用于收集SMART报告的命令行实用程序。 SmarterCTL是一种使用smartctl的机器学习模型，因此它是smart-ER版本。是的，这是一个双关语，但我不是工程师，而是喜剧演员。</p><p>  Before we get into the details of smarterCTL, let’s briefly go over machine learning. At its core, machine learning is the process of applying statistics to a dataset to find patterns in it. Once the patterns are found, they can be applied to new data to make assumptions about what that new data means. The defining feature of machine learning is that the programmer doesn’t have any input in figuring out the patterns in the data; the algorithm does that on its own through trial and error. For a great high level introduction to how machine learning works, check out  this two minute video.</p><p>  在深入介绍smarterCTL的细节之前，让我们简要介绍一下机器学习。机器学习的核心是将统计信息应用于数据集以在其中找到模式的过程。一旦找到模式，就可以将其应用于新数据，以对新数据的含义进行假设。机器学习的定义特征是程序员在确定数据模式方面没有任何输入；该算法通过反复试验自行完成此操作。有关机器学习的工作原理的高级概述，请观看以下两分钟的视频。</p><p> To get a feel for workflow and terminology, let’s walk through a simplified application of machine learning on drive failure prediction. This example glosses over details and makes some leaps of logic to keep the scope broad. For a more accurate look under the hood of decision tree-based machine learning models, check out this  article and the other resources linked throughout this section.</p><p> 为了了解工作流程和术语，让我们逐步学习一下机器学习在驱动器故障预测上的简化应用。该示例掩盖了细节，并进行了一些逻辑上的飞跃，以保持范围广泛。要在基于决策树的机器学习模型的背景下获得更准确的外观，请查看本文以及本节中链接的其他资源。</p><p> To make predictions, we need a dataset to train the model on. In this case, the data points are hard drives and the features of those data points are the attributes SMART provides.</p><p> 为了进行预测，我们需要一个数据集来训练模型。在这种情况下，数据点是硬盘驱动器，而这些数据点的功能就是SMART提供的属性。</p><p>  These data points are labelled as members of the negative or positive class, which in this case means “hard drive operates normally” or “hard drive has failed.” Note that the “positive” in “positive class” doesn’t mean “good.” Instead, it means “this sample exhibits the behavior we’re looking out for.” A machine learning model would read this dataset, then look for patterns in the features that determine why each hard drive ended up in its class.</p><p>  这些数据点被标记为否定或肯定类的成员，在这种情况下表示“硬盘驱动器正常运行”或“硬盘驱动器发生故障”。请注意，“积极阶层”中的“积极”并不意味着“好”。相反，它的意思是“此示例展示了我们正在寻找的行为。”机器学习模型将读取此数据集，然后在功能中寻找确定每个硬盘最终归入其类的原因的模式。</p><p> Normally there would be enough data points and features that a human couldn’t read the whole dataset—let alone spot a pattern in it! This example is simplified enough for us to step through the process that a model might follow. Let’s look for a pattern in each feature:</p><p> 通常，会有足够多的数据点和特征，人们无法阅读整个数据集，更不用说在其中发现模式了！这个例子已经足够简化，使我们逐步完成模型可能遵循的过程。让我们在每个功能中寻找一种模式：</p><p>      Again, we can’t make a determination based on this attribute alone. There isn’t an obvious split, like high numbers being good and low numbers being bad.</p><p>      同样，我们不能仅基于此属性来确定。没有明显的分歧，例如高数字表示好，低数字表示坏。 </p><p>   This time it looks like there is a pattern! Drives with high power-on hours are healthy and drives with low power-on hours will fail. This doesn’t make logical sense though—a drive with low power-on hours should be the healthiest, since it’s the closest to mint condition. Let’s look a little deeper, and see if we can find a correlation between this feature and another that tells us something more logical.</p><p>这次看起来像是有图案！开机时间长的驱动器运行状况良好，开机时间低的驱动器将发生故障。不过，这没有逻辑上的意义-开机时间短的驱动器应该是最健康的，因为它最接近薄荷状态。让我们再深入一点，看看我们是否可以在此功能和另一个功能之间找到关联，从而告诉我们更多的逻辑。</p><p>   A-ha! There’s still a difference in magnitude between the two classes, but this time there’s an explanation for it that makes sense: an older drive with a high spin-up time is aging and degrading normally, while a very young drive with a high spin-up time indicates that there might be something like a factory defect.</p><p>   哈！这两类之间在大小上仍然存在差异，但是这次有一个合理的解释：具有高加速时间的较旧驱动器会正常老化和退化，而具有高加速率的非常年轻的驱动器则会老化时间表明可能存在某种工厂缺陷。</p><p>  Now that we’ve figured out the pattern, let’s see how our theoretical model would classify these new SMART reports:</p><p>  现在我们已经弄清楚了模式，让我们看一下理论模型如何将这些新的SMART报告分类：</p><p>  The ratio of spin-up time over power-on hours for drive X is 0.0002, which indicates that it will remain healthy. The ratio for drive Y is 0.045, pointing toward failure. 	So how do we know if the model made correct predictions? Well, we just have to wait and see. One of the trickier parts of this problem space is verifying results, because the whole point is to allow us to take action before the thing we’re predicting ever happens. Keep this in mind, it’ll come back to bite us later.</p><p>  驱动器X的加速时间与加电时间的比率为0.0002，这表明它将保持健康。驱动器Y的比率为0.045，表示出现故障。那么我们如何知道模型是否做出正确的预测呢？好吧，我们只需要拭目以待。这个问题空间中最棘手的部分之一就是验证结果，因为这样做的主要目的是让我们能够在我们所预测的事情发生之前就采取行动。请记住，以后会再次咬我们。</p><p> SmarterCTL’s job is to classify hard drives as “failing” or “not failing” so Datto can avoid being blindsided by lost drives and preemptively swap them for healthy ones. If there are patterns in SMART stats that indicate drive failure, smarterCTL will learn those patterns from SMART data Datto has collected over the past 3 years. Then smarterCTL can monitor new daily SMART reports and produce an alert when a drive is exhibiting a pattern that indicates failure.</p><p> SmarterCTL的工作是将硬盘驱动器归类为“故障”或“未发生故障”，这样Datto可以避免因丢失的驱动器而失明，并抢先将它们换成健康的驱动器。如果SMART统计信息中有表示驱动器故障的模式，smarterCTL将从Datto在过去3年中收集的SMART数据中学习这些模式。然后，smarterCTL可以监视新的每日SMART报告，并在驱动器呈现出指示故障的模式时发出警报。</p><p>   We have several hundred gigabytes of SMART files spanning 3 years and a relatively beefy server to train a machine learning model on—the next step is to set it up and let it churn through the data, right? Unfortunately the work has just begun. Even though machine learning models have a reputation for taking days, or even weeks to train, preparing the data is often the most time consuming part of the process!</p><p>   我们拥有跨越3年的数百GB的SMART文件，以及用于训练机器学习模型的相对强大的服务器-下一步是对其进行设置并使其遍历数据，对吗？不幸的是，这项工作才刚刚开始。即使机器学习模型以花费数天甚至数周的时间进行训练而闻名，但准备数据通常是该过程中最耗时的部分！</p><p> Software folks know that computers are “dumb” and will only do exactly what they’re instructed to. Therefore, the data fed into a machine learning model needs to be very carefully curated. The model has no intuition about whether the conclusions it’s coming to are logical or not, so data that doesn’t accurately describe the whole problem space will lead to a model that confidently spouts nonsense.</p><p> 软件人员知道计算机是“愚蠢的”，只会按照他们的指示去做。因此，需要非常仔细地整理输入到机器学习模型中的数据。该模型对得出的结论是否合乎逻辑没有直觉，因此无法准确描述整个问题空间的数据将导致该模型毫无疑问地吐出废话。 </p><p> In machine learning, the job of the programmer isn’t to spot the patterns in the data: it’s to spot the antipatterns that the model might fall into along the way. Good data treatment and preparation are the first steps in avoiding those antipatterns.</p><p>在机器学习中，程序员的职责不是发现数据中的模式；而是发现模型可能会遇到的反模式。良好的数据处理和准备工作是避免使用这些反模式的第一步。</p><p>  The first step is turning the human readable SMART reports into something more machine readable.</p><p>  第一步是将人类可读的SMART报告转变为更具机器可读性的内容。</p><p>  I condensed all of the data in each day of smart reports into its own csv file, but some key information was still missing. Since this data will be used to train a model, every data point needs to have a class label associated with it—the model needs to know whether these are healthy or failed drives so it can start to learn about potential patterns.</p><p>  我将智能报表的每一天中的所有数据都压缩到了自己的csv文件中，但是仍然缺少一些关键信息。由于此数据将用于训练模型，因此每个数据点都需要具有与之关联的类标签-模型需要知道这些驱动器是正常运行还是发生故障的驱动器，以便可以开始学习潜在的模式。</p><p> Producing the class labels ended up being a little tricky—if a drive is in poor enough health to be a member of the failing class, it might already be failing badly enough to not report SMART stats. To get a clearer idea of how drives were behaving and when they were failing out, I wrote a script that walked through every day of SMART reports and tracked how and when each drive failed.</p><p> 生产类标签最终会有些棘手-如果驱动器的运行状况足够差，无法成为失败类的成员，则它可能已经严重失败，无法报告SMART统计信息。为了更清楚地了解驱动器的运行方式以及何时发生故障，我编写了一个脚本，该脚本遍历了每天的SMART报告并跟踪每个驱动器发生故障的方式和时间。</p><p> Our servers use the filesystem ZFS, which pools together storage devices into groups called “zpools”. The zpool reports a health status for each drive in the pool. When my script comes across a failing zpool status for a drive, it notes on which day the failure occurred then goes through every daily csv and adds the following features to that drive’s row:</p><p> 我们的服务器使用文件系统ZFS，该文件系统将存储设备汇集在一起​​，称为“ zpools”。 zpool报告池中每个驱动器的运行状况。当我的脚本遇到驱动器的zpool故障状态时，它会记录发生故障的那一天，然后通过每天的csv进行检查，并将以下功能添加到该驱动器的行中：</p><p> If so, in how many days from this csv’s date the failure would occur, and</p><p> 如果是这样，则从该csv日期起的几天内，将会发生故障，并且</p><p>   At this point the dataset is a collection of csv files, one per day, that contain:</p><p>   此时，数据集是一组csv文件，每天一个，其中包含： </p><p>  Seems like enough data to start making predictions, right? Well, kind of! At this point, the data is well-formed enough that a model could understand it and start producing predictions. The quality of those predictions would be poor, though; the data needs a lot more treatment to remove the pitfalls lurking within.</p><p>似乎有足够的数据可以开始进行预测，对吧？好吧，有点！此时，数据的格式已经足够好，模型可以理解它并开始产生预测。但是，这些预测的质量很差。数据需要更多处理以消除潜伏在其中的陷阱。</p><p>  Right now, the data is well formed but messy. It’s in the right shape, but it’s full of missing values and incorrect typing. Here are some real samples from the first iteration of the dataset:</p><p>  目前，数据格式正确但混乱。形状正确，但缺少值和错误的输入。以下是来自数据集第一次迭代的一些真实样本：</p><p>  Unless the data is cleaned, the model will totally trust that  “No device found, FAILED to get smart data!” is a valid hard drive size. To clean the data, the malformed values need to be sniffed out and replaced with one standardized NaN ( Not  A Number) representation. Once the missing values are standardized, the model can handle them appropriately.</p><p>  除非清除数据，否则该模型将完全相信“找不到设备，无法获取智能数据！”是有效的硬盘驱动器大小。要清理数据，需要嗅出格式错误的值，并用一个标准化的NaN（非数字）表示形式替换。一旦缺失值被标准化，模型就可以适当地处理它们。</p><p>  The random error values are gone, but in the 1_val column alone over 1 in 10 entries are NaN. Even though the model knows to overlook those values, having so many unused features can actually skew the results. The non-NaN features associated with a data point will have stronger relative weights when many others are missing. There are two ways to approach this problem: deleting the rows with missing data or recreating the missing data.</p><p>  随机错误值消失了，但仅在1_val列中，超过10个条目中就有1个是NaN。即使模型知道忽略了这些值，但拥有这么多未使用的功能实际上可能会扭曲结果。当缺少许多其他要素时，与数据点关联的非NaN特征将具有更强的相对权重。有两种方法可以解决此问题：删除缺少数据的行或重新创建缺少的数据。</p><p>  If there aren’t too many rows with missing values, it makes sense to drop them. There won’t be too much data loss, and we can be completely sure that the data isn’t skewed by NaNs.</p><p>  如果没有太多缺少值的行，则可以删除它们。不会有太多的数据丢失，而且我们可以完全确定NaN不会扭曲数据。</p><p>   If there are many rows with missing values, first and foremost that’s a red flag that the data may not be complete enough to move forward. If that concern is addressed and the data is sound, then the NaNs can be recreated by imputation: inferring what the missing value would likely be based on other values related to it. There are several ways to do this, and figuring out which is best for any problem is a process of trial and error.</p><p>   如果有很多行缺少值，那么首先是一个危险信号，即数据可能不够完整，无法继续前进。如果解决了这个问题并且数据合理，则可以通过归因来重新创建NaN：根据与之相关的其他值推断出缺失值可能是什么。有几种方法可以做到这一点，找出最适合任何问题的方法是反复试验的过程。</p><p>   What ended up being the right path for smarterCTL was a combination of the two approaches: dropping the rows that had tons of missing values, but keeping and repairing the ones that only had a couple.</p><p>   最终，成为smarterCTL的正确方法是两种方法的组合：删除丢失大量价值的行，但保留并修复仅有缺失的行。 </p><p>   At this point the data is nicely shaped and super clean, which seems like a signal to move forward to predictions. Unfortunately, hard drive manufacturers decided to throw a wrench in that plan. While SMART attribute names and SMART report formatting follow a consistent standard across the board, the values associated with the attributes vary by brand.</p><p>在这一点上，数据的形状和超净度都很好，这似乎是向前进行预测的信号。不幸的是，硬盘制造商决定在该计划中使用扳手。尽管SMART属性名称和SMART报告格式在各个方面都遵循一致的标准，但与属性相关的值随品牌而变化。</p><p>  These are both fresh-off-the-line drives with the default “no errors” value for this attribute. The numbers are very different because they’re produced by two different manufacturers.</p><p>  这些都是全新的驱动器，此属性的默认值为“无错误”。数字是非常不同的，因为它们是由两个不同的制造商生产的。</p><p> A machine learning model doesn’t have the intuition to know that while these values are referring to the same attribute, they are on different scales. It’s like asking someone to describe the difference between something that weighs “10” and something that weighs “40” without telling them that the 10 is in pounds and the 40 is in grams. It’s misleading, and will lead to the model massively favoring certain manufacturers over others when predicting failures.</p><p> 机器学习模型没有直觉来知道，尽管这些值指的是相同的属性，但是它们的规模不同。这就像要某人描述重量为“ 10”的东西和重量为“ 40”的东西之间的区别，而没有告诉他们10的磅数和40的克数。这具有误导性，将导致模型在预测故障时大大偏爱某些制造商。</p><p> To take the difference in scale out of the equation, I split the dataset into many subsets where each subset had only drives from one manufacturer. This worked great for keeping the data consistent, but it introduced a new problem: most of the datasets were too small. We’ll get into that later in Act 4.</p><p> 为了消除比例差异，我将数据集分为许多子集，其中每个子集仅具有一个制造商的驱动器。这对于保持数据一致非常有用，但是却带来了一个新问题：大多数数据集都太小。我们将在第4号法案的后面进行讨论。</p><p> At this point, nothing has been added to the data; we haven’t computed new features or made new data points. What we  have done is get the data in a clean, consistent state that can be used to train a model without falling into any obvious traps.</p><p> 至此，数据没有添加任何内容。我们还没有计算新功能或建立新数据点。我们要做的是使数据处于干净一致的状态，可用于训练模型而不会陷入任何明显的陷阱。</p><p>   Finally it’s time to boot up our machine learning library of choice, XGBoost, and let it rip! Just kidding, there’s actually  even more data processing to do first. The data needs to be split into a set of features and a set of class labels (known as X and Y), then into training and testing sets.</p><p>   最后，该启动我们选择的机器学习库XGBoost了，让它撕裂！只是开个玩笑，实际上还有更多的数据处理要做。数据需要分为一组功能和一组类标签（称为X和Y），然后分为训练和测试集。</p><p>  It’s important to split X and Y to ensure that the class label doesn’t influence predictions—otherwise the model would be able to “cheat” and get the right answer every time without figuring out any real patterns.</p><p>  务必将X和Y分开，以确保类别标签不会影响预测结果；否则，该模型将能够“欺骗”并每次获得正确答案而无需弄清楚任何实际模式。 </p><p> The training set is 30-40% of the full dataset, selected randomly. It’s what’s given to the model to figure out the patterns in the data. During training, the model will have access to the class labels to “check its answer” and see if its suspected patterns are correct.</p><p>训练集是整个数据集的30-40％，是随机选择的。这是提供给模型以找出数据模式的要素。在训练期间，模型将可以访问班级标签以“检查其答案”，并查看其可疑模式是否正确。</p><p> The rest of the data comprises the testing set—this portion of the data is held out and kept secret from the model until after training is complete. After training, the model can make predictions using the testing set as input to get a sneak peek into how the model might perform in production on unseen data. Oftentimes testing sets are smaller, only around 20-30% of the data, but I prefer leaving more data for the testing set when working with imbalanced data (more on imbalanced data later!).</p><p> 其余数据包括测试集，直到训练完成后，这部分数据才被保留并从模型中保密。训练后，该模型可以使用测试集作为输入进行预测，以窥探该模型在看不见的数据中如何在生产中表现。通常，测试集较小，仅占数据的20％到30％，但是我更喜欢在处理不平衡数据时为测试集保留更多数据（稍后再介绍不平衡数据！）。</p><p> Holding out a testing set is really important! It’s the main way to check if the model is overfitting. Overfitting is when a model learns “too much” about the training set, and loses track of the overall trend it&#39;s looking for.</p><p> 提供测试集非常重要！这是检查模型是否过拟合的主要方法。过度拟合是指模型对训练集“了解过多”，而无法跟踪其寻找的总体趋势。</p><p>   Now that the data is split into training and testing, it can finally be chucked into XGBoost and the machines can learn! At first we’ll just use the classifier model with default settings.</p><p>   现在，数据已分为训练和测试，最终可以将其放入XGBoost，机器可以学习了！首先，我们将使用默认设置的分类器模型。</p><p>  At this point,  classifier is our trained model. To check how accurate it is, let’s see what it says about the testing set.</p><p>  在这一点上，分类器是我们训练有素的模型。要检查它的准确性，让我们看看它对测试集的含义。</p><p>      Accuracy is the ratio of correct predictions over total predictions. At first glance, 99.92% accuracy looks incredible! Digging a little deeper reveals why accuracy is actually a misleading measure for this problem: the classes are incredibly imbalanced. There are 400,150 reports from healthy drives and only 215 reports from failed drives. If the model were to predict that every single drive would never fail, then that would still be 99.94% accurate while providing nothing useful.</p><p>      准确性是正确预测与总预测之比。乍看之下，99.92％的准确性看起来令人难以置信！深入研究揭示了为什么精度实际上是解决该问题的一种误导性措施：各类之间的不平衡性令人难以置信。运行状况良好的驱动器有400,150个报告，而故障驱动器只有215个报告。如果该模型预测每个驱动器都不会发生故障，那么它仍然可以达到99.94％的准确度，而没有提供任何有用的信息。</p><p> Matthews Correlation Coefficient is a measure that can handle imbalanced data by taking into account the difference between true and false positives and negatives. MCC is a decimal value between -1 and +1. An MCC of zero is random prediction, +1 is perfect prediction, and -1 is perfectly opposite prediction. In this case an MCC of 0.1975 is low, but still better than random.</p><p> 马修斯相关系数是一种可以通过考虑真假正负之间的差异来处理不平衡数据的度量。 MCC是介于-1和+1之间的十进制值。零的MCC是随机预测，+ 1是完全预测，而-1是完全相反的预测。在这种情况下，MCC为0.1975很低，但仍然好于随机数。 </p><p>   The confusion matrix is a great look into where the model’s strengths and weaknesses are. In this case, the model is great at predicting true negatives; it got 400,000 of them correct. One of the model’s weak points is false negatives. Out of the total 215 failing drives in this sample, it incorrectly predicted that 175 of them would be healthy.</p><p>混淆矩阵可以很好地了解模型的优缺点所在。在这种情况下，该模型非常适合预测真实的负面因素；它得到了其中的40万个正确答案。该模型的弱点之一是假阴性。在此样本中的总共215个故障驱动器中，它错误地预测其中175个将是健康的。</p><p> A model will never get every prediction correct, and the wrong answers will often skew to one side: many more false positives or many more false negatives. Which one a model should favor is a business decision. False negatives represent drives that can’t be preemptively swapped, wasting time and manpower. False positives represent healthy drives that will be swapped regardless, wasting money on new unnecessary hardware. Either could be favorable depending on the target audience, and the confusion matrix can be used to monitor how well the model is being trained to favor either direction.</p><p> 模型永远不会使每个预测正确，错误的答案通常会偏向一侧：更多的误报或更多的误报。模型应该偏向哪一个是业务决策。假阴性代表无法抢先交换的驱动器，浪费时间和人力。误报表示健康的驱动器，无论如何都会交换掉，浪费金钱在新的不必要的硬件上。取决于目标受众，这两种方法中的任何一种都可能是有利的，并且混淆矩阵可用于监视训练模型以偏向任一方向的程度。</p><p>  The data is well formed and describes the problem space well, but the model is producing mediocre results. The first place for improvement is to move beyond the default settings and start tuning hyperparameters. A hyperparameter is an external configuration of the model; it’s something the programmer picks, not something learned from the data. Think of hyperparameters like tuning knobs on an instrument. Here’s an overview of how I tuned the hyperparameters that turned out to be most important to smarterCTL:</p><p>  数据格式正确，可以很好地描述问题空间，但是模型产生的结果中等。改进的首要目标是超越默认设置并开始调整超参数。超参数是模型的外部配置。这是程序员选择的东西，而不是从数据中学到的东西。可以将超参数想像成乐器上的调节旋钮。这是我如何调整对smarterCTL最重要的超参数的概述：</p><p>  eval_metric: The eval_metric is the measure that the model uses to judge how accurate its predictions are while it’s training. By default, XGBoost uses error rate (inverse of accuracy) as the eval_metric. Any measure related to accuracy isn’t helpful when it comes to imbalanced data, so I replaced the eval_metric with a custom function that checks MCC instead.</p><p>  eval_metric：eval_metric是模型用来判断训练期间预测的准确性的度量。默认情况下，XGBoost使用错误率（精度的倒数）作为eval_metric。对于不平衡的数据，任何与准确性相关的度量都无济于事，因此我将eval_metric替换为用于检查MCC的自定义函数。</p><p> early_stopping_rounds: XGBoost trains models in rounds. Every round of training, a change is made and the accuracy is compared to the last version. Without early_stopping_rounds, a model will train for an arbitrary amount of rounds then return the best iteration. With early_stopping_rounds, the model will stop training if accuracy on the test set hasn’t improved in X rounds. The goal is to stop training before the model begins overfitting, even though its accuracy on the training set might still be increasing.</p><p> early_stopping_rounds：XGBoost轮训模型。每轮培训都会进行更改，并将准确性与上一版本进行比较。如果没有early_stopping_rounds，则模型将训练任意数量的回合，然后返回最佳迭代。如果使用Early_stopping_rounds，则模型在X轮中测试集的准确性没有提高的情况下将停止训练。目标是在模型开始过度拟合之前就停止训练，即使模型在训练集上的准确性可能仍在提高。</p><p> scale_pos_weight: Scale_pos_weight is the ratio of positive samples (failing drives) over negative samples (healthy drives). Defining the scale_pos_weight helps improve accuracy when dealing with imbalanced data, because it lets the model know roughly what proportion of classifications should be positive. This metric can be set higher or lower than the real ratio of positive samples to encourage the model to favor false positive or false negatives. A very low scale_pos_weight tells the model to assume that the vast majority of classifications should be negative.</p><p> scale_pos_weight：Scale_pos_weight是阳性样本（故障驱动器）与阴性样本（健康驱动器）的比率。定义scale_pos_weight有助于提高在处理不平衡数据时的准确性，因为它可以使模型大致知道应归为正数的分类比例。可以将该指标设置为高于或低于阳性样本的真实比率，以鼓励模型偏向于假阳性或假阴性。极低的scale_pos_weight告诉模型假定绝大多数分类应为负数。</p><p> base_score: When making a classification, the model gives each data point a score between 0 (negative) and 1 (positive). The closer to 1 the more sure the model is that the classification is positive. Base_score helps handle imbalanced data similarly to scale_pos_weight, but does so through defining the default classification score to encourage the model to err toward either positives or negatives. A base_score of 0.1 tells the model to err on the side of assuming that any classification should be negative.</p><p> base_score：进行分类时，模型会为每个数据点赋予0（负）和1（正）之间的分数。越接近1，模型就越能肯定分类是肯定的。 Base_score与scale_pos_weight相似，可帮助处理不平衡数据，但可以通过定义默认分类得分来鼓励模型向正数或负数误差。如果base_score为0.1，则表明该模型在假定任何分类都为负的方面会出错。 </p><p> Some of these hyperparameter values were decided upon through trial and error, and some were figured out with the help of XGBoost’s cv (cross validation) function. Cv is essentially automated trial and error; it runs through several rounds of boosting using different hyperparameter values and returns the values from the most accurate iteration. Cross validation also provides some other important information relating to statistical significance, which can be read about  here.</p><p>这些超参数值中的一些是通过反复试验确定的，而某些则是借助XGBoost的cv（交叉验证）功能找到的。 Cv本质上是自动试验和错误；它使用不同的超参数值进行了几轮提升，并从最精确的迭代中返回值。交叉验证还提供了其他一些与统计意义有关的重要信息，可以在此处阅读。</p><p>   Definitely better! MCC is significantly higher. Proportionately, true positives are up and false negatives are down considering the total amount of failures randomly sampled into this testing set. This is still a long way from being accurate enough to be useful though, so it’s back to data treatment.</p><p>   绝对更好！ MCC明显更高。相应地，考虑到随机抽取到该测试集中的故障总数，真实的肯定性上升而错误的否定性下降。尽管距离准确到足以有用还还有很长的路要走，所以可以追溯到数据处理。</p><p>  Working through different combinations of these data treatments was an iterative process that spanned a long period of time, and hundreds of different models were trained along the way. I didn’t keep every model so there won’t be accuracy summaries for each data treatment, but I will briefly go over what worked and what di</p><p>  通过这些数据处理的不同组合进行工作是一个反复的过程，跨越了很长一段时间，并且在此过程中训练了数百种不同的模型。我并没有保留所有模型，因此不会为每种数据处理提供准确的摘要，但是我将简要介绍一下哪些有效，哪些</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://datto.engineering/post/predicting-hard-drive-failure-with-machine-learning">https://datto.engineering/post/predicting-hard-drive-failure-with-machine-learning</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/硬盘/">#硬盘</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/hard/">#hard</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>