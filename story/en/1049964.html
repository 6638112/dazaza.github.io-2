<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>在跳上Kubernetes之前先看看Nomad Take a look at Nomad before jumping on Kubernetes</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Take a look at Nomad before jumping on Kubernetes<br/>在跳上Kubernetes之前先看看Nomad </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-01 00:12:04</div><div class="page_narrow text-break page_content"><p>Recently I stumbled upon and then stumbled upon again on  David Anderson’s interesting post about “new Kubernetes”, based on a discussion he had with  Vallery Lancey about what they would do differently if they were rewriting Kubernetes from scratch. Interestingly, a decent part of the proposals for a “new Kubernetes” are design choices made by Hashicorp for  Nomad, which is a pretty underrated orchestrator, and drastically simpler ( one of the main goals of said “new Kubernetes”).</p><p>最近，我偶然发现，然后偶然发现了David Anderson关于“新Kubernetes”的有趣帖子，基于他与Vallery Lancey的讨论，即如果他们从头开始重写Kubernetes，他们会做些什么。有趣的是，关于“新型Kubernetes”的提案中相当不错的一部分是Hashicorp为Nomad设计的选择，Nomad是一个被低估的协调器，而且非常简单（上述“新型Kubernetes”的主要目标之一）。</p><p> Some people are aware that Docker Swarm kinda exists but is abandonware/on life support, and isn’t really recommended anymore, but it still comes up in discussions due to how easy it is to use. For most, that leaves Kubernetes as the only “serious” option, but it is a  very complex piece of software, with a lot of moving parts, which isn’t actually required or need in most cases.</p><p> 有人意识到Docker Swarm确实存在，但是已经放弃了软件/提供生命支持，因此不再推荐使用，但是由于使用起来非常简单，因此仍在讨论中出现。对于大多数人来说，这使Kubernetes成为唯一的“严肃”选项，但这是一个非常复杂的软件，具有许多活动部件，在大多数情况下并不需要或不需要。</p><p>   This inspired me to write a series on Nomad, what it is, why it’s great, where it’s lacking and how to use it.</p><p>   这激发了我写关于Nomad的系列文章，内容是什么，为什么很棒，为什么缺少，以及如何使用。</p><p>  Hashicorp’s Nomad is a simple to run and maintain, yet very flexible task scheduler/orchestrator. It relies on plugins for execution, autoscaling and other features, and can run pretty much anything via its  task drivers - Docker, contairnerd, LXC, rkt, podman, Java, fork/exec, QEMU, firecracker, FreeBSD jails.</p><p>  Hashicorp的Nomad是易于运行和维护的，但非常灵活的任务计划程序/编排器。它依靠插件来执行，自动缩放和其他功能，并且可以通过其任务驱动程序运行几乎所有东西-Docker，contairnerd，LXC，rkt，podman，Java，fork / exec，QEMU，鞭炮，FreeBSD监狱。</p><p> It comes in the form of a single binary, run in two modes ( server, in groups of 3 or 5, which make scheduling decisions and host the APIs and configuration, and an unlimited number of  workers which actually run whatever it is you want to run), and can be automatically clustered via  Consul. The configuration ( both for jobs and of Nomad itself) is in  HCL (I’ll get into more detail about how great that is a bit later) or JSON (mainly for when the jobs are submitted by machines/scripts/tooling and not humans). Multiple clusters can be connected via  multi-region federation for sharing ACLs and for API forwarding ( you can submit a job or request logs to any server for any region and it will be forwarded to the appropriate server). Deployments can be complex out of the box ( rolling, canary, blue/green), and everything is version controlled and rollbackable.</p><p> 它以单个二进制文件的形式出现，以两种模式运行（服务器，以3或5为一组，它们可以制定调度决策并托管API和配置，还有数量不限的工作程序，它们可以实际运行您想要的任何程序）运行），并且可以通过领事自动将其聚类。配置（用于作业和Nomad本身）都在HCL（我稍后会详细介绍它的功能）或JSON（主要用于何时由机器/脚本/工具而非人提交作业）中进行配置）。可以通过多区域联合来连接多个集群，以共享ACL和API转发（您可以向任何区域的任何服务器提交作业或请求日志，并将其转发到适当的服务器）。开箱即用的部署可能很复杂（滚动，金丝雀，蓝色/绿色），并且所有内容都是受版本控制和可回滚的。</p><p> Like most HashiCorp tools, it’s “open core”, meaning that the majority of features are available in an  open source version, and some more advanced/enterprise-y ones ( in Nomad’s case,  multi-region/cluster deployments - deploying something simultaneously to multiple separate clusters, policy as code with  Sentinel and similar ) require upgrading to Nomad Enterprise.</p><p> 像大多数HashiCorp工具一样，它是“开放核心”，这意味着大多数功能都可以在开放源代码版本中使用，以及一些更高级/企业级的功能（在Nomad的情况下，是跨区域/群集部署-可以同时部署一些功能）多个单独的群集，使用Sentinel和类似代码作为策略）需要升级到Nomad Enterprise。</p><p>  job is a declarative file which contains groups of tasks, each task being a container/binary/anything run by an exec driver</p><p>  job是一个声明性文件，其中包含任务组，每个任务都是由exec驱动程序运行的容器/二进制文件/任何文件 </p><p> system jobs (run on all client nodes, equivalent to Kubernetes DaemonSets, for monitoring/logging agents/load balancers)</p><p>系统作业（在所有客户端节点上运行，等效于Kubernetes DaemonSet，用于监视/记录代理/负载均衡器）</p><p>  service, which registers as a Consul service and is thus discoverable ( via API or DNS)</p><p>  服务，注册为领事服务，因此可以发现（通过API或DNS）</p><p> deployment, each version of a job, they’re tracked and can be rollbacked to</p><p> 部署，作业的每个版本，都可以对其进行跟踪并可以回滚到</p><p>  Example of a very basic job that runs a Docker container ( jaegertracing/all-in-one:1.21), with limits of 1000Mhz of CPU and 1024MB of RAM, and registers the service with Consul:</p><p>  运行Docker容器（jaegertracing / all-in-one：1.21）的非常基本的工作示例，其CPU限制为1000Mhz，RAM为1024MB，并向Consul注册服务：</p><p> job  &#34;jaeger&#34; { type  =  &#34;service&#34;  group  &#34;api&#34; {  task  &#34;jaeger&#34; { driver  =  &#34;docker&#34;  config { image  =  &#34;jaegertracing/all-in-one:1.21&#34; }  resources { cpu  =  1000 memory  =  1024 }  service { name  =  &#34;jaeger-query&#34; } } } }</p><p> 工作＆＃34; jaeger＆＃34; {type =＆＃34; service＆＃34;组＆＃34; api＆＃34; {task＆＃34; jaeger＆＃34; {driver =＆＃34; docker＆＃34; config {image =＆＃34; jaegertracing / all-in-one：1.21＆＃34; }资源{cpu = 1000内存= 1024}服务{名称=＆＃34; jaeger-query＆＃34; }}}}</p><p> Note that this is a  very basic job, there are no healthchecks, no persistent storage, no extra configuration, no update strategy, no autoscaling, no exposed ports.</p><p> 请注意，这是一项非常基本的工作，没有运行状况检查，没有持久性存储，没有额外的配置，没有更新策略，没有自动缩放，没有暴露的端口。</p><p>  Nomad tracks each job’s full definitions and deployment history, and allows you to easily rollback and compare them, via the UI, CLI or API, e.g.:</p><p>  Nomad跟踪每个作业的完整定义和部署历史记录，并允许您通过UI，CLI或API轻松回滚和比较它们，例如： </p><p> # List the versions of the job named &#34;opentelemetry-collector&#34;$ nomad job history opentelemetry-collectorVersion  =  1Stable  = falseSubmit Date  = 2021-01-08T21:30:30+01:00Version  =  0Stable  = trueSubmit Date  = 2021-01-08T21:29:48+01:00 # Check the difference between versions$ nomad job history -p opentelemetry-collectorVersion  =  1Stable  = falseSubmit Date  = 2021-01-08T21:30:30+01:00Diff  =+/- Job:  &#34;opentelemetry-collector&#34;+/- Task Group:  &#34;opentelemetry-collector&#34; +/- Task:  &#34;opentelemetry-collector&#34; +/- Config  { args [0 ]:  &#34;--config=local/otel/config.yaml&#34; +/- image:  &#34;otel/opentelemetry-collector-contrib:0.15.0&#34;  =&gt;  &#34;otel/opentelemetry-collector-contrib:0.16.0&#34; ports [0 ]:  &#34;health&#34; ports [1 ]:  &#34;jaeger_thrift_compact&#34;  }Version  =  0Stable  = trueSubmit Date  = 2021-01-08T21:29:48+01:00 # Revert job &#34;opentelemetry-collector&#34; to version 0$ nomad job revert opentelemetry-collector  0</p><p>＃列出名为＆＃34; opentelemetry-collector＆＃34; $ nomad的工作历史记录的版本opentelemetry-collectorVersion = 1Stable = falseSubmit Date = 2021-01-08T21：30：30 + 01：00Version = 0Stable = trueSubmit Date = 2021-01-08T21：29：48 + 01：00＃检查版本之间的差异$游牧工作历史-p opentelemetry-collectorVersion = 1稳定= false提交日期= 2021-01-08T21：30：30 + 01：00差异= + / -工作：＆＃34; +/-采集器＆＃34; +/-任务组：＆＃34; opentelemetry-收集器＆＃34; +/-任务：＆＃34; opentelemetry-collector＆＃34; +/- Config {参数[0]：＆＃34;-config = local / otel / config.yaml＆＃34; +/-图片：＆otel / opentelemetry-collector-contrib：0.15.0＆＃34; =＆gt; ＆＃34; otel / opentelemetry-collector-contrib：0.16.0＆＃34;端口[0]：＆＃34;健康状况＆＃34;端口[1]：＆＃34; jaeger_thrift_compact＆＃34; }版本= 0稳定=正确提交日期= 2021-01-08T21：29：48 + 01：00＃还原作业＆＃34; opentelemetry-collector＆＃34;到版本0 $ nomad job还原opentelemetry-collector 0</p><p>  Nomad keeps the desired state and its history, and with  nomad job plan, similar to  terraform plan, allows us to preview what will change upon applying a new job file. There’s also a feature to verify nothing has changed between the  plan and  run (equivalent to  terraform apply with a plan file) with the  -check-index flag:</p><p>  Nomad保持期望的状态及其历史记录，并且与terraform计划类似，nomad的工作计划使我们可以预览应用新的工作文件时将发生的变化。还有一个功能，可以通过-check-index标志来验证计划和运行之间是否没有任何变化（等同于计划文件中的terraform应用）：</p><p> $ nomad job plan otel.nomad+/- Job:  &#34;otel&#34;+/- Task Group:  &#34;opentelemetry&#34;  ( 1 create/destroy update ) +/- Task:  &#34;opentelemetry-collector&#34;  (forces create/destroy update ) +/- Config  { args [0 ]:  &#34;--config=local/otel/config.yaml&#34; +/- image:  &#34;otel/opentelemetry-collector-contrib:0.15.0&#34;  =&gt;  &#34;otel/opentelemetry-collector-contrib:0.20.0&#34; ports [0 ]:  &#34;health&#34; ports [1 ]:  &#34;jaeger_thrift_compact&#34;  }Scheduler dry-run:- All tasks successfully allocated.Job Modify Index:  413To submit the job with version verification run:nomad job run -check-index  413 otel.nomadWhen running the job with the check-index flag, the job will only be run  if thejob modify index given matches the server-side version. If the index haschanged, another user has modified the job and the plan &#39;s results arepotentially invalid.</p><p> $ nomad工作计划otel.nomad +/-工作：＆＃34; otel＆＃34; +/-任务组：＆＃34; opentelemetry＆＃34; （1个创建/销毁更新）+/-任务：＆＃34; opentelemetry-collector＆＃34; （强制创建/销毁更新）+/- Config {参数[0]：＆＃34;-config = local / otel / config.yaml＆＃34; +/-图片：＆otel / opentelemetry-collector-contrib：0.15.0＆＃34; =＆gt; ＆otel / opentelemetry-collector-contrib：0.20.0＆＃34;端口[0]：＆＃34;健康状况＆＃34;端口[1]：＆＃34; jaeger_thrift_compact＆＃34; }计划程序空运行：-已成功分配所有任务。作业修改索引：413使用版本验证运行提交作业：nomad job run -check-index 413 otel.nomad当运行带有check-index标志的作业时，该作业仅如果给定的作业修改索引与服务器端版本匹配，则运行该命令。如果索引已更改，则其他用户已修改作业，并且该计划的结果可能无效。</p><p> Overall, it’s a very useful feature, especially when collaborating, locally or via CI/CD.</p><p> 总体而言，这是一个非常有用的功能，尤其是在本地或通过CI / CD进行协作时。</p><p>  To check the status of a job, there are a few commands under  nomad job and  nomad alloc</p><p>  要检查作业的状态，在nomad job和nomad alloc下有一些命令</p><p> $ nomad job status otelID  = otelName  = otelSubmit Date  = 2021-02-27T20:41:29+01:00Type  = servicePriority  =  50Datacenters  = dc1Namespace  = defaultStatus  = runningPeriodic  = falseParameterized  = falseSummaryTask Group Queued Starting Running Failed Complete Lostotel  0  0  1  0  0  0Latest DeploymentID  = ea533b6fStatus  = successfulDescription  = Deployment completed successfullyDeployedTask Group Desired Placed Healthy Unhealthy Progress Deadlineotel  1  1  1  0 2021-02-27T20:51:45+01:00AllocationsID Node ID Task Group Version Desired Status Created Modified89031cfd d3cbeb7e otel  0 run running 20s ago 4s ago # logs are at the allocation level ( similar to Kubernetes, where they&#39;re at the container level), so we get them with the alloc id$ nomad alloc logs 89031cfd [... ]</p><p> $ nomad作业状态otelID = otelName = otelSubmit日期= 2021-02-27T20：41：29 + 01：00Type = servicePriority = 50Datacenters = dc1Namespace = defaultStatus = runningPeriodic = falseParameterized = falseSummaryTask组已排队等待开始运行运行失败完成Lostotel 0 0 1 0 0 0最新部署ID = ea533b6f状态=成功描述=部署已成功完成部署的任务组所需放置的健康状况不健康进展Deadlineotel 1 1 1 0 2021-02-27T20：51：45 + 01：00AllocationsID节点ID任务组版本所需的状态已创建已修改89031cfd d3cbeb7e otel 0运行20年前运行4秒钟前＃日志处于分配级别（类似于Kubernetes，它们位于容器级别），因此我们使用alloc id $ nomad alloc日志89031cfd [...]来获取它们。</p><p>  Nomad allows defining the lifecycle of tasks in task groups, and their status, with the  lifecycle stanza. We can have  prestart ( for initialisation ),  poststart ( companion, for proxying (aka ambassador and adapter pattern in Kubernetes )) or  poststop for clean up, and via the  sidecar bool we define whether or not it should run as long as the main task(s), e.g.:</p><p>  Nomad允许使用生命周期节来定义任务组中任务的生命周期及其状态。我们可以使用prestart（用于初始化），poststart（用于伴侣（用于代理）（在Kubernetes中为代理和适配器模式））或poststop（用于清理），并通过sidecar bool定义它是否应与主要任务一样运行（s），例如： </p><p> task  &#34;init&#34; {  lifecycle { hook  =  &#34;prestart&#34; sidecar  =  false } driver  =  &#34;docker&#34;  config { image  =  &#34;alpine/httpie&#34; command  =  &#34;http&#34; args  = [  &#34;POST&#34;,  &#34;https://some-internal-service-for-provisioning-stuff.local/v1/new&#34;, &#34;job_id = &#39; ${ NOMAD_JOB_ID } !&#39;&#34; ] } }  task  &#34;fluentd&#34; {  lifecycle { hook  =  &#34;poststart&#34;  # should start after the main task  sidecar  =  true  # should run as long as the main task does, and be restarted if it fails  } driver  =  &#34;docker&#34;  config { image  =  &#34;fluentd/fluentd&#34; } ... }  task  &#34;main-app&#34; { ... }  task  &#34;cleanup&#34; {  lifecycle { hook  =  &#34;poststop&#34; } driver  =  &#34;docker&#34;  config { image  =  &#34;alpine&#34; command  =  &#34;rm -rf&#34; args  = [  &#34;/var/lib/volume-with-super-secret-data&#34; } }</p><p>任务＆＃34; init＆＃34; {生命周期{hook =＆＃34; prestart＆＃34; sidecar = false}驱动程序=＆＃34; docker＆＃34; config {image =＆＃34; alpine / httpie＆＃34;命令=＆＃34; http＆＃34; args = [＆＃34; POST＆＃34 ;,＆＃34; https：//some-internal-service-for-provisioning-stuff.local/v1/new&#34 ;,＆＃34; job_id =＆＃39 ; $ {NOMAD_JOB_ID}！＆＃39;＆＃34; ]}}任务＆＃34;流利的＆＃34; {生命周期{hook =＆＃34; poststart＆＃34; ＃应该在主要任务sidecar = true之后开始＃应该在主要任务运行的同时运行，并且在失败后重新启动} driver =＆＃34; docker＆＃34; config {image =＆＃34; fluentd / fluentd＆＃34; } ...}任务＆＃34; main-app＆＃34; {...}任务＆＃34;清理＆＃34; {生命周期{挂钩=＆＃34; poststop＆＃34; } driver =＆＃34; docker＆＃34; config {image =＆＃34; alpine＆＃34;命令=＆＃34; rm -rf＆＃34; args = [＆＃34; / var / lib / volume-with-super-secret-data＆＃34; }}</p><p>  ACL ( access-control list ), or RBAC ( role-based access control ) as it’s known in Kubernetes, allow defining who can do what, so that not everyone with network access can have full administrator privileges and run/stop whatever. Nomad’s ACL system is pretty similar to Consul and Vault’s, and uses JSON ( mostly for non-humans ) or HCL to define  policies with  rules, which describe what action is allowed on what object.</p><p>  Kubernetes中众所周知的ACL（访问控制列表）或RBAC（基于角色的访问控制）允许定义谁可以做什么，从而使并非每个具有网络访问权限的人都可以拥有全部管理员权限并可以运行/停止任何操作。 Nomad的ACL系统与Consul和Vault的ACL系统非常相似，并使用JSON（主要用于非人类用户）或HCL来定义带有规则的策略，这些规则描述了可对哪个对象采取何种操作。</p><p> # a basic policy which allows the predefined read policy with read-only access to list and read: # job, volume and scaling details, and extra capabilities for job creation and log access within the default namespace  namespace  &#34;default&#34; { policy  =  &#34;read&#34; capabilities  = [ &#34;submit-job&#34;,&#34;dispatch-job&#34;,&#34;read-logs&#34;]}</p><p> ＃一个基本策略，该策略允许预定义的读取策略具有对列表和读取的只读访问权限：＃作业，卷和扩展详细信息，以及在默认名称空间名称空间＆＃34; default＆＃34中的作业创建和日志访问的额外功能。 {policy =＆＃34; read＆＃34;功能= [＆＃34; submit-job＆＃34;，＆＃34; dispatch-job＆＃34;，＆＃34; read-logs＆＃34;]}</p><p> Assignment of policies is done only via the CLI, unlike Kubernetes where that happens in YAML, as does policy management:</p><p> 策略分配仅通过CLI完成，与Kubernetes在YAML中发生的方式不同，Kubernetes在策略管理中也是如此：</p><p> # create/update the policy within Nomadnomad acl policy apply -description  &#34;Application Developer policy&#34; my-policy my-policy.hclnomad acl token create -name = &#34;Test token&#34; -policy =my-policy -type =clientAccessor ID  = 4e3c1ac7-52d0-6c68-94a2-5e75f17e657eSecret ID  = 0be3c623-cc90-3645-c29d-5f0629084f68Name  = Test tokenType  = clientGlobal  = falsePolicies  =  [my-policy ]Create Time  = 2021-02-10 18:41:53.851133 +0000 UTCCreate Index  =  15Modify Index  =  15</p><p> ＃在Nomadnomad acl策略apply中创建/更新策略-description＆＃34; Application Developer policy＆＃34; my-policy my-policy.hclnomad acl令牌create -name =＆＃34; Test token＆＃34; -policy =我的策略-类型= clientAccessor ID = 4e3c1ac7-52d0-6c68-94a2-5e75f17e657eSecret ID = 0be3c623-cc90-3645-c29d-5f0629084f68Name = Test tokenType = clientGlobal = falsePolicies = [my-policy] Create Time = 2021- 02-10 18：41：53.851133 +0000 UTC创建索引= 15修改索引= 15</p><p> Just for comparison, the (in my opinion weirdly verbose to write due to YAML ) syntax for the equivalent in Kubernetes:</p><p> 只是为了进行比较，Kubernetes中的等效语法（由于YAML，我认为写起来很奇怪冗长）：</p><p> apiVersion:  rbac.authorization.k8s.io/v1 kind:  Role metadata:  namespace:  default  name:  test rules:-  apiGroups: [ &#34;&#34;]  resources: [ &#34;pods&#34;,  &#34;services&#34;]  # a Nomad job contains both the pod equivalents ( task groups ) and services  verbs: [ &#34;get&#34;,  &#34;watch&#34;,  &#34;list&#34;,  &#34;logs&#34;,  &#34;create&#34;,  &#34;update&#34;,  &#34;patch&#34;]</p><p> apiVersion：rbac.authorization.k8s.io/v1种类：角色元数据：命名空间：默认名称：测试规则：-apiGroups：[＆＃34;＆＃34;]资源：[＆＃34; pods＆＃34 ;,＆ ＃34; services＆＃34;]＃一个Nomad作业同时包含pod等效项（任务组）和services动词：[＆＃34; get＆＃34 ;、＆＃34; watch＆＃34 ;、＆＃34; list＆＃ 34;，＆＃34;日志＆＃34;，＆＃34;创建＆＃34;，＆＃34;更新＆＃34;，＆＃34;补丁＆＃34;] </p><p> And with this token, either as an env variable (  NOMAD_TOKEN) or flag ( -token) for the CLI or HTTP header (  X-Nomad-Token) for the API, we can do things.</p><p>使用此令牌，作为CLI的环境变量（NOMAD_TOKEN）或标志（-token）或API的HTTP标头（X-Nomad-Token），我们就可以做。</p><p> ACL policies and tokens are optionally shared with federated clusters for simplified management. We can also have ephemeral tokens via Vault’s  Nomad Secret Backend, which generates single/short-use tokens with specific policies, but there’s no implicit or explicit job role equivalent to Kubernetes&#39;  Service Accounts, one has to pass through Vault for that ( assign a Vault policy to the job).</p><p> ACL策略和令牌可以选择与联合群集共享，以简化管理。我们还可以通过Vault的Nomad Secret Backend拥有临时令牌，该令牌会生成具有特定策略的一次性令牌/短期令牌，但没有与Kubernetes相同的隐式或显式工作角色。服务帐户，为此必须通过Vault（将Vault策略分配给作业）。</p><p>  Nomad is easy to install, maintain, update and scale, even with “advanced” features such as linking multiple clusters across datacenters/regions.</p><p>  Nomad即使具有“高级”功能，例如跨数据中心/区域链接多个群集，也易于安装，维护，更新和扩展。</p><p>  Running Nomad locally for development/testing is just a matter of downloading the binary and running  nomad agent -dev ( significantly easier than  microk8s or  minikube or  kind), and the same goes for Consul and Vault ( which you might need to replicate the production Nomad environment locally).</p><p>  在本地运行Nomad进行开发/测试仅需下载二进制文件并运行Nomad代理-dev（比microk8s或minikube或同类更容易），而Consul和Vault也是如此（您可能需要复制生产Nomad本地环境）。</p><p>  Upgrading Nomad servers is just a matter of replacing the binary and restarting the service. There are detailed  upgrade guides which list the main changes and potential breaking ones/things to take care of, but it’s relatively rare ( and will be even less so since it’s already on 1.0+). Clients need to be drained first before upgrading ( for which there’s also a  detailed guide ), and the behaviour of jobs during that operation can be tweaked via the   migrate stanza.</p><p>  升级Nomad服务器只是替换二进制文件并重新启动服务。有详细的升级指南，其中列出了主要的更改以及可能需要解决的重大变化/事情，但是它相对较少（由于已经在1.0+上，所以会更少）。升级之前，首先需要抽干客户（还有详细的指南），并且可以通过迁移节来调整操作期间作业的行为。</p><p>  Nomad collects  extensive metrics on itself and everything it runs within it, which can be send to compatible agents ( statsD, Datadog ) or scraped by a Prometheus/OpenMetrics-compatible scraper, and they even have  a guide on setting up Prometheus to monitor and alert.</p><p>  Nomad会收集有关其自身以及其中运行的所有内容的大量指标，这些指标可以发送到兼容的代理（statsD，Datadog），也可以由Prometheus / OpenMetrics兼容的抓取工具进行抓取，甚至还提供了有关设置Prometheus进行监视和警报的指南。</p><p>  Forming/joining a cluster can be done manually via  nomad server join, via the  server_join configuration block( which can use static IPs/DNS, or dynamic  cloud auto-join ( based on cloud provider tags or similar)), or via Consul.Federation is done by joining a server in another cluster via its WAN IP/DNS and port:</p><p>  可以通过游牧服务器连接，server_join配置块（可以使用静态IP / DNS或动态云自动连接（基于云提供商标签或类似标签））手动或通过Consul.Federation来完成集群的形成/连接。通过通过其WAN IP / DNS和端口将服务器加入另一个群集中来完成： </p><p>   Nomad has an extensive  API (which includes cool recent additions like the  event stream, allowing the creation integrations and tools that act based on what happens in your Nomad cluster), and it integrates well with a bunch of other tools, some of them from Hashicorp, nicely complementing Nomad to rival the features of the more feature-rich Kubernetes and its ecosystem.</p><p>Nomad具有广泛的API（包括最新的酷炫事件流等添加，允许创建集成和基于Nomad集群中发生的事情而起作用的工具），并且与其他工具（包括Hashicorp的一些工具）很好地集成在一起。 ，很好地补充游牧到对手更丰富的功能Kubernetes及其生态系统的功能。</p><p> Things like  Service Discovery and K/V storage (Services and ConfigMaps respectively) and  secret storage (Secrets) and even features part of the larger Kubernetes ecosystem like  service mesh are delegated to other, existing, well-used and battle tested parts of the HashiStack (Consul and Vault), which makes sense - it follows the Unix philosophy “do one thing and do it well”, and makes things easier for Hashicorp and its users - Consul and Vault are already heavily used, stable and very popular in their respective niches, and the decoupling and separation of concerns allows us to use them outside of Nomad (e.g. you can use Vault, hosted or not on Nomad, for secret storage/dynamic secrets/auth for apps running in Nomad or anywhere else; maybe you even already have a running Vault cluster, and you can just connect your Nomad cluster to it and you have the same storage for all your secrets, regardles of where they are used from).</p><p> 诸如服务发现和K / V存储（分别为服务和ConfigMaps）和秘密存储（秘密）之类的东西，甚至包括更大的Kubernetes生态系统的一部分（如服务网格），都被委托给HashiStack的其他，现有的，经过良好使用和经过测试的部分（Consul和Vault），这很有意义-它遵循Unix的哲学“做一件事，并且做得很好”，并使Hashicorp及其用户更轻松地工作-Consul和Vault已经在各自的领域中得到了广泛使用，稳定和非常流行利基，关注点的分离和分离使我们可以在Nomad之外使用它们（例如，您可以使用Nomad托管或不托管在Vault上的保管箱，以秘密存储/动态机密/对在Nomad或其他地方运行的应用程序进行身份验证；也许您甚至已经有一个正在运行的保管库群集，您只需将Nomad群集连接到该群集，您就可以为所有机密使用相同的存储（无论从何处使用它们）。</p><p> The big downside is that if you just want to run a Nomad cluster, you kinda  need two other tools to install, maintain, stay up to date on, etc. but seeing that all three are similar ( single binary, Raft consensus algorithm, great documentation including very detailed upgrade guides ), it’s not  that complex to achieve.</p><p> 最大的缺点是，如果您只想运行Nomad群集，则还需要另外两个工具来安装，维护，保持最新状态，等等。但是看到这三个都是相似的（单个二进制文件，Raft共识算法，文档（包括非常详细的升级指南），实现起来并不复杂。</p><p> There are also integrations, via Consul, with third-party tools such as Traefik and Fabio for automatic Reverse proxy/Load Balancing.</p><p> 还可以通过Consul与第三方工具（例如Traefik和Fabio）进行集成，以实现自动反向代理/负载平衡。</p><p>   One way that you can make use of Vault and Consul in Nomad job files is via the   template stanza (configuration block) in nomad jobs, which allows the creation of files or environment variables based on templates, and is based Hashicorp’s venerable  consul-template.</p><p>   您可以在Nomad作业文件中使用Vault和Consul的一种方法是通过Nomad作业中的模板节（配置块），该模板节可基于模板创建文件或环境变量，并且基于Hashicorp的尊贵领事模板。</p><p> # creating a YAML configuration file from Consul K/V and services, Nomad metadata and Vault secrets:  template { data  =  &lt;&lt; EOH  ---  bind_port : {{  env  &#34;NOMAD_PORT_db&#34; }} # the port &#34;db&#34;  scratch_dir : {{  env  &#34;NOMAD_TASK_DIR&#34; }}  # the task folder   node_id : {{  env  &#34;node.unique.id&#34; }}  # the unique ID of the Nomad node   service_key : {{  key  &#34;service/my-key&#34; }}  # populated by service/my-key from Consul K/V store   loki_addr : {{  range  service  &#34;loki&#34; }}{{ .Address }}:{{ .Port }}{{ end }} # populated by the IP and port of the service named &#34;loki&#34;  in  Consul &#39; s  Service  catalogue  some_secret : {{  with  secret  \ &#34;secret/data/my-secret\&#34; }}{{.Data.data.value}} {{end}}&#34;  # populated by the secret/my-secret secret in Vault   EOH destination  =  &#34;local/file.yml&#34;}  # doing the same but instead of writing in a YAML file, export the values as env variables:  template { ... destination  =  &#34;secrets/file.env&#34;  # secrets is a special, per-task folder that you can use to store secrets and isn&#39;t browseable via API/CLI/UI. More on that below  env  =  true}</p><p> ＃根据领事K / V和服务，Nomad元数据和保险柜机密创建YAML配置文件：template {data =＆lt;＆lt; EOH --- bind_port：{{env＆＃34; NOMAD_PORT_db＆＃34; }}＃端口＆＃34; db＆＃34; scratch_dir：{{env＆＃34; NOMAD_TASK_DIR＆＃34; }}＃任务文件夹node_id：{{env＆＃34; node.unique.id＆＃34; }}＃Nomad节点service_key的唯一ID：{{key＆＃34; service / my-key＆＃34; }}＃由领事K / V商店loki_addr的service / my-key填充：{{range service＆＃34; loki＆＃34; }} {{.Address}}：{{.Port}} {{end}}＃由名为＆＃34; loki＆＃34;的服务的IP和端口填充。在领事馆s服务目录some_secret：{{带有秘密\＆＃34; secret / data / my-secret \＆＃34; }} {{。Data.data.value}} {{end}}＆＃34; ＃由Vault EOH目标中的secret / my-secret秘密填充=＆＃34; local / file.yml＆＃34;}＃进行相同操作，而不是写入YAML文件，而是将值导出为env变量：template { ...目的地=＆＃34; secrets / file.env＆＃34; ＃secrets是每个任务的特殊文件夹，可用于存储秘密，并且无法通过API / CLI / UI浏览。有关以下内容的更多信息，env = true}</p><p>  HVLv2 greatly improves upon HCLv1 with more dynamism and better type management. One can use variables, for loops, include files, etc. Some examples with common use cases:</p><p>  HVLv2大大增强了HCLv1的动态性和更好的类型管理。可以使用变量，进行循环，包含文件等。一些常见用例的示例： </p><p> Using an “env” variable to determine if we’re in local dev or production, and set variables accordingly; and using a list variable with the default arguments in all cases, default ones for local/production, additional arguments, and merging them.</p><p>使用“ env”变量来确定我们是本地开发人员还是生产人员，并相应地设置变量；并在所有情况下都使用带有默认参数的列表变量，用于本地/生产的默认参数，其他参数并将其合并。</p><p> Note:  local variables are local to the file, and  variable variables are more akin to function parameters ( they can be passed via file, env variable, with defaults, etc.)</p><p> 注意：局部变量是文件的局部变量，而变量变量更类似于函数参数（它们可以通过文件，env变量以及默认值等进行传递）</p><p> variable  &#34;env&#34; { default  =  &#34;local&#34;} variable  &#34;args&#34; { type  =  list( string) default  = []} locals { datacenter  = var.env  ==  &#34;local&#34; ? &#34;dc1&#34; : &#34;eu-west-1&#34; count  = var.env  ==  &#34;local&#34;  ?  1  :  5 default_args  = [ &#34;--something&#34;] local_args  = [ &#34;--verbose&#34;] prod_args  = [ &#34;--production-mode 1&#34;, &#34;--secure&#34;] args  = var.env  ==  &#34;local&#34;  ?  concat( local. default_args,  local. local_args,  var. args)  :  concat( local. default_args,  local. prod_args,  var. args)} job  &#34;test&#34; { ... datacenters  = [ local. datacenter]  # Nomad expects a list here, hence the []   group  &#34;test&#34;{ count  =  local. count  task  &#34;test&#34; { driver  =  &#34;docker&#34;  config { image  =  &#34;registry.test.com:0.1&#34; args  = [ for  a  in  var. args :  a]  # Nomad expects a list here, hence the []  } }}</p><p> 变量＆＃34; env＆＃34; {默认=＆＃34; local＆＃34;}变量＆＃34; args＆＃34; {type = list（string）default = []} locals {datacenter = var.env ==＆＃34; local＆＃34; ？ ＆＃34; dc1＆＃34; ：＆＃34; eu-west-1＆＃34; count = var.env ==＆＃34; local＆＃34; ？ 1：5 default_args = [＆＃34;-something＆＃34;] local_args = [＆＃34;-verbose＆＃34;] prod_args = [＆＃34;-production-mode 1＆＃34 ;,＆＃ 34;-secure＆＃34;] args = var.env ==＆＃34; local＆＃34; ？ concat（local。default_args，local。local_args，var.args）：concat（local。default_args，local。prod_args，var。args）}作业＆＃34; test＆＃34; {...数据中心= [本地。数据中心]＃Nomad在这里需要一个列表，因此[]组＆＃34; test＆＃34; {count = local。计数任务＆＃34; test＆＃34; {driver =＆＃34; docker＆＃34; config {image =＆＃34; registry.test.com：0.1＆＃34; args = [对于in in var。 args：a]＃Nomad需要一个列表，因此[[}}}}</p><p> For comparison, either of those are impossible in Kubernetes-land without third-party tooling - Helm, jsonnet, tanka, ytt, etc. for basic templating/logic, which come with a lot of overhead on top of YAML, and things like  Vault Agent for sidecar secret injection or the  Secrets Store CSI driver, which allows reading secrets as filesystem objects via a CSI driver. Of course, Kubernetes ConfigMaps and Secrets exist for the latter issue, but there are several limitations compared to the Nomad way of doing things:</p><p> 为了进行比较，如果没有第三方工具，那么在Kubernetes-land中这两个都是不可能的-用于基本模板/逻辑的Helm，jsonnet，tanka，ytt等，这在YAML之上有很多开销，并且像Vault这样的东西Sidecar秘密注入的代理程序或Secrets Store CSI驱动程序，它允许通过CSI驱动程序将秘密作为文件系统对象读取。当然，对于后一个问题存在Kubernetes ConfigMap和Secrets，但是与Nomad的处理方式相比存在一些限制：</p><p> Kubernetes Secrets aren’t really secret, they’re base64 encoded strings stored in etcd ( so you’re relying on it being encrypted for “security”)</p><p> Kubernetes Secrets并不是真正的秘密，它们是存储在etcd中的base64编码的字符串（因此，您依赖于对其进行加密以实现“安全性”）</p><p> Kubernetes ConfigMaps and Secrets are fully static, you can’t throw in any dynamism ( an if, a for loop, the IP/port of a service, etc.)</p><p> Kubernetes ConfigMap和Secrets是完全静态的，您无法进行任何动态处理（if，for循环，服务的IP /端口等）。</p><p> both are disconnected from deployments - if you update a ConfigMap used by a Pod, the latter needs to be restarted for the changes to be recongized ( compared to Nomad’s  change_mode which allows you to control how and if the task is restarted/reloaded upon configuration change)</p><p> 两者都与部署断开连接-如果更新Pod使用的ConfigMap，则需要重新启动后者以确认更改（与Nomad的change_mode比较，后者可以控制如何以及是否在配置更改时重新启动/重新加载任务） </p><p> you can’t mix and match, like having a single file with secrets, a service address, K/V config, etc. ( unless you do somethig manually with an initContainer and bash)</p><p>您不能混搭，例如只有一个文件包含机密，服务地址，K / V配置等。（除非您使用initContainer和bash手动进行操作）</p><p> you can’t have dynamic secrets ( like AWS or database credentials with a limited time to live) without external tooling taking care of renews and restarts</p><p> 如果没有外部工具来进行续订和重启，您将无法拥有动态机密（例如AWS或具有有限生存时间的数据库凭证）</p><p>  Nomad supports natively a few deployment methods - rolling updates ( each instance is updated X at a time ), canary ( a small part of the running instances are updated, they’re monitored for anomalies/bugs/errors/etc., and if everything is fine over some period of time, the rest are updated as well) and blue/green (two equally-sized environments are running, blue and green one, but only one is serving traffic; the other one gets the updated version in full, tests are run, and once all is good, traffic is switched to it ), and can do automated rollbacks.</p><p>  Nomad原生支持几种部署方法-滚动更新（每个实例一次更新X），金丝雀（一小部分正在运行的实例已更新，是否受到异常/错误/错误/等等的监视，以及是否所有情况）可以在一段时间内正常运行，其余的也进行更新）和蓝色/绿色（两个大小相同的环境正在运行，蓝色和绿色一个在运行，但是只有一个在服务流量；另一个在完全获取更新后的版本，运行测试，一切正常之后，将流量切换到该测试），并可以执行自动回滚。</p><p>  # generic configuration count  =  3  # 3 allocations   update { max_parallel  =  1  # number of instances to upgrade in parallel  health_check  =  &#34;checks&#34;  # what determines the state of the allocation - it could be health *checks*, *state* (if all tasks are running) or *manual* for human/monitoring/etc. via the API }   # canary count  =  3  # 3 allocations   update { max_parallel  =  1 canary  =  1  # 1 canary allocation }   # blue/green count  =  3  # 3 allocations   update { max_parallel  =  3 canary  = 3 # canary =max_parallel =count  = &gt;  Green  env}</p><p>  ＃通用配置计数= 3＃3分配更新{max_parallel = 1＃并行升级的实例数health_check =＆＃34; checks＆＃34; ＃决定分配状态的因素-对人员/监视/等而言，状态可能是*健康状况*检查*，*状态*（如果所有任务都在运行）或*手动*。通过API}＃canary count = 3＃3个分配更新{max_parallel = 1 canary = 1＃1 canary分配}＃蓝色/绿色count = 3＃3个分配更新{max_parallel = 3 canary = 3＃canary = max_parallel = count = ＆gt;绿色环保}</p><p> Canaries or blue/green deployments can be promoted via the CLI, web UI or API ( e.g. automatically based on metrics/logs/traces data). Hashicorp have a  detailed-ish guide with examples on their HashiCorp Learn platform.</p><p> 可以通过CLI，Web UI或API促进金丝雀或蓝色/绿色部署（例如，根据指标/日志/跟踪数据自动生成）。 Hashicorp在其HashiCorp Learn平台上提供了详细的示例指南。</p><p>  Networking on Nomad is, at its base, very simple and basic, at least when it comes to containers. By default each task gets an IP (as provided by Docker/etc.) in bridge mode, in a shared namespace with the other tasks in its group ( to allow sidecar proxies ). Ports can be exposed by using host networking, dyna</p><p>  从本质上讲，至少在容器方面，Nomad上的网络非常简单和基础。默认情况下，每个任务都以桥接模式在其组中的其他任务共享的命名空间中获得IP（由Docker / etc。提供）（以允许sidecar代理）。通过使用主机网络dyna可以暴露端口</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/">https://atodorov.me/2021/02/27/why-you-should-take-a-look-at-nomad-before-jumping-on-kubernetes/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/看看/">#看看</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/jumping/">#jumping</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/nomad/">#nomad</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>