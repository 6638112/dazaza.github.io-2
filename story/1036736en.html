<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>现代存储非常快。不好的是APIModern storage is plenty fast. It is the APIs that are bad</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Modern storage is plenty fast. It is the APIs that are bad<br/>现代存储非常快。不好的是API</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-27 00:45:17</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/3ad989eb74490fcda4f1bd31b5f9221c.png"><img src="http://img2.diglog.com/img/2020/11/3ad989eb74490fcda4f1bd31b5f9221c.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>I have spent almost the entire last decade in a fairly specialized product company, building high performance I/O systems. I had the opportunity to see storage technology evolve rapidly and decisively. Talking about storage and its developments felt like preaching to the choir.</p><p>在过去的十年中，我几乎已经在一家相当专业的产品公司中度过了，用于构建高性能I / O系统。我有机会看到存储技术迅速而果断地发展。谈论存储及其发展感觉就像向合唱团宣讲。</p><p> This year, I have switched jobs. Being at a larger company with engineers from multiple backgrounds I was taken by surprise by the fact that although every one of my peers is certainly extremely bright, most of them carried misconceptions about how to best exploit the performance of modern storage technology leading to suboptimal designs, even if they were aware of the increasing improvements in storage technology.</p><p> 今年，我换了工作。在一家拥有来自不同背景的工程师的大公司中，我感到惊讶的是，尽管我每个同龄人当然都非常聪明，但他们中的大多数人对如何最好地利用现代存储技术的性能导致次优设计抱有误解。 ，即使他们知道存储技术在不断改进。</p><p> As I reflected about the causes of this disconnect I realized that a large part of the reason for the persistence of such misconceptions is that if they were to spend the time to validate their assumptions with benchmarks, the data would show that their assumptions are, or at least appear to be, true.</p><p> 当我反思这种脱节的原因时，我意识到，这种误解持续存在的很大一部分原因是，如果他们花时间用基准来验证其假设，则数据将表明它们的假设是，或者至少看起来是真的。</p><p>  “Well, it is fine to copy memory here and perform this expensive computation because it saves us one I/O operation, which is even more expensive”.</p><p>  “好吧，可以在这里复制内存并执行这种昂贵的计算，因为它可以节省我们一次I / O操作，这甚至会更加昂贵”。</p><p> “I am designing a system that needs to be fast. Therefore it needs to be in memory”.</p><p> “我正在设计一个需要快速运行的系统。因此它必须在内存中”。</p><p> “If we split this into multiple files it will be slow because it will generate random I/O patterns. We need to optimize this for sequential access and read from a single file”</p><p> “如果将其拆分为多个文件，将会很慢，因为它将生成随机的I / O模式。我们需要对此进行优化以实现顺序访问并从单个文件读取”</p><p> “Direct I/O is very slow. It only works for very specialized applications. If you don’t have your own cache you are doomed”.</p><p> 直接I / O非常慢。它仅适用于非常专业的应用程序。如果您没有自己的缓存，那么您注定要失败。”</p><p> Yet if you skim through specs of modern NVMe devices you see commodity devices with latencies in the microseconds range and several GB/s of throughput supporting several hundred thousands random IOPS. So where’s the disconnect?</p><p>但是，如果您浏览了现代NVMe设备的规格，您会发现商用设备的延迟在微秒范围内，并且吞吐量达到数GB / s，可支持数十万个随机IOPS。那么断开连接在哪里？</p><p> In this article I will demonstrate that while hardware changed dramatically over the past decade, software APIs have not, or at least not enough. Riddled with memory copies, memory allocations, overly optimistic read ahead caching and all sorts of expensive operations, legacy APIs prevent us from making the most of our modern devices.</p><p> 在本文中，我将证明尽管硬件在过去十年中发生了巨大变化，但软件API却没有，或者至少还不够。遗留的API充满了内存副本，内存分配，过于乐观的预读缓存以及各种昂贵的操作，使我们无法充分利用现代设备。</p><p> In the process of writing this piece I had the immense pleasure of getting early access to one of the next generation Optane devices, from Intel. While they are not common place in the market yet, they certainly represent the crowning of a trend towards faster and faster devices. The numbers you will see throughout this article were obtained using this device.</p><p> 在撰写本文的过程中，我感到非常高兴，可以从英特尔早日获得下一代Optane设备之一。尽管它们在市场上并不常见，但它们无疑代表了越来越快的设备趋势的加冕。您将在本文中看到的数字是使用此设备获得的。</p><p> In the interest of time I will focus this article on reads. Writes have their own unique set of issues — as well as opportunities for improvements that I intend to cover in a later article.</p><p> 为了节省时间，我将重点放在阅读文章上。写作有自己独特的问题集，还有改进的机会，我打算在以后的文章中介绍。</p><p>    When legacy APIs need to read data that is not cached in memory they generate a page fault. Then after the data is ready, an interrupt. Lastly, for a traditional system-call based read you have an extra copy to the user buffer, and for mmap-based operations you will have to update the virtual memory mappings.</p><p>    当旧版API需要读取未缓存在内存中的数据时，它们会产生页面错误。然后，在数据准备好之后，产生一个中断。最后，对于传统的基于系统调用的读取，您需要向用户缓冲区添加一个副本，而对于基于mmap的操作，则必须更新虚拟内存映射。</p><p> None of these operations: page fault, interrupts, copies or virtual memory mapping update are cheap. But years ago they were still ~100 times cheaper than the cost of the I/O itself, making this approach acceptable. This is no longer the case as device latency approaches single-digit microseconds. Those operations are now in the same order of magnitude of the I/O operation itself.</p><p> 这些操作都不是便宜的：页面错误，中断，副本或虚拟内存映射更新。但是几年前，它们仍然比I / O本身的价格便宜约100倍，使这种方法可以接受。随着设备延迟接近单位数微秒，情况已不再如此。这些操作现在与I / O操作本身的数量级相同。</p><p> A quick back-of-the-napkin calculation shows that in the worst case, less than half of the total busy cost is the cost of communication with the device per se. That’s not counting all the waste, which brings us to the second problem:</p><p> 快速计算得出的结论是，在最坏的情况下，与设备本身的通信成本占总繁忙成本的不到一半。那还没算完所有的浪费，这把我们引到了第二个问题：</p><p>  Although there are some details I will brush over (like memory used by file descriptors, the various metadata caches in Linux), if modern NVMe support many concurrent operations, there is no reason to believe that reading from many files is more expensive than reading from one. However  the aggregate amount of data read certainly matters.</p><p>尽管有一些细节我会介绍（例如文件描述符使用的内存，Linux中的各种元数据缓存），但是如果现代NVMe支持许多并发操作，则没有理由相信从许多文件中读取比从中读取更为昂贵一。但是，读取的数据总量肯定很重要。</p><p> The operating system reads data in  page granularity, meaning it can only read at a minimum 4kB at a time. That means if you need to read read 1kB split in two files, 512 bytes each, you are effectively reading 8kB to serve 1kB, wasting 87% of the data read. In practice, the OS will also perform read ahead, with a default setting of 128kB, in anticipation of saving you cycles later when you do need the remaining data. But if you never do, as is often the case for random I/O, then you just read 256kB to serve 1kB and wasted 99% of it.</p><p> 操作系统以页面粒度读取数据，这意味着一次只能读取至少4kB的数据。这意味着，如果您需要读取分为两个文件（每个512字节）的1kB读块，则实际上是在读取8kB来提供1kB的数据，浪费了87％的读取数据。实际上，操作系统还将执行默认设置为128kB的预读，以期在您以后需要剩余数据时为您节省周期。但是，如果您从不这样做（通常是随机I / O的情况），那么您只需读取256kB即可提供1kB的服务，而浪费了其中的99％。</p><p> If you feel tempted to validate my claim that reading from multiple files shouldn’t be fundamentally slower than reading from a single file, you may end up proving yourself right, but only because read amplification increased by a lot the amount of data effectively read.</p><p> 如果您想验证我的论点，即从多个文件读取基本上不会比从单个文件读取慢，那么您可能会证明自己是正确的，但这仅仅是因为读取放大使有效读取的数据量增加了很多。</p><p> Since the issue is the OS page cache, what happens if you just open a file with Direct I/O, all else being equal? Unfortunately that likely won’t get faster either. But that’s because of our third and last issue:</p><p> 由于问题是操作系统页面高速缓存，如果仅使用Direct I / O打开文件，而其他条件都相同，会发生什么情况？不幸的是，这可能也不会更快。但这是因为我们的第三个也是最后一个问题：</p><p>  A file is seen as a sequential stream of bytes, and whether data is in-memory or not is transparent to the reader. Traditional APIs will wait until you touch data that is not resident to issue an I/O operation. The I/O operation may be larger than what the user requested due to read-ahead, but is still just one.</p><p>  文件被视为字节的顺序流，并且数据是否在内存中对读取器是透明的。传统的API会一直等到您触摸不驻留的数据以发出I / O操作。由于预读，I / O操作可能大于用户请求的操作，但仍然只是其中之一。</p><p> However as fast as modern devices are, they are still slower than the CPU. While the device is waiting for the I/O operation to come back, the CPU is not doing anything.</p><p> 但是，与现代设备一样快，它们仍然比CPU慢。在设备等待I / O操作返回时，CPU未执行任何操作。</p><p>  Using multiple files is a step in the right direction, as it allows more effectively parallelism: while one reader is waiting, another can hopefully proceed. But if you are not careful you just end up amplifying one of the previous problems:</p><p>  使用多个文件是朝正确方向迈出的一步，因为它可以更有效地并行处理：当一个阅读器在等待时，另一个阅读器有望继续进行。但是，如果您不小心，可能只会放大前面的问题之一：</p><p> In thread-poll based APIs multiple files mean multiple threads, amplifying the amount of work done per I/O operation.</p><p>在基于线程轮询的API中，多个文件意味着多个线程，从而放大了每个I / O操作完成的工作量。</p><p> Not to mention that in many situations that’s not what you want: you may not have that many files to begin with.</p><p> 更不用说在很多情况下都不是您想要的：开始时可能没有那么多文件。</p><p>  I have written extensively in the past about  how revolutionary io_uring is. But being a fairly low level interface it is really just one piece of the API puzzle. Here’s why:</p><p>  过去，我已经写了很多有关革命性创新的文章。但是，作为一个相当低级的接口，它实际上只是API难题的一部分。原因如下：</p><p> I/O dispatched through io_uring will still suffer from most of the problems listed previously if it uses buffered files.</p><p> 如果使用io_uring调度的I / O使用缓冲文件，则仍然会遇到前面列出的大多数问题。</p><p> Direct I/O is full of caveats, and io_uring being a raw interface doesn’t even try (nor should it) to hide these problems: For example, memory must be properly aligned, as well as the positions where you are reading from.</p><p> Direct I / O充满了警告，并且io_uring作为原始接口甚至都不会尝试（也不应该）隐藏以下问题：例如，内存必须正确对齐，以及您要读取的位置。</p><p> It is also very low level and raw. For it to be useful you need to accumulate I/O and dispatch in batches. This calls for a policy of when to do it, and some form of event-loop, meaning it works better with a framework that already provides the machinery for that.</p><p> 这也是非常低的水平和原始的。为了使它有用，您需要累积I / O并分批调度。这就要求制定何时执行该操作的策略，以及某种形式的事件循环，这意味着它可以与已经提供了相应机制的框架更好地协同工作。</p><p> To tackle the API issue I have designed G lommio (formerly known as Scipio), a Direct I/O-oriented thread-per-core Rust library. Glommio builds upon io_uring and supports many of its advanced features like registered buffers and poll-based (no interrupts) completions to make Direct I/O shine. For the sake of familiarity Glommio does support buffered files backed by the Linux page cache in a way that resembles the standard Rust APIs (which we will use in this comparison), but it is oriented towards bringing Direct I/O to the limelight.</p><p> 为了解决API问题，我设计了G lommio（以前称为Scipio），这是一个直接的，面向I / O的线程/内核Rust库。 Glommio建立在io_uring的基础上，并支持其许多高级功能，例如注册缓冲区和基于轮询（无中断）的完成功能，以使Direct I / O发挥作用。为了熟悉起见，Glommio确实以类似于标准Rust API（我们将在此比较中使用的API）的方式支持Linux页面缓存支持的缓冲文件，但它的目的是使Direct I / O成为众人瞩目的焦点。</p><p>  Random access files take a position as an argument, meaning there is no need to maintain a seek cursor. But more importantly: they don’t take a buffer as a parameter. Instead, they use io_uring’s pre-registered buffer area to allocate a buffer and return to the user. That means no memory mapping, no copying to the user buffer — there is only a copy from the device to the glommio buffer and the user get a reference counted pointer to that. And because we know this is random I/O, there is no need to read more data than what was requested.</p><p>随机访问文件将位置作为参数，这意味着无需维护搜索游标。但更重要的是：它们不将缓冲区作为参数。相反，他们使用io_uring的预注册缓冲区来分配缓冲区并返回给用户。这意味着没有内存映射，没有复制到用户缓冲区-只有从设备到glommio缓冲区的副本，并且用户会获得指向该缓冲区的引用计数指针。并且由于我们知道这是随机I / O，因此不需要读取比请求更多的数据。</p><p>  Streams, on the other hand, assume that you will eventually run through the entire file so they can afford to use a larger block size and a read-ahead factor.</p><p>  另一方面，流假定您最终将遍历整个文件，因此它们可以负担使用较大的块大小和预读因子的负担。</p><p> Streams are designed to be mostly familiar to Rust’s default  AsyncRead, so it implements the AsyncRead trait and will still read data to a user buffer. All the benefits of Direct I/O-based scans are still there, but there is a copy between our internal read ahead buffers and the user buffer. That’s a tax on the convenience of using the standard API.</p><p> 流的设计主要是Rust的默认AsyncRead所熟悉的，因此它实现了AsyncRead特征，并且仍将数据读取到用户缓冲区。基于直接I / O的扫描的所有优势仍然存在，但是内部预读缓冲区和用户缓冲区之间存在一个副本。这是对使用标准API的便利性的一种征税。</p><p> If you need the extra performance, glommio provides an  API into the stream that also exposes the raw buffers too, saving the extra copy.</p><p> 如果需要额外的性能，glommio在流中提供了一个API，该API也公开了原始缓冲区，从而节省了额外的副本。</p><p>   To demonstrate these APIs glommio has an  example program that issues I/O with various settings using all those APIs (buffered, Direct I/O, random, sequential) and evaluates their performance.</p><p>   为了演示这些API，glommio提供了一个示例程序，该程序使用所有这些API（缓冲，直接I / O，随机，顺序）对I / O进行各种设置，并评估其性能。</p><p> We start with a file that is around 2.5x the size of memory and start simple by reading it sequentially as a normal buffered file:</p><p> 我们从一个大约是内存大小的2.5倍的文件开始，并通过顺序读取它作为普通的缓冲文件来简单地开始：</p><p>  That is certainly not bad considering that this file doesn’t fit in memory, but here the merits are all on Intel Optane’s out-of-this world performance and the io_uring backend. It still has an effective parallelism of one whenever I/O is dispatched and although the OS page size is 4kB, read-ahead allow us to effectively increase the I/O size.</p><p>  考虑到该文件无法容纳在内存中，这当然不错，但是这里的优点全在于英特尔Optane出色的性能和io_uring后端。每当分派I / O时，它的有效并行度仍为1，尽管OS页面大小为4kB，但预读使我们可以有效地增加I / O大小。</p><p> And in fact, should we try to emulate similar parameters using the Direct I/O API (4kB buffers, parallelism of one), the results would be disappointing, “confirming” our suspicion that Direct I/O is indeed, much slower.</p><p>实际上，如果我们尝试使用Direct I / O API（4kB缓冲区，并行度为1）来模拟相似的参数，结果将令人失望，“证实”我们对Direct I / O的确慢得多的怀疑。</p><p>  But as we discussed, glommio’s Direct I/O file streams can take an explicit read-ahead parameter. If active glommio will issue I/O requests in advance of the position being currently read, to exploit the device’s parallelism.</p><p>  但是，正如我们所讨论的那样，glommio的Direct I / O文件流可以采用显式的预读参数。如果主动式glommio会在当前读取的位置之前发出I / O请求，以利用设备的并行性。</p><p> Glommio’s read-ahead works differently than OS-level read ahead: our goal is to exploit parallelism and not just increase I/O sizes. Instead of consuming the entire read-ahead buffer and only then sending a request for a new batch, glommio dispatches a new request as soon as the contents of a buffer is fully consumed and will always try to keep a fixed number of buffers in-flight, as shown in the image below.</p><p> Glommio的预读与操作系统级别的预读不同：我们的目标是利用并行性，而不仅仅是增加I / O大小。 glommio不会消耗整个预读缓冲区，然后才发送新批处理请求，而是在缓冲区的内容被完全消耗后立即调度一个新请求，并将始终尝试保持固定数量的缓冲区在运行，如下图所示。</p><p>  As initially anticipated, once we exploit parallelism correctly by setting a read-ahead factor Direct I/O is not only on pair with buffered I/O but in fact much faster.</p><p>  如最初预期的那样，一旦我们通过设置预读因子正确地利用了并行性，直接I / O不仅与缓冲I / O配对，而且实际上速度更快。</p><p>  This version is still using Rust’s  AsyncReadExt interfaces, which forces an extra copy from the glommio buffers to the user buffers.</p><p>  该版本仍使用Rust的AsyncReadExt接口，该接口强制从glommio缓冲区到用户缓冲区的额外副本。</p><p> Using the  get_buffer_aligned API gives you raw access to the buffer which avoids that last memory copy. If we use that now in our read test we gain a respectable 4% performance improvement</p><p> 使用get_buffer_aligned API可让您对缓冲区进行原始访问，从而避免了最后一次内存复制。如果我们现在在读取测试中使用它，则可以将性能提高4％</p><p>  The last step is to increase the buffer size. As this is a sequential scan there is no need for us to be constrained by 4kB buffer sizes, except for the sake of comparison with the OS page cache version.</p><p>  最后一步是增加缓冲区大小。由于这是一个顺序扫描，因此我们不需要受4kB缓冲区大小的限制，除非与OS页面缓存版本进行比较。</p><p> At this point, let’s summarize all the things that are going on behind the scenes with glommio and io_uring in the next test:</p><p>现在，让我们在下一个测试中使用glommio和io_uring总结幕后发生的所有事情：</p><p> io_uring is set up for poll mode, meaning there are no memory copies, no interrupts, no context switches.</p><p> io_uring设置为轮询模式，这意味着没有内存副本，没有中断，没有上下文切换。</p><p>   This is more than 7x better than the standard buffered approach. And better yet, the memory utilization was never higher than whatever we set as the read-ahead factor times the buffer size. In this example, 2.5MB.</p><p>   这比标准缓冲方法高出7倍以上。更好的是，内存利用率从未超过我们设置为预读因子乘以缓冲区大小的任何东西。在此示例中，为2.5MB。</p><p>  Scans are notoriously pernicious for the OS page cache. How do we fare with random I/O ? To test that we will read as much as we can in 20s, first restricting ourselves to the first 10% of the memory available (1.65GB)</p><p>  众所周知，扫描对操作系统页面缓存有害。我们如何使用随机I / O进行操作？为了测试我们将在20秒钟内读取尽可能多的内容，首先将自己限制在可用内存的前10％（1.65GB）</p><p>    Direct I/O is 20% slower than buffered reads. While reading entirely from memory is still faster — which shouldn’t surprise anybody, that’s a far cry from the disaster one would expect. In fact, if we keep in mind that the buffered version is keeping 1.65GB of resident memory to achieve this whereas Direct I/O is only using 80kB (20 x 4kB buffers) this may even be  preferable for a particular class of applications that may be better off employing that memory somewhere else.</p><p>    直接I / O比缓冲读取慢20％。尽管完全从内存中读取数据的速度仍然更高-这并不会让任何人感到惊讶，但这与人们所预期的灾难相去甚远。实际上，如果我们要记住，缓冲版本要保留1.65GB的常驻内存来实现此目的，而Direct I / O仅使用80kB（20 x 4kB缓冲区），那么对于可能最好在其他地方使用该内存。</p><p> As any performance engineer would tell you, a good read benchmark needs to read data enough to hit the media. After all, “storage is slow”. So if we now read from the entire file, our buffered performance drops dramatically by 65%</p><p> 正如任何性能工程师都会告诉您的那样，一个良好的读取基准需要读取足以打入媒体的数据。毕竟，“存储速度很慢”。因此，如果现在我们从整个文件中读取数据，我们的缓冲性能将急剧下降65％</p><p>  While Direct I/O, as expected, has the same performance and the same memory utilization irrespectively of the amount of data read.</p><p>  正如预期的那样，直接I / O具有相同的性能和相同的内存利用率，而与读取的数据量无关。</p><p>  If the larger scans are our comparison point, then Direct I/O is 2.3x faster, not slower, than buffered files.</p><p>如果比较大的扫描点是我们的比较点，则直接I / O比缓冲文件快2.3倍，而不是更慢。</p><p>  Modern NVMe devices change the nature of how to best perform I/O in stateful applications. This trend has been going on for a while but has been so far masked by the fact that the APIs, especially the higher level ones, haven’t evolved to match what has been happening in the device — and more recently Linux Kernel layers. With the right set of APIs, Direct I/O is the new black.</p><p>  现代NVMe设备改变了如何在有状态应用程序中最佳执行I / O的性质。这种趋势已经持续了一段时间，但到目前为止，事实已经掩盖了以下事实：API（尤其是高级API）尚未发展到能够与设备以及最近的Linux Kernel层相匹配的水平。通过正确的API集，直接I / O成为了新的选择。</p><p> Newer devices like the newest generation of the Intel Optane just seal the deal. There is no scenario in which standard buffered I/O is undisputedly better than Direct I/O.</p><p> 最新一代的设备，例如最新一代的英特尔Optane，就可以达成协议。毫无疑问，标准缓冲I / O绝对比直接I / O好。</p><p> For scans the performance of well-tailored Direct I/O-based APIs is simply far superior. And while Buffered I/O standard APIs performed 20% faster for random reads that fully fit in memory, that comes at the cost of 200x more memory utilization making the trade offs not a clear cut.</p><p> 对于扫描，量身定制的基于Direct I / O的API的性能简直优越得多。而且，尽管对于完全适合内存的随机读取，缓冲I / O标准API的执行速度提高了20％，但代价是内存利用率提高了200倍，因此折衷方案并非一帆风顺。</p><p> Applications that do need the extra performance will still want to cache some of those results, and providing an easy way to integrate specialized caches for usage with Direct I/O is in the works for glommio.</p><p> 确实需要额外性能的应用程序仍将需要缓存其中一些结果，glommio正在提供一种简便的方法来集成专用缓存以与Direct I / O结合使用。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://itnext.io/modern-storage-is-plenty-fast-it-is-the-apis-that-are-bad-6a68319fbc1a">https://itnext.io/modern-storage-is-plenty-fast-it-is-the-apis-that-are-bad-6a68319fbc1a</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/现代/">#现代</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/storage/">#storage</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/api/">#api</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>