<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>麻省理工学院技术评论 MIT Technology Review</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">MIT Technology Review<br/>麻省理工学院技术评论 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-05 14:50:34</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/31210d1c7c8f0b8f8d6eeda3f5a630da.jpg"><img src="http://img2.diglog.com/img/2020/12/31210d1c7c8f0b8f8d6eeda3f5a630da.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>On the evening of Wednesday, December 2, Timnit Gebru, the co-lead of Google’s ethical AI team, announced  via Twitter that the company had forced her out.</p><p>12月2日星期三晚上，Google道德AI团队的联合负责人Timnit Gebru通过Twitter宣布，该公司已将她驱逐出境。</p><p>  Gebru, a widely respected leader in AI ethics research, is known for coauthoring  a groundbreaking paper that showed facial recognition to be less accurate at identifying women and people of color, which means its use can end up discriminating against them. She also cofounded the Black in AI affinity group, and  champions diversity in the tech industry. The team she helped build at Google is one of the most diverse in AI, and includes many leading experts in their own right. Peers in the field envied it for producing critical work that often challenged mainstream AI practices.</p><p>  Gebru是AI伦理学研究领域广受尊敬的领导者，以与他人合作撰写的开创性论文而著称，该论文表明面部识别在识别有色人种和女性方面不那么准确，这意味着其使用最终可能会歧视他们。她还共同创立了Black in AI亲和力小组，并倡导技术行业的多样性。她在Google协助下建立的团队是AI领域中最多元化的团队之一，并且拥有许多领先的专家。该领域的同行们羡慕它做出的关键工作常常挑战主流AI实践。</p><p>  A  series of tweets,  leaked emails, and  media articles showed that Gebru’s exit was the culmination of a conflict over another paper she co-authored. Jeff Dean, the head of Google AI, told colleagues in an internal email (which he has since  put online) that the paper “didn’t meet our bar for publication” and that Gebru had said she would resign unless Google met a number of conditions, which it was unwilling to meet. Gebru  tweeted that she had asked to negotiate “a last date” for her employment after she got back from vacation. She was cut off from her corporate email account before her return.</p><p>  一系列推文，泄露的电子邮件和媒体文章显示，Gebru的退出是与她共同撰写的另一篇论文发生冲突的最终结果。 Google AI负责人Jeff Dean在一封内部电子邮件（此后已在线发布）中告诉同事，该论文“没有达到我们的出版标准”，Gebru表示除非谷歌遇到许多人，否则她将辞职。不愿满足的条件。格布鲁在推特上说，她从假期回来后已经要求谈判“最后约会”。在她回来之前，她已从公司电子邮件帐户中被切断。</p><p>  Online, many other leaders in the field of AI ethics are arguing that the company pushed her out because of the inconvenient truths that she was uncovering about a core line of its research—and perhaps its bottom line. More than 1,400 Google staff and 1,900 other supporters have also  signed a letter of protest.</p><p>  在线上，人工智能伦理领域的许多其他领导者都在争辩说，由于她发现了研究的核心内容（甚至是底线）方面的不便事实，该公司将她推出了。超过1,400名Google员工和1,900名其他支持者也签署了抗议信。</p><p>   Many details of the exact sequence of events that led up to Gebru’s departure are not yet clear; both she and Google have declined to comment beyond their posts on social media. But MIT Technology Review obtained a copy of the research paper from  one of the co-authors, Emily M. Bender, a professor of computational linguistics at the University of Washington. Though Bender asked us not to publish the paper itself because the authors didn’t want such an early draft circulating online, it gives some insight into the questions Gebru and her colleagues were raising about AI that might be causing Google concern.</p><p>   导致格布鲁（Gebru）离开的确切事件顺序的许多细节尚不清楚；她和Google都拒绝评论他们在社交媒体上的帖子。但是《麻省理工科技评论》（MIT Technology Review）是由其中一位合著者华盛顿大学计算语言学教授Emily M.Bender获得的研究论文的副本。尽管Bender要求我们不要发表论文，因为作者不希望这么早的草案在线发布，但它使人们对Gebru和她的同事提出的有关AI的问题有一定的了解，这些问题可能引起Google的关注。</p><p>  Titled “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” the paper lays out the risks of large language models—AIs trained on staggering amounts of text data. These have grown  increasingly popular—and  increasingly large—in the last three years. They are now extraordinarily good, under the right conditions, at producing what looks like convincing, meaningful new text—and sometimes at estimating meaning from language. But, says the introduction to the paper, “we ask whether enough thought has been put into the potential risks associated with developing them and strategies to mitigate these risks.”</p><p>  标题为“关于随机鹦鹉的危险：语言模型会太大吗？”本文阐述了大型语言模型的风险，这些语言是经过大量文本数据训练的AI。在过去三年中，它们越来越受欢迎，也越来越大。现在，它们在适当的条件下非常擅长于生成看起来令人信服的有意义的新文本，有时甚至可以从语言中估计含义。但是，论文的引言说：“我们问是否已经对与开发这些风险相关的潜在风险以及减轻这些风险的策略进行了足够的思考。”</p><p>    The paper, which builds off the work of other researchers, presents the history of natural-language processing, an overview of four main risks of large language models, and suggestions for further research. Since the conflict with Google seems to be over the risks, we’ve focused on summarizing those here.</p><p>    该文件以其他研究人员的工作为基础，介绍了自然语言处理的历史，概述了大型语言模型的四个主要风险，并提出了进一步研究的建议。由于与Google的冲突似乎要解决风险，因此我们着重在这里进行总结。 </p><p>    Training large AI models consumes a lot of computer processing power, and hence a lot of electricity. Gebru and her coauthors refer to a 2019 paper from Emma Strubell and her collaborators on  the carbon emissions and financial costs of large language models. It found that their energy consumption and carbon footprint have been exploding since 2017, as models have been fed more and more data.</p><p>训练大型AI模型会消耗大量的计算机处理能力，因此会消耗大量电力。 Gebru和她的合著者引用了Emma Strubell及其合作者在2019年发表的关于大型语言模型的碳排放和财务成本的论文。研究发现，自2017年以来，随着向模型提供越来越多的数据，其能耗和碳足迹一直在爆炸式增长。</p><p>   Strubell’s study found that one language model with a particular type of “neural architecture search” (NAS) method would have produced the equivalent of 626,155 pounds (284 metric tons) of carbon dioxide—about the lifetime output of five average American cars. A version of Google’s language model, BERT, which underpins  the company’s search engine, produced 1,438 pounds of CO2 equivalent in Strubell’s estimate—nearly the same as a roundtrip flight between New York City and San Francisco.</p><p>   斯特鲁贝尔（Strubell）的研究发现，使用一种特殊类型的“神经结构搜索”（NAS）方法的语言模型将产生相当于626,155磅（284公吨）的二氧化碳，大约相当于五辆普通美国汽车的使用寿命。谷歌语言模型的一个版本，支撑公司搜索引擎的BERT，产生了1,438磅的二氧化碳当量，据Strubell估计，相当于在纽约市和旧金山之间的往返航班。</p><p>   Gebru’s draft paper points out that the sheer resources required to build and sustain such large AI models means they tend to benefit wealthy organizations, while climate change hits marginalized communities hardest. “It is past time for researchers to prioritize energy efficiency and cost to reduce negative environmental impact and inequitable access to resources,” they write.</p><p>   Gebru的草稿指出，建立和维持如此庞大的AI模型所需的庞大资源意味着它们倾向于使富裕的组织受益，而气候变化对边缘化社区的打击最大。他们写道：“现在研究人员已经优先考虑提高能源效率和成本，以减少对环境的负面影响和对资源的不平等获取。”</p><p>    Large language models are also trained on exponentially increasing amounts of text. This means researchers have sought to collect all the data they can from the internet, so there&#39;s a risk that racist, sexist, and otherwise abusive language ends up in the training data.</p><p>    大型语言模型也接受了有关数量成倍增加的文本训练。这意味着研究人员已寻求从互联网上收集所有可能的数据，因此存在种族主义，性别歧视和其他侮辱性语言最终出现在培训数据中的风险。</p><p>  An AI model taught to view racist language as normal is obviously bad. The researchers, though, point out a couple of more subtle problems. One is that shifts in language play an important role in social change; the MeToo and Black Lives Matter movements, for example, have tried to establish a new anti-sexist and anti-racist vocabulary. An AI model trained on vast swaths of the internet won’t be attuned to the nuances of this vocabulary and won’t produce or interpret language in line with these new cultural norms.</p><p>  一个被教为将种族主义语言视为正常的AI模型显然是不好的。不过，研究人员指出了另外两个细微的问题。一个是语言的改变在社会变革中起着重要的作用。例如，MeToo和Black Lives Matter运动试图建立一种新的反性别主义和反种族主义词汇。经过互联网广泛训练的AI模型不会适应这种词汇的细微差别，也不会按照这些新的文化规范来产生或解释语言。</p><p>  It will also fail to capture the language and the norms of countries and peoples that have less access to the internet and thus a smaller linguistic footprint online. The result is that AI-generated language will be homogenized, reflecting the practices of the richest countries and communities.</p><p>  它还将无法捕捉到互联网访问较少的国家和人民的语言和规范，从而使在线语言足迹更小。结果是，人工智能生成的语言将被同化，以反映最富裕国家和社区的做法。</p><p>  Moreover, because the training datasets are so large, it’s hard to audit them to check for these embedded biases. “A methodology that relies on datasets too large to document is therefore inherently risky,” the researchers conclude. “While documentation allows for potential accountability, [...] undocumented training data perpetuates harm without recourse.”</p><p>  此外，由于训练数据集非常庞大，因此很难对其进行审核以检查这些内在偏差。研究人员总结说：“因此，依赖于太大而无法记录的数据集的方法具有内在的风险。” “尽管文件可以潜在的责任，但没有文件的培训数据却无助于永久损害。” </p><p>    The researchers summarize the third challenge as the risk of “misdirected research effort.” Though most AI researchers acknowledge that large language models  don’t actually  understand language and are merely excellent at  manipulating it, Big Tech can make money from models that manipulate language more accurately, so it keeps investing in them. “This research effort brings with it an opportunity cost,” Gebru and her colleagues write. Not as much effort goes into working on AI models that might achieve understanding, or that achieve good results with smaller, more carefully curated datasets (and thus also use less energy).</p><p>研究人员将第三个挑战概括为“研究方向错误”的风险。尽管大多数AI研究人员都承认大型语言模型实际上并不能理解语言，但仅擅于操纵语言，但是Big Tech可以从能够更准确地操纵语言的模型中获利，因此它会继续投资。 “这项研究工作带来了机会成本，” Gebru和她的同事写道。在可能会达成共识的AI模型上投入较少的精力，或者使用更小，更精心策划的数据集（从而也使用更少的能量）来获得良好的结果。</p><p>    The final problem with large language models, the researchers say, is that because they’re so good at mimicking real human language, it’s easy to use them to fool people. There have been a few high-profile cases, such as the  college student who churned out AI-generated self-help and productivity advice on a blog, which went viral.</p><p>    研究人员说，大型语言模型的最终问题是，由于它们非常擅长模仿真实的人类语言，因此很容易使用它们来欺骗人们。有一些引人注目的案例，例如大学生在博客上发布了AI生成的自助和生产力建议，此事大肆宣传。</p><p>  The dangers are obvious: AI models could be used to generate misinformation about an election or the covid-19 pandemic, for instance. They can also go wrong inadvertently when used for machine translation. The researchers bring up an example: In 2017, Facebook  mistranslated a Palestinian man’s post, which said “good morning” in Arabic, as “attack them” in Hebrew, leading to his arrest.</p><p>  危险是显而易见的：例如，AI模型可用于产生有关选举或covid-19大流行的错误信息。当用于机器翻译时，它们也会无意中出错。研究人员举了一个例子：2017年，Facebook将一个巴勒斯坦人的帖子误译为阿拉伯语，称阿拉伯语为“早上好”，从而将其逮捕，该帖子在阿拉伯语中表示“早上好”。</p><p>    Gebru and Bender’s paper has six co-authors, four of whom are Google researchers. Bender asked to avoid disclosing their names for fear of repercussions. (Bender, by contrast, is a tenured professor: “I think this is underscoring the value of academic freedom,” she says.)</p><p>    Gebru和Bender的论文有六位合著者，其中四位是Google研究人员。本德尔要求避免透露自己的名字，以免引起反响。 （相比之下，本德是终身教授：“我认为这凸显了学术自由的价值，”她说。）</p><p>  The paper’s goal, Bender says, was to take stock of the landscape of current research in natural-language processing. “We are working at a scale where the people building the things can&#39;t actually get their arms around the data,” she said. “And because the upsides are so obvious, it&#39;s particularly important to step back and ask ourselves, what are the possible downsides? … How do we get the benefits of this while mitigating the risk?”</p><p>  班德说，这篇论文的目标是盘点当前自然语言处理研究的前景。她说：“我们的工作规模使得构建事物的人实际上无法掌握数据。” “而且，由于存在如此明显的利弊，因此退后一步并自问，可能存在哪些弊端？ ……我们如何在减少风险的同时获得收益？”</p><p>  In his internal email, Dean, the Google AI head, said one reason the paper “didn’t meet our bar” was that it “ignored too much relevant research.” Specifically, he said it didn’t mention more recent work on how to make large language models more energy-efficient and mitigate problems of bias.</p><p>  Google AI负责人Dean在内部电子邮件中说，该论文“未达到我们的标准”的原因之一是，该论文“忽略了太多相关研究”。具体来说，他说，它没有提及有关如何使大型语言模型更加节能和减轻偏差问题的最新工作。</p><p>  However, the six collaborators drew on a wide breadth of scholarship. The paper’s citation list, with 128 references, is notably long. “It&#39;s the sort of work that no individual or even pair of authors can pull off,” Bender said. “It really required this collaboration.”</p><p>  但是，六个合作者获得了广泛的奖学金。该论文的引文清单有128篇参考文献，特别长。班德说：“这是任何个人甚至一对作家都无法完成的工作。” “确实需要这种合作。” </p><p>  The version of the paper we saw does also nod to several research efforts on reducing the size and computational costs of large language models, and on measuring the embedded bias of models. It argues, however, that these efforts have not been enough. “I&#39;m very open to seeing what other references we ought to be including,” Bender said.</p><p>我们看到的这篇文章的版本也对减少大型语言模型的大小和计算成本以及测量模型的嵌入式偏差进行了一些研究。但是，它认为这些努力还不够。班德说：“我很乐意看到我们应该包括哪些其他参考。”</p><p>  Nicolas Le Roux, a Google AI researcher in the Montreal office, later  noted on Twitter that the reasoning in Dean’s email was unusual. “My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review,” he said.</p><p>  蒙特利尔办公室的Google AI研究人员Nicolas Le Roux随后在Twitter上指出，Dean电子邮件中的推理是不寻常的。他说：“总是检查我提交的材料是否泄露敏感材料，而从未检查其文献综述的质量。”</p><p> Now might be a good time to remind everyone that the easiest way to discriminate is to make stringent rules, then to decide when and for whom to enforce them. My submissions were always checked for disclosure of sensitive material, never for the quality of the literature review.</p><p> 现在可能是时候提醒所有人，区分的最简单方法是制定严格的规则，然后决定何时以及针对谁执行这些规则。我提交的材料始终会检查敏感材料的披露情况，而不是文献综述的质量。</p><p>— Nicolas Le Roux (@le_roux_nicolas)  December 3, 2020</p><p>-尼古拉斯·勒·鲁（Nicolas Le Roux）（@le_roux_nicolas）2020年12月3日</p><p> Dean’s email also says that Gebru and her colleagues gave Google AI only a day for an internal review of the paper before they submitted it to a conference for publication. He wrote that “our aim is to rival peer-reviewed journals in terms of the rigor and thoughtfulness in how we review research before publication.”</p><p> 迪安（Dean）的电子邮件还说，格布鲁（Gebru）和她的同事们只给了Google AI一天时间，以便对该论文进行内部审查，然后再将其提交给会议发表。他写道：“我们的目标是在发表论文之前对我们的研究进行严格和周到的研究，以与同行评审期刊相抗衡。”</p><p> I understand the concern over Timnitâ€™s resignation from Google. Sheâ€™s done a great deal to move the field forward with her research. I wanted to share the email I sent to Google Research and some thoughts on our research process. https://t.co/djUGdYwNMb</p><p> 我了解对Timnit从Google辞职的担忧。她为推动该领域的研究做了很多工作。我想分享我发送给Google Research的电子邮件以及对我们研究过程的一些想法。 https://t.co/djUGdYwNMb</p><p>— Jeff Dean (@ðŸ¡) (@JeffDean)  December 4, 2020</p><p>-杰夫·迪恩（@ðŸ¡）（@JeffDean）2020年12月4日 </p><p> Bender noted that even so, the conference would still put the paper through a substantial review process: “Scholarship is always a conversation and always a work in progress,” she said.</p><p>班德指出，即使如此，会议仍将对论文进行实质性的审查：“奖学金始终是一场对话，始终在进行中，”她说。</p><p>  Others, including William Fitzgerald, a former Google PR manager, have further cast doubt on Dean’s claim:</p><p>  其他人，包括前Google公关经理威廉·菲茨杰拉德（William Fitzgerald），进一步对迪恩的主张提出了疑问：</p><p> This is such a lie. It was part of my job on the Google PR team to review these papers. Typically we got so many we didn&#39;t review them in time or a researcher would just publish &amp; we wouldn&#39;t know until afterwards. We NEVER punished people for not doing proper process.  https://t.co/hNE7SOWSLS  pic.twitter.com/Ic30sVgwtn</p><p> 这真是个谎言。审查这些论文是我在Google PR团队工作的一部分。通常情况下，我们有太多我们没有及时审查它们，否则研究人员只会发布＆amp;直到之后我们才知道。我们绝不惩罚没有适当处理的人。 https://t.co/hNE7SOWSLS pic.twitter.com/Ic30sVgwtn</p><p>— William Fitzgerald (@william_fitz)  December 4, 2020</p><p>-威廉·菲茨杰拉德（@william_fitz）2020年12月4日</p><p> Google pioneered much of the foundational research that has since led to the recent explosion in large language models. Google AI was the first to invent the  Transformer language model in 2017 that serves as the basis for the company’s later model BERT, and OpenAI’s GPT-2 and GPT-3. BERT, as noted above, now also powers Google search, the company’s cash cow.</p><p> Google开创了许多基础研究的先河，此研究导致最近大型语言模型的爆炸式增长。 Google AI于2017年率先发明了Transformer语言模型，该模型是该公司后来的BERT模型以及OpenAI的GPT-2和GPT-3的基础。如上所述，BERT现在也为该公司的摇钱树提供了动力。</p><p>  Bender worries that Google’s actions could create “a chilling effect” on future AI ethics research. Many of the top experts in AI ethics work at large tech companies because that is where the money is. “That has been beneficial in many ways,” she says. “But we end up with an ecosystem that maybe has incentives that are not the very best ones for the progress of science for the world.”</p><p>  Bender担心Google的行为可能会对未来的AI伦理学研究造成“寒蝉效应”。许多AI道德高级专家都在大型科技公司工作，因为这就是金钱所在。她说：“这在许多方面都是有益的。” “但我们最终会拥有一个生态系统，其动机可能不是世界科学进步的最佳动力。” </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/技术/">#技术</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/研究/">#研究</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>