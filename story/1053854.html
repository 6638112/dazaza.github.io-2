<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>GPT neo：开源GPT-3样式型号，可用备用重量 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">GPT neo：开源GPT-3样式型号，可用备用重量 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-22 05:18:04</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/77172b185036025cfaa08e99008cd275.png"><img src="http://img2.diglog.com/img/2021/3/77172b185036025cfaa08e99008cd275.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>实施模型＆amp;数据并行GPT2＆amp; GPT3-Like模型，使用网格 -  Tensorflow库，能够扩展到完整的GPT3尺寸（并且可能更多！）。</p><p>  还包括替代模型架构和线性关注实现，应该使缩放到更大的模型大小＆amp;上下文长度，包括：</p><p>    我们＆＃39＆＃39;重新宣传释放在堆上训练的两种佩带的GPT-neo模型，权重和配置可以从--ey.eu自由下载。</p><p>   有关如何获取这些设置的更多信息，请参阅COLAB笔记本或阅读README的其余部分。</p><p> 当我们将焦点转移到我们的GPU培训回购时，这个存储库将是（大多数）存档，GPT-Neox</p><p>      使用CTPU UP-VM-VM-ocke of google shell（https://ssh.cloud.google.com/）创建VM，以便它可以连接到您的Google Bucket和TPU并使用PIP安装要求（见上文） 。</p><p>   您还可以选择在您的GPU上本地培训GPTNEO。为此，您可以省略上面的Google Cloud Setup步骤，而Git在本地克隆repo。通过下面的培训指南运行，然后运行main.py，您只需省略TPU标志，然后通过GPU ID。 </p><p>Google Colab免费提供TPU-V8S，这应该足以将我们的模型达到GPT3XL（1.5B参数）尺寸。单击以上按钮以通过我们的示例Colab笔记本运行。</p><p>  一旦你有训练有素的模型，或者你下载了我们的预先训练的模型之一（即将推出），生成文本就像运行main.py脚本与--predict标志一样简单。您可以使用-prompt标志将路径传递给提示TXT文件，如：</p><p>      我们建议您使用HuggingFace＆＃39;使用我们的repo（下面提供的说明）使用HuggingFace＆＃39;如果您想培训具有不同词汇大小的模型，我们提供培训您自己的销售器的设施如下：</p><p> python data / train_tokenizer.py \ --base_dir ./path/to/your/txt/files \ --output_dir ./output/path \ --file_type txt \ --vocab_size 50257＃如果它成功了，你应该看到消息＃＆＃39;保存的销售器./output/path/byte-level-bpe.tokenizer.json&#39;</p><p>  如果您只想测试培训，您可以跳过此步骤并下载一些虚拟数据，如下所示：</p><p>    如果使用自己的数据培训，则可以使用数据/ create_tfrecords.py脚本来将文本数据编码为tfrecords。</p><p> 您的数据必须是大量的正常.txt文件的形式（每个文件一个文件），也必须是lm_dataformat支持的任何格式。 </p><p>在文档模式下，TFRecords中的每个示例是一个（可变大小的）文档。 这将与Documents_fixed和DocumentS_Random采样模式一起使用（有关更多详细信息，请参见参数参考部分）.document模式是默认模式。  下面的命令将使用GPT2令终止器在Base_Dir中以可接受的格式令授予所有文件，并将其保存到Output_Dir  INPUT_DIR：定义数据所在的文件夹。 该脚本将编码此文件夹中存在的所有文件。  名称：输出文件的名称将是name_i.tfrocords，其中我是文件编号。  USE_GPT2_Tokenizer：是否使用佩带的HuggingFace GPT2令牌化器，在这种情况下，分隔符将设置为[50256]。  Encoder_Path：如果不使用佩带的GPT2令终止器，请使用此标志为生成的令牌主JSON提供路径。  分隔符：以列表格式编写，分隔符令牌插入文档（例如＆＃34; [0]＆＃34;）。 将取决于您的编码器。 </p><p>minime_size：文件必须具有的最小大小（令牌），否则丢弃。这是稍后将确定鼠标参数：鼠标*最小值必须始终更大或等于n_ctx（有关更多详细信息，请参见参数参考部分）。</p><p>  要在模型中使用数据集，您必须首先寄存在DataSet下面的./configs/dataset_configs文件夹。首先选择具有.json扩展的文件名。该文件名将作为数据集标识。应填写以下方式。</p><p> 如果您使用使用佩带的GPT2令终止器编码了数据集，则可以指定这样的：</p><p>    最后，在“模型配置”中，将上面创建的文件名添加到数据集数组中。</p><p>  ＆＃34;数据集＆＃34 ;: [[＆lt;＆lt; stitch＆gt;，＆lt; datatype＆gt;，＆lt;权重＆gt;]＃datasets键在运行时定义每个数据集如何处理培训</p><p>   在这里，我们使用GPT3-XL大小模型作为示例，但是有更多的内容，所有这些都在可用配置部分中有很短的摘要。</p><p> 您需要做的就是编辑如上所述的数据集ID，编辑Model_Path（如果日志和检查点将被保存）指向云桶，您可以在（如果使用GPU）对（或本地路径）的写访问权限。 </p><p>{＆＃34; n_head＆＃34 ;: 32，＆＃34; n_vocab＆＃34 ;: 50257，＆＃34; embed_dropout＆＃34 ;: 0.1，＆＃34; lr＆＃34 ;:0.:0.02，＆ ＃34; lr_decay＆＃34 ;:＆＃34;舒适＆＃34 ;,64; harmup_steps＆＃34 ;: 3000，＆＃34; beta1＆＃34 ;:0.9，＆＃34; beta2＆＃34; beta2＆＃34; 34 ;: 0.95，＆＃34; epsilon＆＃34 ;: 1e-8，＆＃34; Opt_name＆＃34 ;:＆＃34;亚当＆＃34 ;,＆＃34; weight_decay＆＃34 ;: 0.1 ，＆＃34; train_batch_size＆＃34 ;: 512，＆＃34; attn_dropout＆＃34 ;: 0.1，＆＃34; train_steps＆＃34 ;: 286150，＆＃34; eval_steps＆＃34 ;: 0，＆ ＃34; predict_steps＆＃34 ;: 1，＆＃34; res_dropout＆＃34 ;: 0.1，＆＃34; eval_batch_size＆＃34 ;: 128，＆＃34; predict_batch_size＆＃34 ;: 1，＆＃34 ;迭代＆＃34 ;: 2500，＆＃34; n_embd＆＃34 ;:2048，＆＃34; datasets＆＃34 ;: [＆＃34; your_dataset_name＆＃34; 25，＆＃34; domecess_random ＆＃34;，1.0]]，＆＃34; model_path＆＃34 ;:＆＃34; gs：// neo-models / gpt3_xl＆＃34 ;,＆＃34; n_ctx＆＃34 ;: 2048，＆ ＃34; n_layer＆＃34 ;: 24，＆＃34; scale_by_depth＆＃34; true，＆＃34; scale_by_in＆＃34 ;: false，＆＃34; peptrain_types＆＃34; ：[[＆＃34;全球＆＃34;]，24]，＆＃34; mesh_shape＆＃34 ;:＆＃34; x：128，y：2＆＃34 ;,＆＃34;布局＆＃34 ;:＆＃34;批次：x，memory_length：y，emd：y＆＃34 ;,＆＃34; sectivation_function＆＃34 ;:＆＃34; gelu＆＃34 ;,＆＃34; recompute_grad ＆＃34;：真实，＆＃34; gradient_clipping＆＃34 ;: 1.0，＆＃34; tokens_per_mb_per_replica＆＃34 ;: 2048}</p><p>   --auto_layout和--auto_layout_and_mesh_shape（可选）：禁用培训，然后自动生成内存有效的布局（和mesh_shape）</p><p> GPU_IDS：如果使用GPU的培训，请省略TPU标志并通过GPU的ID。在下面的示例中，我们在3个GPU上培训，指定由空格分隔的设备ID：</p><p>   我们有多种型号尺寸可用，但我们的一些配置需要大型TPU，需要调整以在较小的机器上运行或GPU。下面是CONFIGS目录中每个模型的简短指南：</p><p>        确保Model_DIR并在其中包含任何度量标准日志（它浏览Tensorboard的公制内容，这假设它＆＃39;延续现有运行的延续）。您可以使用gsutil rm -r ...删除模型目录</p><p>  您可以访问http：// server_ip_goes_he：8081 /以查看omniboard概述。如果您更愿意看到一个Tensorboard，脚本也会旋转一次，并自动将其分配一个端口。脚本应打印出日志顶部附近的Tensorboard端口。</p><p>  如果您曾经被特定配置文件的数据集混淆，则可以轻松地使用单个命令检查最小和最大令牌ID。这对于确保模型的词汇大小至少与最大令牌ID一样大。如果您尝试使用界限指数在矩阵上收集矩阵，则tensorflow不会出现错误，因此您需要确保您的词汇量足够大。 </p><p>除了能够训练大GPT＆＃39; s，这个存储库还允许您轻松地进行屏蔽语言建模（BERT，ROBERTA）。为此，您必须遵循两个额外的步骤。</p><p>  ＆＃34; mlm_training＆＃34; true，＃必须设置为true＆＃34; mlm_mask_id＆＃34 ;:＆lt;面具ID＆gt; ＃您从上面保留的面具ID</p><p> 所有这些都需要用MLM目标训练模型，适用于您正确编码的任何类型的数据。如果您想调整其他相关的Quand参数，请继续阅读。</p><p> ＆＃34; mlm_cls_token_id＆＃34 ;:＆lt; CLS令牌ID＆gt;，＃自动附加指定的CLS左侧和＃34; MLM_MASK_PROB＆＃34 ;: 0.15，＃掩盖令牌的概率，默认为15％＆＃34; mlm_same_token_prob＆＃34 ;: 0.10， ＃保持令牌的概率相同，默认为10％＆＃34; mlm_random_token_prob＆＃34 ;: 0.10，用随机令牌所替换的令牌概率，由伯爵纸和＃34推荐10％; mlm_mask_ignore_ids＆＃34 ;：[＆lt; CLS令牌＆gt;，＆lt; sep token＆gt;]＃忽略掩蔽其他特殊令牌，如果有的话</p><p>   Warmup_steps：达到全部学习率之前的步数（线性斜坡从0到LR）。</p><p>     权重_decay：重量衰减参数，如果不存在，则没有使用重量衰减（使用亚当的重量衰减修复）（默认值：0.01）（可选）。</p><p>  TRAIN_STEPS：培训步骤数（批量），设置为大约〜1时代（每批次的数据集/令牌的总数（= train_batch_size / n_ctx））。 </p><p>eval_steps：每次评估运行的步数。设置为0无评估。即，在每个检查点之后，测试模型对于eval_steps</p><p> 迭代：排队到TPU的步骤数必须小于stable_per_checkpoint。 （默认：500）</p><p> 数据集：用于使用的TFRecords数据集列表。每个数据集是一个包含以下参数的列表：[火车局，evallotholl，针，Sampling_Mode，重量]。例如，例如单个数据集（注意双列表）：[[＆＃34; Bundestag _ *。Tfrecords＆＃34 ;,＆＃34;＆＃34;，10，＆＃34; random_sample＆＃34;，1.0]缝线：如果使用SAMPLING_MODE ACMANE_SAMPLE，则输入流水线将此量的文本示例到一个以上样本。您必须选择缝线，使缝针*最小值_document_length＆gt; = n_ctx</p><p> Sampling_Mode：块（TFRecords被预处理到正确的长度并按顺序读取）或Domecody_random（拼接文件量串联，然后将N_CTX块随机分配）</p><p> 型号：培训哪种模型。目前只支持GPT，如果不存在，它默认为此。</p><p> Model_Path：Google存储桶位置（或本地路径，如果使用GPU）以保存模型检查点和日志。</p><p>   scale_by_depth：如果为true，则图层的重量初始化由它们在GPT2纸上的深度缩放。 </p><p>scale_by_in：如果为true，则图层的重量初始化由它们的输入数量缩放为GPT2纸张。</p><p> mesh_shape：网格是具有用于网格 -  Tensorflow库中的并行的命名维数的N维数组。根据布局（见下文），每个张量均匀地均匀地均匀地横跨网状尺寸（见下文）。 ＆＃39; mesh_shape＆＃39;是该阵列的形状，必须等于处理器的数量。例如，对于V3-128 TPU＆＃34; mesh_shape＆＃34 ;:“x：16，y：8”。</p><p> 布局：在每个处理器上用一块切片布置张量。张量＆＃34;布局＆＃34;，是一个注射部分地图，指定张量的哪个尺寸（均匀地）横跨该网格的尺寸。没有张量的尺寸可以在其网格的两个尺寸上分开，并且可以在其网格的相同尺寸上分开张量的两个维度。用户以（Tensor-Dimension-name，网格维值）对的形式定义全局的布局规则集。如果存在匹配规则，则张量的尺寸在其网格的维度上分开，例如， （对于上面的示例mesh_shape：＆＃34;布局＆＃34;：＆＃34;批次：x，头部：y＆＃34;</p><p> Activation_Function：Selu（自归一化）或GELU（由OA使用），前馈通量使用的激活功能。 （默认：GELU）</p><p> 注意事项：以下格式列表中的每层的关注类型[[[＆＃34;注意力_type＆＃34;]，n_layers]]。例如对于12层网[[[＆＃34;全球＆＃34;]，12]或[[＆＃34;本地＆＃34;]，10]，[＆＃34;全球＆＃34;]， 2]。选择：线性，全局，本地或无。我们已经找到了50/50混合的全球和线性才能运作良好。无允许您为更高效的PAR变压器模型创建馈送前进层。</p><p>  tokens_per_mb_per_replica：如果不是，则不会将批次拆分为包含tokens_per_mb_per_replica令牌的较小的微匹配以避免OOM。梯度在本地累积并减少一次。重要提示：MB指的是在这里的小纤维不是兆字节。</p><p>  moe_layers：将专家层的混合物附加到附加层数的层数。例如：[2,4,6,8,10,12]。我们已经通过实验发现了每两个自我注意层的MOE层工作。 </p><p>moe_params：额外的kwarg的字典传递到moe层。 例如{＆＃34; moe_dropout_rate＆＃34 ;: 0.0}  num_mem_kv：从集中注意力添加内存/键值。 Param是一个int，具有所需的MEM /键值的数量。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/EleutherAI/gpt-neo/">https://github.com/EleutherAI/gpt-neo/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/开源/">#开源</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/gpt/">#gpt</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1053146.html"><img src="http://img2.diglog.com/img/2021/3/thumb_5cf91ee5c0589d68138fd350ddcdf1b9.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1053146.html">太阳的OSPO做了什么？ </a></div><span class="my_story_list_date">2021-3-18 6:44</span></div><div class="col-sm"><div><a target="_blank" href="/story/1052969.html"><img src="http://img2.diglog.com/img/2021/3/thumb_2aa38c6b916d623004416967c043419d.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1052969.html">完全开源，带有Ubuntu，LXD和雾的多罩堆栈 </a></div><span class="my_story_list_date">2021-3-17 19:30</span></div><div class="col-sm"><div><a target="_blank" href="/story/1052438.html"><img src="http://img2.diglog.com/img/2021/3/thumb_59103c366cac2bfa263c57587db4fb73.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1052438.html">开源工具有助于鼓励您的WhatsApp联系人切换到信号 </a></div><span class="my_story_list_date">2021-3-15 16:22</span></div><div class="col-sm"><div><a target="_blank" href="/story/1052274.html"><img src="http://img2.diglog.com/img/2021/3/thumb_97765d2ba768f8901e02f47d3894fd82.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1052274.html">开源克隆的流行地点（例如Airbnb，Clubhouse） </a></div><span class="my_story_list_date">2021-3-14 10:8</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>