<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>无缝交换Netflix Android应用程序的API后端</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">无缝交换Netflix Android应用程序的API后端</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-23 07:18:57</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/084859d72b5b9b1233be677f4ccee877.png"><img src="http://img2.diglog.com/img/2020/10/084859d72b5b9b1233be677f4ccee877.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>As Android developers, we usually have the luxury of treating our backends as magic boxes running in the cloud, faithfully returning us JSON. At Netflix, we have adopted the  Backend for Frontend (BFF) pattern: instead of having one general purpose “backend API”, we have one backend per client (Android/iOS/TV/web). On the Android team, while most of our time is spent working on the app, we are also responsible for maintaining this backend that our app communicates with, and its orchestration code.</p><p>作为Android开发人员，我们通常可以将我们的后端视为在云中运行的魔盒，忠实地返回给我们JSON。在Netflix，我们采用前端后端(BFF)模式：每个客户端(Android/iOS/TV/Web)有一个后端，而不是一个通用的“后端API”。在Android团队中，虽然我们的大部分时间都花在应用程序上，但我们也负责维护我们的应用程序与之通信的后端及其编排代码。</p><p> Recently, we completed a year-long project rearchitecting and decoupling our backend from the centralized model used previously. We did this migration without slowing down the usual cadence of our releases, and with particular care to avoid any negative effects to the user experience. We went from an essentially serverless model in a monolithic service, to deploying and maintaining a new microservice that hosted our app backend endpoints. This allowed Android engineers to have much more control and observability over how we get our data. Over the course of this post, we will talk about our approach to this migration, the strategies that we employed, and the tools we built to support this.</p><p>最近，我们完成了一个为期一年的项目，重新设计了我们的后端，并将其与以前使用的集中式模型分离。我们在没有放慢发布节奏的情况下进行了这次迁移，并且特别小心地避免了对用户体验的任何负面影响。我们从单一服务中本质上无服务器的模型发展到部署和维护托管我们的应用程序后端端点的新微服务。这使得Android工程师可以更好地控制和观察我们获取数据的方式。在这篇文章中，我们将讨论我们的迁移方法，我们采用的策略，以及我们为支持这一迁移而构建的工具。</p><p>  The Netflix Android app uses the  falcor data model and query protocol. This allows the app to query a list of “paths” in each HTTP request, and get specially formatted JSON ( jsonGraph) that we use to cache the data and hydrate the UI. As mentioned earlier, each client team owns their respective endpoints: which effectively means that we’re writing the resolvers for each of the paths that are in a query.</p><p>Netflix Android应用程序使用Falcor数据模型和查询协议。这允许应用程序查询每个HTTP请求中的“路径”列表，并获得我们用来缓存数据和更新UI的特殊格式的JSON(JsonGraph)。如前所述，每个客户端团队都拥有各自的端点：这实际上意味着我们正在为查询中的每个路径编写解析器。</p><p>  As an example, to render the screen shown here, the app sends a query that looks like this:</p><p>例如，要呈现此处显示的屏幕，应用程序发送如下所示的查询：</p><p>  A   path starts from a  root object, and is followed by a sequence of  keys that we want to retrieve the data for. In the snippet above, we’re accessing the  detail key for the  video object with id  80154610.</p><p>路径从根对象开始，后跟我们要检索其数据的键序列。在上面的代码片段中，我们正在访问ID为80154610的视频对象的细节键。</p><p>    In the example you see above, the data that the app needs is served by different backend microservices. For example, the artwork service is separate from the video metadata service, but we need the data from both in the  detail key.</p><p>在上面的示例中，应用程序需要的数据由不同的后台微服务提供服务。例如，Artwork服务与视频元数据服务是分开的，但我们需要详细信息键中两者的数据。</p><p> We do this orchestration on our endpoint code using a library provided by our API team, which exposes an RxJava API to handle the downstream calls to the various backend microservices. Our endpoint route handlers are effectively fetching the data using this API, usually across multiple different calls, and massaging it into data models that the UI expects. These handlers we wrote were deployed into a service run by the API team, shown in the diagram below.</p><p>我们使用API团队提供的库在端点代码上进行此编排，该库公开RxJava API来处理对各种后端微服务的下游调用。我们的端点路由处理程序使用此API有效地获取数据(通常跨越多个不同的调用)，并将其传递到UI期望的数据模型中。我们编写的这些处理程序被部署到API团队运行的服务中，如下图所示。</p><p>  As you can see, our code was just a part (#2 in the diagram) of this monolithic service. In addition to hosting our route handlers, this service also handled the business logic necessary to make the downstream calls in a fault tolerant manner. While this gave client teams a very convenient “serverless” model, over time we ran into multiple operational and devex challenges with this service. You can read more about this in our previous posts here:  part 1,  part 2.</p><p>如您所见，我们的代码只是这个整体服务的一部分(图中的#2)。除了托管我们的路由处理程序之外，该服务还处理以容错方式进行下游调用所需的业务逻辑。虽然这为客户团队提供了一个非常方便的“无服务器”模型，但随着时间的推移，我们在这项服务上遇到了多个运营和Devex挑战。您可以在我们以前的帖子中阅读更多关于这方面的内容：第1部分，第2部分。</p><p>  It was clear that we needed to isolate the endpoint code (owned by each client team), from the complex logic of fault tolerant downstream calls. Essentially, we wanted to break out the client-specific code from this monolith into its own service. We tried a few iterations of what this new service should look like, and eventually settled on a modern architecture that aimed to give more control of the API experience to the client teams. It was a Node.js service with a composable JavaScript API that made downstream microservice calls, replacing the old Java API.</p><p>很明显，我们需要将端点代码(归每个客户端团队所有)与容错下游调用的复杂逻辑隔离。从本质上讲，我们希望将客户端特定的代码从这个整体分解到它自己的服务中。我们尝试了这个新服务应该是什么样子的几次迭代，最终确定了一个旨在为客户端团队提供更多API体验控制的现代架构。它是一个Node.js服务，使用可组合的JavaScript API进行下游微服务调用，取代了旧的Java API。</p><p>  As Android developers, we’ve come to rely on the safety of a strongly typed language like Kotlin, maybe with a side of Java. Since this new microservice uses Node.js, we had to write our endpoints in JavaScript, a language that many people on our team were not familiar with. The context around why the Node.js ecosystem was chosen for this new service deserves an article in and of itself. For us, it means that we now need to have ~15  MDN tabs open when writing routes :)</p><p>作为Android开发人员，我们已经开始依赖像Kotlin这样的强类型语言的安全性，也许还有Java的一面。因为这个新的微服务使用Node.js，所以我们不得不用JavaScript编写端点，这是我们团队中的许多人都不熟悉的语言。关于为什么选择Node.js生态系统作为这项新服务的上下文值得一篇文章。对我们来说，这意味着在编写路由时，我们现在需要打开大约15个MDN选项卡：)。</p><p> Let’s briefly discuss the architecture of this microservice. It looks like a very typical backend service in the Node.js world: a combination of  Restify, a stack of HTTP middleware, and the Falcor-based API. We’ll gloss over the details of this stack: the general idea is that we’re still writing resolvers for paths like  [videos, &lt;id&gt;, detail], but we’re now writing them in JavaScript.</p><p>让我们简要讨论一下这个微服务的体系结构。它看起来像Node.js世界中非常典型的后端服务：Restify、HTTP中间件堆栈和基于Falcor的API的组合。我们将略过这个堆栈的细节：总体思路是我们仍在为[Videos，&lt；id&gt；，Detail]这样的路径编写解析器，但我们现在用JavaScript编写它们。</p><p> The big difference from the monolith, though, is that this is now a standalone service deployed as a separate “application” (service) in our cloud infrastructure. More importantly, we’re no longer just getting and returning requests from the context of an endpoint script running in a service: we’re now getting a chance to handle the HTTP request in its entirety. Starting from “terminating” the request from our public gateway, we then make downstream calls to the  api application (using the previously mentioned JS API), and build up various parts of the response. Finally, we return the required JSON response from our service.</p><p>然而，与单一的服务最大的不同在于，它现在是一个独立的服务，在我们的云基础设施中作为单独的“应用程序”(服务)部署。更重要的是，我们不再只是从服务中运行的端点脚本的上下文中获取和返回请求：我们现在有机会完整地处理HTTP请求。从“终止”公网网关的请求开始，然后向下调用API应用程序(使用前面提到的JS API)，并构建响应的各个部分。最后，我们从服务返回所需的JSON响应。</p><p>  Before we look at what this change meant for us, we want to talk about how we did it. Our app had ~170 query paths (think: route handlers), so we had to figure out an iterative approach to this migration. Let’s take a look at what we built in the app to support this migration. Going back to the screenshot above, if you scroll a bit further down on that page, you will see the section titled “more like this”:</p><p>在我们看这一变化对我们意味着什么之前，我们想先谈谈我们是如何做到这一点的。我们的应用程序有大约170条查询路径(想想路由处理程序)，所以我们必须找出一种迭代的方法来进行迁移。让我们来看看我们在应用程序中构建了什么来支持此迁移。回到上面的屏幕截图，如果你在该页面再往下滚动一点，你会看到标题为“More Like This”的部分：</p><p>  As you can imagine, this does not belong in the video details data for this title. Instead, it is part of a different  path:  [videos, &lt;id&gt;, similars]. The general idea here is that each UI screen ( Activity/ Fragment) needs data from multiple query paths to render the UI.</p><p>正如您可以想象的那样，这不属于此标题的视频详细信息数据。相反，它是一条不同道路的一部分：[视频，&lt；id&gt；，Similars]。这里的总体思路是，每个UI屏幕(活动/片段)都需要来自多个查询路径的数据来呈现UI。</p><p> To prepare ourselves for a big change in the tech stack of our endpoint, we decided to track metrics around the time taken to respond to queries. After some consultation with our backend teams, we determined the most effective way to group these metrics were by UI screen. Our app uses a version of the repository pattern, where each screen can fetch data using a list of query paths. These paths, along with some other configuration, builds a  Task. These  Tasks already carry a  uiLabel that uniquely identifies each screen: this label became our starting point, which we passed in a header to our endpoint. We then used this to log the time taken to respond to each query, grouped by the  uiLabel. This meant that we could track any possible regressions to user experience by screen, which corresponds to how users navigate through the app. We will talk more about how we used these metrics in the sections to follow.</p><p>为了让我们自己为端点技术堆栈的重大变化做好准备，我们决定跟踪响应查询所用的时间。在与我们的后端团队进行了一些协商之后，我们确定了按UI屏幕对这些指标进行分组的最有效方式。我们的应用程序使用存储库模式的一个版本，其中每个屏幕都可以使用查询路径列表获取数据。这些路径与其他一些配置一起构建一个任务。这些任务已经带有唯一标识每个屏幕的uiLabel：该标签成为我们的起始点，我们将其作为头传递给我们的端点。然后，我们使用它来记录响应每个查询所用的时间，按uiLabel分组。这意味着我们可以通过屏幕跟踪用户体验的任何可能的倒退，这与用户在应用程序中导航的方式相对应。我们将在后面的章节中更多地讨论我们如何使用这些指标。</p><p> Fast forward a year: the 170 number we started with slowly but surely whittled down to 0, and we had all our “routes” (query paths) migrated to the new microservice. So, how did it go…?</p><p>快进一年：我们开始时的170个数字缓慢但肯定地减少到0，我们所有的“路由”(查询路径)都迁移到了新的微服务。那么，事情进行得怎么样了，…。？</p><p>  Today, a big part of this migration is done: most of our app gets its data from this new microservice, and hopefully our users never noticed. As with any migration of this scale, we hit a few bumps along the way: but first, let’s look at good parts.</p><p>今天，这种迁移已经完成了很大一部分：我们的大部分应用程序都是从这项新的微服务中获取数据的，希望我们的用户不会注意到这一点。与任何这种规模的迁移一样，我们在这一过程中遇到了一些坎坷：但首先，让我们看看好的部分。</p><p>  Our monolith had been around for many years and hadn’t been created with functional and unit testing in mind, so those were independently bolted on by each UI team. For the migration, testing was a first-class citizen. While there was no technical reason stopping us from adding full automation coverage earlier, it was just much easier to add this while migrating each query path.</p><p>我们的“巨石”已经存在很多年了，在创建时并没有考虑到功能和单元测试，所以这些都是由每个UI团队独立地固定在一起的。对于移民来说，Testing是一等公民。虽然没有技术原因阻止我们更早地添加完整的自动化覆盖范围，但在迁移每个查询路径时添加它要容易得多。</p><p> For each route we migrated, we wanted to make sure we were not introducing any regressions: either in the form of missing (or worse, wrong) data, or by increasing the latency of each endpoint. If we pare down the problem to absolute basics, we essentially have two services returning JSON. We want to make sure that for a given set of paths as input, the returned JSON is always exactly the same. With lots of guidance from other platform and backend teams, we took a 3-pronged approach to ensure correctness for each route migrated.</p><p>对于我们迁移的每条路由，我们希望确保不会引入任何倒退：无论是以丢失(或更糟糕的是，错误的)数据的形式，还是通过增加每个端点的延迟。如果我们将问题缩减到绝对基础，我们实际上有两个返回JSON的服务。我们希望确保对于作为输入的一组给定路径，返回的JSON始终完全相同。在其他平台和后端团队的大量指导下，我们采取了三管齐下的方法来确保每条迁移路线的正确性。</p><p> Functional Testing Functional testing was the most straightforward of them all: a set of tests alongside each path exercised it against the old and new endpoints. We then used the excellent  Jest testing framework with a set of custom matchers that sanitized a few things like timestamps and uuids. It gave us really high confidence during development, and helped us cover all the code paths that we had to migrate. The test suite automated a few things like setting up a test user, and matching the query parameters/headers sent by a real device: but that’s as far as it goes. The scope of functional testing was limited to the already setup test scenarios, but we would never be able to replicate the variety of device, language and locale combinations used by millions of our users across the globe.</p><p>功能测试是所有测试中最直接的：每条路径旁边的一组测试都对新旧端点进行测试。然后，我们使用了优秀的Jest测试框架和一组自定义匹配器，这些匹配器清理了一些东西，比如时间戳和uuid。它在开发过程中给了我们很高的信心，并帮助我们覆盖了我们必须迁移的所有代码路径。测试套件自动化了一些事情，比如设置测试用户，以及匹配真实设备发送的查询参数/头：但仅此而已。功能测试的范围仅限于已经设置的测试场景，但我们永远无法复制全球数百万用户使用的各种设备、语言和区域设置组合。</p><p>   It was a self-contained flow that, by design, captured entire requests, and not just the one path we requested. This test was the closest to production: it replayed real requests sent by the device, thus exercising the part of our service that fetches responses from the old endpoint and stitches them together with data from the new endpoint. The thoroughness and flexibility of this replay pipeline is best described in its own post. For us, the replay test tooling gave the confidence that our new code was nearly bug free.</p><p>这是一个自包含的流，按照设计，它捕获了整个请求，而不仅仅是我们请求的一条路径。此测试最接近生产：它重放设备发送的真实请求，从而执行我们的服务部分，即从旧端点获取响应，并将它们与来自新端点的数据缝合在一起。这个重放管道的彻底性和灵活性在它自己的帖子中得到了最好的描述。对我们来说，重放测试工具让我们确信我们的新代码几乎没有bug。</p><p> Canaries Canaries were the last step involved in “vetting” our new route handler implementation. In this step, a pipeline picks our candidate change, deploys the service, makes it publicly discoverable, and redirects a small percentage of production traffic to this new service. You can find a lot more details about how this works in the  Spinnaker canaries documentation.</p><p>金丝雀金丝雀是“审查”我们新的路由处理程序实现的最后一步。在此步骤中，管道选择我们的候选更改，部署服务，使其可公开发现，并将一小部分生产流量重定向到此新服务。您可以在Spinnaker Canaries文档中找到更多关于这是如何工作的详细信息。</p><p> This is where our previously mentioned  uiLabel metrics become relevant: for the duration of the canary,  Kayenta was  configured to capture and compare these metrics for all requests (in addition to the system level metrics already being tracked, like server CPU and memory). At the end of the canary period, we got a report that aggregated and compared the percentiles of each request made by a particular UI screen. Looking at our high traffic UI screens (like the homepage) allowed us to identify any regressions caused by the endpoint before we enabled it for all our users. Here’s one such report to get an idea of what it looks like:</p><p>这就是我们前面提到的uiLabel指标变得重要的地方：在金丝雀期间，Kayenta被配置为捕获并比较所有请求的这些指标(除了已经被跟踪的系统级指标之外，比如服务器CPU和内存)。在金丝雀周期结束时，我们得到了一份报告，该报告汇总并比较了特定UI屏幕发出的每个请求的百分位数。通过查看我们的高流量UI屏幕(如主页)，我们可以在为所有用户启用端点之前确定端点造成的任何倒退。这里有一份这样的报告，可以让你对它的样子有个大概的了解：</p><p>  Each identified regression (like this one) was subject to a lot of analysis: chasing down a few of these led to previously unidentified performance gains! Being able to canary a new route let us verify latency and error rates were within acceptable limits. This type of tooling required time and effort to create, but in the end, the feedback it provided was well worth the cost.</p><p>每个确定的回归(像这个)都要经过大量的分析：追查其中的几个会导致以前未确定的性能收益！能够为新路由设置金丝雀，使我们可以验证延迟和错误率是否在可接受的范围内。这种类型的工具需要时间和精力来创建，但最终，它提供的反馈是非常物有所值的。</p><p>  Many Android engineers will be familiar with systrace or one of the excellent profilers in Android Studio. Imagine getting a similar tracing for your endpoint code, traversing along many different microservices: that is effectively what distributed tracing provides. Our microservice and router were already integrated into the Netflix request tracing infrastructure. We used  Zipkin to consume the traces, which allowed us to search for a trace by path. Here’s what a typical trace looks like:</p><p>许多Android工程师会熟悉Systrace或Android Studio中优秀的分析器之一。想象一下，为您的端点代码获得类似的跟踪，遍历许多不同的微服务：这就是分布式跟踪提供的有效功能。我们的微服务和路由器已经集成到Netflix请求跟踪基础设施中。我们使用Zipkin来使用跟踪，这允许我们按路径搜索跟踪。下面是典型的跟踪：</p><p>  Request tracing has been critical to the success of Netflix infrastructure, but when we operated in the monolith, we did not have the ability to get this detailed look into how our app interacted with the various microservices. To demonstrate how this helped us, let us zoom into this part of the picture:</p><p>请求跟踪对于Netflix基础设施的成功至关重要，但当我们在单机版中运行时，我们无法详细了解我们的应用程序是如何与各种微服务交互的。为了说明这对我们有何帮助，让我们放大图片的这一部分：</p><p>  It’s pretty clear here that the calls are being serialized: however, at this point we’re already ~10 hops disconnected from our microservice. It’s hard to conclude this, and uncover such problems, from looking at raw numbers: either on our service or the   testservice above, and even harder to attribute them back to the exact UI platform or screen. With the rich end-to-end tracing instrumented in the Netflix microservice ecosystem and made easily accessible via Zipkin, we were able to pretty quickly triage this problem to the responsible team.</p><p>很明显，这里的调用正在被序列化：然而，在这一点上，我们已经与微服务断开了大约10个跃点。通过查看原始数据，很难得出这样的结论，也很难发现这样的问题：无论是在我们的服务上还是在上面的测试服务上，更难将它们归因于确切的UI平台或屏幕。有了Netflix微服务生态系统中丰富的端到端跟踪功能，并且可以通过Zipkin轻松访问，我们能够非常迅速地将此问题分流给负责的团队。</p><p>  As we mentioned earlier, our new service now had the “ownership” for the lifetime of the request. Where previously we only returned a Java object back to the api middleware, now the final step in the service was to flush the JSON down the request buffer. This increased ownership gave us the opportunity to easily test new optimisations at this layer. For example, with about a day’s worth of work, we had a prototype of the app using the binary msgpack response format instead of plain JSON. In addition to the flexible service architecture, this can also be attributed to the Node.js ecosystem and the rich selection of npm packages available.</p><p>正如我们前面提到的，我们的新服务现在拥有请求生命周期的“所有权”。以前我们只将Java对象返回给API中间件，现在服务中的最后一步是将JSON刷新到请求缓冲区。所有权的增加使我们有机会在这一层轻松测试新的优化。例如，通过大约一天的工作，我们有了一个使用二进制msgpack响应格式而不是普通JSON的应用程序原型。除了灵活的服务架构外，这还可以归功于Node.js生态系统和丰富的NPM包选择。</p><p>  Before the migration, developing and debugging on the endpoint was painful due to slow deployment and lack of local debugging ( this post covers that in more detail). One of the Android team’s biggest motivations for doing this migration project was to improve this experience. The new microservice gave us fast deployment and debug support by running the service in a local Docker instance, which has led to significant productivity improvements.</p><p>在迁移之前，端点上的开发和调试非常痛苦，因为部署缓慢且缺乏本地调试(本文详细介绍了这一点)。Android团队做这个迁移项目的最大动机之一就是改善这种体验。新的微服务通过在本地Docker实例中运行该服务，为我们提供了快速部署和调试支持，从而显著提高了工作效率。</p><p>  In the arduous process of breaking a monolith, you might get a sharp shard or two flung at you. A lot of what follows is not specific to Android, but we want to briefly mention these issues because they did end up affecting our app.</p><p>在打破一块巨石的艰难过程中，你可能会得到一两块锋利的碎片。下面的很多内容都不是Android特有的，但是我们想简单地提一下这些问题，因为它们最终确实影响了我们的应用程序。</p><p>  The old  api service was running on the same “machine” that also cached a lot of video metadata (by design). This meant that data that was static (e.g. video titles, descriptions) could be aggressively cached and reused across multiple requests. However, with the new microservice, even fetching this cached data needed to incur a network round trip, which added some latency.</p><p>旧的API服务运行在同一台“机器”上，这台机器也缓存了大量的视频元数据(根据设计)。这意味着静态数据(例如视频标题、描述)可以跨多个请求被积极缓存和重用。然而，使用新的微服务，即使获取这些缓存的数据也需要网络往返，这增加了一些延迟。</p><p> This might sound like a classic example of “monoliths vs microservices”, but the reality is somewhat more complex. The monolith was also essentially still talking to a  lot of downstream microservices: it just happened to have a custom-designed cache that helped a lot. Some of this increased latency was mitigated by better observability and more efficient batching of requests. But, for a small fraction of requests, after a lot of attempts at optimization, we just had to take the latency hit: sometimes, there are no silver bullets.</p><p>这听起来可能像是“巨无霸VS微服务”的经典例子，但实际情况要复杂得多。这块巨石本质上还在与许多下游微服务对话：它只是碰巧有一个定制的缓存，这对它有很大帮助。通过更好的可观察性和更高效的请求批处理，部分增加的延迟得到了缓解。但是，对于一小部分请求，在进行了大量优化尝试之后，我们不得不承担延迟损失：有时，没有什么灵丹妙药。</p><p>  As each call to our endpoint might need to make multiple requests to the api service, some of these calls can fail, leaving us with partial data. Handling such partial query errors isn’t a new problem: it is baked into the nature of composite protocols like Falcor or GraphQL. However, as we moved our route handlers into a new microservice, we now introduced a network boundary for fetching  any data, as mentioned earlier.</p><p>由于对端点的每个调用可能需要向API服务发出多个请求，其中一些调用可能会失败，从而给我们留下部分数据。处理这样的部分查询错误并不是一个新问题：它是Falcor或GraphQL等复合协议的本质。然而，正如前面提到的，当我们将路由处理程序移到一个新的微服务中时，我们现在引入了用于获取任何数据的网络边界。</p><p> This meant that we now ran into partial states that weren’t possible before because of the custom caching. We were not completely aware of this problem in the beginning of our migration: we only saw it when some of our deserialized data objects had  null fields. Since a lot of our code uses Kotlin, these partial data objects led to immediate crashes, which helped us notice the problem early: before it ever hit production.</p><p>这意味着我们现在遇到了以前由于自定义缓存而不可能实现的部分状态。我们在迁移开始时并没有完全意识到这个问题：我们只有在一些反序列化的数据对象具有空字段时才能看到这个问题。因为我们的很多代码都使用Kotlin，所以这些部分数据对象会立即导致崩溃，这有助于我们及早注意到问题：在它投入生产之前。</p><p> As a result of increased partial errors, we’ve had to improve overall error handling approach and explore ways to minimize the impact of the network errors. In some cases, we also added custom retry logic on either the endpoint or the client code.</p><p>由于局部错误的增加，我们必须改进整体错误处理方法，并探索将网络错误的影响降至最低的方法。在某些情况下，我们还在端点或客户端代码上添加了自定义重试逻辑。</p><p>  This has been a long (you can tell!) and a fulfilling journey for us on the Android team: as we mentioned earlier, on our team we typically work on the app and, until now, we did not have a chance to work with our endpoint with this level of scrutiny. Not only did we learn more about the intriguing world of microservices, but for us working on this project, it provided us the perfect opportunity to add observability to our app-endpoint interaction. At the same time, we ran into some unexpected issues like partial errors and made our app more resilient to them in the process.</p><p>这是很久以前的事了(你看得出来！)。对于我们Android团队来说，这是一个令人满意的旅程：正如我们前面提到的，在我们的团队中，我们通常会开发这款应用程序，直到现在，我们还没有机会使用我们的端点进行这种级别的审查。我们不仅了解了更多有关微服务的有趣世界，而且对于我们从事此项目的人员来说，它为我们提供了一个完美的机会，可以为我们的应用程序-端点交互添加可观察性。与此同时，我们遇到了一些意想不到的问题，如部分错误，并在此过程中使我们的应用程序对它们更具弹性。</p><p> As we continue to evolve and improve our app, we hope to share more insights like these with you.</p><p>随着我们不断发展和完善我们的应用程序，我们希望能与您分享更多这样的真知灼见。</p><p>  The planning and successful migration to this new service was the combined effort of multiple backend and front end teams.</p><p>规划并成功迁移到这项新服务是多个后端和前端团队共同努力的结果。</p><p> On the Android team, we ship the Netflix app on Android to millions of members around the world. Our responsibilities include extensive A/B testing on a wide variety of devices by building highly performant and often custom UI experiences. We work on data driven optimizations at scale in a diverse and sometimes unforgiving device and network ecosystem. If you find these challenges interesting, and want to work with us, we have an   open position .</p><p>在Android团队中，我们将Android上的Netflix应用程序发送给世界各地的数百万会员。我们的职责包括通过构建高性能且通常是自定义的UI体验，在各种设备上进行广泛的A/B测试。我们致力于在多样化、有时不可饶恕的设备和网络生态系统中进行大规模的数据驱动优化。如果您对这些挑战感兴趣，并想与我们合作，我们有一个开放的职位。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://netflixtechblog.com/seamlessly-swapping-the-api-backend-of-the-netflix-android-app-3d4317155187">https://netflixtechblog.com/seamlessly-swapping-the-api-backend-of-the-netflix-android-app-3d4317155187</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/android/">#android</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/交换/">#交换</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/swapping/">#swapping</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/服务/">#服务</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1030633.html"><img src="http://img2.diglog.com/img/2020/10/thumb_21054de7abd23919431e2a8a7caa1f96.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030633.html">使用Bakeware将Elixir应用程序编译为单个可执行的二进制文件</a></div><span class="my_story_list_date">2020-10-23 2:54</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030616.html"><img src="http://img2.diglog.com/img/2020/10/thumb_5b552713e2b268cc98eb7ba4b741bd2d.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030616.html">苹果开始隐藏其Mac应用程序的流量</a></div><span class="my_story_list_date">2020-10-23 1:53</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030597.html"><img src="http://img2.diglog.com/img/2020/10/thumb_e5af8a548c3bdd923adf907e1c5c862a.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030597.html">PS5在发布时将缺少一些PS4视频流应用程序</a></div><span class="my_story_list_date">2020-10-23 0:24</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030591.html"><img src="http://img2.diglog.com/img/2020/10/thumb_774914de6bb18c065982d36c366149e8.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030591.html">亲身体验华为新款Mate 40 Pro：令人难以置信的摄像头和强大的处理器，但应用程序选择很糟糕，很难推荐这款手机</a></div><span class="my_story_list_date">2020-10-23 0:19</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>