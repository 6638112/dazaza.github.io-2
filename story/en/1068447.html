<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>让你的群集游泳（2020） Make your cluster SWIM (2020)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Make your cluster SWIM (2020)<br/>让你的群集游泳（2020） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-29 22:34:55</div><div class="page_narrow text-break page_content"><p>In this blog post we&#39;ll cover how systems form clusters, what clusters actually are and what are their responsibilities. We&#39;ll also present different protocols responsible to serve the needs of the clusters with a various tradeoffs associated with them.</p><p>在这个博客文章中我们＆＃39; ll涵盖系统如何形成集群，实际上是什么集群以及他们的职责是什么。我们＆＃39; LL还呈现了不同的协议，以满足与他们相关的各种权衡的群集的需求。</p><p> To make this blog post a bit more practical, we&#39;ll also go in depth of one of the membership protocols - known as under SWIM acronym - from theoretical standpoint up to example implementation in F#.</p><p> 为了使这个博客发表一点更实用，我们也会深入了解了众议员协议之一 - 从游泳首字母缩略词中的一个隶属协议 - 从理论上的角度来看，在F＃中实现了示例。</p><p>  From a user perspective, cluster is supposed to serve as an illusion of a &#34;single machine&#34; and keep it  safe from all of complexity coming from communicating with - usually dynamically changing - network of interconnected servers.</p><p>  从用户的角度来看，群集应该用作A＆＃34;单机＆＃34的幻觉;并将其免于与 - 通常动态变化 - 互联服务器网络的所有复杂性安全。</p><p> As it turns out, there&#39;s a multitude of protocols and responsibilities that services being a part of cluster need to solve to maintain that illusion. Going bottom-up, here are some of the questions our cluster usually needs to know how to answer for:</p><p> 正如事实证明，那里的一系列协议和责任是群集的一部分需要解决以维持这种幻觉。走下自下而上，这里是我们集群通常需要知道如何回答的一些问题：</p><p> How to join the cluster? Usually when we have a new server node, which we want to join to the cluster, it needs to know how to communicate with another node that&#39;s already part of the cluster itself. Where can it find that information? It&#39;s the question that needs to be answered: The simplest trick is to provide a static list of contact points as a configuration parameter provided at the start of new service.</p><p> 如何加入群集？通常，当我们有一个我们想要加入群集的新服务器节点时，它需要知道如何与已有群集本身的一部分的另一个节点通信。哪里可以找到这个信息？它＆＃39;是需要回答的问题：最简单的诀窍是为新服务开始时提供的配置参数提供静态的联系点列表。</p><p> Another way is to use 3rd party service (like database,  Consul,  etcd or  ZooKeeper) to serve as a node registry. Our cluster usually doesn&#39;t live in vacuum, and sometimes we can reuse other already established services for our advantage.</p><p> 另一种方法是使用第三方服务（如数据库，Scenul，etcd或zookeeper）用作节点注册表。我们的集群通常不会生活在真空中，有时我们可以重复使用其他已经建立的服务以获得优势。</p><p> In some situations, we could leverage capabilities of lower layers - like  Kubernetes DNS service or  mDNS - that are specific to a host environment to dynamically discover other devices living in the same network.</p><p> 在某些情况下，我们可以利用低层Kubernetes DNS服务或MDNS的功能 - 特定于主机环境，以动态发现生活在同一网络中的其他设备。 </p><p> How do we know what other nodes are part of the cluster? This is area of so called membership protocols on which we&#39;ll focus in second part of this blog post. In dynamic clusters it&#39;s usually done by keeping track of a active discovered nodes, then updating and gossiping them once a node joins/leaves the cluster. A lot of decisions here depend on deployment scenario - services forming a cluster within the same datacenter have different characteristics from eg. meshes of mobile devices. In cases of common backend services hosted in the same datacenter, each node keeps a full information about the state of the cluster. This is how protocols like  SWIM operate.</p><p>我们如何知道其他节点是群集的一部分？这是所谓的会员协议的区域，我们＆＃39; ll焦点在本博客文章的第二部分。在动态集群中，它通常通过跟踪活动发现的节点来完成，然后在节点连接/离开群集时更新和闲置它们。这里的许多决定依赖于部署方案 - 在同一数据中心内形成群集的服务具有不同的特征，从例如eg。移动设备的网格。在托管在同一数据中心托管的常见后端服务的情况下，每个节点都会保持有关群集状态的完整信息。这就是游泳操作的协议。</p><p> Other membership protocols (like  HyParView) enable to have only partial view on the cluster. This is preferable in cases when our cluster operates on much higher scale eg. thousands of nodes (usually most clustered services living in datacenters don&#39;t reach over dozens-to-hundreds of servers).</p><p> 其他成员资格协议（如hyparview）启用只能在群集中映像。如果我们的群集在更高的尺度上操作时，则这是优选的。成千上万的节点（通常生活在数据中心的大多数集群服务Don＆＃39; t达到几十多个服务器）。</p><p> How do we send messages from one node to another? This usually is also related to a membership protocol. Most datacenter-oriented systems can make conservative assumption that every node in the system can connect to every other node, forming (potentially) a fully connected mesh.</p><p> 我们如何将消息从一个节点发送到另一个节点？这通常也与隶属协议有关。大多数以数据中心为导向的系统可以保守假设系统中每个节点都可以连接到每个其他节点，形成（潜在地）完全连接的网格。</p><p> In other scenarios, we need to take into account that some of the nodes may not be able to connect to others - because of the underlying network characteristics. While the most common case of that is client-server architecture, when we&#39;re talking about cluster protocols, often more advanced scenarios need to be applied (as we mentioned, &#34;server&#34; itself is not a single entity).</p><p> 在其他场景中，我们需要考虑到某些节点可能无法连接到其他节点 - 因为网络特征潜在的网络特征。虽然当我们谈论群集协议时，最常见的情况是客户服务器架构，但常常需要应用更高级的方案（如我们所提到的，＆＃34;服务器＆＃34;本身不是单一的实体）。</p><p> How do we detect dead nodes? Also known as failure detection. The oldest known trick in that area is a simple exchange of PING 🡘 ACK messages within expected timeout boundaries every now and then, or simply expecting every connection to send a heartbeat message within given time interval. Heartbeats are often implemented directly into transport layer (like TCP), sometimes we&#39;re able to piggyback failure detector directly on top of it. Downside of that solution is that while transport layer itself is responsive - because it&#39;s usually managed by underlying OS - our application layer could in fact be not (eg. because it hangs deadlocked indefinitely). Another thing is that temporal failure of network connection doesn&#39;t has to mean, that node won&#39;t try to restart it and continue to work.</p><p> 我们如何检测到死区？也称为失败检测。该领域最古老的知识是每立即在预期超时边界内的Ping🡘ACK消息的简单交换，或者只是期望在给定时间间隔内发送心跳消息。心跳通常直接进入运输层（如TCP），有时是我们＆＃39;重新能够直接捎带失败探测器。该解决方案的缺点是，虽然运输层本身是响应的 - 因为它通常由底层操作系统管理 - 我们的应用层实际上不是（例如，因为它无限期地挂起了僵化的僵局）。另一件事是网络连接的时间失败并不是意味着，那个节点赢得了＆＃39; t试着重新启动并继续工作。</p><p> More often, membership protocols promote their own heartbeat algorithms. What&#39;s worth noticing here, missing heartbeat doesn&#39;t necessarily mean, that our node is dead. It could as well be overwhelmed with serving other incoming requests. Modern algorithms like  Phi accural failure dector or  Lifeguard (an extension to SWIM protocol mentioned above) take that behavior into account.</p><p> 更常见的是，会员协议促进自己的心跳算法。什么＆＃39;在这里值得注意，缺少心跳并不一定意味着，我们的节点已经死亡。它也可能不堪重负，提供其他传入请求。像Phi Accuctuct Dector或Lifeguard这样的现代算法（上面提到的游泳协议的扩展）考虑到这一行为。</p><p> These are the most basic questions cluster needs to know how to answer. We could call it a layer 0 of any cluster. The main part of this blog post will cover how to implement protocol addressing these issues. On top of it, there are many other higher level features, aiming to solve problems like:</p><p> 这些是最基本的问题需要知道如何回答。我们可以称之为任何群集的第0层。本博客文章的主要部分将涵盖如何实施解决这些问题的协议。在它之上，还有许多其他更高的级别功能，旨在解决以下问题： </p><p> How can we detect/respond to periodic network partitions? A problem often known as  split-brain scenario. It comes from basic observation that  it&#39;s not possible to differentiate dead node from unresponsive one over network boundary. It can lead to very risky situations, like splitting our cluster in two, each one believing, it&#39;s the only one alive and causing data inconsistency or even corruption. There&#39;s no one simple cure for that, that&#39;s why some systems (like  Akka.NET split brain resolvers) offer different strategies depending on the tradeoffs, that we care for.</p><p>我们如何检测/响应定期网络分区？通常被称为分裂性情景的问题。它来自基本观察，即它＆＃39;■不可能在网络边界中区分死区。它可以导致风险非常危险的情况，比如将我们的集群分成两个，每个人都相信，它＆＃39;唯一一个活着，导致数据不一致甚至腐败。没有一个简单的治疗方法，那就是为什么一些系统（如akka.net分裂大脑resolvers）根据我们关心的权衡提供不同的策略。</p><p> How different nodes can reason and decide about the state in the cluster? This case is usually common in systems that are responsible for data management (like distributed databases). Since nodes must be able to serve incoming requests, sometimes they may possibly run into making conflicting decisions about the state of the system. Without going into too much details, two generic approaches two this problem are: Avoiding conflicts, which usually assumes that nodes must establish consensus about state of the system before committing to their decisions. It usually requires establishing (and maintaining) some leader among the nodes, synchronizing via quorum of nodes or combination of two. This is area of popular protocols such as  Raft,  ZAB or Paxos.</p><p> 不同的节点如何推理和决定群集中的状态？这种情况通常在负责数据管理（如分布式数据库）的系统中是常见的。由于节点必须能够为传入请求提供服务，因此有时它们可​​能会遇到对系统状态的冲突决策。在没有过多的细节中，两个通用方法两个问题是：避免冲突，这通常假设节点必须在提交决策之前建立关于系统状态的共识。它通常需要建立（和维护）节点之间的一些领导者，通过仲裁节点或两者组合同步。这是筏，ZAB或Paxo等流行协议的区域。</p><p> Resolving conflicts which accepts the possibility of state conflict to appear - mostly as a tradeoff happening in face of long latencies or periodically unreachable servers - but enriches it with enough metadata so that all nodes individually can reach the same conclusion about the result state without need of consensus. This is a dominant area of Conflict Free Replicated Data Types, which you could read about eg. in a  blog post series published here.</p><p> 解决接受国家冲突的可能性出现的冲突 - 主要是在长期延迟或周期性地发生的权衡或定期无法访问的服务器中进行的权衡 - 但是以足够的元数据丰富，使得所有节点可以单独达到与结果状态的相同结论共识。这是一个主导地区的免除自由复制数据类型，您可以读取eg。在这里发布的博客文章中。</p><p> How to route requests to a given resource inside of cluster? The state of our system usually consists of multiple addressable entities - which are often replicated for higher availability and resiliency. However usually the entire state is too big to fit into any single node. For this reason it&#39;s often partitioned all over the cluster  dynamically. Now this begs a question: how to tell, which node contains an entity identified by some virtual key? Naive approach would be to ask some subset of nodes in hope that at least one of them will have a data we hope for. Given cluster of  N nodes and entity replicated  R times, we should be able to reach our resource after calling  (N/R)+1 nodes.</p><p> 如何将请求路由到集群内部的给定资源？我们的系统状态通常由多种可寻址实体组成 - 通常复制，以便更高的可用性和弹性。然而，通常整个状态太大，无法适合任何一个节点。出于这个原因，它经常动态地在整个群集中划分。现在恳求一个问题：如何告诉，哪个节点包含一些虚拟键标识的实体？天真的方法是询问一些节点的子集希望，其中至少有一个将有我们希望的数据。给定群集的N个节点和实体复制了R次，我们应该能够在调用（n / r）+1节点后到达我们的资源。</p><p> More common way is to keep a registry having an information about current localization of every single entity in a system. Since this approach doesn&#39;t scale well, in practice we group and co-locate entities together within partitions and therefore compress the registry to store information about entire partition rather than individual entity. In this case resource ID is composite key of  (partitionID, entityID). This is how eg.  Akka.Cluster.Sharding or  riak core works. Frequently some subset of hot (frequently used) partitions may be cached on each node for to reduce asking central registry or even the registry itself may be a replicated store.</p><p> 更常见的方式是保留注册表，该注册表具有有关系统中每个单个实体的当前本地化的信息。由于这种方法并不展示，在实践中，我们在实践中，在分区中组合并将实体共同定位在一起，因此压缩了注册表以存储有关整个分区的信息而不是单个实体。在这种情况下，资源ID是（PartitionD，EntityID）的复合键。这是如何。 akka.cluster.arding或Riak核心作品。通常可以在每个节点上缓存一些热（常用）分区的一些子集，以便减少询问中央注册表甚至注册表本身可以是复制的商店。</p><p> We could also use  distributed hash tables - where our entity key is hashed and then mapped into specific node that is responsible for holding resources belonging to that specific subset of key space (a range of all possible hash values). Sometimes this may mean, that we miss node at first try eg. because cluster state is changing, and more hops need to apply.  Microsoft Orleans and Cassandra are popular solutions using that approach.</p><p> 我们还可以使用分布式哈希表 - 散列我们的实体密钥，然后映射到特定节点，该节点负责将属于关键空间的特定空间子集的资源（所有可能的哈希值的范围）。有时这可能意味着，首先尝试我们错过节点。因为群集状态正在发生变化，因此需要应用更多跳跃。 Microsoft Orleans和Cassandra使用该方法是流行的解决方案。</p><p> As you can see, even though we didn&#39;t asked all questions, there&#39;s already a lot of things happening here and the same problems may be solved with different approaches depending on the tradeoffs our system is willing to take.</p><p> 正如你所看到的，即使我们没有问过所有的问题，那里有很多事情发生了很多事情，并且可以根据我们的系统愿意采取的权衡来解决不同的方法。 </p><p> While there&#39;s a chance to make a separate article about each of these in the future, today we&#39;ll focus solely on a  SWIM - used in systems such as Consul and hugely popularized over last few years - as it&#39;s easy to implement for a good start. To improve its resiliency and reduce false positive failure detection, checkout this talk about  Lifeguard - a set of extensions and observations about original SWIM protocol.</p><p>在那里＆＃39;今天的机会在未来做出一个单独的文章，今天我们＆＃39; ll完全专注于游泳 - 在领事和持续几年中的系统中的系统中使用 - 如它＆＃39 ;很容易实现良好的开始。为了提高其弹性，减少假阳性故障检测，结账关于救生员的谈判 - 一系列关于原始游泳协议的扩展和观察。</p><p>   New node wants to join the cluster - how to make that happen and how to inform other nodes in the cluster about that event?</p><p>   新节点想要加入群集 - 如何使其发生，以及如何在群集中通知关于该事件的其他节点？</p><p>  Node was abruptly terminated or cannot be reached any longer. How to detect that an inform others about the fact?</p><p>  节点突然终止或者无法达到任何更长。如何检测到其他事实的信息？</p><p> While first two cases are pretty easy, all of the complexity comes with the third case. We need to discover if node cannot be reached, but that&#39;s not enough - since temporary network failures may happen even inside the same datacenter, they usually get healed fairly fast. We don&#39;t want to panic and throw the node out of the cluster just because we couldn&#39;t reach it in split second. This would lead to very shaky and fragile cluster.</p><p> 虽然前两种情况很容易，但所有的复杂性都带有第三种情况。我们需要发现节点是否无法达到，但是＆＃39; s还不够 - 由于临时网络故障可能发生在同一数据中心内，他们通常会迅速愈合。我们不想恐慌并将节点扔出群集，因为我们可以在分开第二个中达到它。这将导致非常摇摇欲坠和脆弱的集群。</p><p> We&#39;ve already mentioned heartbeat mechanism - every now and then we&#39;re going to send a PING message to other random node. This node is expected to answer with ACK before expected timeout. Easy.</p><p> 我们已经提到了心跳机制 - 每一个现在，然后我们将ping消息发送到其他随机节点。预计此节点将在预期超时之前用ACK回答。简单的。</p><p>  Now, what happens if node didn&#39;t respond under timeout? As we said, we don&#39;t want to overreact. So we don&#39;t consider this node dead yet. Instead we consider it to be  suspect and inform others about our suspicion. Other nodes that received suspect gossip expect it to be confirmed within specified timeout - otherwise they will consider it a hoax, and remove node from their suspected list.</p><p>  现在，如果节点没有在超时回应的情况下会发生什么？正如我们所说，我们不想过度反应。所以我们不要考虑这个节点已经死了。相反，我们认为它是怀疑并告知别人我们的怀疑。接收嫌疑八卦的其他节点预计它将在指定的超时中确认 - 否则他们将考虑一个恶作剧，并从其疑似列表中删除节点。</p><p>  Now all we need is confirmation - in order to do so, we need to ask someone else for verification. So we&#39;re going pick another unsuspected node and ask it to ping  suspect for us (this request is known as PING-REQ). Now if that mediator managed to receive ACK from the  suspect, we know that node is alive, just our network connection was severed for some reason:</p><p>  现在我们所需要的只是确认 - 为了这样做，我们需要向别人询问别人进行验证。所以我们＆＃39;重新选择另一个未经用的节点，并要求它为我们的ping嫌疑人（此请求被称为ping-req）。现在，如果该调解员从嫌疑人那里收到ACK，我们知道节点是活着的，只是我们的网络连接被切断了原因： </p><p>  On the other side, if our mediator didn&#39;t received ACK either, we now have two-hand verification that node is unresponsive for significant time frame - therefore it can be confirmed as dead to everyone.</p><p>在另一边，如果我们的调解员也没有收到Ack，我们现在有两只手验证，节点对大量时间框架无响应 - 因此它可以确认为每个人死亡。</p><p>  Of course this doesn&#39;t mean that node is indeed dead - one escape mechanism is that if suspect will get a suspect notification about itself, it can override it by broadcasting alive message. There maybe different reasons, why  suspect didn&#39;t respond on time: it may be too occupied (either by backlog of other requests or eg. stop-the-world garbage collection of underlying VM). It may also happen, that our network have split the cluster apart - this phenomenon is known as a split-brain.</p><p>  当然这一点并不意味着节点确实死了 - 一个转义机制是，如果怀疑将获得关于自己的可疑通知，它可以通过广播活动来覆盖它。可能有不同的原因，为什么怀疑＆＃39; t按时回应：它可能太占用（由其他请求的积压或ev。停止 - 世界垃圾收集的底层VM）。也可能发生，我们的网络分开了分开的集群 - 这种现象被称为分裂性。</p><p> Ultimately there&#39;s no way to ensure 100% reliable failure detection. We&#39;re just trading reasonable ratio of false failure and how quickly can we detect node as dead - usually these two goals work against each other. In terms of SWIM, I again recommend to lookup for  Lifeguard, which addresses some of the mentioned scenarios. However we&#39;ll not cover it in our implementation below to keep it succinct.</p><p> 最终没有办法保证100％可靠的故障检测。我们＆＃39;重新交易合理的错误失败比，我们如何迅速地检测到节点作为死亡 - 通常这两个目标相互互相努力。在游泳方面，我再次建议查找救生员，该救生员地解决了一些提到的方案。然而，我们＆＃39; LL在下面的实施中没有涵盖它，以保持简洁。</p><p>  Now, we&#39;re going to actually implement that protocol. In order to do so we&#39;re going to apply some simplifications - we&#39;ll do so to make protocol easier to comprehend:</p><p>  现在，我们＆＃39;重新实际实施该协议。为了这样做＆＃39;重新应用一些简化 - 我们＆＃39; ll这样做，使协议更容易理解：</p><p> In general to associate specific request/response messages, we should use some form of sequence numbers. This can result in scenario, when A sends PING1 request to B for which is responds with ACK1, some time later A sends another PING2 to B and receives ACK1 back - the problem was that it cannot tell which PING this message was acknowledgment for. This kind of situation is very rare, but not impossible. If may also lead to &#34;resurrecting&#34; dead nodes.</p><p> 一般来说要关联特定请求/响应消息，我们应该使用某种形式的序列号。这可能导致场景，当A向B发送Ping1请求时，其与Ack1响应，一段时间后者将另一个Ping2发送到B并接收Ack1回来 - 问题是它不能告诉该消息的问题是为了确认这条消息。这种情况非常罕见，但不是不可能的。如果也可能导致＆＃34;复活和＃34;死亡节点。</p><p> While we&#39;re going to piggyback gossip on top of pings - like the paper suggests - we&#39;re simply  gossip entire membership state every time. It&#39;s not the most optimal way, but it&#39;ll work good enough.</p><p> 虽然我们＆＃39;重新进入肩背上的肩带 - 像纸张暗示 - 我们每次都只是八卦整个会员国。它不是最优越的方式，但它＆＃39; ll工作足够好。</p><p> PS:  Other way of avoiding sending entire state with every gossip is to compute consistent hash of current node membership view and put that hash inside of ping instead of full view. This way our PING will carry just a hash value, which responder may compare with hash of its own state. Only if hashes differ (in clusters with small churn of nodes most of the time they don&#39;t), we&#39;re putting membership state on top of ACK.</p><p> PS：避免使用每个八卦发送整个状态的其他方式是计算当前节点成员身份视图的一致哈希，并将该散列放在ping内而不是完整视图。这样我们的ping就会携带哈希值，这是响应者可以与自己状态的哈希相比。只有当哈希不同（大多数时间都有大部分时间的集群，他们在他们不时的时间和＃39; t），我们＆＃39;重新将会员状态放在Ack之上。 </p><p> The code used here is available in  this gist, which I recommend to use, as we&#39;ll not cover everything here.</p><p>此处使用的代码可在本发明员中提供，我建议使用，如我们＆＃39; LL在这里不涵盖所有内容。</p><p>  Since we don&#39;t want to deal with all complexities of node-to-node communication, we&#39;ll just build a model on top of abstractions, that will let us evaluate the algorithm without derailing into solving other problems. The basic prerequisites here are:</p><p>  由于我们不想处理节点到节点通信的所有复杂性，我们＆＃39; LL只是在抽象之上构建一个模型，这将让我们评估算法，而不会导致解决其他问题。以下基本先决条件是：</p><p> A transport layer that will just let us send message to another (possibly remote)  endpoint. We don&#39;t want to deal with managing network connections or serialization details.</p><p> 一个传输层，让我们向另一个（可能远程）端点发送消息。我们希望处理管理网络连接或序列化详细信息。</p><p> An agent accessible behind the  endpoint, able to serve multiple requests and change its state in thread safe manner.</p><p> 可访问端点后面的代理，能够为多个请求提供服务并以线程安全方式更改其状态。</p><p> For these reasons, I&#39;m going to use Akka.NET/Akkling and model our nodes using actors living in the same process. You can easily adopt it to different actor systems connected over Akka.Remote. We keep this as a training exercise - in practice if you want to use clusters in akka, you already have entire ecosystem built on top of Akka.Cluster, which handles membership - and other problems mentioned in introduction - for you.</p><p> 由于这些原因，我将使用Akka.net/Kkling和模拟我们的节点使用生活在同一过程中的演员。您可以轻松地采用它以通过Akka.Remote连接的不同演员系统。我们将此作为培训练习 - 在实践中，如果您想在Akka中使用群集，您已经在Akka.Cluster顶部建立了整个生态系统，这些生态系统都在处理会员资格 - 以及引言中提到的其他问题 - 适合您。</p><p> Our actor will be created by providing it an initial list of contact points: these are the endpoints known to be part of the cluster:</p><p> 我们的演员将通过提供它的初始联系点列表来创建：这些是已知为群集的一部分的端点：</p><p> open Akklinguse sys = System.create &#34;swim-cluster&#34; &lt;| Configuration.parse config// In this example `a` and `b` are actor refs for other // SWIM cluster members living on other nodes. let c = spawn sys &#34;node-b&#34; &lt;| props (Swim.membership [a; b])</p><p> 打开akklinguse sys = system.create＆＃34;游泳群和＃34; ＆lt; | configuration.parse config //在此示例中，其他//`b`是生活在其他节点上的其他//游泳集群成员的演员​​refs。让C =产卵系统＆＃34;节点B＆＃34; ＆lt; |道具（游泳.Membership [A; B]） </p><p> In order to create that actor first we need to deal with a joining process. The idea is that we&#39;ll try to pick endpoints from the provided list (it corresponds to seed node addresses in Akka.Cluster ) and send a  Join request to them. If cluster node actor will receive such request, it&#39;s obliged to accept it by sending  Joined response to ALL cluster members (including requester).</p><p>为了创建该演员，我们需要处理加入过程。这个想法是我们＆＃39; ll尝试从提供的列表中挑选端点（它对应于Akka.Cluster中的种子节点地址）并向它们发送加入请求。如果群集节点演员将接收这样的请求，则＆＃39; s义务通过向所有群集成员发送加入响应（包括请求者）来接受它。</p><p> In case when  Joined wouldn&#39;t arrive eg. because we misconfigured our cluster and other endpoint couldn&#39;t be reached, we wait for some time and try another node from the list. This could be represented as:</p><p> 如果加入WORN和＃39; T到达，例如。因为我们错误地配置了我们的群集和其他端点，所以达到了＆＃39; t and＃39;我们等待一段时间并从列表中尝试另一个节点。这可以表示为：</p><p> let membership (seeds: Endpoint list) (ctx: Actor&lt;_&gt;) = let state = { Myself = { Endpoint = ctx.Self } Active = Set.empty Suspects = Map.empty } (* rest of the actor code... *) let rec joining state cancel = actor { match! ctx.Receive() with | JoinTimeout [] -&gt; return Stop // failed to join any members | JoinTimeout (next::remaining) -&gt; next &lt;! Join state.Myself let cancel = ctx.Schedule joinTimeout ctx.Self (JoinTimeout remaining) return! joining state cancel | Joined gossip -&gt; cancel.Cancel() // cancel join timeout return! becomeReady state gossip | Join peer when peer = state.Myself -&gt; // establish new cluster cancel.Cancel() // cancel join timeout return! becomeReady state (Set.singleton state.Myself) | _ -&gt; return Unhandled } match seeds with | [] -&gt; becomeReady state (Set.singleton state.Myself) | seed::remaining -&gt; seed &lt;! Join state.Myself let cancel = ctx.Schedule joinTimeout ctx.Self (JoinTimeout remaining) joining state cancel</p><p> 让成员身份（种子：端点列表）（CTX：Actor＆lt; _＆gt;）=设态= {mylising = {endpoint = ctx.self} Active = set.empty嫌疑人= map.empty}（* actor代码的其余部分.. 。*）让REC加入州取消=演员{匹配！ ctx.receive（）与|职工[]  - ＆gt;返回停止//未能加入任何会员|职工（下一个::剩下） - ＆gt;下一个＆lt;加入State.Myself让Call取消= CTX.schedule Connectimeout CTX.Self（剩余剩余）返回！加入州取消|加入八卦 - ＆gt; CANCEL.CANCEL（）//取消加入超时返回！成为州八卦|当peer = statem.myself  - ＆gt; //建立新群集取消.Cancel（）//取消加入超时返回！成为州（set.singleton state.myself）| _  - ＆gt;返回未处理的}匹配种子| []  - ＆gt;成为州（set.singleton state.myself）|种子::剩下 - ＆gt;种子＆lt;加入State.Myself让Call = CTX.Schedule Connectimeout CTX.Self（剩余剩余）加入州取消</p><p> After receiving the  Joined request actor associates itself as a operating member of the cluster, ready to work. From now on it will trigger itself to periodically check if others remain responsive:</p><p> 接收到加入的请求Actor将自己视为群集的操作成员，准备工作。从现在开始，它将触发自身，以定期检查其他人是否保持响应：</p><p> let merge gossip state = { state with Active = state.Active + gossip }let becomeReady state gossip = ctx.Schedule pingInterval ctx.Self NextRound |&gt; ignore ready (merge gossip state)</p><p> 让合并八卦状态= {youst active = state.active + gossip}让berseready状态gossip = ctx.schedule pinginterval ctx.self nextround |＆gt;忽略就绪（合并八卦状态）</p><p> As mentioned before, in this case  gossip is just a full set of active cluster members, while  ready is actor behavior used for standard cluster activities:</p><p> 如前所述，在这种情况下，八卦只是一整套活动集群成员，而准备好是用于标准群集活动的演员行为：</p><p> let rec ready state = actor { match! ctx.Receive() with // other message handlers | _ -&gt; return Unhandled}</p><p> 让Recrade State =演员{匹配！ ctx.receive（）使用//其他消息处理程序| _  - ＆gt;返回未处理} </p><p> Since we covered joining procedure from requestor side, let&#39;s do that for operating cluster member as well:</p><p>由于我们从请求者方面覆盖了加入程序，因此为操作集群成员提供了：</p><p> match! ctx.Receive with| Join peer -&gt; let gossip = Set.add peer state.Active let msg = Joined gossip gossip |&gt; Set.remove state.Myself |&gt; Set.iter (fun peer -&gt; peer.Endpoint &lt;! msg) return! ready { state with Active = gossip } | Joined gossip -&gt; return! ready (merge gossip state)// other handlers</p><p> 比赛！ ctx.receive |.加入peer  - ＆gt;让Gossip = Set.Add Peer状态。Active Let Msg =加入八卦八卦|＆gt; set.remove state.myself |＆gt; set.iter（有趣的peer  - ＆gt; peer.endpoint＆lt; msg）返回！准备就绪{状态与active = gossip} |加入八卦 - ＆gt;返回！准备好（合并八卦状态）//其他处理程序</p><p> As we said, once new member tries to join, we simply update our active members state and gossip it to all other members in  Joined message, so they could update it as well.</p><p> 正如我们所说，一旦新成员试图加入，我们只需将我们的活动成员状态更新并将其汇报到加入消息中的所有其他成员，因此它们也可以更新。</p><p> Now, let&#39;s cover pinging process. First we mentioned that we want to trigger our member every now and then - you could already see that when we scheduled  NextRound event in  becomeReady function. Now how it will work? We&#39;re going to pick one node at random - other than current one and not being suspected (suspects are nodes that didn&#39;t reply to pings on time) - and send a  Ping request to it. In the meantime we also schedule a timeout for that request to complete:</p><p> 现在，让＆＃39; s覆盖ping过程。首先，我们提到我们现在想要触发我们的会员，然后 - 当我们计划成比赛中的Nextround事件时，您可以看到。现在它如何工作？我们＆＃39;重新选择一个节点随机 - 除了当前的一个没有疑似（嫌疑人是＆＃39; t按时回复ping的节点） - 并向它发送ping请求。与此同时，我们还安排了该请求完成的超时：</p><p> let pick peers = match Set.count peers with | 0 -&gt; None | count -&gt; let idx = ThreadLocalRandom.Current.Next count Some (Seq.item idx peers) let ready state = actor { match! ctx.Receive() with | NextRound -&gt; ctx.Schedule pingInterval ctx.Self NextRound |&gt; ignore let suspects = state.Suspects |&gt; Map.toSeq |&gt; Seq.map fst |&gt; Set.ofSeq let others = (Set.remove state.Myself state.Active) - suspects // pick one member at random, other than self and not marked as suspected match pick others with | None -&gt; return! ready state | Some peer -&gt; // send Ping request to that peer and schedule timeout peer.Endpoint &lt;! Ping(state.Active, ctx.Self) let cancel = ctx.Schedule pingTimeout ctx.Self (PingTimeout peer) let skipList = Set.ofArray [| state.Myself; peer |] return! ready { state with Suspects = Map.add peer (cancel, WaitPingAck skipList) state.Suspects } | Ping (gossip, sender) -&gt; // reply to incoming ping right away sender &lt;! PingAck(state.Myself, state.Active) return! ready (merge gossip state) // other handlers}</p><p> 让选择同行=匹配set.count对等体0  - ＆gt;没有|数 - ＆gt;让idx = threadlocalrandom.current.next计数一些（SEQ.ITEM IDX对等体）让就绪状态= ACTOR {匹配！ ctx.receive（）与| nextround  - ＆gt; ctx.schedule pinginterval ctx.self nextround |＆gt;忽略让嫌疑人= state.suspects |＆gt; Map.Toseq |＆Gt; seq.map fst |＆gt; set.ofseq让别人=（set.remove state.myself state.active） - 嫌疑人//随意选择一个成员，除了自我而不是标记为怀疑匹配选择没有 - ＆gt;返回！就绪状态|一些同伴 - ＆gt; //将ping请求发送到该对等体和调度超时peer.endpoint＆lt; ping（state.active，ctx.self）让取消= ctx.schedule pingtimeout ctx.self（pingtimeout peer）让skiplist = set.ofArray [|州。同伴|]回来！准备{state with suspects = map.add peer（取消，waitpingack skiplist）state.suspects} | ping（八卦，发件人） - ＆gt; //回复传入ping ricking sender＆lt ;! Pingack（州.MYSELF，State.active）返回！准备好（合并八卦状态）//其他处理程序}</p><p> Here we&#39;re using our suspects map to keep track of timeout cancellation and skip list. What&#39;s a skip list? We&#39;ll cover it soon. As we mentioned, when  PingAck doesn&#39;t arrive on time, we&#39;re going to pick another member (at random) and ask it to ping our suspect for us using  PingReq message. This way we try to mitigate risk of false negatives in our failure detection algorithm:</p><p> 在这里，我们使用我们的嫌疑人映射来跟踪超时取消和跳过列表。什么＆＃39;跳过列表？我们很快掩盖了。正如我们所提到的那样，当Pingack＆＃39; t准时到达时，我们会选择另一个成员（随意）并要求它使用pingreq消息来为我们审视我们的嫌疑人。这样，我们试图在我们的故障检测算法中减轻假否定的风险：</p><p> let ready state = actor { match! ctx.Receive() with | PingReq (suspect, gossip, sender) -&gt; let cancel = ctx.Schedule indirectPingTimeout ctx.Self (PingTimeout suspect) suspect.Endpoint &lt;! Ping(state.Active, ctx.Self) return! ready { merge gossip state with Suspects = Map.add suspect (cancel, WaitPingReqAck) state.Suspects</p><p> 让准备状态=演员{匹配！ ctx.receive（）与| Pingreq（嫌疑人，八卦，发件人） - ＆gt;让取消= ctx.schedule IndirectpingTimeout CTX.Self（PingTimeout Suspect）嫌疑次数.EndPoint＆lt; ping（state.active，ctx.self）退货！准备{Merge Gossip州与嫌疑人= map.add嫌疑人（取消，等待，等待reqack）state.suspects </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://bartoszsypytkowski.com/make-your-cluster-swim/">https://bartoszsypytkowski.com/make-your-cluster-swim/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/2020/">#2020</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/群集/">#群集</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cluster/">#cluster</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/节点/">#节点</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1068381.html"><img src="http://img2.diglog.com/img/2021/6/thumb_d1615bb7dc31e3f074082558ea885b96.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1068381.html">论梦想分享及其目的（2020年） </a></div><span class="my_story_list_date">2021-6-26 11:37</span></div><div class="col-sm"><div><a target="_blank" href="/story/1068267.html"><img src="http://img2.diglog.com/img/2021/6/thumb_06ea97da1ba19fd0da53a6a87d67e332.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1068267.html">David Dobrik的概况，一个愚蠢的恶作剧臭名昭着的愚蠢恶作剧，他们在2020年赚取了1550万美元，现在在火灾中遭到造成创伤 </a></div><span class="my_story_list_date">2021-6-26 0:28</span></div><div class="col-sm"><div><a target="_blank" href="/story/1067914.html"><img src="http://img2.diglog.com/img/2021/6/thumb_07fceb2227c245f6b5c2c09bec8685b9.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1067914.html">肥皂的历史（2020） </a></div><span class="my_story_list_date">2021-6-24 15:26</span></div><div class="col-sm"><div><a target="_blank" href="/story/1067690.html"><img src="http://img2.diglog.com/img/2021/6/thumb_ac47bd9ae7ffd39361a85c87a05d50c5.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1067690.html">谷歌照片是如此2020-欢迎来到自主照片管理世界 </a></div><span class="my_story_list_date">2021-6-23 20:38</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>