<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>超越曲线拟合的ML：因果推理和Do-微积分入门 ML Beyond Curve Fitting: An Intro to Causal Inference and Do-Calculus</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">ML Beyond Curve Fitting: An Intro to Causal Inference and Do-Calculus<br/>超越曲线拟合的ML：因果推理和Do-微积分入门 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-06 02:05:16</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/1/2916bd6015af3e9e1e962504cf897b6a.png"><img src="http://img2.diglog.com/img/2021/1/2916bd6015af3e9e1e962504cf897b6a.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>You might have come across  Judea Pearl&#39;s new book, and a  related interview which was widely shared in my social bubble. In the interview, Pearl dismisses most of what we do in ML as curve fitting. While I believe that&#39;s an overstatement (conveniently ignores RL for example), it&#39;s a nice reminder that most productive debates are often triggered by controversial or outright arrogant comments. Calling machine learning alchemy was a great recent example. After reading the article, I decided to look into his famous do-calculus and the topic causal inference once  again.</p><p>您可能会遇到Judea Pearl的新书，以及在我的社交圈中广泛分享的相关采访。在采访中，Pearl认为我们在ML中所做的大部分工作都是曲线拟合。尽管我认为这是一个夸大的陈述（例如，通常忽略了RL），但它很好地提醒您，大多数富有成效的辩论通常是由有争议的或完全自大的评论引发的。调用机器学习炼金术就是一个很好的例子。阅读文章后，我决定再次研究他著名的do-微积分和主题因果推论。</p><p> Again, because this happened to me semi-periodically. I first learned do-calculus in a (very unpopular but advanced) undergraduate course Bayesian networks. Since then, I have re-encountered it every 2-3 years in various contexts, but somehow it never really struck a chord. I always just thought &#34;this stuff is difficult and/or impractical&#34; and eventually forgot about it and moved on. I never realized how fundamental this stuff was, until now.</p><p> 再一次，因为这是我半周期性地发生的。我最初是在贝叶斯网络（非常不受欢迎但很高级）本科课程中学习微积分的。从那时起，我每隔2-3年在各种情况下都会遇到它，但是以某种方式它从未真正引起过共鸣。我一直以为这东西很困难和/或不切实际。最终忘记了，继续前进。直到现在，我还没有意识到这些东西的根本性。</p><p> This time around, I think I fully grasped the significance of causal reasoning and I turned into a full-on believer. I know I&#39;m late to the game but I almost think it&#39;s basic hygiene for people working with data and conditional probabilities to understand the basics of this toolkit, and I feel embarrassed for completely ignoring this throughout my career.</p><p> 这次，我想我已经完全理解了因果推理的重要性，因此我成为了一个全面的信徒。我知道我来晚了，但是我几乎认为这是使用数据和条件概率来理解该工具包基础知识的人们的基本卫生习惯，我为在整个职业生涯中完全忽略这一点而感到尴尬。</p><p> In this post I&#39;ll try to explain the basics, and convince you why you should think about this, too. If you work on deep learning, that&#39;s an even better reason to understand this. Pearl&#39;s comments may be unhelpful if interpreted as contrasting deep learning with causal inference. Rather, you should interpret it as highlighting causal inference as a huge, relatively underexplored, application of deep learning. Don&#39;t get discouraged by causal diagrams looking a lot like Bayesian networks (not a coincidence seeing they were both pioneered by Pearl) they don&#39;t compete with, they complement deep learning.</p><p> 在这篇文章中，我将尝试解释基础知识，并说服您为什么也应该考虑这一点。如果您从事深度学习，那是理解这一点的一个更好的理由。如果将Pearl的评论解释为将深度学习与因果推理进行对比，则可能无济于事。相反，您应该将其解释为强调因果推理是深度学习的一种巨大，相对未开发的应用。不要因果关系图而灰心丧气，这些因果图看上去很像贝叶斯网络（并非偶然，因为它们都是由Pearl率先开发的），它们不与之竞争，而是对深度学习的补充。</p><p>  First of all, causal calculus differentiates between two types of conditional distributions one might want to estimate.  tldr: in ML we usually estimate only one of them, but in some applications we should actually try to or have to estimate the other one.</p><p>  首先，因果演算区分了人们可能想要估计的两种类型的条件分布。 tldr：在ML中，我们通常只估计其中之一，但是在某些应用中，我们实际上应该尝试或不得不估计另一个。</p><p> To set things up, let&#39;s say we have i.i.d. data sampled from some joint $p(x,y,z,\ldots)$. Let&#39;s assume we have lots of data and the best tools (say, deep networks) to fully estimate this joint distribution, or any property, conditional or marginal distribution thereof. In other words, let&#39;s assume $p$ is known and tractable. Say we are ultimately interested in how variable $y$ behaves given $x$. At a high level, one can ask this question in two ways:</p><p> 要进行设置，假设我们有i.i.d。从某些联合$ p（x，y，z，\ ldots）$采样的数据。假设我们拥有大量数据和最好的工具（例如深层网络）来完全估计这种联合分布或其任何属性，条件或边际分布。换句话说，假设$ p $是已知的并且易于处理。假设我们最终对变量$ y $在给定$ x $时的行为感兴趣。在较高的层次上，可以通过两种方式提出这个问题：</p><p> observational $p(y\vert x)$: What is the distribution of $Y$ given that I  observe variable $X$ takes value $x$. This is what we usually estimate in supervised machine learning. It is a conditional distribution which can be calculated from $p(x,y,z,\ldots)$ as a ratio of two of its marginals. $p(y\vert x) = \frac{p(x,y)}{p(x)}$. We&#39;re all very familiar with this object and also know how to estimate this from data.</p><p> 观察性$ p（y \ vert x）$：鉴于我观察到的变量$ X $取值$ x $，$ Y $的分布是什么。这是我们通常在有监督的机器学习中估计的。它是一个条件分布，可以根据$ p（x，y，z，\ ldots）$作为其两个边际的比率来计算。 $ p（y \ vert x）= \ frac {p（x，y）} {p（x）} $。我们都非常熟悉此对象，并且也知道如何根据数据进行估算。 </p><p> interventional $p(y\vert do(x))$: What is the distribution of $Y$ if I were to  set the value of $X$ to $x$. This describes the distribution of $Y$ I would observe if I intervened in the data generating process by artificially forcing the variable $X$ to take value $x$, but otherwise simulating the rest of the variables according to the original process that generated the data. (note that the data generating procedure is NOT the same as the joint distribution $p(x,y,z,\ldots)$ and this is an important detail).</p><p>介入性$ p（y \ vert do（x））$：如果我将$ X $的值设置为$ x $，则$ Y $的分布是什么。这描述了$ Y $的分布，如果我通过人为地强迫变量$ X $取值$ x $干预数据生成过程，则可以观察到这些分布，但是根据生成该变量的原始过程模拟其余的变量数据。 （请注意，数据生成过程与联合分布$ p（x，y，z，\ ldots）$不同，这是一个重要的细节）。</p><p>  No. $p(y\vert do(x))$ and $p(y\vert x)$ are not generally the same, and you can verify this with several simple thought experiments. Say, $Y$ is the pressure in my espresso machine&#39;s boiler which ranges roughly between $0$ and $1.1$ bar depending on how long it&#39;s been turned on. Let $X$ be the reading of the built-in barometer. Let&#39;s say we jointly observe X and Y at random times. Assuming the barometer functions properly $p(y|x)$ should be a unimodal distribution centered around $x$, with randomness due to measurement noise. However, $p(y|do(x))$ won&#39;t actually depend on the value of $x$ and is generally the same as $p(y)$, the marginal distribution of boiler pressure. This is because artificially setting my barometer to a value (say, by moving the needle) won&#39;t actually cause the pressure in the tank to go up or down.</p><p>  否。$ p（y \ vert do（x））$和$ p（y \ vert x）$通常并不相同，您可以通过几个简单的思想实验来验证这一点。假设$ Y $是我的意式咖啡机锅炉中的压力，其压力范围大约在$ 0 $和$ 1.1 $ bar之间，具体取决于开启时间。假设$ X $是内置气压计的读数。假设我们在随机时间共同观察X和Y。假设气压计正常运行，则$ p（y | x）$应该是一个以$ x $为中心的单峰分布，并且由于测量噪声而具有随机性。但是，$ p（y | do（x））$实际上不会取决于$ x $的值，并且通常与锅炉压力的边际分布$ p（y）$相同。这是因为人为地将气压计设置为某个值（例如，通过移动指针）实际上不会导致水箱中的压力升高或降低。</p><p> In summary, $y$ and $x$ are correlated or statistically dependent and therefore seeing $x$ allows me to predict the value of $y$, but $y$ is not caused by $x$ so setting the value of $x$ won&#39;t effect the distribution of $y$. Hence, $p(y\vert x)$ and $p(y\vert do(x))$ behave very differently. This simple example is just the tip of the iceberg. The differences between interventional and observational conditionals can be a lot more nuanced and hard to characterize when there are lots of variables with complex interactions.</p><p> 总而言之，$ y $和$ x $是相关的或在统计上相关的，因此看到$ x $可以预测$ y $的值，但是$ y $不是由$ x $引起的，因此设置$ x的值$不会影响$ y $的分配。因此，$ p（y \ vert x）$和$ p（y \ vert do（x））$的行为非常不同。这个简单的例子只是冰山一角。当存在大量具有复杂交互作用的变量时，干预条件和观察条件之间的差异可能会更加细微，难以描述。</p><p>  Depending on the application you want to solve, you should seek to estimate one of these conditionals. If your ultimate goal is diagnosis or forecasting (i.e. observing a naturally occurring $x$ and inferring the probable values of $y$) you want the observational conditional $p(y\vert x)$. This is what we already do in supervised learning, this is what Judea Pearl called curve fitting. This is all good for a range of important applications such as classification, image segmentation, super-resolution, voice transcription, machine translation, and many more.</p><p>  根据您要解决的应用程序，您应该尝试估算这些条件之一。如果您的最终目标是诊断或预测（即观察自然发生的$ x $并推断$ y $的可能值），则需要观测条件条件$ p（y \ vert x）$。这就是我们在监督学习中已经做过的事情，这就是Judea Pearl所谓的曲线拟合。这对于一系列重要的应用程序都是有好处的，例如分类，图像分割，超分辨率，语音转录，机器翻译等等。</p><p> In applications where you ultimately want to control or choose $x$ based on the conditional you estimated, you should seek to estimate $p(y\vert do(x))$ instead. For example, if $x$ is a medical treatment and $y$ is the outcome, you are not merely interested in observing a naturally occurring treatment $x$ and predicting the outcome, we want to  proactively choose the treatment $x$ given our understanding of how it effects the outcome $y$. Similar situations occur in system identification, control and online recommender systems.</p><p> 在最终希望根据估计的条件来控制或选择$ x $的应用程序中，应改为尝试估计$ p（y \ vert do（x））$。例如，如果$ x $是药物治疗，而$ y $是结果，则您不仅对观察自然发生的治疗$ x $和预测结果感兴趣，我们还想根据自己的意愿主动选择治疗$ x $了解它如何影响结果$ y $。在系统识别，控制和在线推荐系统中也会发生类似情况。</p><p>  This is perhaps the main concept I haven&#39;t grasped before. $p(y\vert do(x))$ is in fact a vanilla conditional distribution, but it&#39;s not computed based on $p(x,z,y,\ldots)$, but a different joint $p_{do(X=x)}(x,z,y,\ldots)$ instead. This $p_{do(X=x)}$ is the joint distribution of data which we would observe if we actually carried out the intervention in question. $p(y\vert do(x))$ is the conditional distribution we would learn from data collected in  randomized controlled trials or A/B tests where the experimenter controls $x$. Note that actually carrying out the intervention or randomized trials may be impossible or at least impractical or unethical in many situations. You can&#39;t do an A/B test forcing half your subjects to smoke weed and the other half to smoke placebo to understand the effect on marijuana on their health. Even if you can&#39;t directly estimate $p(y\vert do(x))$ from randomized experiments, the object still exists. The main point of causal inference and do-calculus is:</p><p>  这也许是我以前没有掌握的主要概念。 $ p（y \ vert do（x））$实际上是一种普通的条件分布，但是它不是基于$ p（x，z，y，\ ldots）$计算的，而是基于不同的联合$ p_改为{do（X = x）}（x，z，y，\ ldots）$。 $ p_ {do（X = x）} $是数据的联合分布，如果我们实际进行了相关干预，我们将观察到。 $ p（y \ vert do（x））$是条件分布，我们将从实验者控制$ x $的随机对照试验或A / B测试中收集的数据中学到。注意，在许多情况下，实际上不可能进行干预或随机试验，或者至少不切实际或不道德。您不能进行A / B测试，以迫使您的受试者一半抽大麻，另一半抽安慰剂以了解大麻对其健康的影响。即使您不能从随机实验中直接估算$ p（y \ vert do（x））$，该对象仍然存在。因果推理和微积分的要点是：</p><p> If I cannot measure $p(y\vert do(x))$ directly in a randomized controlled trial, can I estimate it based on data I observed outside of a controlled experiment?</p><p> 如果我无法在随机对照试验中直接测量$ p（y \ vert do（x））$，是否可以根据在对照实验之外观察到的数据进行估算？ </p><p>  Let&#39;s start with a diagram that shows what&#39;s going on if we only care about $p(y\vert x)$, i.e. the simple supervised learning case:</p><p>让我们从一个图表开始，该图表显示了如果我们只关心$ p（y \ vert x）$（即简单的监督学习案例）时发生的情况：</p><p>  Let&#39;s say we observe 3 variables, $x, z, y$, in this order. Data is sampled i.i.d. from some observable joint distribution over 3 variables, denoted by the blue factor graph labelled &#39;observable joint&#39;. If you don&#39;t know what a factor graph is, it&#39;s not important, the circles represent random variables, the little square represents a joint distribution of the variables it&#39;s connected to. We are interested in predicting $y$ from $x$, and say that $z$ is a third variable which we do not want to infer but we can also measure (I included this for completeness). The observational conditional $p(y\vert x)$ is calculated from this joint via simple conditioning. From the training data we can build a model $q(y\vert x;\theta)$ to approximate this conditional, for example using a deep net minimizing cross-entropy or whatever.</p><p>  假设我们按此顺序观察3个变量$ x，z，y $。数据被内采样从3个变量的一些可观察到的关节分布中得到，由标记为“可观察到的关节”的蓝色因子图表示。如果您不知道什么是因子图，那并不重要，圆圈代表随机变量，小方块代表所连接变量的联合分布。我们有兴趣从$ x $预测$ y $，并说$ z $是我们不想推断的第三个变量，但我们也可以测量（为完整性起见，我将其包括在内）。观测条件条件$ p（y \ vert x）$是通过简单条件从该关节计算出来的。从训练数据中，我们可以建立模型$ q（y \ vert x; \ theta）$来近似此条件，例如使用使交叉熵最小的深网或其他方法。</p><p> Now, what if we&#39;re actually interested in $p(y\vert do(x))$ rather than $p(y\vert x)$? This is what it looks like:</p><p> 现在，如果我们实际上对$ p（y \ vert do（x））$而不是$ p（y \ vert x）$感兴趣呢？看起来是这样的：</p><p>  So, we still have the blue observed joint and data is still sampled from this joint. However, the object we wish to estimate is on the bottom right, the red intervention conditional $p(y\vert do(x))$. This is related to the intervention joint which is denoted by the red factor graph above it. It&#39;s a joint distribution over the same domain as $p$ but it&#39;s a different distribution. If we could sample from this red distribution (e.g. actually run a randomized controlled trial where we get to pick $x$), the problem would be solved by simple supervised learning. We could generate data from the red joint, and estimate a model directly from there. However, we assume this is not possible, and all we have is data sampled from the blue joint. We have to see if we can somehow estimate the red conditional $p(y\vert do(x))$ from the blue joint.</p><p>  因此，我们仍然具有观察到的蓝色关节，并且仍从该关节中采样数据。但是，我们希望估计的对象位于右下角，即红色干预条件$ p（y \ vert do（x））$。这与介入关节有关，该关节由上方的红色因子图表示。它是与$ p $在同一域中的联合分布，但是是不同的分布。如果我们可以从这个红色分布中取样（例如，实际上进行了一项随机对照试验，我们可以选择$ x $），那么该问题将通过简单的监督学习来解决。我们可以从红色关节生成数据，然后从那里直接估算模型。但是，我们认为这是不可能的，而我们所拥有的全部是从蓝色接头采样的数据。我们必须看看我们是否能够以某种方式从蓝色关节估计红色条件$ p（y \ vert do（x））$。</p><p>  If we want to establish a connection between the blue and the red joints,  we must introduce additional assumptions about the causal structure of the data generating mechanism. The only way we can make predictions about how our distribution changes as a consequence of an interaction is if we know how the variables are causally related. This information about causal relationships is not captured in the joint distribution alone. We have to introduce something more expressive than that. Here is how what this looks like:</p><p>  如果要在蓝色和红色关节之间建立连接，则必须引入有关数据生成机制的因果结构的其他假设。我们可以预测由于交互作用而导致的分布变化的唯一方法是知道变量之间的因果关系。关于因果关系的信息不能仅在联合分布中捕获。我们必须引入比这更具表现力的东西。这是什么样子：</p><p>  In addition to the observable joint we now also have a causal model of the world (top left) This causal model contains more detail than the joint distribution: it knows not only that pressure and barometer readings are dependent but also that pressure causes the barometer to go up and not the other way around. The arrows in this model correspond to the assumed direction of causation, and the absence of an arrow represents the absence of direct causal influence between variables. The mapping from causal diagrams to joint distributions is many-to-one: several causal diagrams are compatible with the same joint distribution. Thus, it is generally impossible to conclusively choose between different causal explanations by looking at observed data only.</p><p>  除了可观察的关节之外，我们现在还拥有一个世界的因果模型（左上）。该因果模型包含比关节分布更多的细节：它不仅知道压力和气压计读数是相关的，而且还知道压力导致气压计上升，而不是相反。此模型中的箭头对应于假定的因果关系方向，而没有箭头则表示变量之间没有直接因果关系。从因果图到联合分布的映射是多对一的：几个因果图与相同的联合分布兼容。因此，通常不可能仅通过观察数据来在不同的因果解释之间做出结论性的选择。</p><p> Coming up with a causal model is a modeling step where we have to consider assumptions about how the world works, what causes what. Once we have a causal diagram, we can emulate the effect of intervention by mutilating the causal network: deleting all edges that lead into nodes in a $do$ operator. This is shown on the middle-top panel. The mutilated causal model then gives rise to a joint distribution denoted by the green factor graph. This joint has a corresponding conditional distribution $\tilde{p}(y\vert do(x))$, which we can use as our approximation of $p(y\vert do(x))$. If we got the causal structure qualitatively right (i.e. there are no missing nodes and we got the direction of arrows all correct), this approximation is exact and $\tilde{p}(y\vert do(x)) = p(y\vert do(x))$. If our causal assumptions are wrong, the approximation may be bogus.</p><p> 提出因果模型是一个建模步骤，在该步骤中，我们必须考虑有关世界运转方式，原因的假设。一旦有了因果图，我们就可以通过破坏因果网络来模拟干预的效果：删除$ do $运算符中所有导致结点的边。这显示在中间顶部面板上。残缺的因果模型然后产生一个由绿色因子图表示的联合分布。该关节具有相应的条件分布$ \ tilde {p}（y \ vert do（x））$，我们可以将其用作我们的$ p（y \ vert do（x））$的近似值。如果我们定性地确定了因果结构（即没有丢失的节点，并且箭头的方向都正确），则此近似值是精确的，并且$ \ tilde {p}（y \ vert do（x））= p（y \ vert do（x））$。如果我们的因果假设是错误的，则近似值可能是虚假的。 </p><p> Critically, to get to this green stuff, and thereby to establish the bridge between observational data and interventional distributions, we had to combine data with additional assumptions, prior knowledge if you wish. Data alone would not enable us to do this.</p><p>至关重要的是，为了获得这些绿色的东西，从而在观察数据和干预性分布之间建立桥梁，我们必须将数据与其他假设（如果需要的话）相结合。单凭数据并不能使我们做到这一点。</p><p>  Now the question is, how can we say anything about the green conditional when we only have data from the blue distribution. We are in a better situation than before as we have the causal model relating the two. To cut a long story short, this is what the so-called  do-calculus is for. Do-calculus allows us to massage the green conditional distribution until we can express it in terms of various marginals, conditionals and expectations under the blue distribution. Do-calculus extends our toolkit of working with conditional probability distributions with four additional rules we can apply to conditional distributions with the $do$ operators in them. These rules take into account properties of the causal diagram. The details can&#39;t be compressed into a single blog post, but here is  an introductory paper on them..</p><p>  现在的问题是，当我们只有蓝色分布中的数据时，我们怎么能说绿色条件。我们处于比以前更好的状况，因为我们有了将两者联系起来的因果模型。长话短说，这就是所谓的“演算”的目的。 Do-演算使我们可以对绿色的条件分布进行按摩，直到我们可以根据蓝色分布下的各种边际，条件和期望来表达它为止。 Do-演算扩展了我们的工具包，用于处理条件概率分布，并提供了四个附加规则，我们可以将这些规则应用于带有$ do $运算符的条件分布。这些规则考虑了因果图的属性。这些详细信息无法压缩到单个博客文章中，但这是其中的介绍性文章。</p><p> Ideally, as a result of a do-calculus derivation you end up with an equivalent formula for $\tilde{p}(y\vert do(x))$ which no longer has any do operators in them, so you estimate it from observational data alone. If this is the case we say that the causal query $\tilde{p}(y\vert do(x))$ is  identifiable. Conversely, if this is not possible, no matter how hard we try applying do-calculus, we call the causal query  non-identifiable, which means that we won&#39;t be able to estimate it from the data we have. The diagram below summarizes this causal inference machinery in its full glory.</p><p> 理想情况下，作为do-演算推导的结果，您最终得到了$ \ tilde {p}（y \ vert do（x））$的等效公式，该公式中不再包含任何do运算符，因此您可以根据以下公式进行估算仅观察数据。在这种情况下，我们说因果查询$ \ tilde {p}（y \ vert do（x））$是可识别的。相反，如果这不可能，则无论我们尝试应用do-演算多么努力，我们都将因果查询称为不可识别的，这意味着我们将无法从所拥有的数据中估计出因果查询。下图全面总结了这种因果推理机制。</p><p>  The new panel called &#34;estimable formula&#34; shows the equivalent expression for $\tilde{p}(y\vert do(x))$ obtained as a result of the derivation including several do-calculus rules. Notice how the variable $z$ which is completely irrelevant if you only care about $p(y\vert x)$ is now needed to perform causal inference. If we can&#39;t observe $z$ we can still do supervised learning, but we won&#39;t be able to answer causal inference queries $p(y\vert do(x))$.</p><p>  新的面板称为“可估算公式”。显示了通过推导获得的$ \ tilde {p}（y \ vert do（x））$的等效表达式，其中包括几个do-演算规则。注意，现在只需要变量$ z $来执行因果推断，如果只关心$ p（y \ vert x）$，则该变量完全不相关。如果我们不能观察$ z $，我们仍然可以进行监督学习，但是我们将无法回答因果推理查询$ p（y \ vert do（x））$。</p><p>  You can never fully verify the validity and completeness of your causal diagram based on observed data alone. However, there are certain aspects of the causal model which are empirically testable. In particular, the causal diagram implies certain conditional independence or dependence relationships between sets of variables. These dependencies or independencies can be empirically tested, and if they are not present in the data, that is an indication that your causal model is wrong. Taking this idea forward you can attempt to perform full causal discovery: attempting to infer the causal model or at least aspects of it, from empirical data.</p><p>  您永远无法仅根据观察到的数据完全验证因果图的有效性和完整性。但是，因果模型的某些方面可以凭经验进行检验。特别是，因果图暗示了变量集之间的某些条件独立性或依存关系。可以凭经验测试这些依赖性或独立性，如果数据中不存在这些依赖性，则表明您的因果模型是错误的。将此想法向前推进，您可以尝试执行完整的因果发现：尝试从经验数据推断因果模型或至少是因果模型。</p><p> But the bottom line is: a full causal model is a form of prior knowledge that you have to add to your analysis in order to get answers to causal questions without actually carrying out interventions. Reasoning with data alone won&#39;t be able to give you this. Unlike priors in Bayesian analysis - which are a nice-to-have and can improve data-efficiency - causal diagrams in causal inference are a must-have. With a few exceptions, all you can do without them is running randomized controlled experiments.</p><p> 但最重要的是：完整的因果模型是先验知识的一种形式，您必须将其添加到分析中，以便在不实际进行干预的情况下获得因果问题的答案。仅凭数据进行推理就无法为您提供帮助。与贝叶斯分析中的先验方法不同（前者很不错并且可以提高数据效率），因果推理中的因果图是必须具备的。除少数例外，如果没有它们，您所能做的就是运行随机对照实验。</p><p>  Causal inference is indeed something fundamental. It allows us to answer &#34;what-if-we-did-x&#34; type questions that would normally require controlled experiments and explicit interventions to answer. And I haven&#39;t even touched on counterfactuals which are even more powerful.</p><p>  因果推理确实是基本的东西。它使我们能够回答＆＃34;如果我们做了x＆＃34;输入通常需要受控实验和明确干预才能回答的问题。而且我什至都没有谈到过更强大的反事实。 </p><p> You can live without this in some cases. Often, you really just want to do normal inference. In other applications such as model-free RL, the ability to explicitly control certain variables may allow you to sidestep answering causal questions explicitly. But there are several situations, and very important applications, where causal inference offers the only method to solve the problem in a principled way.</p><p>在某些情况下，您可以没有这种生活。通常，您实际上只是想进行正常的推断。在其他应用程序中，例如无模型RL，显式控制某些变量的功能可能使您可以回避显式回答因果问题。但是，在几种情况下以及非常重要的应用中，因果推理是唯一以有原则的方式解决问题的方法。</p><p> I wanted to emphasize again that this is not a question of whether you work on deep learning or causal inference. You can, and in many cases you should, do both. Causal inference and do-calculus allows you to understand a problem and establish what needs to be estimated from data based on your assumptions captured in a causal diagram. But once you&#39;ve done that, you still need powerful tools to actually estimate that thing from data. Here, you can still use deep learning, SGD, variational bounds, etc. It is this cross-section of deep learning applied to causal inference which the recent article with Pearl claimed was under-explored.</p><p> 我想再次强调，这不是您从事深度学习还是因果推理的问题。您可以并且在很多情况下应该两者都做。因果推论和微积分使您能够理解问题并根据因果图中捕获的假设来确定需要从数据中估计的内容。但是，一旦完成此操作，您仍然需要强大的工具才能从数据中实际估算出该值。在这里，您仍然可以使用深度学习，SGD，变化范围等。这是应用于因果推理的深度学习的这一部分，而最近有关Pearl的文章则未得到充分研究。</p><p> UPDATE: In the comments below people actually pointed out some relevant papers (thanks!). If you are aware of any work, please add them there.</p><p> 更新：在下面的评论中，人们实际上指出了一些相关的论文（谢谢！）。如果您知道有任何工作，请在此处添加。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.inference.vc/untitled/">https://www.inference.vc/untitled/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/曲线拟合/">#曲线拟合</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/curve/">#curve</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>