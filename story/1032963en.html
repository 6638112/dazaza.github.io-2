<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>CTO可怕的停电故事</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">CTO可怕的停电故事</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-03 22:14:59</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/1b8d43f0863065c4f954f4168059ebc7.jpg"><img src="http://img2.diglog.com/img/2020/11/1b8d43f0863065c4f954f4168059ebc7.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>You’re sound asleep when the alarms go off. It’s 3 a.m. You wipe your eyes, check your phone. You know something is wrong. Very wrong.</p><p>闹钟响的时候你睡得正香。现在是凌晨3点。你擦擦眼睛，看看手机。你知道有些不对劲。大错特错。</p><p> The website is down. Your application is broken. The only light in the room is coming from your computer monitor. The Gremlin in the system can be hiding anywhere, and it’s your team’s job to find it.</p><p>网站瘫痪了。您的应用程序已损坏。房间里唯一的光来自你的电脑显示器。系统中的Gremlin可能隐藏在任何地方，找到它是您的团队的工作。</p><p>  As someone who runs PR for various DevOps startups, I’ve seen this story play out over and over. The reputation cost alone of a major outage is enough to instill fear in even the most seasoned engineer!</p><p>作为一个为多家DevOps初创公司管理公关的人，我看到这个故事一遍又一遍地上演。即使是最有经验的工程师，光是一次重大停机的声誉成本就足以让人感到恐惧！</p><p> But the truth is, every company has system failure. And we’re still a bit away from getting online systems to look more like utilities, where you flip a switch and it just works. So sharing stories and normalizing failure (e.g. transparent and blameless postmortems) are positive trends for the industry; it makes everyone feel less scared and alone.</p><p>但事实是，每家公司都有系统故障。我们离让在线系统看起来更像公用事业公司还有一段路要走，在这种情况下，只要轻轻一按开关，它就能正常工作。因此，分享故事和将失败正常化(例如，透明和无可指责的事后处理)是该行业的积极趋势；它让每个人都不那么害怕和孤独。</p><p> I’m not going to cite generic numbers about the cost of downtime. For Amazon it may be millions per hour; for your company, it may be confined to a frustrating customer experience, if dealt with swiftly. But ultimately these kinds of situations lose businesses money, hurt reputations, drain engineering resources and fuel interest in the competition.</p><p>我不打算引用有关停机成本的通用数字。对亚马逊来说，这可能是每小时数百万美元；对你的公司来说，如果处理迅速，可能只会限制在令人沮丧的客户体验上。但最终，这些情况会让企业赔钱，损害声誉，耗尽工程资源，并激发人们对竞争的兴趣。</p><p> So in the spirit of Halloween, and more importantly in the spirit of sharing experiences to better prevent them from happening in the future, let’s take a look at six scary outages stories, as told by CTOs themselves.</p><p>因此，本着万圣节的精神，更重要的是本着分享经验以更好地防止未来发生这种情况的精神，让我们来看看CTO们自己讲述的六个可怕的停电故事。</p><p>      “Push can’t possibly be down. Our pushes are in a queue, and I am receiving pushes.”</p><p>“推盘不可能往下推。我们的推送在排队，而我正在接受推送。“。</p><p> “It’s been five days, and push is STILL down. People are filing all kinds of tasks.”</p><p>“已经五天了，推盘还是往下。人们正在将各种任务归档。“</p><p> … so I reluctantly started poking around. All our push metrics looked relatively normal, every test push I sent was promptly delivered. Yet the support team was right — people had been steadily complaining for five full days about pushes not succeeding. What on earth could it be?</p><p>…。所以我很不情愿地开始四处打听。我们所有的推送指标看起来都比较正常，我发送的每一次测试推送都是及时交付的。然而，支持团队是对的-人们整整五天来一直在抱怨推动没有成功。到底是什么呢？</p><p> These were Android push notifications, and Android devices needed to hold a socket open to the server to subscribe to push notifications. We had tens of millions of Android devices, so we ran the push notification service in an autoscaling group. To load-balance connections across the group, we used round-robin DNS, and to increase capacity we would simply increase the size of the ASG [auto-scaling group]. Eventually, we figured out that the complaints had begun right around the last time we increased the size of the ASG, so that was a good clue. Another clue was that all the people complaining seemed to be in Eastern Europe. We asked a few of them to run a verbose trace, and that’s when we learned that the DNS record was coming back as … missing?</p><p>这些都是Android推送通知，Android设备需要向服务器打开一个套接字才能订阅推送通知。我们有数以千万计的Android设备，所以我们在一个自动伸缩组中运行推送通知服务。为了在整个组中实现连接负载平衡，我们使用了循环DNS，而要增加容量，我们只需增加ASG[自动伸缩组]的大小即可。最终，我们发现投诉是在我们最后一次增加ASG大小的时候开始的，所以这是一个很好的线索。另一条线索是，所有抱怨的人似乎都在东欧。我们要求他们中的一些人运行详细的跟踪，就在那时我们了解到dns记录将以…的形式返回。失踪了吗？</p><p> Turns out that when we increased the size of the ASG, the round-robin DNS record exceeded the UDP packet size. Normally this is no big deal; the protocol says it should fall back to using TCP in that instance. And it did, for almost everyone. Except for users behind one major router in Romania. We delegated DNS for that record from route53 to a small local python DNS server that let us return a random subset of four Android push notification servers, and everything was fine again. 💀</p><p>结果发现，当我们增加ASG的大小时，循环DNS记录超过了UDP数据包的大小。通常这没什么大不了的；协议说在这种情况下应该退回到使用TCP。它确实发生了，对几乎每个人来说都是如此。除了罗马尼亚一台主要路由器后面的用户。我们将该记录的DNS从route53委托给一个小型的本地python DNS服务器，该服务器允许我们返回4个Android推送通知服务器的随机子集，然后一切都恢复正常。💀。</p><p>   The outage occurred on a Friday afternoon, just as we were about to head out to Halloween Happy Hour. The page came in that we were serving exclusively 500s — a bad, bad experience for customers. After some digging, we realized that our hosts had filled up their disks, and we started failing because we couldn’t write logs (also scary because we were flying blind).</p><p>停电发生在一个周五的下午，当时我们正准备去万圣节欢乐时光。传来的页面显示，我们只为500提供服务-这对客户来说是一种非常非常糟糕的体验。经过一些挖掘，我们意识到我们的主机已经填满了他们的磁盘，我们开始失败，因为我们不能写日志(也很可怕，因为我们是在盲目飞行)。</p><p> We ended up refreshing the hosts, implementing log rotation to prevent that from happening in the future, and creating an alarm to warn us if we were ever getting close again. But the most interesting thing we did is have one of our engineers write a new Gremlin for our platform: the disk Gremlin to make sure we could proactively exercise the fixes to make sure we never failed that way again. Then we automated that test and that test still lingers, running randomly in our production environment to this very day. 😱</p><p>我们最终刷新了主机，实现了日志轮换以防止将来发生这种情况，并创建了警报以警告我们是否再次接近。但最有趣的是，我们让一位工程师为我们的平台编写了一个新的Gremlin：Disk Gremlin，以确保我们可以主动执行修复，确保我们不会再以这种方式失败。然后，我们自动化了该测试，而该测试仍然挥之不去，直到今天仍在我们的生产环境中随机运行。😱。</p><p>   Remember that urban legend about a server going down everyday, at the same specific hour? And after weeks of investigations, someone looked at the security camera footage… and found out that the maid was disconnecting the server to connect the vacuum cleaner! Well, we all know that the Gremlin in the closet isn’t always as scary or mysterious as we initially think :)</p><p>还记得那个关于服务器每天在同一特定时间停机的都市传说吗？经过几周的调查，有人看了监控录像…。发现女佣正在断开服务器连接吸尘器！嗯，我们都知道，衣柜里的Gremlin并不总是像我们最初想象的那样可怕或神秘：)。</p><p>  Several times a week, we’d been seeing the backend’s latency metrics going through the roof. And each time we investigated it, we noticed one of the tables getting locked and queries kept timing out all over. We wondered: Is one of our customers redeploying their application non-stop? The main suspect was a complex query which fetches the list of all our customers’ servers’ information, so they’ll be able to choose which of them they would like to debug. We started optimizing that query and saw huge improvements, yet those latency spikes kept happening.</p><p>每周有好几次，我们都看到后端的延迟指标达到了顶峰。每次我们调查它时，我们都会注意到其中一个表被锁定，并且所有查询都不断超时。我们想知道：我们的一个客户是否不间断地重新部署他们的应用程序？主要的疑点是一个复杂的查询，它获取我们所有客户的服务器信息的列表，这样他们就可以选择要调试的服务器。我们开始优化该查询，看到了巨大的改进，但这些延迟峰值仍在不断发生。</p><p> Then a couple of weeks ago, while attending the weekly “Customer Success Briefing,” the latency spike was happening again and it hit me like a brick. I noticed a query that we barely used, from our application’s back office, that was really slow because we never prioritized fixing it (it was scarcely used). Apparently, our customer success manager had been collecting the data for the meeting, and every time the query didn’t return fast enough, he just kept hitting refresh and retrying. That rarely used query was locking up our database and challenging our customer success manager’s sanity! Looking back at the data, we confirmed that all of the latency peaks were aligned with Customer Success briefings. Eventually, after about 20 minutes of optimizing that query, everything returned to normal. 🎃</p><p>几周前，在参加每周一次的“客户成功简报会”时，延迟高峰再次出现，这对我打击很大。我从应用程序的后台注意到一个我们很少使用的查询，它非常慢，因为我们从来没有优先修复它(它几乎没有使用过)。显然，我们的客户成功经理一直在为会议收集数据，每次查询返回得不够快时，他都会不停地点击刷新和重试。那个很少使用的查询锁定了我们的数据库，并挑战我们客户成功经理的理智！回顾数据，我们确认所有延迟峰值都与客户成功简报保持一致。最终，在优化该查询大约20分钟后，一切都恢复正常。🎃</p><p>   It was a clear, sunny day in San Francisco. I was working at a small internet company, when suddenly our app stopped loading for me. Not just one view, but the whole app. Hard reload, but no luck. I looked around and my teammates were also confused; the app wasn’t working for them either. Our users weren’t complaining (yet?) but we started digging in anyway. No deployments had happened yet that day, no infrastructure had changed; yet it was broken consistently across OS types and browsers. What could have changed?</p><p>那是旧金山一个晴朗阳光灿烂的日子。我在一家小互联网公司工作，突然我们的应用程序停止为我加载。不只是一个视图，而是整个应用程序。重新装弹很辛苦，但运气不佳。我环顾四周，我的队友们也很困惑；这个应用程序也不能为他们工作。我们的用户没有抱怨(还没有吗？)。但不管怎样，我们还是开始挖坑了。那一天还没有进行任何部署，基础设施也没有改变；然而，它在各种操作系统类型和浏览器中都是一致崩溃的。会有什么改变呢？</p><p> We found some errors in a critical (but boring-and-hadn’t-changed-in-forever) API call, without which the app wouldn’t load. But why were the errors only happening for people that worked at the company? And why now? It turned out that for internal users, the API returned some extra data…extra data that had been slowly growing over the last few weeks, until it had finally exceeded the request’s maximum payload size that afternoon. 👻</p><p>我们在一个关键的(但很无聊而且没有永远改变的)API调用中发现了一些错误，如果没有这些错误，应用程序将无法加载。但为什么这些错误只发生在公司的员工身上呢？为什么是现在？原来，对于内部用户，该接口返回了一些额外的数据…。额外的数据在过去几周内一直在缓慢增长，直到那天下午最终超过请求的最大有效负载大小。👻。</p><p>   The AddTrust Root Certificate Authority (CA) we relied on expired at roughly 4 a.m. Pacific Time on Saturday morning, May 30, 2020.</p><p>我们所依赖的AddTrust Root证书颁发机构(CA)大约在凌晨4点过期。太平洋时间2020年5月30日星期六上午。</p><p> At the time, we were transitioning some of our infrastructure to Let’s Encrypt, a nonprofit certificate authority, as part of our move to Kubernetes. Legacy Syslog clients required AddTrust/UserTrust/Comodo. We run our own SaaS environment in addition to a number of worldwide environments for a major cloud partner. In our SaaS environment, a single certificate chain is used everywhere, including our ingestion endpoint, Syslog endpoint, and web app. We thought we were ready for this root certificate expiry… we were not.</p><p>当时，作为迁移到Kubernetes的一部分，我们正在将我们的一些基础设施过渡到一个非营利性的证书颁发机构“让我们加密”(We‘s Encrypt)。旧式系统日志客户端需要AddTrust/UserTrust/Comodo。除了为主要云合作伙伴运行多个全球环境外，我们还运行自己的SaaS环境。在我们的SaaS环境中，到处都使用单个证书链，包括我们的接收端点、系统日志端点和Web应用程序。我们认为我们已经为这个根证书过期…做好了准备。我们没有。</p><p> Quick primer on certificate chains: All certificate-based security relies on chains of trust. Browsers and operating systems ship with these trust stores of root certificates.</p><p>证书链快速入门：所有基于证书的安全性都依赖于信任链。浏览器和操作系统附带这些根证书的信任存储区。</p><p> LogDNA Chain: AddTrust Root CA (expired May 30) -&gt; UserTrust CA -&gt; Sectigo -&gt; *. logdna.com</p><p>LogDNA链：AddTrust Root CA(5月30日到期)-&gt；UserTrust CA-&gt；Sectigo-&gt；*。Logdna.com。</p><p>  UserTrust CA itself is also part of root trust stores of many browsers, so even if AddTrust is expired, it’s ignored since the chain leading up to the UserTrust CA is still valid.</p><p>UserTrust CA本身也是许多浏览器的根信任存储的一部分，因此即使AddTrust过期，它也会被忽略，因为通向UserTrust CA的链仍然有效。</p><p>  Turns out, old legacy systems will only see the LogDNA chain, which is considered an invalid chain if any of the four certificates are expired. They also don’t recognize UserTrust as a trusted root certificate.</p><p>原来，旧的遗留系统将只看到LogDNA链，如果四个证书中的任何一个过期，它就被认为是无效的链。它们也不会将UserTrust识别为受信任的根证书。</p><p> All of the support tickets we received mentioned that our v1 agent was no longer sending logs to our ingestion endpoints, but our v2 agent and other modern implementations of REST API-based clients were all working fine.</p><p>我们收到的所有支持票证都提到我们的v1代理不再向我们的接收端点发送日志，但是我们的v2代理和其他基于rest API的客户端的现代实现都工作得很好。</p><p> We erroneously started working on an update to our v1 agent. Ironically, our CI/CD provider also had an outage of their own due to the same AddTrust Root CA expiration, which further complicated our rollout of that agent. Once we realized that the issue was with the actual certificate chain and how older legacy systems behaved with that chain, we quickly rectified it by switching in a new certificate chain based on Let’s Encrypt. 🧟</p><p>我们错误地开始对v1代理进行更新。具有讽刺意味的是，由于相同的AddTrust Root CA到期，我们的CI/CD提供商自己也发生了故障，这进一步使我们推出该代理变得更加复杂。一旦我们意识到问题出在实际的证书链上，以及旧的遗留系统在该链上的行为如何，我们就会通过切换到一个基于“让我们加密”的新证书链来快速纠正这个问题。🧟。</p><p>   Full-on site outages are horrible — but they don’t make your skin crawl the same way that the random, unpredictable failures really can. I was working on the mobile web version of Twitter, and we got requests that, for some random unlucky campers, caused a scary error page whenever they visited the site. For everyone else, the sky was blue and the birds were chirping. But now and again, someone else would get hit. And, once they were hit, they were stuck in a pit of despair, unable to read any tweets from their phone.</p><p>全面的站点停机是可怕的-但它们不会让您的皮肤像随机的、不可预测的故障那样爬行。我当时正在开发Twitter的移动网络版本，我们收到了一些请求，对于一些随机倒霉的露营者来说，每当他们访问该网站时，都会导致一个可怕的错误页面。对于其他人来说，天空是蓝色的，鸟儿在啁啾。但时不时地，会有其他人被击中。而且，一旦他们被击中，他们就陷入了绝望的深渊，无法阅读手机上的任何推文。</p><p> Slowly, as the number of these tarnished accounts increased, the 500s started creeping up to critical levels. We were able to see that the new library we were using failed to parse session cookies with a specific character. So every time you logged back in, you were rolling the dice on getting bit by this pesky bug, and you couldn’t be cured without the wizardly powers to reset your cookies on a phone. Eventually, we fixed the bug in the library, and everyone was able to go back to reading their tweets… which, as we know, can be a very scary thing on its own! 🕸️</p><p>慢慢地，随着这些受损账户数量的增加，500开始攀升到临界水平。我们可以看到，我们正在使用的新库无法解析具有特定字符的会话cookie。所以每次你重新登录的时候，你都是在掷骰子，被这个讨厌的漏洞咬了一口，如果没有重新设置手机cookie的神奇力量，你就不可能被治愈。最终，我们修复了图书馆中的错误，每个人都可以回去阅读他们的推文…。正如我们所知，这本身就是一件非常可怕的事情！🕸️</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://thenewstack.io/6-scary-outage-stories-from-ctos/">https://thenewstack.io/6-scary-outage-stories-from-ctos/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/停电/">#停电</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/outage/">#outage</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/服务器/">#服务器</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>