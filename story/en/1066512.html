<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>在创建世界上最大的图表数据库的幕后 Behind the Scenes of Creating the World’s Biggest Graph Database</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Behind the Scenes of Creating the World’s Biggest Graph Database<br/>在创建世界上最大的图表数据库的幕后 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-18 01:06:47</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/9fab2a12cd244163506de0601600bcaf.jpeg"><img src="http://img2.diglog.com/img/2021/6/9fab2a12cd244163506de0601600bcaf.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Each forum shard contains 900 million relationships and 182 million nodes. The person shard contains 3 billion people and 16 billion relationships between them.</p><p>每个论坛碎片包含9亿个关系和182百万个节点。该人员碎片包含30亿人和他们之间的160亿个关系。</p><p>  “100 machines isn’t cool. You know what’s cool? One trillion relationships.”</p><p>  “100台机器不酷。你知道什么很酷？一万亿的关系。“</p><p> I’m not saying that someone actually uttered these exact words, but I’m pretty sure we all thought them. That was a month ago, when we’d decided to try and build the biggest graph database that has ever existed.</p><p> 我并不是说有人实际说出了这些确切的话语，但我很确定我们都认为他们。那是一个月前，当我们决定尝试并建立曾经存在的最大的图形数据库。</p><p>   When we introduced  Neo4j Fabric, we also created a  proof of concept benchmark that was presented at FOSDEM 2020.</p><p>   当我们推出Neo4J面料时，我们还创建了在FOSDem 2020呈现的概念基准证据。</p><p> It showed that, for a  1TB database, throughput and latency  improve linearly with the number of shards that it’s distributed across. More shards, more performance.</p><p> 它表明，对于1TB数据库，吞吐量和延迟随着它分布在的碎片数量而直线地改善。更多的碎片，更性能。</p><p>  The results looked good and confirmed that we had a very good understanding of the approach to scaling a graph database. Development of Fabric continued toward making it an integral part of Neo4j.</p><p>  结果看起来很好，并确认我们对缩放图形数据库的方法非常了解。开发面料继续使其成为Neo4J的一个组成部分。</p><p> New technologies were created and improved upon (server-side routing is a good example) and made useful for non-Fabric setups as well.</p><p> 创建并改进了新技术（服务器端路由是一个很好的例子），也适用于非结构设置。 </p><p> But, that 1TB dataset from FOSDEM was always nagging us.  1TB is not that big, at least for Neo4j. We routinely have production setups with 10TB or more and, although they run on considerably large machines, Neo4j scales up pretty well.</p><p>但是，来自FOSDEM的1TB数据集总是唠叨我们。 1TB至少是那么大，至少是Neo4J。我们经常具有10TB或更大的生产设置，虽然它们在大型机器上运行，但Neo4j展现得很好。</p><p> We didn’t really need a solution for that; we needed a solution for  really big databases. That’s why we had created Fabric, but we hadn’t found its limit yet.</p><p> 我们并没有真正需要解决方案;我们需要一个真正大数据库的解决方案。这就是我们创造了面料的原因，但我们还没有找到它的极限。</p><p> And what would that scale be? A year ago we built clusters of 40 machines, and they worked out pretty well. Going to 100 machines didn’t seem that much of a challenge. Billions of nodes, perhaps? Well, billions of nodes is the same as a few TB of data.</p><p> 那个规模是什么？一年前，我们建造了40台机器的集群，他们效果很好。前往100台机器似乎没有挑战。数十亿个节点，也许？嗯，数十亿个节点与几TB数据相同。</p><p> Plus, the  richness of a graph schema comes from relationships between nodes, not the nodes themselves. Remember: We’re trying to see how far we can push Neo4j, not just make ourselves feel or look good.</p><p> 此外，图形模式的丰富性来自节点之间的关系，而不是节点本身。记住：我们正试图看到我们可以推动neo4j，不仅让自己感觉或看起来不错。</p><p> Every now and then the same discussion would come around, and it was becoming clear that we were looking for an opportunity, a lightning rod, that would ground us in a realistic goal.</p><p> 每一个现在都会出现同样的讨论，并且变得明显，我们正在寻找一个机遇，一个避雷针，将我们在一个现实的目标。</p><p> Turns out, that opportunity was  NODES 2021 on June 17, our biggest online developer conference.</p><p> 事实证明，我们最大的在线开发人员会议，该机会是2021年6月17日的节点。</p><p>  We decided that we needed to show the world what we mean when we talk about scale. Not  just that Neo4j can retain or even improve performance when scaled horizontally, but to show how far we can go while still retaining performance.</p><p>  我们决定，我们需要向世界展示我们谈论规模时的意思。不仅仅是Neo4J水平缩放时可以保留甚至提高性能，但显示我们仍然可以在仍然保持性能时进行多远。 </p><p> We’re going to need bigger numbers. Something that demands attention. Not hundreds of machines or billions of nodes.</p><p>我们将需要更大的数字。需要注意的东西。不是数百台机器或数十亿节点。</p><p>   Where do you even find a database with a trillion relationships? Using a production setup would present logistics problems — the data transfer itself would be complicated, not to mention obfuscating the data so it’s appropriate for public view.  We’ll need to generate it for ourselves, from a known model. And we knew we needed to start with the data model, since that would give us the number and size of machines, the queries we’d run, and the tests we’d create.  In short, start with the data model to get a sense of the effort it would take.</p><p>   你甚至在哪里找到一个带有万亿关系的数据库？使用生产设置将出现物流问题 - 数据传输本身将是复杂的，更不用说混淆数据，因此可以适合公共视图。我们需要从已知模型生成它。我们知道我们需要从数据模型开始，因为它会给我们机器的数量和大小，我们运行的查询，以及我们创建的测试。简而言之，从数据模型开始，以获得它需要的努力。</p><p> LDBC is a good candidate. It is a social network that contains people, forums, and posts. It’s easy to understand and explain and we are very familiar with it. We decided on  3 billion people, surpassing the largest social network on the planet. The degree of social connectivity was chosen so that the person shard would come out 850GB, and every forum shard would come at 250GB with about 900 million relationships each.  To get to the target of 1 trillion relationships we’d need about  1110 forum shards in total, each its own machine.</p><p> LDBC是一个很好的候选人。它是一个包含人，论坛和帖子的社交网络。它很容易理解和解释，我们非常熟悉它。我们决定了30亿人，超越了地球上最大的社交网络。选择了社交连接程度，以便该人员碎片将出现850GB，每个论坛碎片都会以250GB来到250GB，每个人都有约900万个关系。为了获得1万亿的关系，我们需要大约1110个论坛碎片，每个人都是自己的机器。</p><p>  That’s 1110 forum shards, plus a few more for redundancy, each of which will need to have a store generated for it.</p><p>  这是1110个论坛碎片，加上冗余的更多信息，每个都需要有一个为它生成的商店。</p><p> We also wanted 3  Fabric proxies connecting to 10, 100 and all the shards to see how the system scales as data sizes grow. And, with the clock ticking, we needed a plan to orchestrate all these machines.</p><p> 我们还想要连接到10,100和所有碎片的3个面料代理，以了解系统尺度如何随数据尺寸的增长。并且，随着时钟滴答作响，我们需要一个计划来协调所有这些机器。</p><p> It was all about managing risk. We expected that, if trouble found us, it would be either during store generation or something glitching badly in the network. As it turned out, we were half right.</p><p> 这一切都是关于管理风险。我们预期，如果麻烦发现我们，它是在商店生成期间或在网络中严重发出故障的东西。事实证明，我们就是一半。</p><p>   One was the  large number of machines that would host the shards. The other was generating the shard data, which we knew needed to happen in parallel and would, therefore, also need orchestrating a lot of machines.</p><p>   一个是举办碎片的大量机器。另一个正在生成碎片数据，我们知道需要并行发生，因此也需要协调大量机器。 </p><p> We needed a two-step approach. The first step would be full-sized stores but a small number of Neo4j processes. That would let us test the parallel store creation, the generators, the installation of the stores from the buckets to the shards, and also test the MVP of the latency-measuring client.</p><p>我们需要一个两步的方法。第一步将是全尺寸的商店，而是少数NEO4J进程。这将让我们测试并行商店创建，发电机，从桶到分片的存储器，也测试延迟测量客户端的MVP。</p><p> It wouldn’t put any stress on our orchestration tools (that would come later), and allows us to focus on getting everything wired properly for our proof of concept. Things worked out pretty well, and we had all the pieces in place. Everything seemed to work together, and now we could hold our breath and go for the second phase — full size.</p><p> 它不会对我们的编排工具（以后来）对任何压力进行压力，并允许我们专注于为我们的概念证明正确地接线。事情很好地锻炼身体，我们都有所有的作品。一切似乎都在一起工作，现在我们可以屏住呼吸并获得第二阶段 - 全尺寸。</p><p> With two weeks left, we moved to step two:   We pulled out all the stops and braced for impact.</p><p> 剩下两周后，我们搬到第二步：我们拔出了所有的挡块并支撑了影响。</p><p> The first issue that came up was that AWS instance provisioning started failing unpredictably at around 800 machines. Some detective work led to discovering that we have a vCore limit on our AWS account that didn’t let us create any more machines. AWS Support lifted it promptly and we continued creating instances only to hit a more serious limitation:</p><p> 提到的第一个问题是AWS实例配置开始在左右800台机器时不可预测地失败。一些侦探工作导致发现我们对我们的AWS帐户有一个vcore限制，这些账户没有让我们创建任何机器。 AWS支持迅速提升，我们继续创建实例只会达到更严重的限制：</p><p> “We currently do not have sufficient x1e.4xlarge capacity in zones with support for ‘gp2’ volumes. Our system will be working on provisioning additional capacity.”</p><p> “我们目前没有足够的x1e.4xlarge容量，具有支持”GP2“卷。我们的系统将致力于提供额外的容量。“</p><p> Hmm. It would seem   Amazon had run out of capacity. This left us with two options. Either go for multiple Availability Zones or for smaller instance types. We decided that there’s more complexity and unknowns going for multi AZ, so we’d do smaller instances for now and, if performance was a problem, we’d deal with it later. Getting to the full number of instances was the most important goal.</p><p> 唔。看起来亚马逊缺乏容量。这让我们留下了两个选择。要么用于多个可用性区域或较小的实例类型。我们决定多AZ的复杂性和未知数，所以我们现在做了更小的实例，如果表现是一个问题，我们稍后会处理它。到达全部的实例是最重要的目标。</p><p> Using smaller instances did the trick, and the next day we had the full contingent of 1129 shards up and running. The latency measuring demo app was almost ready so we decided to take some measurements to see where we stand.</p><p> 使用较小的实例执行了诀窍，第二天我们拥有1129个碎片的全部或跑步。延迟测量演示应用程序几乎准备就绪，因此我们决定拍摄一些测量以查看我们的立场。 </p><p>  The complete fabric proxy, the one pointing to all 1129 shards, was timing out. Neo4j log files didn’t have any relevant error messages, just the timeouts. No GC pauses, no firewall misconfigurations; none of the usual suspects were to blame. Each individual shard was responding normally, and the Fabric proxy didn’t show any problems either. It took an evening of investigative work to find that the issue was   DNS query limiting. As AWS documentation points out:</p><p>完整的面料代理，指向所有1129个碎片的一个，是时机。 Neo4j日志文件没有任何相关的错误消息，只是超时。没有GC暂停，没有防火墙错位;常规嫌疑人都没有责备。每个单独的碎片正常响应，并且面料代理也没有显示任何问题。它花了一个晚上的调查工作，发现这个问题是DNS查询限制。随着AWS文件指出：</p><p> “Amazon provided DNS servers enforce a limit of 1024 packets per second per elastic network interface (ENI). Amazon provided DNS servers reject any traffic exceeding this limit.”</p><p> “亚马逊提供DNS服务器每秒每个弹性网络接口（ENI）执行每秒1024个数据包的限制。亚马逊提供DNS服务器拒绝超过此限制的任何流量。“</p><p> Yup. That should do it. Our Fabric configuration was using DNS names for the shards, so that limit was reached immediately on every query we submitted. The solution was quite simple — just make the DNS entries static on the Fabric instance and no longer depend on DNS ( /etc/hosts FTW).</p><p> 是的。这应该这样做。我们的Fabric配置正在使用碎片的DNS名称，因此我们在我们提交的每个查询上立即到达了该限制。解决方案非常简单 - 只需使DNS条目静态在结构实例上，不再依赖于DNS（/ etc / hosts ftw）。</p><p> And that was the last limit we had to overcome. Our initial latency numbers were looking very nice and we decided there was no reason to move to larger instance types, which would also help keep the costs reasonable.</p><p> 这是我们必须克服的最后一个限制。我们的初始延迟编号看起来非常好，我们决定没有理由转移到更大的实例类型，这也有助于保持成本合理。</p><p> Overall, we had build tools that allowed us, at the press of a button, to set up a  1129 shard cluster hosting 280 TB of data, with 3 Fabric proxies, in under 3 hours. Yay! And it took us only 16 days to get there.</p><p> 总的来说，我们在按一下按钮时，我们建立了允许我们的工具，以设置1129个碎片群托管280 TB数据，3小时内的3个面料代理。耶！它只需要16天到达那里。</p><p> We spent the rest of the time fine-tuning the configurations and the queries, and trying out the latency measuring app. We also played around with the graph itself, created new ad-hoc queries to get a feel of working with such a large setup.</p><p> 我们剩下的时间清除配置和查询，并尝试延迟测量应用程序。我们还在图表本身上播放，创建了新的Ad-hoc查询，以获得使用如此大的设置。</p><p>     We didn’t want to create a pure demo-ware, so we decided to make everything public under the ASL, you can try to run the setup yourself (at different scales) by using the   trillion-graph repository. Just remember to watch your AWS bill 💸!</p><p>     我们不想创建一个纯粹的演示程序，因此我们决定通过使用万亿图表存储库尝试自己（在不同的尺度）上运行Setup（以不同的尺度）运行设置。记得要注意你的AWS账单♥！ </p><p>   This is just the first step in a long journey. We have a lot more work to do to understand how large graph databases behave, how network variations aggregate as noise and how we should improve the system to scale beyond the numbers we achieved for this demo.</p><p>这只是长途旅程的第一步。 我们有很多工作要做，以了解大图数据库的表现方式，网络变化如何聚合为噪音以及我们应该如何改进系统以超出我们为此演示所实现的数字。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/neo4j/behind-the-scenes-of-creating-the-worlds-biggest-graph-database-cd22f477c843">https://medium.com/neo4j/behind-the-scenes-of-creating-the-worlds-biggest-graph-database-cd22f477c843</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/database/">#database</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/创建/">#创建</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/creating/">#creating</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>