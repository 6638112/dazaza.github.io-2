<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>如何在AWS上以更高的性能和更低的成本运行数据库How to Run a Database on AWS with Better Performance and Lower Cost</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">How to Run a Database on AWS with Better Performance and Lower Cost<br/>如何在AWS上以更高的性能和更低的成本运行数据库</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-25 11:13:43</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/9f146995af2256e25c80a630b1865a42.jpg"><img src="http://img2.diglog.com/img/2020/11/9f146995af2256e25c80a630b1865a42.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Amazon Web Services (AWS) is one of the most popular providers of public cloud services. Running databases on AWS is an essential part of using its cloud services. But how can we best leverage the cloud resources and have the database run at its best performance?</p><p>亚马逊网络服务（AWS）是最受欢迎的公共云服务提供商之一。在AWS上运行数据库是使用其云服务的重要部分。但是，我们如何才能最好地利用云资源并以最佳性能运行数据库？</p><p> This post demonstrates how we at  PingCAP deploy and optimize  TiDB for production on AWS Cloud. It&#39;s our hope that recommendations provided here will also help you configure the TiDB service most suitable for your own workload.</p><p> 这篇文章演示了我们在PingCAP上如何部署和优化TiDB以在AWS Cloud上进行生产。我们希望这里提供的建议还可以帮助您配置最适合您自己的工作负载的TiDB服务。</p><p> To evaluate the performance of different configurations, we ran  TPC-C and Yahoo! Cloud Serving Benchmark ( YCSB) tests on a typical three-node cluster. TPC-C is a common benchmark standard that simulates an intensive online transaction workload, while YCSB covers most common cloud application scenarios. This article discusses the results in detail.</p><p> 为了评估不同配置的性能，我们运行了TPC-C和Yahoo!。在典型的三节点群集上进行云服务基准（YCSB）测试。 TPC-C是通用基准标准，可模拟密集的在线交易工作量，而YCSB涵盖最常见的云应用方案。本文详细讨论了结果。</p><p>  When an application is built on top of a persistent database like TiDB, the database eventually translates user interactions into reads and writes on disk storage. This is why this article focuses on how to better utilize AWS cloud storage as a new storage medium for TiDB.</p><p>  当在TiDB之类的持久数据库之上构建应用程序时，该数据库最终会将用户交互转换为磁盘存储上的读取和写入。因此，本文着重介绍如何更好地利用AWS云存储作为TiDB的新存储介质。</p><p>  In this post, we consider using AWS Elastic Block Storage (EBS) as the primary storage for the TiDB service. Unlike traditional local disks, EBS is:</p><p>  在本文中，我们考虑将AWS Elastic Block Storage（EBS）用作TiDB服务的主要存储。与传统的本地磁盘不同，EBS是：</p><p> Elastic. The storage layer is decoupled from the Elastic Compute Cloud (EC2) instance, and therefore is resilient to host failure and is easy to scale.</p><p> 有弹性。存储层与弹性计算云（EC2）实例解耦，因此可以弹性应对主机故障，并且易于扩展。</p><p> Cloud-friendly. EBS integrates naturally with the AWS cloud ecosystem; for example, it is easy to backup and restore through an S3 snapshot.</p><p> 云友好。 EBS与AWS云生态系统自然集成；例如，通过S3快照进行备份和还原很容易。</p><p> Even though EBS has a smaller annual failure rate (AFR) than commodity hardware, it&#39;s still not reliable enough for business-critical applications. They require cross-region availability, which is only possible with distributed databases replicated in multiple availability zones (AZ). In this regard, TiDB is a good match for EBS.</p><p>尽管EBS的年故障率（AFR）比商品硬件要小，但对于关键业务应用程序仍然不够可靠。它们要求跨区域可用性，只有在多个可用性区域（AZ）中复制的分布式数据库才有可能。在这方面，TiDB非常适合EBS。</p><p>  EBS volumes are defined by their performance limits, including maximum and sustained Input/Output Operations Per Second (IOPS) and throughput. In certain cases, EBS volumes can &#34;burst&#34; to higher performance for short periods. In order for our product to perform predictably, our post focuses more on the  sustained EBS performance.</p><p>  EBS卷由其性能限制定义，包括最大和持续的每秒输入/输出操作（IOPS）和吞吐量。在某些情况下，EBS卷可以在短期内“爆发”以达到更高的性能。为了使我们的产品具有可预测的性能，我们的帖子主要关注可持续的EBS性能。</p><p>  The IOPS limit acts like a hard ceiling. Below the limit, I/O latency is relatively stable with respect to IOPS changes; once IOPS reaches the limit, latency experiences a dramatic penalty, often more than 10 ms.</p><p>  IOPS限制就像硬顶一样。低于限制，就IOPS更改而言，I / O延迟相对稳定。一旦IOPS达到限制，延迟就会受到严重的惩罚，通常会超过10毫秒。</p><p> Compared to IOPS, the throughput limit is much softer. This means that even below the limit, I/O latency grows proportionally to I/O throughput.</p><p> 与IOPS相比，吞吐量限制要软得多。这意味着即使低于该限制，I / O延迟也会与I / O吞吐量成比例地增长。</p><p> EBS connects to EC2 instances through a network interface. Therefore, for a given EC2/EBS pair, the actual disk performance is determined not only by the EBS specs, but also by the EC2&#39;s capability of dedicated bandwidth to cloud storage. The following table summarizes the performance of several instance types:</p><p> EBS通过网络接口连接到EC2实例。因此，对于给定的EC2 / EBS对，实际的磁盘性能不仅取决于EBS规范，还取决于EC2专用于云存储的带宽的能力。下表总结了几种实例类型的性能：</p><p>  1: As per AWS official document, low-power machines can only reach maximum performance for 30 minutes at least once every 24 hours.</p><p>  1：根据AWS官方文档，低功率机器至少每24小时只能达到30分钟的最高性能。</p><p> The  Amazon EBS-optimized instances document includes detailed specifications on the EC2-to-EBS limit. Generally, EC2&#39;s capability is proportional to its computing power. However, larger machines (like  m5.8xlarge with 32 cores) tend to have fewer resources per core.</p><p> Amazon EBS优化的实例文档包括有关EC2-to-EBS限制的详细规范。通常，EC2的能力与其计算能力成正比。但是，较大的机器（例如具有32个内核的m5.8xlarge）倾向于每个内核具有更少的资源。</p><p> The following graphic showcases how EC2 bandwidth limits directly affect service performance. Two clusters are cross compared using different types of instances as node servers; namely, m5.2xlarge and m5.4xlarge. Each cluster node is equipped with a 1 TB AWS provisioned IOPS volume of 6,000 IOPS. In theory, this disk has a maximum throughput of 1,500 MiB/s for 256 KiB I/O, but the actual performance is tightly bounded by EC2 bandwidth. After we switched EC2 instances, the performance doubled with nearly the same CPU usage.</p><p>下图展示了EC2带宽限制如何直接影响服务性能。使用不同类型的实例作为节点服务器对两个集群进行交叉比较；即m5.2xlarge和m5.4xlarge。每个群集节点都配备了1 TB的AWS预置IOPS，容量为6,000 IOPS。从理论上讲，该磁盘的最大吞吐量为256 KiB I / O时为1,500 MiB / s，但实际性能受EC2带宽的严格限制。切换EC2实例后，在CPU使用率几乎相同的情况下，性能提高了一倍。</p><p>    For AWS general-purpose SSDs, the IOPS performance scales linearly to the volume size with a ratio of 3 IOPS per GB, until it reaches an upper bound of 3,000 IOPS at 1 TB. This means a gp2 volume larger than 1 TB is not an economically-optimal choice. Theoretically, gp2&#39;s bandwidth limit is always 250 MiB/s. However, because internally  EBS splits user requests into 256 KiB chunks, only volumes larger than 334 GB have enough IOPS quota to deliver that amount of throughput.</p><p>    对于AWS通用SSD，IOPS性能以每GB 3 IOPS的比例线性扩展至卷大小，直到达到1 TB的3,000 IOPS上限为止。这意味着大于1 TB的gp2体积不是经济上最佳的选择。理论上，gp2的带宽限制始终为250 MiB / s。但是，由于EBS在内部将用户请求分为256 KiB块，因此只有大于334 GB的卷才具有足够的IOPS配额来交付该数量的吞吐量。</p><p> Notably, gp2 is one of the credit-based burstable instances. Based on the number of credits you have, gb2 can burst from baseline to maximum performance for short periods. This feature could be tempting for users with a highly dynamic workload profile that only demands high performance for a limited time. But for our target user base,  we suggest setting a gp2 volume size greater than 1 TB to avoid unpredictable performance caused by bursting.</p><p> 值得注意的是，gp2是基于信用的可爆实例之一。根据您拥有的积分数，gb2可能会在短期内从基准达到最高性能。对于具有仅在有限时间内要求高性能的高度动态工作负载配置文件的用户而言，此功能可能很诱人。但是对于我们的目标用户群，我们建议将gp2的卷大小设置为大于1 TB，以避免由于突发而导致不可预测的性能。</p><p>  Compared to gp2, provisioned IOPS SSDs further raise the maximum throughput and IOPS capacity. You can provision IOPS up to 64,000 and throughput up to 1,000 MiB/s. In addition to high performance, the new generation io2 offers 99.999% durability.</p><p>  与gp2相比，预配置的IOPS SSD进一步提高了最大吞吐量和IOPS容量。您最多可以配置IOPS 64,000，吞吐量最多可以配置1,000 MiB / s。除了高性能之外，新一代io2还提供了99.999％的耐用性。</p><p> For baseline performance, we suggest configuring your io1/io2 volume to 6,000 IOPS per TB. Thanks to the elasticity of cloud storage, you can  dynamically modify your EBS volume performance. This means you can scale the storage performance of individual nodes up or down by different amounts depending on your application&#39;s pressure on the storage layer. You can monitor the I/O load of running instances using our Grafana panel or AWS&#39;s CloudWatch.</p><p> 为了获得基准性能，我们建议将io1 / io2卷配置为每TB 6,000 IOPS。借助云存储的弹性，您可以动态修改EBS卷性能。这意味着您可以根据应用程序对存储层的压力，以不同的方式向上或向下扩展各个节点的存储性能。您可以使用我们的Grafana面板或AWS的CloudWatch监视正在运行的实例的I / O负载。</p><p> In general,  a provisioned IOPS SSD volume is a perfect fit for large-scale, data-intensive applications that require the best storage performance available on cloud. To best leverage this storage power, users considering this option should take extra caution in choosing a properly sized EC2 instance.</p><p> 通常，预配置的IOPS SSD卷非常适合需要云上最佳存储性能的大规模数据密集型应用程序。为了最大程度地利用此存储功能，考虑使用此选项的用户在选择大小合适的EC2实例时应格外小心。</p><p>  One EBS volume has limited performance, but  you can stripe several volumes together as a RAID-0 array to multiply storage power. According to our benchmark, the performance of a RAID 0 array with two 1 TB gp2 volumes is comparable to io1 provisioned with 6 K IOPS, but with only 40% of the cost.</p><p>  一个EBS卷的性能有限，但是您可以将多个卷作为RAID-0阵列条带化以增加存储能力。根据我们的基准，具有两个1 TB gp2卷的RAID 0阵列的性能可与配备6 K IOPS的io1相媲美，但成本仅为40％。</p><p>   However, a RAID-0 configuration lacks data redundancy. Therefore, a RAID array&#39;s service guarantee is slightly weaker than that of a single volume. For instance, striping two gp2 volumes of 99.8% durability produces an array of 99.6% durability.</p><p>但是，RAID-0配置缺少数据冗余。因此，RAID阵列的服务保证要弱于单个卷的服务保证。例如，剥离两个gp2体积的99.8％的耐久性，将产生一系列99.6％的耐久性。</p><p> Also, resizing a RAID-0 array involves service downtime. So for users adopting disk striping, we suggest you plan the volume size of each storage node before deploying it.</p><p> 同样，调整RAID-0阵列的大小会导致服务停机。因此，对于采用磁盘条带化的用户，建议您在部署每个存储节点之前先计划它们的卷大小。</p><p>  So far, we have discussed the key elements of storage performance on AWS cloud. An application&#39;s workload determines the actual performance requirements. More specifically, queries like batch insert or table scan put significant pressure on storage bandwidth, while transactional processing or random reads rely more on storage IOPS resources.</p><p>  到目前为止，我们已经讨论了AWS云上存储性能的关键要素。应用程序的工作负载决定了实际的性能要求。更具体地说，诸如批量插入或表扫描之类的查询给存储带宽带来了巨大压力，而事务处理或随机读取则更多地依赖于存储IOPS资源。</p><p> The table below is a list of storage node configurations that we consider cost effective for most common workloads. Think of these configurations as general recommendations or building blocks on which you can create your own custom configurations. Besides this list, users can select from a large range of options provided by AWS, based on the principles proposed here.</p><p> 下表列出了我们认为对于大多数常见工作负载而言具有成本效益的存储节点配置。将这些配置视为一般建议或构建块，可以在其上创建自己的自定义配置。除了此列表之外，用户还可以根据此处提出的原理从AWS提供的各种选项中进行选择。</p><p>    For applications that require extra quality of service, we provide several techniques that can further boost baseline performance with minimal cost. However, these strategies require additional maintenance, and, therefore, are only recommended for professional users.</p><p>    对于需要更高服务质量的应用程序，我们提供了几种可以以最低成本进一步提高基准性能的技术。但是，这些策略需要额外的维护，因此仅建议专业用户使用。</p><p>  Logically, TiKV stores the Raft logs separately from key-value data. Because the database only maintains the Raft logs of the most recent transactions, the Raft component has low throughput usage. Compared to sharing a disk with key-value data,  using a separate Raft volume reduces write latency on the critical path. For this reason, we recommend deploying a separate EBS disk as the Raft volume for latency-sensitive applications.</p><p>  从逻辑上讲，TiKV将Raft日志与键值数据分开存储。由于数据库仅维护最新事务的Raft日志，因此Raft组件的吞吐量使用率较低。与使用键值数据共享磁盘相比，使用单独的Raft卷可以减少关键路径上的写入延迟。因此，我们建议为延迟敏感的应用程序部署一个单独的EBS磁盘作为Raft卷。</p><p> You can specify the path to a separate Raft volume in the TiKV configuration file:</p><p> 您可以在TiKV配置文件中指定单独的Raft卷的路径：</p><p> [  ] ## The path to RaftDB directory. ## If not set, it will be `{data-dir}/raft`. ## If there are multiple disks on the machine, storing the data of Raft RocksDB on different disks ## can improve TiKV performance. # raftdb-path = &#34;/path/to/raft&#34;</p><p>[] ## RaftDB目录的路径。 ##如果未设置，则为`{data-dir} / raft`。 ##如果计算机上有多个磁盘，请将Raft RocksDB的数据存储在不同的磁盘上##可以提高TiKV性能。 ＃raftdb-path =“ / path / to / raft”</p><p> To guarantee data persistence, Raftstore frequently issues synchronous writes. Therefore, the top priorities for the Raft volume are disk IOPS and fsync frequency. Several types of EBS can meet these needs. For the best system stability, we suggest using a provisioned IOPS SSD with a small volume size; for example, a 40 GB io1 provisioned with 2 K IOPS. For applications that can tolerate some degree of performance fluctuation, it&#39;s appropriate to go for a cheaper general purpose SSD with larger size for acceptable baseline IOPS; for example, a 350 GB gp2 capable of 1,050 IOPS.</p><p> 为了保证数据的持久性，Raftstore经常发出同步写入。因此，木筏卷的最高优先级是磁盘IOPS和fsync频率。几种类型的EBS可以满足这些需求。为了获得最佳的系统稳定性，我们建议使用体积较小的预配置IOPS SSD。例如，配备2 K IOPS的40 GB io1。对于可以容忍某种程度的性能波动的应用程序，应该选择价格更便宜的通用型SSD，并且其更大的尺寸可以接受可接受的基准IOPS。例如，一个350 GB gp2，容量为1050 IOPS。</p><p>   Several EC2 types have one or more local  instance stores attached. These SSD drives co-locate physically with the instance you create, and therefore perform much better than cloud storage. However, the content in instance stores can&#39;t persist or be transferred after the instance is terminated. These limitations make instance stores vulnerable to host failures or updates and unfit for storing critical application data. The following table compares network disk and local instance store, tested with fio on  m5d.2xlarge:</p><p>   几种EC2类型已附加一个或多个本地实例存储。这些SSD驱动器与您创建的实例在物理上位于同一位置，因此，其性能比云存储好得多。但是，实例终止后，实例存储中的内容将无法保留或传输。这些限制使实例存储容易受到主机故障或更新的影响，并且不适合存储关键的应用程序数据。下表比较了在m5d.2xlarge上用fio测试的网络磁盘和本地实例存储：</p><p>  Nonetheless, as a fast tier device,  instance stores can be used as cache to offload read pressure from cloud disks. Because  RocksDB (the storage engine of TiKV) groups on-disk data by their access recency, cache with a least-recently used (LRU) policy naturally benefits from the way data files are organized.</p><p>  但是，作为快速层设备，实例存储可以用作缓存来减轻来自云磁盘的读取压力。由于RocksDB（TiKV的存储引擎）通过访问新近度对磁盘数据进行分组，因此使用最近最少使用（LRU）策略的缓存自然会受益于数据文件的组织方式。</p><p> To demonstrate the importance of SSD caching, we use EnhanceIO (an  open-source solution for SSD caching) to optimize several workloads that are bounded by read IOPS.</p><p> 为了证明SSD缓存的重要性，我们使用EnhanceIO（SSD缓存的开源解决方案）来优化由读取IOPS限制的几种工作负载。</p><p>     SSD caching does not bring extra risk to data integrity because we only use local disks as read-only cache. Furthermore, several SSD caching solutions, including EnhanceIO, support hot plugging, so you can dynamically configure the caching strategy while the service stays up.</p><p>     SSD缓存不会给数据完整性带来额外的风险，因为我们仅将本地磁盘用作只读缓存。此外，包括EnhanceIO在内的多种SSD缓存解决方案均支持热插拔，因此您可以在服务保持运行时动态配置缓存策略。</p><p>  As discussed before, the access latency of EBS is sensitive to I/O throughput. This poses a great threat to performance stability for systems like TiKV that rely on background compaction to provide sustained service. The following graph shows that an increase in the read/write flow causes a decrease in the write operations per second:</p><p>  如前所述，EBS的访问延迟对I / O吞吐量很敏感。对于像TiKV这样的依靠后台压缩来提供持续服务的系统，这对性能稳定性构成了极大的威胁。下图显示，读/写流的增加导致每秒写操作的减少：</p><p>    To maintain background I/O flow at a stable level, we recommend you set  rate-bytes-per-sec__ at a moderately low value. In TiKV 4.0.8 and later, you can dynamically change the I/O rate limit, and a DBA can optimize this parameter as the workload evolves. Both methods are listed below.</p><p>为了将后台I / O流量保持在稳定水平，建议您将rate-bytes / sec__设置为适中的较低值。在TiKV 4.0.8及更高版本中，您可以动态更改I / O速率限制，并且DBA可以随着工作负载的发展来优化此参数。两种方法都在下面列出。</p><p> [  ] ## Limit the disk IO of compaction and flush. ## Compaction and flush can cause terrible spikes if they exceed a certain threshold. Consider ## setting this to 50% ~ 80% of the disk throughput for a more stable result. However, in heavy ## write workload, limiting compaction and flush speed can cause write stalls too. ## 1. rate-bytes-per-sec is the only parameter you want to set most of the time. It controls the ## total write rate of compaction and flush in bytes per second. Currently, RocksDB does not ## enforce rate limit for anything other than flush and compaction, e.g. write to WAL. rate-bytes-per-sec  =  &#34;100MB&#34;</p><p> [] ##限制压缩和刷新的磁盘IO。 ##如果压实和冲洗超过一定阈值，则可能导致严重的峰值。考虑将##设置为磁盘吞吐量的50％〜80％，以获得更稳定的结果。但是，在繁重的##写工作负荷中，限制压缩和刷新速度也会导致写停顿。 ## 1. rate-bytes-per-sec是您大多数时候要设置的唯一参数。它控制压缩和刷新的##总写入率，以每秒字节数为单位。目前，除了刷新和压缩外，RocksDB不会##强制执行速率限制，例如写入WAL。 rate-bytes / sec =“ 100MB”</p><p> However, constant manual intervention is inconvenient. Our ultimate goal is a more autonomous, &#34;self-driving&#34; database. As a step in that direction, we recently introduced a new auto-tuned algorithm that automatically configures rate limits based on user workload, which will be available in the next major release.</p><p> 然而，持续的手动干预是不方便的。我们的最终目标是建立一个更加自治的“自动驾驶”数据库。作为朝这个方向迈出的一步，我们最近推出了一种新的自动调整算法，该算法可根据用户工作负载自动配置速率限制，该功能将在下一个主要版本中提供。</p><p>  Based on your workload and budget, you now should decide on your specific cluster configurations. This section describes some internal tuning tricks that can better customize the database service for your needs.</p><p>  现在，根据工作量和预算，您应该确定特定的群集配置。本节介绍一些内部调整技巧，可以更好地根据您的需求自定义数据库服务。</p><p>  Underneath TiKV, we use RocksDB as the storage engine. It uses block cache to store recent reads in uncompressed format. Configuring the block cache size essentially adjusts the proportion of uncompressed and compressed content (system page) stored in memory.</p><p>  在TiKV下，我们使用RocksDB作为存储引擎。它使用块高速缓存以未压缩的格式存储最近的读取。配置块高速缓存大小实际上可以调整存储在内存中的未压缩内容和压缩内容（系统页面）的比例。</p><p>   Reading from block cache is faster than reading from page cache, but it isn&#39;t always ideal to allocate large block cache. If you set a relatively small block cache, memory can hold more blocks because they are stored compactly in system page cache, thus avoiding reads to these blocks from hitting disk storage.</p><p>   从块缓存读取比从页面缓存读取更快，但是分配大块缓存并不总是理想的。如果设置相对较小的块高速缓存，则内存可以容纳更多的块，因为它们被紧凑地存储在系统页面高速缓存中，从而避免了对这些块的读取而影响磁盘存储。</p><p> When the application workload is read-heavy and the read set can&#39;t fit in the block cache (for example, the dataset is too large or the read pattern is too sparse), it&#39;s likely that many read requests will fall through to the persistent layer. Users would normally experience an increase in read and update latency and notice high read I/O pressure on the storage monitor.  In these situations, it&#39;s advisable to tune down  block-cache-size. In the 5 K warehouse TPC-C benchmark, to avoid read IOPS from hitting the EBS limit, we reserve 4 GB for block cache in EC2 instances with 32 GB memory.</p><p> 当应用程序的工作负载非常繁重且读取集无法放入块缓存中时（例如，数据集太大或读取模式太稀疏），很可能许多读取请求将落入持久性层。用户通常会体验到读取和更新延迟的增加，并且会注意到存储监视器上的读取I / O压力很高。在这种情况下，建议调低块缓存大小。在5 K仓库TPC-C基准测试中，为了避免读取IOPS达到EBS限制，我们在具有32 GB内存的EC2实例中为块缓存保留4 GB。</p><p>  [  ] ## Size of the shared block cache. Normally it should be tuned to 30%-50% of system&#39;s total memory. ## When the config is not set, it is decided by the sum of the following fields or their default ## value: ## * rocksdb.defaultcf.block-cache-size or 25% of system&#39;s total memory ## * rocksdb.writecf.block-cache-size or 15% of system&#39;s total memory ## * rocksdb.lockcf.block-cache-size or 2% of system&#39;s total memory ## * raftdb.defaultcf.block-cache-size or 2% of system&#39;s total memory ## ## To deploy multiple TiKV nodes on a single physical machine, configure this parameter explicitly. ## Otherwise, the OOM problem might occur in TiKV. # capacity = &#34;4GB&#34;</p><p>[] ##共享块缓存的大小。通常应将其调整为系统总内存的30％-50％。 ##如果未设置配置，则由以下字段的总和或其默认##值决定：## * rocksdb.defaultcf.block-cache-size或系统总内存的25％## * rocksdb。 writecf.block-cache-size或系统总内存的15％## * rocksdb.lockcf.block-cache-size或系统总内存的2％## * raftdb.defaultcf.block-cache-size或系统总内存的2％总内存## ##要在单个物理机上部署多个TiKV节点，请显式配置此参数。 ##否则，TiKV中可能会发生OOM问题。 ＃容量=“ 4GB”</p><p>  In RocksDB, a key-value pair is read to memory by first retrieving the physical block it resides in. In this case, the physical I/O size per logical read, also known as the read amplification, is at least the size of one physical block. Decreasing the block size reduces the unnecessary reads involved in a point-get. A downside is that read requests with spatial locality produce more read I/Os. Therefore,  when disk I/O pressure (especially IOPS) is below the EBS limit, users can decrease the block size for better read performance.</p><p>  在RocksDB中，先通过检索键值对所在的物理块将其读取到内存中。在这种情况下，每个逻辑读取的物理I / O大小（也称为读取放大）至少为一个大小。物理块。减小块大小可减少点获取中涉及的不必要读取。不利之处是具有空间局部性的读取请求会产生更多的读取I / O。因此，当磁盘I / O压力（尤其是IOPS）低于EBS限制时，用户可以减小块大小以获得更好的读取性能。</p><p> Block size is also useful in balancing I/O throughput and IOPS. For example, in the m5.4xlarge+io1 configuration, EC2 limits the I/O throughput to 593 MiB/s, which might not be enough for intensive workloads. In this case, users can decrease the block size to reduce the read throughput. However, this comes at the cost of more read I/Os, which can be satisfied by provisioned IOPS. As a rule of thumb, changing block size from 64 KiB to 4 KiB doubles read IOPS and halves read throughput.</p><p> 块大小对于平衡I / O吞吐量和IOPS也很有用。例如，在m5.4xlarge + io1配置中，EC2将I / O吞吐量限制为593 MiB / s，这可能不足以满足密集型工作负载。在这种情况下，用户可以减小块大小以降低读取吞吐量。但是，这是以更多读取I / O为代价的，可以通过预配置的IOPS来满足。根据经验，将块大小从64 KiB更改为4 KiB可使读取IOPS加倍，并使读取吞吐量减半。</p><p> Because block size determines the encoding of disk files, you should do a few test runs with different settings before deciding the best configuration for your application.</p><p> 由于块大小决定了磁盘文件的编码，因此在为应用程序确定最佳配置之前，应使用不同的设置进行几次测试运行。</p><p>  [  ] ## The data block size. RocksDB compresses data based on the unit of block. ## Similar to page in other databases, block is the smallest unit cached in block-cache. Note that ## the block size specified here corresponds to uncompressed data. # block-size = &#34;64KB&#34; [  ] # block-size = &#34;64KB&#34;</p><p>  [] ##数据块大小。 RocksDB基于块单位压缩数据。 ##与其他数据库中的页面相似，块是在块缓存中缓存的最小单位。请注意，##此处指定的块大小对应于未压缩的数据。 ＃块大小=“ 64KB” []＃块大小=“ 64KB”</p><p>  To complete user requests, internal executors must conduct different types of work, in specific CPU and I/O operations. On platforms with higher I/O latency like AWS Cloud, these executors spend more time on I/O operations and cannot efficiently process incoming requests. To resolve this problem, the TiKV team is developing a new asynchronous framework that separates I/O operations from request processing loops. Until this feature is production-ready,  we suggest that users set a larger size for the thread pool of critical components to increase TiKV&#39;s overall processing efficiency.</p><p>  为了完成用户请求，内部执行者必须在特定的CPU和I / O操作中执行不同类型的工作。在具有更高I / O延迟的平台（如AWS Cloud）上，这些执行器将更多时间用于I / O操作，并且无法有效处理传入的请求。为了解决这个问题，TiKV团队正在开发一个新的异步框架，该框架将I / O操作与请求处理循环分开。在此功能投入生产之前，我们建议用户为关键组件的线程池设置更大的大小，以提高TiKV的整体处理效率。</p><p> Inadequate thread pool size is likely to cause slow client response time. Under the  TiKV FastTune Grafana panel, users can monitor the wait duration of different thread pools and make adjustments accordingly.</p><p> 线程池大小不足可能会导致客户端响应时间变慢。在TiKV FastTune Grafana面板下，用户可以监视不同线程池的等待时间并进行相应的调整。</p><p>  [  ] ## Scheduler&#39;s worker pool size, i.e. the number of write threads. ## It should be less than total CPU cores. When there are frequent write operations, set it to a ## higher value. More specifically, you can run `top -H -p tikv-pid` to check whether the threads ## named `sched-worker-pool` are busy. # scheduler-worker-pool-size = 4 [  ] ## Use how many threads to handle log apply # apply-pool-size = 2 ## Use how many threads to handle raft messages # store-pool-size = 2</p><p>[] ##调度程序的工作池大小，即写线程数。 ##应该少于CPU核心总数。当频繁进行写操作时，请将其设置为##更高的值。更具体地说，您可以运行“ top -H -p tikv-pid”来检查名为“ sched-worker-pool”的线程##是否繁忙。 ＃scheduler-worker-pool-size = 4 [] ##使用多少个线程来处理日志应用＃apply-pool-size = 2 ##使用多少个线程来处理筏消息＃store-pool-size = 2</p><p>  Through a series of in-depth investigations, we have demonstrated the best practices for running TiDB on AWS cloud. By adhering to these practices, you&#39;ll be able to have a TiDB cluster with better performance at a reasonable cost. Next, you can try it out by reading our  tutorial and  Aurora migration guide. Feel free to  join us on Slack to share your experience and raise questions.</p><p>  通过一系列深入的调查，我们展示了在AWS云上运行TiDB的最佳实践。通过遵循这些实践，您将能够以合理的成本获得性能更高的TiDB集群。接下来，您可以阅读我们的教程和Aurora迁移指南，进行尝试。欢迎加入Slack，与我们分享您的经验并提出问题。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://pingcap.com/blog/run-database-on-aws-with-better-performance-and-lower-cost/">https://pingcap.com/blog/run-database-on-aws-with-better-performance-and-lower-cost/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/database/">#database</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/性能/">#性能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/运行/">#运行</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/存储/">#存储</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>