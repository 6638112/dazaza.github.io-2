<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>我们如何扩展ClickHouse集群How we scale out our ClickHouse cluster</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">How we scale out our ClickHouse cluster<br/>我们如何扩展ClickHouse集群</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-14 15:19:29</div><div class="page_narrow text-break page_content"><p>Any service that sees significant growth will eventually need to scale. This means growing the infrastructure larger (in compute, storage and networking) so that the applications riding on that infrastructure can serve more people at a time.It’s critical that scaling is done in a way that ensures the integrity of the data. Before starting, you need to understand the limitations to determine which solution provides the greatest benefit for you. Scaling operations can easily become expensive and complex in terms of performance and usability.</p><p>任何出现显著增长的服务最终都需要扩展。这意味着扩大基础设施（在计算、存储和网络方面），以便在基础设施上运行的应用程序可以同时为更多的人服务。以确保数据完整性的方式进行缩放是至关重要的。在开始之前，您需要了解这些限制，以确定哪种解决方案为您提供了最大的好处。在性能和可用性方面，扩展操作很容易变得昂贵和复杂。</p><p>            Here at  Contentsquare, we decided to scale out ourcluster. In ClickHouse, the scaling operation is made of two parts. You firstneed to reshard (adding new shards), then rebalance (distributing your existingdata across all those shards).</p><p>在Contentsquare，我们决定扩大我们的集群。在ClickHouse中，缩放操作由两部分组成。您首先需要重新硬处理（添加新的碎片），然后重新平衡（将现有数据分布到所有这些碎片上）。</p><p>       Rebalancing is needed to actually benefit from the scaling. If you do not rebalance your data, the new data will be distributed across your new topology but the existing and current data is still stored in your previous configuration. You will not benefit from the improvement of the scaling and will not preserve your capacity planning in terms of storage across all the shards.</p><p>需要重新平衡，才能真正从规模扩张中获益。如果不重新平衡数据，新数据将分布在新拓扑中，但现有和当前数据仍存储在以前的配置中。您将不会从扩展的改进中受益，也不会在所有碎片的存储方面保留容量规划。</p><p> The issue we faced was that ClickHouse doesn&#39;t automatically rebalance data in the cluster when we add new shards.</p><p>我们面临的问题是ClickHouse没有&#39；当我们添加新碎片时，不会自动重新平衡集群中的数据。</p><p> That being said, ClickHouse comes with utilities to help you rebalance your data. Depending on your data topology and size, they all have their limitations in terms of performance and usability.</p><p>尽管如此，ClickHouse附带了一些实用程序，可以帮助您重新平衡数据。根据您的数据拓扑和大小，它们在性能和可用性方面都有其局限性。</p><p> At Contentsquare, with the amount of data we add to ClickHouse every day, it was paramount that we kept 100% data integrity without impacting production speed. This article describes all the ways we explored before building our own. We hope it will help you better understand the tradeoffs and pick the one that provides the greatest benefit for your situation.</p><p>在Contentsquare，随着我们每天向ClickHouse添加的数据量的增加，我们保持100%的数据完整性而不影响生产速度是至关重要的。本文介绍了我们在构建自己的系统之前探索的所有方法。我们希望它能帮助您更好地理解权衡，并选择对您的情况有最大好处的权衡。</p><p>   The first idea we explored was using Kafka to externalize the process of rebalancing the data. Kafka is an important part of our data pipeline, and we already use it extensively.</p><p>我们探索的第一个想法是使用卡夫卡将重新平衡数据的过程具体化。卡夫卡是我们数据管道的重要组成部分，我们已经广泛使用它。</p><p> ClickHouse can natively read messages from a Kafka topic. It uses the Kafka table engine coupled with a materialized view to fetch messages and push them to a ClickHouse target table. ClickHouse can also write to Kafka by inserting into a Kafka table. This allowed us to use Kafka as the infrastructure tool to rebalance data between two ClickHouse clusters.</p><p>ClickHouse可以在本机上阅读卡夫卡主题的消息。它使用Kafka表引擎和物化视图来获取消息并将其推送到ClickHouse目标表。ClickHouse还可以通过插入卡夫卡表来写入卡夫卡。这使我们能够使用卡夫卡作为基础设施工具，在两个ClickHouse集群之间重新平衡数据。</p><p>    -- Kafka tables engine to create in source and target cluster CREATE  TABLE  [src |dest ]_table_queue  ( name1  [type1 ] , name2  [type2 ] ,  .  .  . )  ENGINE  = KafkaSETTINGS kafka_broker_list  =  &#39;hostname:port&#39; , kafka_topic_list  =  &#39;topic1, topic2, ...&#39; kafka_group_name  =  &#39;group_name&#39; , kafka_format  =  &#39;data_format&#39; kafka_max_block_size  = N  ; -- Create MV in target cluster CREATE MATERIALIZED  VIEW dest_table_queue_mv TO dest_table AS SELECT  *  FROM dest_table_queue  ; -- Transfer data from source to target table INSERT  INTO src_table_queue  SELECT  *  FROM src_table  ;</p><p>--Kafka表引擎在源集群和目标集群中创建表[src | dest]_TABLE_队列（name1[type1]，name2[type2]，…）ENGINE=KafkaSETTINGS kafka#u broker#u list=&39；主机名：端口号&#39，卡夫卡主题列表=&#39；主题1，主题2&#39; 卡夫卡大学组(u name=#39 ;；组名&#39，卡夫卡形式=&#39；数据格式&#39；卡夫卡（kafka）_max_block_size=N；——在目标集群中创建MV创建物化视图dest_table_queue_MV到dest_table AS SELECT*FROM dest_table_queue；——将数据从源表传输到目标表插入src_table_队列从src_table选择*；</p><p> Each Kafka topic will contain data “ready” to be inserted into the rebalanced target cluster.</p><p>每个卡夫卡主题都将包含“准备好”插入重新平衡的目标集群的数据。</p><p>    -- n is number of shard in target cluster for each src_shard:  for each src_table:  for each date_partition:  for each dest_shard  (i:  0. . n - 1 ):  INSERT  INTO src_table_queue (topic i )  SELECT  *  FROM src_table  WHERE  date =date_partition  and shard_key %n  = i -- this part is fully automatic for each dest_shard:  for each dest_table:  is defined dest_table_queue attached  to kakfa topic  with materialized  view</p><p>--n是每个src_shard的目标集群中的碎片数：对于每个src_表：对于每个日期分区：对于每个dest_shard（i:0..n-1）：插入到src_表_队列（主题i）从src_表中选择*，其中日期=日期分区和碎片键%n=i——这部分对于每个dest_shard是完全自动的：对于每个dest_表：是使用物化视图将定义的dest_table_队列附加到kakfa主题</p><p> This Kafka solution was promising. Because it is externalized to Kafka, we could tweak the ingestion traffic speed and resources automatically.But, we decided not to use it. At least, not yet. We thought that the  Kafka table engine in ClickHouse was not mature enough. Specifically, it is lacking a lot of logging features and we did not want to run it in production without a way to monitor it closely and debug if needed.</p><p>卡夫卡的这个解决方案很有希望。因为它被外部化到卡夫卡，我们可以自动调整摄取流量和资源。但是，我们决定不使用它。至少还没有。我们认为ClickHouse中的卡夫卡桌面引擎不够成熟。具体来说，它缺少很多日志功能，我们不想在生产中运行它，而不想在需要时对其进行密切监视和调试。</p><p>   clickhouse-copier is part of standard ClickHouse server distribution, it copies data from the tables in one cluster to tables in another (or the same) cluster. Zookeeper is used for syncing the copy and tracking the changes.</p><p>clickhouse copier是标准clickhouse server分发版的一部分，它将数据从一个集群中的表复制到另一个（或同一个）集群中的表。Zookeeper用于同步副本和跟踪更改。</p><p>  scenario #1: We thought about building an entire new cluster, and then migrate data with  clickhouse-copier from source_cluster to target_cluster.</p><p>场景#1：我们考虑构建一个全新的集群，然后使用clickhouse copier将数据从源集群迁移到目标集群。</p><p>    This solution is working well but, because it was operating on the production cluster directly (during the migration), it would have had an impact on our production performance. We considered it was too risky, so we looked for an alternative.</p><p>该解决方案运行良好，但由于它直接在生产集群上运行（在迁移期间），因此会对我们的生产性能产生影响。我们认为这太冒险了，所以我们寻找了一个替代方案。</p><p> scenario #2: We then thought about using  clickhouse-copier and creating a new database with more shards in the same cluster. We would have copied data from source_db to target_db. This is called  resharding in place, because we stay in the same cluster.</p><p>场景#2：然后我们考虑使用clickhouse copier，在同一集群中创建一个包含更多碎片的新数据库。我们会将数据从源数据库复制到目标数据库。这被称为原地重装，因为我们待在同一个集群中。</p><p>    This  Resharding in place solution is powerful and cost effective: a single cluster is used during the migration. However, we did not go with it because it would operate on our production server. It was not completely ignored however because we do use it in our  built-in solution.</p><p>这种就地重装解决方案功能强大且经济高效：在迁移过程中使用单个集群。然而，我们没有使用它，因为它将在我们的生产服务器上运行。然而，它并没有被完全忽略，因为我们确实在内置解决方案中使用了它。</p><p>   All the solutions we tried had their drawbacks. We wanted something  simple to reason about so it would be easy to automate and monitor. It also had to be  scalable (should run quickly even with a large amount of data) and  robust (so it could be stopped and restarted at any time). Finally, we also needed it to be  cost-effective.</p><p>我们尝试的所有解决方案都有缺点。我们想要一些简单的理由，这样就可以很容易地实现自动化和监控。它还必须具有可扩展性（即使有大量数据也应该快速运行）和健壮性（因此可以随时停止和重新启动）。最后，我们还需要它具有成本效益。</p><p> Before I describe our solution, it is important to introduce a vital tool that we use on a daily basis in Contentsquare: the mechanism of  clickhouse backup/restore to easily save and restore our  daily data with cloud storage support.</p><p>在我描述我们的解决方案之前，重要的是要介绍一个我们每天在Contentsquare中使用的重要工具：clickhouse备份/恢复机制，它可以通过云存储支持轻松保存和恢复我们的日常数据。</p><p>    What we came up with uses  clickhouse-copier, as well as  our daily backups.We call it our  ClickHouse cooker, because it acts as some kind of pot where we put our ingredients (our backups), let them cook for some time, and then we extract new, rebalanced, backups from it.We built this based on our findings exploring all the above ideas.</p><p>我们想出的方法使用clickhouse复印机，以及我们的日常备份。我们称之为ClickHouse炊具，因为它就像一个锅，我们把配料（我们的备份）放在里面，让它们煮一段时间，然后我们从中提取新的、重新平衡的备份。我们在探索上述所有想法的基础上得出了这一结论。</p><p>    Let&#39;s cook it! Below is the “recipe” to scale out our ClickHouse cluster from n to m shards</p><p>让&#39；我们来煮吧！下面是将ClickHouse集群从n个碎片扩展到m个碎片的“秘诀”</p><p>  The  ClickHouse resharding- cooker: creates ClickHouse cluster (m shards) with 2 empty databases: source_db installed on n shards and default database on m shards (n&lt;m)</p><p>ClickHouse resharding-cooker：创建ClickHouse集群（m个碎片），其中包含两个空数据库：源数据库安装在n个碎片上，默认数据库安装在m个碎片上（n&lt；m）</p><p>    for each partition_key of table data    # step 1  [clickhouse-client ] remove/truncate data  in source_db and default databases  # step 2  [clickhouse-backup ] restore data from external_storage to source_db  # step 3  [clickhouse-copier ] migrate data from source_db to default  # step 4  [clickhouse-client ] check data between source_db and default  (make sure all data from source_db have been fully migrated to default database )  # step 5  [clickhouse-backup ] backup data from default to external_storage</p><p>对于表数据的每个分区#键#步骤1[点击房屋客户端]删除/截断源#数据库和默认数据库中的数据#步骤2[点击房屋备份]将数据从外部#存储恢复到源#数据库#步骤3[点击房屋复印机]将数据从源#数据库迁移到默认数据库#步骤4[点击房屋客户端]检查源#默认（确保源数据库中的所有数据都已完全迁移到默认数据库）#第5步[点击房屋备份]将默认数据备份到外部存储</p><p> When you’ve finished the previous loop operation for all the dates of your database (daily backup ready in the new topology), you need to:</p><p>完成数据库所有日期的上一个循环操作（新拓扑中的每日备份就绪）后，需要：</p><p> This solution is fully scalable: you can deploy several clusters (with m instances each) and execute in a loop the daily operation described above.</p><p>这个解决方案是完全可扩展的：您可以部署几个集群（每个集群有m个实例），并在一个循环中执行上述日常操作。</p><p>  Exploring so many different ideas was worth our while because we finally came up with one we are happy with. It is simple enough that it can be automated and scaled horizontally. Because of that, it also does not cost much and we can tweak its speed as needed.</p><p>探索这么多不同的想法是值得的，因为我们最终找到了一个我们满意的想法。它非常简单，可以自动化并水平缩放。正因为如此，它的成本也不高，我们可以根据需要调整它的速度。</p><p>  Thanks to the amazing DT team and in particular to Ryad for being a great support every day of this reflection and all the team for approving it: Vincent, Nir and Aryeh. And Thanks to Tim for helping me write this article.</p><p>感谢令人惊叹的DT团队，尤其是Ryad每天都大力支持这一反思，感谢所有团队的支持：Vincent、Nir和Aryeh。谢谢蒂姆帮我写这篇文章。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>