<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>随CUDA 11发布PyTorch 1.7，FFT新API，赢得分布式培训</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">随CUDA 11发布PyTorch 1.7，FFT新API，赢得分布式培训</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-28 03:09:17</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/1a7043e793fc66b6b19c490439b68d22.jpeg"><img src="http://img2.diglog.com/img/2020/10/1a7043e793fc66b6b19c490439b68d22.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>The PyTorch 1.7 release includes a number of new APIs including support for NumPy-Compatible FFT operations, profiling tools and major updates to both distributed data parallel (DDP) and remote procedure call (RPC) based distributed training. In addition, several features moved to  stable including custom C++ Classes, the memory profiler, the creation of custom tensor-like objects, user async functions in RPC and a number of other features in torch.distributed such as Per-RPC timeout, DDP dynamic bucketing and RRef helper.</p><p>PyTorch 1.7版本包括许多新的API，包括支持与NumPy兼容的FFT操作、评测工具以及对基于分布式数据并行(DDP)和远程过程调用(RPC)的分布式培训的重大更新。此外，几个功能已经稳定下来，包括自定义C++类、内存分析器、创建自定义类张量对象、RPC中的用户异步函数以及Torch.Distributed中的许多其他功能，如每RPC超时、DDP动态分组和RRef帮助器。</p><p>  Updates and additions to profiling and performance for RPC, TorchScript and Stack traces in the autograd profiler</p><p>对Autograd分析器中的RPC、TorchScript和堆栈跟踪的分析和性能进行了更新和添加。</p><p> To reiterate, starting  PyTorch 1.6, features are now classified as stable, beta and prototype. You can see the detailed announcement  here. Note that the prototype features listed in this blog are available as part of this release.</p><p>再次重申，从PyTorch1.6开始，特性现在分为稳定、测试版和原型。你可以在这里看到详细的公告。请注意，此博客中列出的原型功能已作为此版本的一部分提供。</p><p>   FFT-related functionality is commonly used in a variety of scientific fields like signal processing. While PyTorch has historically supported a few FFT-related functions, the 1.7 release adds a new torch.fft module that implements FFT-related functions with the same API as NumPy.</p><p>与FFT相关的功能通常用于各种科学领域，如信号处理。虽然PyTorch过去一直支持一些与FFT相关的函数，但1.7版本添加了一个新的torch.fft模块，该模块使用与NumPy相同的API实现与FFT相关的函数。</p><p> This new module must be imported to be used in the 1.7 release, since its name conflicts with the historic (and now deprecated) torch.fft function.</p><p>这个新模块必须导入才能在1.7版本中使用，因为它的名称与历史悠久(现已弃用)的torch.fft函数冲突。</p><p>  &gt;&gt; &gt;  import  torch. fft &gt;&gt; &gt;  t  =  torch. arange( 4) &gt;&gt; &gt;  t tensor([ 0,  1,  2,  3]) &gt;&gt; &gt;  torch. fft. fft( t) tensor([  6. + 0.j,  - 2. + 2.j,  - 2. + 0.j,  - 2. - 2.j]) &gt;&gt; &gt;  t  =  tensor([ 0. + 1.j,  2. + 3.j,  4. + 5.j,  6. + 7.j]) &gt;&gt; &gt;  torch. fft. fft( t) tensor([ 12. + 16.j,  - 8. + 0.j,  - 4. - 4.j,  0. - 8.j])</p><p>&gt；&gt；&gt；导入手电筒。FFT&gt；&gt；&gt；t=火炬。Arange(4)&gt；&gt；&gt；t张量([0，1，2，3])&gt；&gt；火炬。快速傅立叶变换。FFT(T)张量([6.+0.j，-2.+2.j，-2.+0.j，-2.-2.j])&gt；&gt；&gt；t=张量([0.。+1.j，2.+3.j，4.+5.j，6.+7.j])&gt；&gt；&gt；火炬。快速傅立叶变换。FFT(T)张量([12.+16.j，-8.+0.j，-4.-4.j，0.。-8.j])。</p><p>   Since  PyTorch 1.5, we’ve continued to maintain parity between the python and C++ frontend APIs. This update allows developers to use the nn.transformer module abstraction from the C++ Frontend. And moreover, developers no longer need to save a module from python/JIT and load into C++ as it can now be used it in C++ directly.</p><p>从PyTorch1.5开始，我们继续保持Python和C++前端API之间的一致性。此更新允许开发人员使用C++前端的nn.former模块抽象。此外，开发人员不再需要将模块从python/JIT保存并加载到C++中，因为它现在可以在C++中直接使用。</p><p>   Reproducibility (bit-for-bit determinism) may help identify errors when debugging or testing a program. To facilitate reproducibility, PyTorch 1.7 adds the  torch.set_deterministic(bool) function that can direct PyTorch operators to select deterministic algorithms when available, and to throw a runtime error if an operation may result in nondeterministic behavior. By default, the flag this function controls is false and there is no change in behavior, meaning PyTorch may implement its operations nondeterministically by default.</p><p>再现性(逐位确定性)可能有助于在调试或测试程序时识别错误。为了便于重现性，PyTorch 1.7添加了torch.set_defiristic(Bool)函数，该函数可以指导PyTorch操作符选择确定性算法(如果可用)，并在操作可能导致不确定性行为时抛出运行时错误。默认情况下，此函数控制的标志为false，并且行为没有变化，这意味着默认情况下PyTorch可能不确定地实现其操作。</p><p>  Operations with deterministic variants use those variants (usually with a performance penalty versus the non-deterministic version); and</p><p>具有确定性变体的操作使用这些变体(与非确定性版本相比，通常会降低性能)；以及。</p><p> Note that this is necessary,  but not sufficient, for determinism  within a single run of a PyTorch program. Other sources of randomness like random number generators, unknown operations, or asynchronous or distributed computation may still cause nondeterministic behavior.</p><p>请注意，对于PyTorch程序的单次运行中的确定性而言，这是必要的，但不是充分的。其他随机性来源，如随机数生成器、未知操作、异步或分布式计算，仍可能导致不确定行为。</p><p>     Users can now see not only operator name/inputs in the profiler output table but also where the operator is in the code. The workflow requires very little change to take advantage of this capability. The user uses the  autograd profiler as before but with optional new parameters:  with_stack and  group_by_stack_n. Caution: regular profiling runs should not use this feature as it adds significant overhead.</p><p>用户现在不仅可以看到分析器输出表中的操作员名称/输入，还可以看到操作员在代码中的位置。工作流程只需极少的更改即可利用此功能。用户像以前一样使用自动评分探查器，但带有可选的新参数：WITH_STACK和GROUP_BY_STACK_n。注意：常规分析运行不应使用此功能，因为它会增加大量开销。</p><p>    Torchelastic offers a strict superset of the current  torch.distributed.launch CLI with the added features for fault-tolerance and elasticity. If the user is not be interested in fault-tolerance, they can get the exact functionality/behavior parity by setting  max_restarts=0 with the added convenience of auto-assigned  RANK and  MASTER_ADDR|PORT (versus manually specified in  torch.distributed.launch).</p><p>Torchelastic提供了当前Torch.Distributed的严格超集。启动CLI时增加了容错和弹性功能。如果用户对容错不感兴趣，他们可以通过设置max_restarts=0获得精确的功能/行为奇偶校验，同时增加了自动分配RANK和MASTER_ADDR|PORT的便利性(而不是在torch.Distributed.Launch中手动指定)。</p><p> By bundling  torchelastic in the same docker image as PyTorch, users can start experimenting with torchelastic right-away without having to separately install  torchelastic. In addition to convenience, this work is a nice-to-have when adding support for elastic parameters in the existing Kubeflow’s distributed PyTorch operators.</p><p>通过将Torchelastic捆绑在与PyTorch相同的底座图像中，用户可以立即开始尝试Torchelastic，而不必单独安装Torchelastic。除了方便之外，当在现有Kubeflow的分布式PyTorch操作符中添加对弹性参数的支持时，这项工作也是非常有用的。</p><p>   PyTorch 1.7 introduces a new context manager to be used in conjunction with models trained using  torch.nn.parallel.DistributedDataParallel to enable training with uneven dataset size across different processes. This feature enables greater flexibility when using DDP and prevents the user from having to manually ensure dataset sizes are the same across different process. With this context manager, DDP will handle uneven dataset sizes automatically, which can prevent errors or hangs at the end of training.</p><p>PyTorch1.7引入了一个新的上下文管理器，该管理器将与使用torch.nn.parallel.DistributedDataParallel训练的模型结合使用，以支持跨不同进程使用不均匀的数据集进行训练。此功能在使用DDP时提供了更大的灵活性，使用户不必手动确保不同流程中的数据集大小相同。使用此上下文管理器，DDP将自动处理不均匀的数据集大小，这可以防止培训结束时出现错误或挂起。</p><p>   In the past, NCCL training runs would hang indefinitely due to stuck collectives, leading to a very unpleasant experience for users. This feature will abort stuck collectives and throw an exception/crash the process if a potential hang is detected. When used with something like torchelastic (which can recover the training process from the last checkpoint), users can have much greater reliability for distributed training. This feature is completely opt-in and sits behind an environment variable that needs to be explicitly set in order to enable this functionality (otherwise users will see the same behavior as before).</p><p>在过去，NCCL的训练运行会因为集体卡住而无限期地挂起，给用户带来了非常不愉快的体验。如果检测到潜在的挂起，此功能将中止挂起的集合，并抛出异常/使进程崩溃。当与torchelastic(它可以从最后一个检查点恢复训练过程)之类的东西一起使用时，用户可以获得更高的分布式训练可靠性。此功能完全是选择加入的，并且位于需要显式设置才能启用此功能的环境变量之后(否则用户将看到与以前相同的行为)。</p><p>   torch.distributed.rpc.rpc_async has been available in TorchScript in prior releases. For PyTorch 1.7, this functionality will be extended the remaining two core RPC APIs,  torch.distributed.rpc.rpc_sync and  torch.distributed.rpc.remote. This will complete the major RPC APIs targeted for support in TorchScript, it allows users to use the existing python RPC APIs within TorchScript (in a script function or script method, which releases the python Global Interpreter Lock) and could possibly improve application performance in multithreaded environment.</p><p>在早期版本中，Torch.Distributed.rpc.rpc_async已在TorchScript中提供。对于PyTorch1.7，此功能将扩展到剩下的两个核心RPCAPI：torch.Distributed.rpc.rpc_sync和torch.Distributed.rpc.remote。这将完成计划在TorchScript中支持的主要RPC API，它允许用户在TorchScript中使用现有的python RPC API(在脚本函数或脚本方法中，这将释放python全局解释器锁)，并可能提高多线程环境中的应用程序性能。</p><p>   PyTorch provides a broad set of optimizers for training algorithms, and these have been used repeatedly as part of the python API. However, users often want to use multithreaded training instead of multiprocess training as it provides better resource utilization and efficiency in the context of large scale distributed training (e.g. Distributed Model Parallel) or any RPC-based training application). Users couldn’t do this with with distributed optimizer before because we need to get rid of the python Global Interpreter Lock (GIL) limitation to achieve this.</p><p>PyTorch提供了一系列用于训练算法的优化器，这些优化器已作为python API的一部分重复使用。然而，用户通常希望使用多线程训练而不是多进程训练，因为它在大规模分布式训练(例如，分布式模型并行)或任何基于RPC的训练应用程序的上下文中提供了更好的资源利用率和效率。用户以前不能使用分布式优化器做到这一点，因为我们需要摆脱python全局解释器锁(GIL)的限制才能实现这一点。</p><p> In PyTorch 1.7, we are enabling the TorchScript support in distributed optimizer to remove the GIL, and make it possible to run optimizer in multithreaded applications. The new distributed optimizer has the exact same interface as before but it automatically converts optimizers within each worker into TorchScript to make each GIL free. This is done by leveraging a functional optimizer concept and allowing the distributed optimizer to convert the computational portion of the optimizer into TorchScript. This will help use cases like distributed model parallel training and improve performance using multithreading.</p><p>在PyTorch1.7中，我们启用了分布式优化器中的TorchScript支持来删除GIL，并使优化器能够在多线程应用程序中运行。新的分布式优化器具有与以前完全相同的接口，但是它会自动将每个Worker中的优化器转换为TorchScript，从而使每个GIL空闲。这是通过利用功能优化器概念并允许分布式优化器将优化器的计算部分转换为TorchScript来实现的。这将有助于使用分布式模型并行训练等用例，并使用多线程提高性能。</p><p> Currently, the only optimizer that supports automatic conversion with TorchScript is  Adagrad and all other optimizers will still work as before without TorchScript support. We are working on expanding the coverage to all PyTorch optimizers and expect more to come in future releases. The usage to enable TorchScript support is automatic and exactly the same with existing python APIs, here is an example of how to use this:</p><p>目前，唯一支持使用TorchScript进行自动转换的优化器是Adagrad，所有其他优化器在没有TorchScript支持的情况下仍然可以像以前一样工作。我们正在努力将覆盖范围扩大到所有PyTorch优化器，并期待在未来的版本中提供更多内容。启用TorchScript支持的用法是自动的，与现有的python API完全相同，下面是一个如何使用的示例：</p><p> import  torch. distributed. autograd  as  dist_autograd import  torch. distributed. rpc  as  rpc from  torch  import  optim from  torch. distributed. optim  import  DistributedOptimizer with  dist_autograd. context()  as  context_id:  # Forward pass.  rref1  =  rpc. remote( &#34;worker1&#34;,  torch. add,  args =( torch. ones( 2),  3))  rref2  =  rpc. remote( &#34;worker1&#34;,  torch. add,  args =( torch. ones( 2),  1))  loss  =  rref1. to_here()  +  rref2. to_here()  # Backward pass.  dist_autograd. backward( context_id, [ loss. sum()])  # Optimizer, pass in optim.Adagrad, DistributedOptimizer will  # automatically convert/compile it to TorchScript (GIL-free)  dist_optim  =  DistributedOptimizer(  optim. Adagrad, [ rref1,  rref2],  lr = 0.05, )  dist_optim. step( context_id)</p><p>导入手电筒。分布式的。自动分级为dist_autograd导入手电筒。分布式的。RPC作为来自TORCH的RPC从TORCH导入Optim。分布式的。Optim import DistributedOptimizer with dist_autograd。Context()as context_id：#转发。Rref1=RPC。远程(&#34；工人1&#34；，手电筒。ADD，args=(手电筒。1(2)，3))rref2=RPC。远程(&#34；工人1&#34；，手电筒。ADD，args=(手电筒。1(2)，1))损耗=rref1。To_here()+rref2。To_here()#向后传递。Dist_autograd。向后(context_id，[丢失。Sum()])#Optimizer，传入Optim.Adagrad，DistributedOptimizer会#自动将其转换/编译为TorchScript(GIL-free)dist_optim=DistributedOptimizer(Optim.。Adagrad，[rref1，rref2]，LR=0.05，)dist_optim。步骤(Context_Id)。</p><p>   Support for using the PyTorch profiler in conjunction with the RPC framework was first introduced in PyTorch 1.6. In PyTorch 1.7, the following enhancements have been made:</p><p>PyTorch 1.6首次引入了对结合使用PyTorch分析器和RPC框架的支持。在PyTorch 1.7中，进行了以下增强：</p><p>  User are now able to use familiar profiling tools such as  with torch.autograd.profiler.profile() and  with torch.autograd.profiler.record_function, and this works transparently with the RPC framework with full feature support, profiles asynchronous functions, and TorchScript functions.</p><p>用户现在可以使用熟悉的性能分析工具，比如torch.autograd.profiler.profile()和torch.autograd.profiler.record_function，这可以透明地与RPC框架配合使用，这些RPC框架具有全功能支持、配置文件异步函数和TorchScript函数。</p><p>   PyTorch 1.7 brings prototype support for  DistributedDataParallel and collective communications on the Windows platform. In this release, the support only covers Gloo-based  ProcessGroup and  FileStore. To use this feature across multiple machines, please provide a file from a shared file system in  init_process_group.</p><p>PyTorch1.7为Windows平台上的DistributedDataParallel和集合通信提供了原型支持。在此版本中，仅支持基于Gloo的ProcessGroup和FileStore。要跨多台计算机使用此功能，请在init_process_group中提供来自共享文件系统的文件。</p><p> # initialize the process group dist. init_process_group(  &#34;gloo&#34;,  # multi-machine example:  # Shared files need six &#34;/&#34;  # init_method = `&#34;file://////{machine}/{share_folder}/file&#34;`  # Local file need three &#34;/&#34;  init_method = &#34;file:///{your local file path}&#34;,  rank = rank,  world_size = world_size) model  =  DistributedDataParallel( local_model,  device_ids =[ rank])</p><p>#初始化进程组dist。INIT_PROCESS_GROUP(&#34；GLOO&#34；，#多机示例：#共享文件需要六个&#34；/&#34；#INIT_METHOD=`&#34；file:/{machine}/{share_folder}/file&#34；`#本地文件需要三个&#34；/&#34；INIT_METHOD=&#34；file:///{your本地文件路径}&#34；，RANK=RANK，WORLD_SIZE=WORLD_SIZE)model=DistributedDataParallel(LOCAL_MODEL，DEVICE_ID=[RANK])</p><p>   PyTorch Mobile supports both  iOS and  Android with binary packages available in  Cocoapods and J Center respectively. You can learn more about PyTorch-Mobile  here.</p><p>PyTorch Mobile支持iOS和Android，CocoaPods和J Center分别提供二进制软件包。您可以在这里了解有关PyTorch-Mobile的更多信息。</p><p>  On some mobile platforms, such as Pixel, we observed that memory is returned to the system more aggressively. This results in frequent page faults as PyTorch being a functional framework does not maintain state for the operators. Thus outputs are allocated dynamically on each execution of the op, for the most ops. To ameliorate performance penalties due to this, PyTorch 1.7 provides a simple caching allocator for CPU. The allocator caches allocations by tensor sizes and, is currently, available only via the PyTorch C++ API. The caching allocator itself is owned by client and thus the lifetime of the allocator is also maintained by client code. Such a client owned caching allocator can then be used with scoped guard,  c10::WithCPUCachingAllocatorGuard, to enable the use of cached allocation within that scope.</p><p>在一些移动平台上，比如Pixel，我们观察到内存被更积极地归还给系统。这导致频繁的页面错误，因为作为功能框架的PyTorch不维护操作符的状态。因此，对于大多数操作，每次执行操作时都会动态分配输出。为了改善由此造成的性能损失，PyTorch1.7为CPU提供了一个简单的缓存分配器。分配器按张量大小缓存分配，目前只能通过PyTorch C++API使用。缓存分配器本身归客户端所有，因此分配器的生命周期也由客户端代码维护。这样的客户端拥有的缓存分配器然后可以与作用域保护c10：：WithCPUCachingAllocatorGuard一起使用，以允许在该作用域内使用缓存分配。</p><p>  # include   &lt;c10/mobile/CPUCachingAllocator.h &gt;.....c10::CPUCachingAllocator caching_allocator;   // Owned by client code. Can be a member of some client class so as to tie the   // the lifetime of caching allocator to that of the class......{ c10::optional&lt;c10::WithCPUCachingAllocatorGuard&gt; caching_allocator_guard;  if (FLAGS_use_caching_allocator) { caching_allocator_guard. emplace(&amp;caching_allocator); } .... model. forward(..);}.....</p><p>#include&lt；c10/mobile/CPUCachingAllocator.h&gt；.c10：：CPUCachingAllocator CACHING_ALLOCATOR；//客户端代码拥有。可以是某个客户端类的成员，以便//将缓存分配器的生存期与类的生存期捆绑在一起.{c10：：optional&lt；c10：：WithCPUCachingAllocatorGuard&gt；CACHING_ALLOCATOR_GROUDE；IF(FLAGS_USE_CACHING_ALLOCATOR){CACHING_ALLOCATOR_GROUTE。Emplace(&amp；cachingallocator)；}....。模特。前进(……)；}……。</p><p> NOTE: Caching allocator is only available on mobile builds, thus the use of caching allocator outside of mobile builds won’t be effective.</p><p>注意：缓存分配器仅在移动版本上可用，因此在移动版本之外使用缓存分配器将不会有效。</p><p>     Previously,  torch.conj and  Tensor.conj were making a clone for Tensors of real dtype. It now returns the Tensor as-is to improve performance. You can recover the original behavior by adding a  .clone() for real Tensors. Note that this behavior is different from  numpy for which  np.conj returns a new ndarray and  ndarray.conj returns the ndarray as-is.</p><p>以前，torch.conj和Tensor.conj正在为real dtype的张量创建克隆。它现在按原样返回张量以提高性能。可以通过为实张量添加.clone()来恢复原始行为。注意，此行为与numpy不同，在numpy中，np.conj返回新的ndarray，而ndarray.conj按原样返回ndarray。</p><p>    torch.tensor,  torch.as_tensor, and  torch.sparse_coo_tensor now use the input Tensor’s device when it is not specified ( #41984)</p><p>Torch.tensor、torch.as_tensor和torch.parse_coo_tensor现在使用未指定的输入张量设备(#41984)。</p><p> This will change the device on which the Tensor is created and so the user can start seeing device mismatch errors. It also means for sparse Tensors that both of the provided Tensors must be on the same device if the device is not specified. You can recover the original behavior by passing the  device argument.</p><p>这将更改在其上创建张量的设备，因此用户可以开始看到设备不匹配错误。这也意味着对于稀疏张量，如果未指定设备，则提供的两个张量必须位于同一设备上。您可以通过传递设备参数来恢复原始行为。</p><p>  &gt;&gt; &gt;  t. device device( type =‘ cuda: 0’) &gt;&gt; &gt;  # tensor constructor &gt;&gt; &gt;  torch. tensor( t,  dtype = torch. float32). device device( type =‘ cpu’) &gt;&gt; &gt;  # sparse constructor &gt;&gt; &gt;  torch. sparse_coo_tensor(  torch. tensor(([ 0], [ 2]),  device = &#34;cpu&#34;),  torch. tensor(([ 1.],),  device = &#34;cuda&#34;),  size =( 3,  3,  1)). device device( type = &#39;cuda&#39;,  index = 0)</p><p>&gt；&gt；&gt；t设备设备(type=‘cuda：0’)&gt；&gt；&gt；#张量构造函数&gt；&gt；火炬。张量(t，dtype=火炬。浮动32)。设备设备(type=‘cpu’)&gt；&gt；&gt；#稀疏构造函数&gt；&gt；火炬。稀疏Coo张量(手电筒。张量(([0]，[2])，设备=&#34；CPU&#34；)，手电筒。张量(([1.]，)，设备=&#34；Cuda&#34；)，大小=(3，3，1))。设备设备(type=&#39；cuda&#39；，index=0)</p><p>  &gt;&gt; &gt;  t. device device( type =‘ cuda: 0’) &gt;&gt; &gt;  # tensor constructor &gt;&gt; &gt;  torch. tensor( t,  dtype = torch. float32). device device( type =‘ cuda: 0’) &gt;&gt; &gt;  # Specify the device to get the same behavior as 1.6 &gt;&gt; &gt;  torch. tensor( t,  dtype = torch. float32,  device = &#39;cpu&#39;). device device( type =‘ cpu’) &gt;&gt; &gt;  # sparse constructor &gt;&gt; &gt;  torch. sparse_coo_tensor(  torch. tensor(([ 0], [ 2]),  device = &#34;cpu&#34;),  torch. tensor(([ 1.],),  device = &#34;cuda&#34;),  size =( 3,  3,  1)). device RuntimeError:  backend  of  indices ( CPU)  must  match  backend of  values ( CUDA) &gt;&gt; &gt;  # Specify the device to get the same behavior as 1.6 &gt;&gt; &gt;  torch. sparse_coo_tensor(  torch. tensor(([ 0], [ 2]),  device = &#34;cpu&#34;),  torch. tensor(([ 1.],),  device = &#34;cuda&#34;),  size =( 3,  3,  1),  device = &#34;cuda:0&#34;). device device( type = &#39;cuda&#39;,  index = 0)</p><p>&gt；&gt；&gt；t设备设备(type=‘cuda：0’)&gt；&gt；&gt；#张量构造函数&gt；&gt；火炬。张量(t，dtype=火炬。浮动32)。Device device(type=‘cuda：0’)&gt；&gt；&gt；#指定设备以获得与1.6&gt；&gt；&gt；Torch相同的行为。张量(t，dtype=火炬。Flat32，device=&#39；cpu&#39；)。设备设备(type=‘cpu’)&gt；&gt；&gt；#稀疏构造函数&gt；&gt；火炬。稀疏Coo张量(手电筒。张量(([0]，[2])，设备=&#34；CPU&#34；)，手电筒。张量(([1.]，)，设备=&#34；Cuda&#34；)，大小=(3，3，1))。设备运行错误：索引后端(CPU)必须与值后端(CUDA)&gt；&gt；&gt；#指定设备以获得与1.6&gt；&gt；&gt；Torch相同的行为。稀疏Coo张量(手电筒。张量(([0]，[2])，设备=&#34；CPU&#34；)，手电筒。张量(([1.]，)，Device=&#34；Cuda&#34；)，大小=(3，3，1)，Device=&#34；Cuda：0&#34；)。设备设备(type=&#39；cuda&#39；，index=0)。</p><p>   Before this change, when calling  torch.norm with  keepdim=True and  p=&#39;fro&#39; or  p=number, leaving all other optional arguments as their default values, the keepdim argument would be ignored. It is now properly respected. Also, any time  torch.norm was called with  p=&#39;nuc&#39; and  keepdim=True, the result would have one fewer dimension than the input, and the dimensions could be out of order depending on which dimensions were being reduced. It is now properly keeping all the dimensions. You can recover the original behavior by setting  keepdim=False.  NOTE: this function is now deprecated (see below) and we recommend you use  torch.linalg.norm, which follows NumPy’s conventions.</p><p>在进行此更改之前，当使用Keepdim=True和p=&#39；fro&#39；或p=number调用torch.norm时，将所有其他可选参数保留为其默认值时，Keepdim参数将被忽略。它现在受到了适当的尊重。此外，任何时候使用p=#39；nuc&39；和Keepdim=True调用torch.norm时，结果都会比输入少一个维度，并且维度可能会乱序，具体取决于要减少的维度。它现在正确地保留了所有的维度。您可以通过设置Keepdim=False来恢复原始行为。注意：该函数现在已弃用(见下文)，我们建议您使用torch.linalg.norm，它遵循NumPy的约定。</p><p>  &gt;&gt; &gt;  t. size() torch. Size([ 4,  4]) &gt;&gt; &gt;  t. norm( p =‘ fro’,  keepdim = True). size() torch. size([]) &gt;&gt; &gt;  t. norm( p = 3,  keepdim = True). size() torch. size([]) &gt;&gt; &gt;  t. norm( p =‘ nuc’,  keepdim = True). size() torch. size([ 1])</p><p>&gt；&gt；&gt；T.size()手电筒。Size([4，4])&gt；&gt；&gt；t.norm(p=‘fro’，Keepdim=True)。大小()手电筒。Size([])&gt；&gt；&gt；t.norm(p=3，Keepdim=True)。大小()手电筒。Size([])&gt；&gt；&gt；t.norm(p=‘nuc’，Keepdim=True)。大小()手电筒。大小([1])。</p><p>  &gt;&gt; &gt;  t. size() torch. Size([ 4,  4]) &gt;&gt; &gt;  t. norm( p =‘ fro’,  keepdim = True). size() torch. size([ 1,  1]) &gt;&gt; &gt;  t. norm( p = 3,  keepdim = True). size() torch. size([ 1,  1]) &gt;&gt; &gt;  t. norm( p =‘ nuc’,  keepdim = True). size() torch. size([ 1,  1])</p><p>&gt；&gt；&gt；T.size()手电筒。Size([4，4])&gt；&gt；&gt；t.norm(p=‘fro’，Keepdim=True)。大小()手电筒。Size([1，1])&gt；&gt；&gt；t.Norm(p=3，Keepdim=True)。大小()手电筒。Size([1，1])&gt；&gt；&gt；t.norm(p=‘nuc’，Keepdim=True)。大小()手电筒。大小([1，1])。</p><p>   The autograd system is able to correctly handle modifications through views of Tensors by explicitly tracking known view operations. In prior releases,  torch.split and  torch.chunk were not marked as known view operations, which could lead to silently wrong gradients.</p><p>自动分级系统能够通过显式跟踪已知的视图操作来正确地处理通过张量的视图的修改。在以前的版本中，torch.plit和torch.chunk没有被标记为已知的视图操作，这可能会导致静默错误的渐变。</p><p> Note that since v1.5, inplace modification of views created by functions that return multiple views is deprecated. Such case is not properly handled by the autograd and can lead to internal errors or wrong gradients. So, as a side effect of this view fix, inplace modifications of the outputs of  torch.split and  torch.chunk will now raise a warning and can lead to internal errors or wrong gradients while they were previously silently computing wrong gradients. If you see such a warning, you should replace the inplace operation with an out of place one. You can recover the original behavior by using the new  torch.unsafe_split and  torch.unsafe_chunk. Note that these functions are only here to ease the transition and will also be removed in a future version.</p><p>请注意，从V1.5开始，不建议对返回多个视图的函数创建的视图进行就地修改。自动分级器没有正确处理这种情况，可能会导致内部错误或错误的渐变。因此，作为此视图修复的副作用，就地修改torch.plit和torch.chunk的输出现在将引发警告，并可能导致内部错误或错误的渐变，而它们以前是静默地计算错误的渐变。如果您看到这样的警告，您应该将就地操作替换为不适当的操作。您可以使用新的torch.unsafe_plit和torch.unsafe_chunk恢复原始行为。请注意，此处的这些函数只是为了简化过渡，在将来的版本中也将删除。</p><p>  torch.argmin ( torch.argmax) now always returns the index of the first minimum (maximum) element. This choice is consistent with NumPy. Previously if there were multiple minima (maxima) the index returned could be the index of any of them. You cannot recover the original behavior as it was platform dependent and not guaranteed. If your code was relying on a specific index for your specific platform, you should update it to work with the first index and this new code will work on all platforms.</p><p>Torch.argmin(torch.argmax)现在总是返回第一个最小(最大)元素的索引。此选择与NumPy一致。以前，如果存在多个最小值(最大值)，则返回的索引可能是其中任何一个的索引。您无法恢复原始行为，因为它依赖于平台且不能保证。如果您的代码依赖于特定平台的特定索引，则应将其更新为使用第一个索引，此新代码将在所有平台上运行。</p><p>  When no dimension is specified, full reduction is performed and the gradient will now flow back evenly towards all the input that realized the output value. The old behavior was to propagate the gradient only for one of such input selected arbitrarily. This should improve stability of training by gradient descent. To recover the previous behavior, you can perform the reduction with the  dim= argument. It will ensure that the gradient only flows back for the input whose index was returned.</p><p>如果未指定尺寸，则执行完全缩减，并且渐变现在将均匀地向实现输出值的所有输入回流。旧的行为是只为任意选择的一个这样的输入传播梯度。这应该会通过梯度下降来提高训练的稳定性。要恢复之前的行为，可以使用dim=参数执行缩减。它将确保渐变仅针对其索引被返回的输入回流。</p><p>     This is the end of the deprecation cycle for this op to make sure it does not have different broadcasting semantic compared to numpy’s broadcasting semantic used everywhere else in PyTorch’s codebase. You need to make sure all inputs are the same size to avoid the error.</p><p>这是该OP的弃用周期的结束，以确保它与PyTorch代码库中所有其他地方使用的numpy的广播语义相比没有不同的广播语义。您需要确保所有输入的大小相同，以避免错误。</p><p>  &gt;&gt; &gt;  bceloss  =  nn. BCELoss() &gt;&gt; &gt;  a  =  torch. rand( 25) &gt;&gt; &gt;  b  =  torch. rand( 25,  1) &gt;&gt; &gt;  bceloss( a,  b) UserWarning:  Using  a  target  size ( torch. Size([ 25,  1])) that  is  different  to  the  input  size ( torch. Size([ 25])) is  deprecated.  Please  ensure  they  have  the  same  size. tensor( 1.0604)</p><p>&gt；&gt；&gt；bceloss=nn.。BCELoss()&gt；&gt；&gt；a=手电筒。兰德(25)&gt；&gt；b=火炬。Rand(25，1)&gt；&gt；bceloss(a，b)UserWarning：使用目标大小(手电筒。尺寸([25，1]))与输入尺寸(手电筒)不同。大小([25]))已弃用。请确保它们的尺寸是一样的。张量(1.0604)。</p><p>  &gt;&gt; &gt;  bceloss  =  nn. BCELoss() &gt;&gt; &gt;  a  =  torch. rand( 25) &gt;&gt; &gt;  b  =  torch. rand( 25,  1) &gt;&gt; &gt;  bceloss( a,  b) ValueError:  Using  a  target  size ( torch. Size([ 25,  1])) that  is  different  to  the  input  size ( torch. Size([ 25])) is  deprecated.  Please  ensure  they  have  the  same  size. &gt;&gt; &gt;  b  =  b. reshape( 25) &gt;&gt; &gt;  bceloss( a,  b) tensor( 1.0604)</p><p>&gt；&gt；&gt；bceloss=nn.。BCELoss()&gt；&gt；&gt；a=手电筒。兰德(25)&gt；&gt；b=火炬。Rand(25，1)&gt；&gt；&gt；bceloss(a，b)值错误：使用目标大小(手电筒。尺寸([25，1]))与输入尺寸(手电筒)不同。大小([25]))已弃用。请确保它们的尺寸是一样的。&gt；&gt；&gt；b=B.重塑(25)&gt；&gt；&gt；bceloss(a，b)张量(1.0604)。</p><p>   To improve performance, the custom  autograd.Function will not create a Tensor full of zeros when an input is differentiable but the user’s  backward function returns  None for it. This means that code for which the  .backward() or  autograd.grad() final result will now be  None while it used to be a Tensor full of zeros. You can recover the previous behavior by having your custom  autograd.Function materialize the zero Tensor with  torch.zeros_like(input) to replace the  None output for the  backward method.</p><p>为了提高性能，当输入是可微的，但用户的Backup函数没有为其返回None时，定制的autograd.Function将不会创建一个满是零的张量。这意味着.backward()或autograd.grad()最终结果现在将为None，而它过去是一个满是零的张量。您可以通过自定义自动分级来恢复以前的行为。函数使用torch.zeros_like(输入)实体化零张量，以替换向后方法的None输出。</p><p> import  torch # Custom Function that returns None for the gradient class  GetTwos( torch. autograd. Function):  @ staticmethod  def  forward( ctx,  inp):  return  inp. clone(). fill_( 2)  @ staticmethod  def  backward( ctx,  grad_out):  # To recover the 1.6 behavior, replace the line below with `return torch.zeros_like(grad_out)`  return  None a  =  torch. rand( 10,  requires_grad = True) b  =  GetTwos. apply( a) b. sum(). backward() print( a. grad) # In PyTorch 1.6 this will print # tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]) # In PyTorch 1.7 this will print # None</p><p>Import torch#为渐变类GetTwos(Torch.。自动升级。函数)：@staticmethod def ward(ctx，inp)：返回inp。克隆()。Fill_(2)@staticmethod def back(ctx，grad_out)：#要恢复1.6行为，请将下面一行替换为`Return torch.zeros_like(Grad_Out)`return NONE a=torch。Rand(10，Requires_grad=True)b=GetTwos。申请(A)B.sum()。Backward()print(a.grad)#在PyTorch 1.6中，这将打印#tensor([0.，0.，0.，0.，0.，0.，0.，0.，0.，0.，0.，0.])#在PyTorch 1.7中，这将打印#NONE。</p><p>  We fixed a bug in the inplace detection code that was preventing the detection of some inplace operations for output that are not differentiable (like integer type Tensors). This can lead to code that used to run fine to throw the error “a Tensor that was needed for backward was modified in an inplace operation”. Such failure is true and the user code must be fixed to compute proper gradients. In general, this involves cloning the Tensor before modifying it inplace to make sure the backward pass can happen safely.</p><p>我们修复了就地检测代码中的一个错误，该错误阻止检测一些不可微的输出就地操作(如整型张量)。这可能会导致过去运行良好的代码抛出错误“在原地操作中修改了向后所需的张量”。这种故障是真实的，必须修复用户代码才能计算正确的梯度。通常，这涉及在就地修改张量之前克隆张量，以确保可以安全地进行后向传递。</p><p> import  torch a  =  torch. rand( 10,  requires_grad = True) with  torch. no_grad():  a[ 2]  =  10 b,  ind  =  a. max( dim = 0) # ind is 2 here with  torch. no_grad():  t  =  torch. rand( 10)  t[ 4]  =  10  res  =  torch. max( t,  dim = 0,  out =( torch. Tensor(),  ind))  # ind becomes 4 here # This backward runs in 1.6 but will fail in 1.7 b. sum(). backward() print( a. grad) # tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]) # The value is wrong is at index 4 while it should be at index 2 # The issue is avoided by not modifying ind inplace by replacing the line # above with: # res = torch.max(t, dim=0, out=(torch.Tensor(), ind.clone()))</p><p>导入手电筒a=手电筒。带手电筒的RAND(10，Requires_grad=True)。No_grad()：a[2]=10b，ind=A。max(dim=0)#ind在手电筒中为2。No_grad()：t=手电筒。兰德(10)t[4]=10 RES=手电筒。MAX(t，Dim=0，Out=(手电筒。Tensor()，ind)#ind在这里变成4#这个向后运行在1.6中运行，但在1.7B.sum()中会失败。Backward()print(a.grad)#tensor([0.，0.，0.，0.，1.，0.，0.，0.，0.，0.])#错误的值在索引4处，而它应该在索引2处#通过将上面的行#替换为：#res=torch.max(t，dim=0，out=(torch.张量器()，ind.clone()可以避免这个问题。</p><p>    The old behavior of “any operations on your subclass produces a torch.Tensor instead of the subclass” can be recover</p><p>可以恢复“子类上的任何操作都会产生火炬。张量器而不是子类”的旧行为</p><p>......</p><p>.</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/pytorch/pytorch/releases/tag/v1.7.0">https://github.com/pytorch/pytorch/releases/tag/v1.7.0</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/发布/">#发布</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/赢得/">#赢得</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ffts/">#ffts</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1031577.html"><img src="http://img2.diglog.com/img/2020/10/thumb_75e60354f9d7216fe8ff99728318d528.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031577.html">Ubuntu Groovy Gorilla将Raspberry Pi添加为“一等公民”</a></div><span class="my_story_list_date">2020-10-28 2:52</span></div><div class="col-sm"><div><a target="_blank" href="/story/1031540.html"><img src="http://img2.diglog.com/img/2020/10/thumb_9c4390135e7f62612d01cfd4298fd889.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031540.html">Feddora 33已发布</a></div><span class="my_story_list_date">2020-10-28 1:8</span></div><div class="col-sm"><div><a target="_blank" href="/story/1031498.html"><img src="http://img2.diglog.com/img/2020/10/thumb_87192005a7cf243845577de2a49c2acc.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031498.html">
TCL发布售价400美元的5G手机</a></div><span class="my_story_list_date">2020-10-27 22:43</span></div><div class="col-sm"><div><a target="_blank" href="/story/1031462.html"><img src="http://img2.diglog.com/img/2020/10/thumb_3fc7f9c30dfdb8de31d78dddb9939322.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031462.html">OpenMPTCP路由器v0.56发布，提供聚合互联网连接的免费解决方案</a></div><span class="my_story_list_date">2020-10-27 20:35</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>