<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Gans轻松学习现实生活分布 GANs learn real-life distributions easily</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">GANs learn real-life distributions easily<br/>Gans轻松学习现实生活分布 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-11 21:46:28</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/832b4f01bf32ed6d6f0a0c4c23c6a9dc.jpg"><img src="http://img2.diglog.com/img/2021/6/832b4f01bf32ed6d6f0a0c4c23c6a9dc.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>A Generative adversarial network, or GAN, is one of the most powerful machine learning models proposed by  Goodfellow  et al. for learning to generate samples from complicated real-world distributions. GANs have sparked millions of applications, ranging from generating realistic images or cartoon characters to text-to-image translations. Turing award laureate Yann LeCun called GANs “the most interesting idea in the last 10 years in ML.”</p><p>一种生成的对抗性网络或GaN是Goodfellow等人提出的最强大的机器学习模型之一。用于学习从复杂的现实世界分布中生成样本。 GANS已经引发了数百万的应用程序，从产生了逼真的图像或卡通人物到图像到图像翻译。 Tying奖奖Laureate Yann Lecun叫Gans“最近10年来最有趣的想法。</p><p> In the context of generating images, GANs consist of two parts. 1) A parameterized (deconvolutional) generator network \(G\) that takes input \(z\) which is a random Gaussian vector and outputs a fake image \(G(z)\). 2) A parameterized (convolutional) discriminator network \(D\) that takes as input an image \(X\)and outputs a real value \(D(X)\). To learn a target distribution \(\mathcal{X}\) of images, the training process of GANs involves finding a generator \(G\) (typically by gradient descent ascent) where the distribution \(G(z)\) of fake images is indistinguishable from the distribution \(\mathcal{X}\) of real images, using any discriminator \(D\) in its parameter family. We illustrate GANs in Figure 1.</p><p> 在生成图像的背景下，GAN由两部分组成。 1）采用输入\（Z \）的参数化（Deconvolulate）生成器网络\（g \），其是随机高斯向量，并输出假图像\（g（z）\）。 2）参数化（卷积）鉴别器网络\（d \），其作为输入图像\（x \）并输出实值\（d（x）\）。要学习图像的目标分布\（\ Mathcal {x}），GAN的培训过程涉及找到一个发电机\（g \）（通常通过梯度下降升级），其中分布\（g（z）\）假图像与实际图像的分布\（\ Mathcal {x}）无法区分，使用其参数系列中的任何鉴别器\（d \）。我们说明了图1中的GAN。</p><p>   In sharp contrast to the great empirical success, GAN remains one of the least understood machine learning models in theory. How can the generator transfer random vectors from a non-structured spherical Gaussian distribution to highly-structured images? How can the generator be found simply by local search algorithms such as gradient descent ascent (GDA)? What is the role of the discriminator during the training process?</p><p>   与大实证成功鲜明对比，GaN仍然是理论上最不理解的机器学习模型之一。发电机如何将随机向量从非结构化球形高斯分布转移到高度结构的图像？如何通过当地搜索算法（如梯度下降Ascent（GDA））来找到发电机？判别在培训过程中的作用是什么？</p><p> All these theoretical questions remain essentially unanswered. This is perhaps not surprising, since even learning a linear transformation of some known distributions can be computationally hard (even NP-hard in the worst case), not to mention learning a transformation given by neural networks with ReLU activations.</p><p> 所有这些理论问题仍然没有答案。这可能并不令人惊讶，因为即使学习一些已知的分布的线性变换也可以计算地硬化（即使是最坏的情况下的NP  - 硬），更不用说学习通过Relu激活的神经网络给出的转换。</p><p> Does it mean GAN theory reaches a dead end? No. To understand the great empirical success of GANs, our new paper “  Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions ”, investigates the special structures of real-world distributions, in particular the structure of images, to understand how GANs can work effectively beyond the worst-case, theoretical bounds.</p><p> 这是否意味着GaN理论达到死胡同？不可以。要了解GAN的巨大经验成功，我们的新论文“前瞻性超级分辨率：如何将GAN学习现实世界分布的分层生成模型”，调查现实世界分布的特殊结构，特别是图像结构，了解GAN如何有效地超越最坏情况，理论界。</p><p>  Most real-life images can be viewed in different resolutions without losing the semantics. We often can reduce the resolution of a 1080p car image to as small as, say 16 pixel by 16 pixel, while still maintaining the outline of a car that can be identified by humans.</p><p>  大多数现实生活图像可以在不同的分辨率中查看而不丢失语义。我们经常可以将1080p轿厢图像的分辨率降低到小于16像素，同时仍然保持可以由人类识别的汽车的轮廓。</p><p> Consequently, one can expect a progressive generative model for real-life images–with the lower layers of the generator producing lower-resolution versions of the images, and higher layers producing higher resolutions. An experimental verification of this progressive generative model was done by NVIDIA researchers  Karras  et al., and is also illustrated in Figure 2 below.</p><p> 因此，人们可以期望用于现实寿命图像的渐进生成模型 - 利用发电机的下层产生图像的下分辨率版本，以及产生更高分辨率的更高层。 NVIDIA研究人员Karras等人对该进步生成模型进行了实验验证，并且还在下面的图2中说明。 </p><p>  In this work, we look deeply into this structural property of images. We formalize it as the forward super-resolution property. Mathematically, we consider learning a distribution of images generated progressively as follows. Let \(X_1\),…\(X_{L-1}\) be images of \(X_L\) at lower resolutions (that can be computed from \(X_L\) via image down-scaling), with the resolution of \(X_1\) being the lowest (for example 8×8), and the resolution of \(X_L\) being the highest (for example 128×128). We assume there is a target generator network \(G^*\) (to be learned) with hidden layers \(S{_1^*}\),\(S{_2^*}\),…,\(S{_L^*}\) computed as:</p><p>在这项工作中，我们深入了解了这种图像的结构属性。我们将其正式形式化为前瞻性分辨率财产。在数学上，我们考虑学习如下逐渐生成的图像的分布。让\（x_1 \），... \（x_ {l-1} \）是较低分辨率的\（x_l \）的图像（可以通过图像下缩放从\（x_l \）计算），分辨率\（x_1 \）是最低（例如8×8），并且\（x_l \）的分辨率最高（例如128×128）。我们假设用隐藏的图层\（g ^ * \）（要学习）\（s {_1 ^ *} \），\（s {_2 ^ *} \），...，\（s {_L ^ *} \）计算为：</p><p>  where \(z\) is a random Gaussian input and Deconv(⋅) is any deconvolution operator (such as nn.ConvTranspose2d in PyTorch). We also assume:</p><p>  其中\（z \）是随机高斯输入，Deconv（⋅）是任何折耦合运算符（如pytorch中的nn.convtransose2d）。我们还假设：</p><p>  Above, we call \(Deconv_{output}\) (⋅) the  output deconvolution layer. They are responsible for representing the “edge-color features” at different resolutions (see Figure 3).</p><p>  以上，我们调用\（Deconv_ {输出} \）（⋅）输出折块层。它们负责代表不同分辨率的“边缘颜色功能”（参见图3）。</p><p>  In other words, each hidden layer \(S_l^*\) of the target network \(G\)* is responsible for generating weights to determine “which edge-color features” are to be used to “paint” the output image. The lower-level hidden layers \(S_l^*\) are responsible for generating weights that are used to paint lower-resolution images; and the higher-level hidden layers responsible for painting higher-resolution images.</p><p>  换句话说，目标网络\（g \）*的每个隐藏层\（S_L ^ * \）负责生成权重，以确定要用于“绘制”输出图像的“哪个边缘颜色功能”。较低级别的隐藏层\（S_L ^ * \）负责生成用于绘制下层分辨率图像的权重;以及负责绘制更高分辨率图像的更高级别的隐藏层。</p><p> As a result, when using GANs to learn real-life images, one should expect the higher hidden layers of the  learner generator \(G\) to learn – via compositions of hidden deconvolution layers – how to combine lower-resolution features to generate higher-resolution images via  super-resolution.</p><p> 结果，当使用GAN学习现实生活图像时，人们应该期望学习者发生器的更高隐藏层\（g \）来学习 - 通过隐藏的解卷积层的组成 - 如何将较低分辨率的功能组合产生更高的功能 - 通过超级分辨率进行研究。</p><p>  Real-life images are also very sharp: meaning that one can clearly see the “edge of contrast” at object boundaries and the “consistency of color” within objects in the image. In a generative model, we attribute  image sharpness to a   hierarchical   sparse coding property of hidden layers .</p><p>  现实生活图像也很敏锐：含义可以清楚地看到物体边界的“对比度”和图像中对象中的“颜色一致性”。在生成模型中，我们将图像清晰度属性到隐藏图层的分层稀疏编码属性。</p><p> To see this, let us recall that real-life images are usually generated via  sparse combinations of “edge-color features” in the output deconvolution layer. For instance (see Figure 4), when a patch in an image is associated with the boundary of an object, an “edge feature” can be selected to generate the pixels in that patch, while all other features become unlikely to show up; in contrast, when a patch is in the middle of an object, a “color feature” can be selected to paint these pixels, while all other features are unlikely to show up.</p><p> 为了看到这一点，让我们回顾该现场图像通常通过输出解卷积层中的“边缘颜色特征”的稀疏组合生成。例如（参见图4），当图像中的贴片与对象的边界相关联时，可以选择“边缘特征”以在该补丁中生成像素，而所有其他功能都不太可能出现;相反，当补丁处于对象的中间时，可以选择“颜色特征”以绘制这些像素，而所有其他功能都不太可能出现。 </p><p>  Mathematically, we model this by assuming for every layer l and every patch p, the restriction of the hidden layer \(S_l^*\) to this patch – denoted by \([S_l^* ]_p\)– is a sparse vector. This means only a few channels are non-zero in every patch (although these non-zero channels can be different at different patches). We have empirically verified this assumption by measuring the sparsity of hidden-layer activations in Figure 5.</p><p>在数学上，我们通过假设每层L和每个补丁p来模拟这一点，将隐藏层\（s_l ^ * \）的限制限制为此补丁 - 由\（[s_l ^ *] _p \）表示 - 是稀疏的矢量。这意味着在每个补丁中只有几个通道是非零（尽管这些非零通道可以在不同的补丁下不同）。通过测量图5中的隐藏层激活的稀疏性，我们经验经验验证了这一假设。</p><p>    In this work, we show that the two structural properties of the distributions for images, namely  forward super-resolution which ensures semantic consistency of images at different resolutions, and  hierarchical sparse coding which ensures image sharpness, are   sufficient for GANs to learn such distributions  efficiently. Mathematically, we prove the following theorem in our new  paper.</p><p>    在这项工作中，我们表明，图像的分布的两个结构特性，即要确保不同分辨率的图像的语义一致性，以及确保图像清晰度的分层稀疏编码，足以为GAN有效地学习此类分布。在数学上，我们在新论文中证明了以下定理。</p><p> Theorem (informal): for any D-dimensional distribution with the forward super-resolution and hierarchical sparse coding properties, for every ε&gt;0, we can learn such distribution up to error ε with sample and time complexity polynomial in D and 1/ε, simply by training GANs using gradient descent ascent (GDA).</p><p> 定理（非正式）：对于每个ε＆gt; 0，对于前进超分辨率和分层稀疏编码属性的任何D维分布，我们可以使用D和1 /ε中的样本和时间复杂度多项式来学习此类分配。 ，只需使用梯度下降上升（GDA）培训GAN。</p><p> In this blog, let us pin down how GANs can learn this distribution without diving into math details. We consider layer-wise training: first train the first layer of the learner generator \(G\) (together with an output deconvolution layer) to generate lowest-resolution images \(X_1\), then train the second layer of \(G\) (together with an output deconvolution layer) to generate \(X_2\), the third layer to generate \(X_3\), and so on. We separate the learning into different phases.</p><p> 在这篇博客中，让我们识别GAN如何在没有潜入数学细节的情况下学习这个分布。我们考虑层展培训：首先将学习者生成器\（g \）的第一层列车（与输出解卷积）一起生成最低分辨率图像\（x_1 \），然后训练第二层\（g \）（与输出解卷层）一起生成\（x_2 \），第三层生成\（x_3 \），等等。我们将学习分成不同的阶段。</p><p>  To learn the output deconvolution layers (i.e. the deconvolutional operator from \(X_l\)=\(Deconv_{output}\) \((S_l^* )\) for any layer \(l\)), we show it suffices for the generator to learn to match the moments between the generator’s output images and the target distribution’s real images. This is a special property of distributions generated from the sparse coding model known from earlier theoretical works such as  Anandkumar et al.</p><p>  要学习输出解卷积图层（即，来自\（x_l \）= \（deconv_ {output} \）\（（s_l ^ *）\）的任何一层\（l \）），我们显示足够的生成器学习匹配发电机输出图像和目标分布的真实图像之间的时刻。这是从早期的理论工作中已知的稀疏编码模型产生的分布的特殊属性，例如Andkumar等人。</p><p> Intuitively, the output deconvolution layer can be written as a linear operator \(X=Ay\) where each column of matrix A represents an edge-color feature, and y is a sparse vector that determines which edge-color features to use to paint the output image \(X\). It is known under standard regularity conditions (such as almost column orthogonality of \(A\) and sufficient sparsity of \(y\)), for any integer C &gt; 0, the C-th order moment \(\mathbb{E}\)\([X^{⊗C}]≈\) \(∑_i {A_i^{⊗C}} \)\(\mathbb{E}[y{_i^C}]\) where \(A_i\) is the \(i\)-th column of \(A\). When this happens, matching the moments \(\mathbb{E}[X^{⊗C}]\) effectively determines the matrix \(A\) up to a column permutation.</p><p> 直观地，输出解卷积层可以写成线性运算符\（x = ay \），其中矩阵a的每列代表边缘颜色特征，y是稀疏向量，该稀疏向量确定用于油漆的哪个边缘颜色功能输出图像\（x \）。它在标准规律性条件下（例如几乎列正交的\（a \）和\（y \）的足够稀疏性），适用于任何整数c＆gt; 0，第c-tround songe \（\ mathbb {e} \）\（[x ^ {⊗c}]≈\）\（Σ_i{a_i ^ {⊗c}} \（\ mathbb {e [y {_i ^ c}] \）其中\（a_i \）是\（i \） -  th列（a \）。发生这种情况时，匹配瞬间\（\ mathbb {e} [x ^ {⊗c}]有效地确定矩阵\（a \）达到列排列。</p><p> In other words, to learn the output deconvolution layer, it suffices to ensure that the GAN generator matches moments between its output images and real images. On the theoretical side, we show that a ReLU-type discriminator can discriminate the mismatch between the moments on images, and thus through gradient descent ascent the output deconvolution layers can be efficiently learned. On the empirical side, Figure 6 (see also  previous work) shows that GANs are indeed doing moment matching during the earlier stage of training.</p><p> 换句话说，为了学习输出折耦层，它足以确保GaN发生器与其输出图像和真实图像之间的瞬间匹配。在理论方面，我们表明Relu型鉴别器可以区分图像上的时刻之间的错配，因此通过梯度下降开始，可以有效地学习输出折码层。在经验方面，图6（另见上文）表明，GAN确实在训练阶段期间匹配的时刻匹配。 </p><p>   To learn the first hidden layer (i.e. the deconvolutional operator from \((S_{1^*}\)=\(ReLU(Deconv(\)\(z))) \), we show it also suffices to match moments.</p><p>学习第一个隐藏层（即，来自\的解压缩运算符（（s_ {1 ^ *} \）= \（Relu（CofOnv（\）\（z）））\），我们表明它还足以匹配时刻。</p><p> Indeed, each coordinate of the first hidden layer \(S{_1^*}\) can be written as \([S{_1^*}]_i\)=\(ReLU(α_i⋅g_i-β_i)\) for \(g_i\) being standard Gaussian. Given the moments of \(S{_1^*}\) even just to any constant order, this uniquely determines \(α_i\) and \(β_i\) for every \(i\) as well as determines the pairwise correlations ⟨\(g_i\), \(g_j\)⟩. In other words, if the discriminator can discriminate the moments of hidden layer \(S_1^*\) from the target generator \(G^*\) and the moments of hidden layer \(S_1\) from the learner generator \(G\), then this effectively determines the first deconvolution layer Deconv(⋅) up to unitary transformation. The main difference from Phase (1) is that, unlike output images, the discriminator does not have access to hidden layer \(S_1^*\) and cannot directly implement this moment matching process.</p><p> 实际上，第一隐藏层\（s {_1 ^ *} \）的每个坐标都可以写成\（[s {_1 ^ *}] _ i \）= \（Relu（α_i⋅g_i-β_i）\） \（g_i \）是标准高斯。鉴于\（s {_1 ^ *} \）的时刻即使只是任何常量顺序，这个唯一地确定每个\（i \）的\（α_i\）和\（β_i\），以及确定成对相关性⟨ \（g_i \），\（g_j \）⟩。换句话说，如果鉴别者可以从目标生成器\（g ^ * \）和来自学习者生成器的隐藏层\（s_1 \）的瞬间区分隐藏层\（s_1 ^ * \）的时刻（g \），然后，这有效地确定了第一折叠层DECONV（⋅）直至整体变换。与阶段（1）的主要区别是，与输出图像不同，鉴别器没有访问隐藏层\（S_1 ^ * \），不能直接实现此刻匹配过程。</p><p> Fortunately, using the sparse coding property, Phase (1) tells us the discriminator can learn the output deconvolution layers (recall those are “edge-color features”) to descent accuracy, and thus it can use them to perform decoding of \(S_1^*\) from the real images \(X_1\). This requires the first layer of the discriminator to use (approximately) the same set of edge-color features comparing to the output layer of the generator and is consistent to what happens in practice (see Figure 7).</p><p> 幸运的是，使用稀疏的编码属性，阶段（1）告诉我们鉴别器可以学习输出折叠层（调用它们是“边缘颜色的特征”）到下降准确性，因此它可以使用它们来执行\（S_1）进行解码^ * \）来自真实图像\（x_1 \）。这需要第一层鉴别器使用（大约）与发电机的输出层相同的相同的边缘颜色特征，并且与在实践中发生的情况一致（见图7）。</p><p>  Putting Phases (1) and (2) together, the generator can learn not only the output deconvolution layer but also the first hidden deconvolution layer, and thus learn the distribution of \(X_1\), namely, the “most coarse grind” global structure of the image.</p><p>  将阶段（1）和（2）放在一起，发电机不仅可以学习输出折折叠层，还可以学习第一个隐藏的解卷积层，从而了解\（X_1 \）的分布，即“最粗磨”全局图像的结构。</p><p>  For any other hidden layer (i.e. the deconvolutional operator from \(S{_l^*}\)=\(ReLU(Deconv(\)\(S_{l-1}^*))\) for \(l\)≥2), since it captures more and more sharp details of the image, method of moments is no longer known to be sufficient.</p><p>  对于任何其他隐藏层（即来自\的解卷积运算符（s {_l ^ *} \）= \（Relu（reconv（\）\（s_ {l-1} ^ *））\（l \） ≥2），由于它捕获了图像的越来越尖锐的细节，因此不再知道矩的方法是足够的。</p><p> In this case, we show the discriminator learns to discriminate the statistical difference of the pair \(((X_{l-1},X_l))\) between real images and \(G\)’s output images. For example, the discriminator can discriminate the case that “a black dot in \(X_2\) always becomes an eye in \(X_3\)” or not. When this is so, the generator can learn how images \(X_{l-1}\) can perform forward super-resolution into \(X_l\), layer by layer for each \(l\)≥2.</p><p> 在这种情况下，我们示出了鉴别者学会鉴别真实图像和\（g \）输出图像之间的对（（（（x_ {l-1}，x_l））\）的统计差异。例如，鉴别器可以区分“\（X_2 \”始终成为\（X_3 \）的眼睛的情况。当这是这样的时候，发电机可以了解图像\（X_ {L-1} \）如何将超级分辨率执行到\（X_L \），每个\（L \）≥2的层。</p><p> The learning process of forward super-resolution should be reminiscent of supervised learning: the goal is to learn a one-hidden-layer neural network that takes as inputs lower-resolution images and outputs higher-resolution ones. Note one main difference from supervised learning is that, in \(S{_l^*}\)=\(ReLU(Deconv(\)\(S_{l-1}^*))\), both the inputs \(S_{l-1}^*\) and the outputs \(S{_l^*}\) are hidden features as opposed to low- and high-resolution images \(X_{l-1}\),\(X_l\). Again, thanks to the sparse coding property and Phase (1), we show that the discriminator can decode these hidden features from their corresponding images \(X_{l-1}\),\(X_l\). This allows GANs to simulate supervised learning and learn how the lower-level features are being combined to generate higher-level features efficiently, purely using gradient descent ascent.</p><p> 前向超分辨率的学习过程应让人想起监督学习：目标是学习一个隐藏层神经网络，其作为输入下分辨率图像并输出更高分辨率的地图。注意从监督学习的一个主要区别在于，在\（s {_l ^ *} \）= \（reconv（\）\（s_ {l-1} ^ *））中，\），输入\（ s_ {l-1} ^ * \）和输出\（s {_l ^ *}）是隐藏的特征，而不是低分辨线和高分辨率图像\（x_ {l-1} \），\（x_l \）。同样，由于稀疏编码属性和阶段（1），我们表明鉴别器可以从其相应的图像\（X_ {L-1} \），\（X_L \）中解码这些隐藏特征。这允许GANS模拟监督学习，并了解如何将较低级别的功能合并以有效地产生更高级别的功能，纯粹地使用梯度下降升级。 </p><p>  Another key reason that GANs can learn these super-resolution operations efficiently is that such operations are very local: to perform super-resolution on a patch of an image, the generator only needs to look at nearby patches instead of the entire image. Indeed, the global structure has already been taken care of in the lower resolution layers. In other words, learning each hidden layer can be done essentially patch-wise instead of over the entire image, as we illustrate in Figure 9 below.</p><p>GAN可以有效地学习这些超分辨率操作的另一个关键原因是此类操作非常本地：在图像的斑点上执行超分辨率，发电机仅需要查看附近的补丁而不是整个图像。实际上，全球结构已经在下层分辨率层中得到了处理。换句话说，学习每个隐藏层可以基本上修补而不是整个图像，如下图9所示。</p><p>   In our work, we also conduct an experiment showing that the features learned from lower-resolution images are indeed extremely helpful for learning higher-resolution images. In Figure 10, we consider layer-wise training of GAN, where we first train only the first hidden layer of the generator, then freeze it and train only the second layer of the generator, and so on. One can obviously see that the features learned from lower-resolution images can indeed be used to generate very non-trivial realistic images at higher resolutions. We believe this is strong evidence that the  forward super-resolution property makes the GAN training easy on distributions of real-life images, despite the worst-case hardness bounds.</p><p>   在我们的工作中，我们还开展了一个实验，表明从下层分辨率图像中学到的特征确实非常有助于学习更高分辨率的图像。在图10中，我们考虑了甘甘的层面训练，我们首先首先训练发电机的第一个隐藏层，然后冻结它并仅训练发电机的第二层，等等。显然可以看到从较低分辨率图像中学到的特征确实可以用于在更高的分辨率下产生非常非琐碎的现实图像。我们相信这是强有力的证据表明，前瞻性超决议性的财产使GAN培训容易掌握现实生活的分布，尽管有最糟糕的硬度界限。</p><p>   We point out that our work is still preliminary and far from capturing the  full picture of GANs. Most notably, we have focused on the “realizable” setting for proving our main theorem. Thus from a theoretical standpoint, it suffices to perform layer-wise learning: namely, first learn the distribution \(X_1\) and hidden layer \(S_1^*\) , then learn distribution \(X_2\) and hidden layer \(S_2^*\), and so on.</p><p>   我们指出，我们的作品仍然初步，远远捕获了全部的GAN的全貌。最值得注意的是，我们专注于证明我们主要定理的“可实现的”设置。因此，从理论上的角度来看，它足以执行层次的学习：即，首先学习分布\（X_1 \）和隐藏的层\（S_1 ^ * \），然后学习分发\（X_2 \）和隐藏层\（ s_2 ^ * \），等等。</p><p> As we show in Figure 10, layer-wise forward super-resolution is already performing much better than learning from random lower-level features. However, in practice we might consider the more challenging “agnostic” setting, where the target distributions of images are generated with error. If we cannot learn hidden layer sufficiently well during layer-wise training, this error propagates to deeper layers and may blow up if we perform layer-wise learning. This is okay for generating simple images, see Figure 10. For more complicated images, we expect the generator network to reduce over-fitting to such errors on lower-level layers, through training higher-level layers altogether. In other words, when training all layers together, we expect the lower-level layers to be able to also capture higher resolution details (as opposed to solely learning lower resolution images). This phenomenon is known as   Backward Feature Correction (BFC), which is equipped with provable guarantees in supervised deep learning. Extending the scope of BFC to GANs is an important next step.</p><p> 正如我们在图10所示，层面前进超分辨率已经比从随机较低级别的特征的学习更好地执行。但是，在实践中，我们可能会考虑更具挑战性的“不可知论”设置，其中图像的目标分布是错误的。如果我们在层展培训期间无法充分学习隐藏层，则此错误将传播到更深层层，如果我们执行层次学习，可能会爆炸。这是可以生成简单的图像，请参见图10.对于更复杂的图像，我们预计发电机网络通过完全训练更高级别的层，可以将发电机网络减少到较低级别的层上的这种误差。换句话说，当训练所有层次时，我们预期较低级别的层能够捕获更高的分辨率细节（而不是仅仅学习下部分辨率图像）。这种现象称为后向功能校正（BFC），其配备有监督深度学习的可证明的保证。扩展BFC的范围到GANS是一个重要的下一步。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.microsoft.com/en-us/research/blog/how-can-generative-adversarial-networks-learn-real-life-distributions-easily/">https://www.microsoft.com/en-us/research/blog/how-can-generative-adversarial-networks-learn-real-life-distributions-easily/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/现实生活/">#现实生活</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learn/">#learn</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/图像/">#图像</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>