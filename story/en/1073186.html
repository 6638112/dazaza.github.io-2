<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>GhOSt：Linux调度的快速灵活的用户空间委派GhOSt: Fast and Flexible User-Space Delegation of Linux Scheduling</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">GhOSt: Fast and Flexible User-Space Delegation of Linux Scheduling<br/>GhOSt：Linux调度的快速灵活的用户空间委派</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-14 23:49:28</div><div class="page_narrow text-break page_content"><p>This is one of the last papers I’m writing about from SOSP - I am trying out something new and publishing the queue of papers I plan on reading  here. These paper reviews can  be delivered weekly to your inbox, or you can subscribe to the  Atom feed. As always, feel free to reach out on  Twitter with feedback or suggestions!</p><p>这是我在SOSP上写的最后几篇论文之一——我正在尝试一些新的东西，并在这里发布我计划阅读的论文队列。这些论文评论可以每周发送到你的收件箱，也可以订阅Atom订阅源。一如既往，请随时在Twitter上发表反馈或建议！</p><p>  The ghOSt paper describes a system for implementing Linux scheduling   This paper is about CPU scheduling, not data center scheduling (like I covered in a  previous paper review).  policies in user space   See  What is difference between User space and Kernel space?. . Operating system scheduling is more complicated for data center workloads, as there are additional factors to consider when deciding what to run and when (like ensuring low latency for user queries). Previous research aims to take higher-level context about applications into consideration when making scheduling decisions   One example scheduler,  Shinjuku, is designed to reduce tail latency. The approach is able to achieve up to 6.6× higher throughput and 88% lower tail latency by implementing a custom scheduling policy. , with dramatic positive results.</p><p>这篇ghOSt论文描述了一个实现Linux调度的系统。这篇论文是关于CPU调度的，而不是数据中心调度的（就像我在之前的论文回顾中提到的）。用户空间中的策略查看用户空间和内核空间之间的区别。对于数据中心工作负载来说，操作系统调度更为复杂，因为在决定要运行什么时候以及何时（例如，确保用户查询的低延迟）时需要考虑其他因素。以前的研究旨在在做出调度决策时考虑应用程序的更高级别上下文。一个示例调度器新宿（Shinjuku）旨在减少尾部延迟。通过实施定制的调度策略，该方法能够实现高达6.6倍的吞吐量和88%的尾部延迟，取得了显著的积极成果。</p><p> Unfortunately, custom schedulers can be difficult to implement, deploy, and maintain.  Shinjuku is an example   The paper also cites a set of Dune-themed projects, like  Caladan and  Shenango as prior work in the space that runs into the coupling problem.  of a custom scheduler facing these problems - it is designed to reduce tail latency for data center applications, but requires tight coupling between an application and the scheduler. This tight coupling means that changes to the kernel could also unintentionally impact applications using the approach, potentially causing a brittle implementation with high ongoing maintenance costs.</p><p>不幸的是，定制调度器可能很难实现、部署和维护。新宿就是一个例子。论文还引用了一系列以沙丘为主题的项目，比如卡拉丹和雪南戈，作为该领域遇到耦合问题的前期工作。一个定制调度器面临这些问题的例子——它旨在减少数据中心应用程序的尾部延迟，但需要应用程序和调度器之间的紧密耦合。这种紧密耦合意味着对内核的更改也可能会无意中影响使用这种方法的应用程序，可能会导致脆弱的实现和高昂的持续维护成本。</p><p> ghOSt aims to address the problems faced by custom schedulers and those who implement them, while facilitating the dramatic performance and scalability gains workload-specific schedulers allow. The key to its approach is separating scheduling logic and the components that interact with the kernel. Custom schedulers, called  policies, are moved into user space.</p><p>ghOSt旨在解决定制调度器及其实现者所面临的问题，同时促进特定于工作负载的调度器所允许的显著性能和可伸缩性提升。其方法的关键是分离调度逻辑和与内核交互的组件。被称为策略的自定义调度程序被移动到用户空间中。</p><p> In contrast, relatively stable code that interacts directly with the Linux kernel remains in kernel-space, and exposes an API for the user-space schedulers to interact with. This split approach means that custom schedulers run just like any other application - as a result, they can be implemented in variety of languages, tested using existing infrastructure, and deployed a faster rate for a wider set of workloads.</p><p>相比之下，直接与Linux内核交互的相对稳定的代码仍保留在内核空间中，并为用户空间调度器提供了一个与之交互的API。这种拆分方法意味着定制调度器的运行与任何其他应用程序一样——因此，它们可以用多种语言实现，使用现有基础设施进行测试，并以更快的速度部署到更广泛的工作负载中。</p><p>  The paper makes three main contributions: design and implementation of a system that allows custom scheduling logic to run in user space, implementations of several custom schedulers using the system, and evaluation of the architecture (including in a production setting).</p><p>本文的主要贡献有三个：设计和实现一个允许自定义调度逻辑在用户空间中运行的系统，使用该系统实现多个自定义调度程序，以及评估体系结构（包括在生产环境中）。</p><p>   Implementing schedulers is hard because of the constraints posed on kernel code, like restrictions on languages   Support for  Rust in the kernel is a work in progress.  and debug tooling   See a previous discussion on difficulties with kernel debugging on  HN. .</p><p>由于对内核代码的限制，实现调度器很困难，比如对语言的限制，在内核中支持生锈是一项正在进行的工作。调试工具见前面关于HN上内核调试困难的讨论。</p><p> Deploying schedulers is even harder because upgrading a kernel requires   Technically, not all changes to the kernel require a  reboot.  a time-consuming multi-step process of shifting workloads and rebooting the machine. The potential for kernel upgrades to introduce performance regressions make the process more difficult.</p><p>部署调度程序更加困难，因为升级内核需要技术支持，而不是所有对内核的更改都需要重新启动。转移工作负载和重新启动机器的耗时多步骤过程。内核升级可能会导致性能下降，这使得这个过程更加困难。</p><p> Custom schedulers must schedule kernel-level threads, not user-level threads   See  Difference between user-level and kernel-supported threads?.  - scheduling user-level threads on top of kernel-level threads does not guarantee that the associated kernel-level threads are actually run   The paper notes two approaches that allow developers to overcome the limitations of user-level threads: “(1) Dedicate CPUs to the native threads running the user-threads, thus guaranteeing implicit control. However, this option wastes resources at low workload utilization, because the dedicated CPUs cannot be shared with another application (see §4.2), and requires extensive coordination around scaling capacity. Alternatively, developers can (2) stay at the mercy of the native thread scheduler, allowing CPUs to be shared, but ultimately losing the control over response time that they turned to a user-level runtime for.” .</p><p>自定义调度程序必须调度内核级线程，而不是用户级线程。请参阅用户级线程和内核支持线程之间的区别？-在内核级线程之上调度用户级线程并不能保证相关的内核级线程实际运行。本文指出了两种允许开发人员克服用户级线程限制的方法：“（1）将CPU专用于运行用户线程的本机线程，从而保证隐式控制。然而，此选项在低工作负载利用率下浪费资源，因为专用CPU无法与其他应用程序共享（请参见§4.2），并且需要围绕扩展容量进行广泛协调。或者，开发人员可以（2）任由本机线程调度器摆布，允许共享CPU，但最终会失去对响应时间的控制，而这正是他们转向用户级运行时的原因。" .</p><p> Custom schedulers tailored to specific workloads pose their own challenges because they do not adapt well to different use cases (not to mention their internals are complex and potentially not shared across multiple schedulers).</p><p>为特定工作负载定制的定制调度器也带来了挑战，因为它们不能很好地适应不同的用例（更不用说它们的内部结构很复杂，可能无法在多个调度器之间共享）。</p><p> Existing custom scheduling techniques are not sufficient, in particular Berkeley Packet Filter (BPF)   Julia Evans has a great post on  BPF, which was originally designed to capture and filter packets inside of the kernel. More recently,  eBPF extends the idea to other parts of the kernel - see  A thorough introduction to eBPF for more details on how BPF/eBPF works. There is also an exciting ecosystem building around eBPF tooling, like  Cilium and Isovalent, the company behind the tool, recently raised money from  Andreessen Horowitz. . While BPF programs are amazingly cool, they run synchronously and block the CPU - non-ideal from a performance perspective   It is worth noting that the paper does mention using BPF for fast-path .</p><p>现有的定制调度技术是不够的，尤其是伯克利数据包过滤器（BPF）Julia Evans在BPF上发表了一篇很好的文章，它最初设计用于捕获和过滤内核内部的数据包。最近，eBPF将这个想法扩展到了内核的其他部分——有关BPF/eBPF如何工作的更多细节，请参阅eBPF的全面介绍。围绕eBPF工具还有一个令人兴奋的生态系统建设，比如该工具背后的公司Cilium and Isopalent最近从Andreessen Horowitz那里筹集了资金。虽然BPF程序非常酷，但它们同步运行并阻塞CPU——从性能角度来看，这并不理想。值得注意的是，本文确实提到使用BPF实现快速路径。</p><p>  Custom scheduling logic should be easy to implement and test: separating scheduling logic from the kernel simplifies development and testing.</p><p>自定义调度逻辑应该易于实现和测试：将调度逻辑与内核分离可以简化开发和测试。</p><p> It should be possible to easily create scheduling logic for many different use cases: unlike previous specialized schedulers built into the kernel,  ghOSt aims to be a generic platform that schedulers can be built on top of.</p><p>应该可以轻松地为许多不同的用例创建调度逻辑：与以前内置在内核中的专用调度程序不同，ghOSt旨在成为一个通用平台，可以在其上构建调度程序。</p><p> Scheduling should be able to operate across multiple CPUs: existing Linux schedulers make per-CPU scheduling decisions and it is difficult to execute scheduling decisions over a set of CPUs to optimize for other properties, like tail latency   The paper cites a number of previous systems (like  Shenango: Achieving high CPU efficiency for latency-sensitive datacenter workloads) that achieve their goals by scheduling across multiple CPUs .</p><p>调度应该能够跨多个CPU运行：现有的Linux调度程序会做出每个CPU的调度决策，并且很难在一组CPU上执行调度决策以优化其他属性，像tail latency一样，本文引用了许多以前的系统（比如Shenango：为延迟敏感的数据中心工作负载实现高CPU效率），这些系统通过跨多个CPU进行调度来实现其目标。</p><p> Non-disruptive updates and fault isolation: it should be easy to deploy scheduling logic like one would with other tasks running on a machine, allowing updates without requiring a reboot. Furthermore, failures or regressions in scheduling policies should not crash the whole machine.</p><p>无中断更新和故障隔离：应该很容易部署调度逻辑，就像在机器上运行其他任务一样，允许在不需要重新启动的情况下进行更新。此外，调度策略中的故障或倒退不应使整个机器崩溃。</p><p>  To achieve the goals of the system, ghOSt introduces  policies (custom scheduling logic).  Policies are executed in user-space and associated scheduling decisions are communicated to the kernel.</p><p>为了实现系统的目标，ghOSt引入了策略（自定义调度逻辑）。策略在用户空间中执行，相关的调度决策被传送到内核。</p><p>  Policies (and their scheduling decisions) propagate over three main components running across kernel and user space:</p><p>策略（及其调度决策）在运行于内核和用户空间的三个主要组件上传播：</p><p> The  ghOSt scheduling class   Here is a great article about scheduling classes and Linux’s  Completely Fair Scheduler. There is also the  man page about the related  sched system call.  runs inside of the Linux kernel and provides a syscall interface that other components use to communicate scheduling decisions.</p><p>这里的ghOSt调度类是一篇关于调度类和Linux的完全公平调度程序的伟大文章。还有一个关于相关sched系统调用的手册页。在Linux内核内部运行，并提供一个系统调用接口，其他组件使用该接口来传达调度决策。</p><p> Agents run  policies (custom scheduling logic) in user-space, and make scheduling decisions that they communicate to the  ghOSt scheduling class running in kernel-space.</p><p>代理在用户空间中运行策略（自定义调度逻辑），并做出调度决策，并与内核空间中运行的ghOSt调度类通信。</p><p> Enclaves are groups of  agents. Each  enclave has a primary agent that makes the scheduling decisions. Assigning multiple  agents to an enclave provides redundancy in the case of the primary agent failing.</p><p>飞地是一群特工。每个飞地都有一个主要代理，负责制定调度决策。将多个代理分配给一个enclave可以在主代理失败的情况下提供冗余。</p><p>   ghOSt components running in kernel or user-space need a way to provide information and feedback to each other. The paper discusses the two primary communication flows:  kernel-to-agent and  agent-to-kernel.</p><p>在内核或用户空间中运行的ghOSt组件需要一种相互提供信息和反馈的方式。本文讨论了两种主要的通信流：内核到代理和代理到内核。</p><p>  In the  kernel-to-agent flow, the  kernel communicates to  agents using messages and message queues   Definition of the messages  here. . The kernel sends messages on queues when events happen in the kernel that could impact scheduling decisions. Each CPU has an associated queue, and each queue is associated with an enclave   Not every agent has a message queue because in some configurations there is a single primary agent for the enclave that is receiving information from the kernel - reference the enclave diagram above for a visual representation of this idea. . While there are several existing queue approaches (including  io_uring or  BPF ring buffers), not all kernel versions support them - the authors argue that this makes ghOSt’s queue abstraction necessary.</p><p>在内核到代理的流程中，内核使用消息和消息队列与代理进行通信。此处定义了消息。当内核中发生可能影响调度决策的事件时，内核会在队列上发送消息。每个CPU都有一个关联的队列，每个队列都与一个enclave关联。并非每个代理都有一个消息队列，因为在某些配置中，enclave只有一个主代理从内核接收信息——请参考上面的enclave图，以获得此想法的可视化表示。虽然有几种现有的队列方法（包括io__-uring或BPF环形缓冲区），但并不是所有的内核版本都支持它们——作者认为这使得ghOSt的队列抽象成为必要。</p><p> In the  agent-to-kernel direction, the  agent communicates by making system calls to communicate scheduling decisions and to perform management operations on the shared queue. To send scheduling decisions, the  agent creates and commits transactions (like  TXN_CREATE() and  TXNS_COMMIT()). Transactions are important because they allow a policy to make scheduling decisions across a range of CPUs, ensuring all or none succeed, while batching scheduling information - batching is critical because it limits the number of interrupts that impact the to-be-scheduled CPUs (as the kernel component of ghOSt needs to respond to agent transactions).</p><p>在代理到内核的方向上，代理通过进行系统调用进行通信，以传达调度决策，并对共享队列执行管理操作。为了发送调度决策，代理创建并提交事务（比如TXN_CREATE（）和TXNS_COMMIT（））。事务非常重要，因为它们允许策略跨一系列CPU做出调度决策，确保所有或所有事务都能成功，批处理调度信息——批处理非常关键，因为它限制了影响待调度CPU的中断数量（因为ghOSt的核心组件需要响应代理事务）。</p><p> Lastly, there is a challenge to both  kernel-to-agent and  agent-to-kernel communication: keeping up to date with the state of the system. The kernel needs to ensure that it doesn’t execute out of date scheduling decisions, and the agent need to make sure that it doesn’t make scheduling decisions based on an old state of the world. The key piece of information used to track state is a  sequence number that exists for every agent.</p><p>最后，内核到代理和代理到内核的通信都面临一个挑战：跟上系统的状态。内核需要确保它不会执行过时的调度决策，代理需要确保它不会根据旧的世界状态做出调度决策。用于跟踪状态的关键信息是存在于每个代理的序列号。</p><p> In  kernel-to-agent commmunication, the kernel provides the  sequence number to agents in each message, and in a shared memory region. The sequence number in shared memory is updated by the kernel whenever it publishes a new message. The agent consumes the  sequence number from shared memory when reading messages from the queue, comparing the value to the  sequence number in shared memory. When the sequence number from consumed messages matches the value in shared memory, the agent knows it has read an up to date state.</p><p>在内核到代理的通信中，内核在每条消息和共享内存区域中为代理提供序列号。每当内核发布新消息时，共享内存中的序列号就会被更新。当从队列中读取消息时，代理会使用共享内存中的序列号，并将该值与共享内存中的序列号进行比较。当已使用消息的序列号与共享内存中的值匹配时，代理知道它已读取最新状态。</p><p> In  agent-to-kernel communication, the agent includes the  sequence number when sending scheduling decisions (via transactions) to the kernel. The kernel compares the  sequence number from the agent’s transaction with the most recent sequence number the kernel is aware of. If the transaction’s sequence number is too old, the kernel doesn’t execute the scheduling decision.</p><p>在代理到内核的通信中，代理在向内核发送调度决策（通过事务）时包含序列号。内核将代理事务中的序列号与内核知道的最新序列号进行比较。如果事务的序列号太旧，内核就不会执行调度决策。</p><p>  To evaluate ghOSt, the paper considers the overheads associated with the system, compares ghOSt to previous custom scheduler implementations, and evaluates the system in production.</p><p>为了评估ghOSt，本文考虑了与系统相关的开销，将ghOSt与以前的定制调度器实现进行了比较，并评估了生产中的系统。</p><p>  To evaluate the overheads of the system, the paper includes microbenchmarks that show the time spent in the different parts of the scheduling system, showing that it is competitive.</p><p>为了评估系统的开销，本文包括了微基准，显示了在调度系统的不同部分花费的时间，表明它具有竞争力。</p><p>  The paper also determines the performance of a global scheduler (that schedules all cores on a system) implemented with ghOSt - previous research shows the potential advantage of this approach as the scheduler has more complete knowledge of the system. The evaluation shows that ghOSt is able to scale to millions of transactions, even when responsible for many CPUs.</p><p>本文还确定了使用ghOSt实现的全局调度器（对系统上的所有核心进行调度）的性能——之前的研究表明，随着调度器对系统有更全面的了解，这种方法的潜在优势。评估表明，ghOSt能够扩展到数百万个事务，即使它负责许多CPU。</p><p>   Next, the paper compares ghOSt to Shinjuku   See the  Shinjuku paper. , an example of a custom scheduling system tailored to reduce tail latency. The goal of this evaluation is to see whether  ghOSt performs similarly to a custom scheduler (which theoretically could achieve higher performance by using tailored optimization techniques). Shinjuku has a number of differences from  ghOSt - it uses dedicated resources (spinning threads that consume all of a CPU or set of CPUs), is constrained to a physical set of cores, and takes advantage of virtualization features to increase performance (like  posted interrupts). The authors also port the Shinjuku scheduling policy itself so that it is compatible with ghOSt.</p><p>接下来，这篇论文将鬼魂与新宿进行了比较。参见新宿论文，这是一个定制的调度系统的例子，可以减少尾部延迟。此评估的目标是查看ghOSt的性能是否与定制调度器类似（理论上，通过使用定制的优化技术，定制调度器可以实现更高的性能）。新宿与ghOSt有很多不同之处——它使用专用资源（占用全部CPU或一组CPU的旋转线程），受限于一组物理内核，并利用虚拟化功能来提高性能（如发布中断）。作者还移植了新宿调度策略本身，以便与ghOSt兼容。</p><p> The two systems run a generated workload, “in which each request includes a GET query to an in-memory RocksDB key-value store and performs a small amount of processing”.</p><p>这两个系统运行生成的工作负载，“其中每个请求都包括对内存中RocksDB键值存储的GET查询，并执行少量处理”。</p><p>  ghOSt is competitive with Shinjuku for 𝜇s-scale tail workloads, even though its Shinjuku policy is implemented in 82% fewer lines of code than the custom Shinjuku data plane system. ghOSt has slightly higher tail latencies than Shinjuku at high loads and is within 5% of Shinjuku’s saturation throughput.</p><p>鬼魂与新宿竞争𝜇s规模的尾部工作负载，尽管其新宿策略的代码行比定制的新宿数据平面系统少82%。ghOSt在高负载下的尾部延迟略高于新宿，在新宿饱和吞吐量的5%以内。</p><p>   Lastly, the paper runs a production workload against ghOSt and compares the results to the same workload executed by machines using the completely fair scheduler (CFS)   More info on the Completely Fair Scheduler  here - on the older side, but seems like it was updated relatively recently. .</p><p>最后，本文针对ghOSt运行了一个生产工作负载，并将结果与使用完全公平调度程序（CFS）的机器执行的相同工作负载进行了比较。关于完全公平调度程序的更多信息，请参阅此处的“较旧”部分，但它似乎是最近更新的。</p><p> The workload contains three query types (CPU and memory bound, IO and memory bound, and CPU-bound) - ghOSt is able to reduce tail-latency for the first two types of requests, but doesn’t have a huge impact for the third   The paper does note that it is possible to impact compute bound tasks by extending the ghOSt policy with similar logic to what Linux’s CFS contains around  nice values. .</p><p>工作负载包含三种查询类型（CPU和内存绑定、IO和内存绑定以及CPU绑定）-ghOSt能够减少前两种请求的尾部延迟，但是对第三个没有太大的影响。论文确实指出，通过使用类似于Linux的CFS包含的nice值的逻辑来扩展ghOSt策略，可以影响计算绑定的任务。</p><p> What stood out to me the most about this section is actually ghOSt’s impact on developer productivity:</p><p>在我看来，这一部分最突出的是ghOSt对开发人员生产力的影响：</p><p> When developing a kernel scheduler, the write-test-write cycle includes (a) compiling a kernel (up to 15 minutes), (b) deploying the kernel (10-20 minutes), and (c) running the test (1 hour due to database initialization following a reboot). As a result, the enthusiastic kernel developer experiments with 5 variants per day. With ghOSt, compiling, deploying and launching the new agent is comfortably done within one minute.</p><p>在开发内核调度器时，写测试写周期包括（a）编译内核（最多15分钟），（b）部署内核（10-20分钟），以及（c）运行测试（由于重启后数据库初始化，需要1小时）。因此，这位热心的内核开发人员每天都要用5种变体进行实验。有了ghOSt，新代理的编译、部署和启动可以轻松地在一分钟内完成。</p><p>  The ghOSt paper builds on a body of previous research that demonstrates how critical scheduling is to the scalability and performance of datacenter workloads. Scheduling is far from a solved problem, especially because of the “rise of the killer microsecond” and new device types - I’m looking forward to following along future work on the  ghOSt open source project!</p><p>ghOSt论文建立在之前的一系列研究的基础上，这些研究证明了调度对数据中心工作负载的可伸缩性和性能有多么重要。调度还远远不是一个解决的问题，尤其是因为“杀手微秒的崛起”和新的设备类型——我期待着在ghOSt开源项目的未来工作中继续跟进！</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/fast/">#fast</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/内核/">#内核</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>