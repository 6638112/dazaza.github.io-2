<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>评估搜索算法 Evaluating Search Algorithms</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Evaluating Search Algorithms<br/>评估搜索算法 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-02 04:41:02</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/98dc6da09855eb47eb7be88af21719dc.png"><img src="http://img2.diglog.com/img/2021/4/98dc6da09855eb47eb7be88af21719dc.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Over 2 million users visit  Shopify’s Help Center every month to find help solving a problem. They come to the Help Center with different motives: learn how to set up their store, find tips on how to market, or get advice on troubleshooting a technical issue. Our search product helps users narrow down what they’re looking for by surfacing what’s most relevant for them. Algorithms empower search products to surface the most suitable results, but how do you know if they’re succeeding at this mission?</p><p>超过200万用户每月访问Shopify的帮助中心，以找到解决问题。他们来到不同的动机帮助中心：了解如何设置他们的商店，找到关于如何上市的提示，或者获得关于技术问题的故障排除建议。我们的搜索产品可帮助用户缩小他们通过对它们最相关的东西寻找的内容。算法授权搜索产品到表面最合适的结果，但你如何知道他们是否在这个任务中取得成功？</p><p> Below, we’ll share the three-step framework we built for evaluating new search algorithms. From collecting data using Kafka and annotation, to conducting offline and online A/B tests, we’ll share how we measure the effectiveness of a search algorithm.</p><p> 下面，我们将分享我们为评估新搜索算法而构建的三步框架。从使用Kafka和注释收集数据，在线和在线A / B测试，我们将分享我们如何测量搜索算法的有效性。</p><p>  Search is a difficult product to build. When you input a search query, the search product sifts through its entire collection of documents to find suitable matches. This is no easy feat as a search product’s collection and matching result set might be extensive. For example, within the Shopify Help Center’s search product lives thousands of articles, and a search for “shipping” could return hundreds.</p><p>  搜索是一个难以构建的产品。当您输入搜索查询时，搜索产品通过其整个文档集中筛选，以查找合适的匹配。这不是一个简单的壮举，因为搜索产品的收集和匹配结果集可能是广泛的。例如，在Shopify Help Center的搜索产品中，千篇文章，搜索“运输”可以返回数百个。</p><p> We use an algorithm we call  Vanilla Pagerank to power our search. It boosts results by the total number of views an article has across all search queries. The problem is that it also surfaces non-relevant results. For example, if you conducted a search for “delete discounts” the algorithm may prefer results with the keywords “delete” or “discounts”, but not necessarily results on “deleting discounts”.</p><p> 我们使用算法致电香草PageRank来为我们的搜索供电。它通过跨所有搜索查询的总数促使结果。问题是它还曲面突出了无相关的结果。例如，如果您进行了“删除折扣”，则算法可能更愿意使用关键字“删除”或“折扣”，但不一定会导致“删除折扣”。</p><p> We’re always trying to improve our users’s experience by making our search algorithms smarter. That’s why we built a new algorithm,  Query-specific Pagerank, which aims to boost results with high click frequencies (a popularity metric) from historic searches containing the search term. It basically boosts the most frequently clicked-on results from similar searches.</p><p> 我们始终努力通过使我们的搜索算法更智能地提高用户的经验。这就是为什么我们构建了一种新的算法查询特定的PageRank，这旨在通过包含搜索词的历史搜索来提高高点击频率（一种流行度量）的结果。它基本上提升了来自类似搜索的最常用的结果。</p><p> The challenge is any change to the search algorithms might improve the results for some queries, but worsen others. So to have confidence in our new algorithm, we use data to evaluate its impact and performance against the existing algorithm. We implemented a simple three-step framework for evaluating experimental search algorithms.</p><p> 挑战是搜索算法的任何改变可能会改善一些查询的结果，但却恶化了其他疑问。因此，为了对我们的新算法有信心，我们使用数据来评估其对现有算法的影响和性能。我们实施了一个简单的三步框架，用于评估实验搜索算法。</p><p>  Before we evaluate our search algorithms, we need a “ground truth” dataset telling us which articles are relevant for various intents. For Shopify’s Help Center search, we use two sources: Kafka events and annotated datasets.</p><p>  在我们评估搜索算法之前，我们需要一个“地面真理”数据集告诉我们哪些文章与各种意图相关。对于ShopIfy的帮助中心搜索，我们使用两个来源：Kafka事件和注释数据集。 </p><p>  Users interact with search products by entering queries, clicking on results, or leaving feedback. By using a messaging system like Kafka, we collect all live user interactions in schematized streams and model these events in an ETL pipeline. This culminates in a search fact table that aggregates facts about a search session based on behaviours we’re interested in analyzing.</p><p>用户通过输入查询，单击“结果”或“留下反馈”来互动与搜索产品进行交互。通过使用像Kafka这样的消息传递系统，我们将在ETL管道中收集示意图流中的所有Live用户交互并在这些事件中进行模型。这在搜索事实表中达到了基于我们对分析的行为的行为聚集了关于搜索会话的事实。</p><p>  With data generated from Kafka events, we can continuously evaluate our online metrics and see near real-time change. This helps us to:</p><p>  通过从Kafka事件生成的数据，我们可以不断评估我们的在线指标并查看近实时更改。这有助于我们：</p><p> Assign users to A/B tests in real time. We can set some users’s experiences to be powered by search algorithm A and others by algorithm B.</p><p> 将用户实时分配给A / B测试。我们可以通过算法B通过搜索算法A和其他人来设置一些用户的经验。</p><p>  Annotation is a powerful tool for generating labeled datasets. To ensure high-quality datasets within the Shopify Help Center, we leverage the expertise of the Shopify Support team to annotate the relevance of our search results to the input query. Manual annotation can provide high-quality and trustworthy labels, but can be an expensive and slow process, and may not be feasible for all search problems. Alternatively,  click models can build judgments using data from user interactions. However, for the Shopify Help Center, human judgments are preferred since we value the expert ratings of our experienced Support team.</p><p>  注释是一种强大的工具，可以生成标记的数据集。为确保Shopify Help Center内的高质量数据集，我们利用Shopify支持团队的专业知识向输入查询注释我们的搜索结果的相关性。手动注释可以提供高质量和值得信赖的标签，但可以是昂贵且慢的过程，可能对所有搜索问题不可行。或者，单击模型可以使用来自用户交互的数据构建判断。但是，对于Shopify帮助中心，人类判断是首选，因为我们重视我们经验丰富的支持团队的专家评级。</p><p>  A query is paired with a document or a result set that might represent a relative match.</p><p>  查询与文档或可能表示相对匹配的结果集配对。</p><p> An annotator judges the relevance of the document to the question and assigns the pair a rating.</p><p> 注释器将文档与问题的相关性判断，并将该货币对分配评级。</p><p>   Binary ratings: Given a user’s query and a document, an annotator can answer the question  Is this document relevant to the query? by providing a binary answer (1 = yes, 0 = no).</p><p>   二进制评级：给定用户的查询和文档，注释器可以回答问题是与查询相关的此文档？通过提供二进制答案（1 =是，0 =否）。 </p><p> Scale ratings: Given a user’s query and a document, a rater can answer the question  How  relevant is this document to the query? on a scale (1 to 5, where 5 is the most relevant). This provides interval data that you can turn into categories, where a rating of 4 or 5 represents a hit and anything lower represents a miss.</p><p>尺度评级：给定用户的查询和文档，rater可以回答问题是如何对查询相关的？在一个范围内（1到5，其中5个是最相关的）。这提供了可以变成类别的间隔数据，其中4或5的评级代表了一个命中，任何更低的代表错过。</p><p> Ranked lists: Given a user’s query and a set of documents, a rater can answer the question  Rank the documents from most relevant to least relevant to the query. This provides ranked data, where each query-document pair has an associated rank.</p><p> 排名列表：给定用户的查询和一组文档，rater可以回答问题对待与查询最不相关的文档等级。这提供了排名的数据，其中每个查询文档对具有相关等级。</p><p> The design of your annotation options depends on the evaluation measures you want to employ. We used  Scale Ratings with a worded list (bad, ok, good, and great) that provides explicit descriptions on each label to simplify human judgment. These labels are then quantified and used for calculating performance scores. For example, when conducting a search for “shipping”, our Query-specific Pagerank algorithm may return documents that are more relevant to the query where the majority of them are labelled “great” or “good”.</p><p> 注释选项的设计取决于您想要采用的评估措施。我们使用了符号评级，具有措辞列表（坏，确定，好的），在每个标签上提供明确的描述，以简化人类判断。然后量化这些标签并用于计算性能分数。例如，在进行搜索“运输”时，我们的查询特定的PageRank算法可能会返回与查询更相关的文档，其中大多数标记为“伟大”或“好”。</p><p> One thing to keep in mind with annotation is dataset staleness. Our document set in the Shopify Help Center changes rapidly, so datasets can become stale over time. If your search product is similar, we recommend re-running annotation projects on a regular basis or augmenting existing datasets that are used for search algorithm training with unseen data points.</p><p> 用注释记住的一件事是数据集僵局。我们的文档在Shopify Help Center中设置快速更改，因此数据集会随着时间的推移变得陈旧。如果您的搜索产品类似，我们建议定期重新运行注释项目或增强用于搜索算法培训的现有数据集，并使用未经说明的数据点进行培训。</p><p>  After collecting our data, we wanted to know whether our new search algorithm, Query-specific Pagerank, had a positive impact on our ranking effectiveness  without impacting our users. We did this by running an offline evaluation that uses relevance ratings from curated datasets to judge the effectiveness of search results before launching potentially-risky changes to production. By using offline metrics, we run thousands of historical search queries against our algorithm variants and use the labels to evaluate statistical measures that tell us how well our algorithm performs. This enables us to iterate quickly since we simulate thousands of searches per minute.</p><p>  在收集我们的数据后，我们想知道我们的新搜索算法，特定于特定的PageRank，对我们的排名效果产生了积极影响，而不会影响用户。我们通过运行一个离线评估来实现这一目标，该评估使用来自策划数据集的相关性评估来判断搜索结果的有效性，然后在发起潜在风险的生产变化之前。通过使用脱机指标，我们对我们的算法变体运行数千个历史搜索查询，并使用标签来评估告诉我们算法如何执行的统计措施。这使我们能够迅速迭代，因为我们模拟每分钟数千搜索。</p><p> There are a number of measures we can use, but we’ve found  Mean Average Precision and  Normalized Discounted Cumulative Gain to be the most useful and approachable for our stakeholders.</p><p> 我们可以使用许多措施，但我们发现平均平均精度和标准化的折扣累积增益是我们利益相关者最有用和最平衡的。</p><p>  Mean Average Precision (MAP) measures the relevance of each item in the results list to the user’s query with a specific cutoff N. As queries can return thousands of results, and only a few users will read all of them, only top N returned results need to be examined. The top N number is usually chosen arbitrarily or based on the number of paginated results. Precision@N is the percentage of relevant items among the first N recommendations. MAP is calculated by averaging the AP scores for each query in our dataset. The result is a measure that penalizes returning irrelevant documents before relevant ones. Here is an example of how MAP is calculated:</p><p>  平均平均精度（MAP）测量结果列表中的每个项目的相关性，以特定的截止N.返回数千个结果，只有少数用户读取所有结果，只有顶部n返回结果需要审查。顶部n号通常是任意选择的或基于分枝术结果的数量。 Precision @ n是第一个建议中相关项目的百分比。通过对我们数据集中的每个查询进行平均来计算地图。结果是一种衡量相关文件前返回无关文件的措施。这是如何计算地图的示例： </p><p>   To compute MAP, you need to classify if a document is a good recommendation by determining a cutoff score. For example, document and search query pairs that have  ok,  good, and  great labels (that is scores greater than 0) will be categorized as relevant. But the differences in the relevancy of  ok and  great pairs will be neglected.</p><p>为了计算地图，您需要对文档进行分类，如果文档是通过确定截止分数的好建议。例如，具有OK，WEAD和GREAT标签的文档和搜索查询对（即大于0的分数）将被分类为相关。但是，OK和大对相关性的差异将被忽视。</p><p> Discounted Cumulative Gain (DCG) addresses this drawback by maintaining the non-binary score while adding a logarithmic reduction factor—the denominator of the DCG function to penalize the recommended items with lower positions in the list. DCG is a measure of ranking quality that takes into account the position of documents in the results set.</p><p> 折扣累积增益（DCG）通过维护非二进制分数来解决该缺点，同时添加对数减少因素 -  DCG函数的分母来惩罚列表中具有较低位置的推荐项目。 DCG是一种排名质量的衡量标准，但考虑到结果集中的文档的位置。</p><p>  One issue with DCG is that the length of search results differ depending on the query provided. This is problematic because the more results a query set has, the better the DCG score, even if the ranking doesn’t make sense. Normalized Discounted Cumulative Gain Scores (NDCG) solves this problem. NDCG is calculated by dividing DCG by the maximum possible DCG score—the score calculated from the sorted search results.</p><p>  DCG的一个问题是搜索结果的长度因提供的查询而异。这是有问题的，因为结果越多，查询集具有，即使排名没有意义，DCG得分也越好。标准化的折扣累计增益分数（NDCG）解决了这个问题。通过将DCG除以最大可能的DCG分数来计算NDCG  - 从分类的搜索结果计算的分数。</p><p> Comparing the numeric values of offline metrics is great when the differences are significant. The higher value, the more successful the algorithm. However, this only tells us the ending of the story. When the results aren’t significantly different, the insights we get from the metrics comparison are limited. Therefore, understanding  how we got to the ending is also important for future model improvements and developments. You gather these insights by breaking down the composition of queries to look at:</p><p> 比较差异很大的离线度量的数值很大。较高的值，算法越成功。然而，这只告诉我们故事的结束。当结果没有显着差异时，我们从度量标准比较中获得的见解是有限的。因此，了解我们如何实现结束对于未来的模型改进和发展也很重要。您通过分解查询的构成来收集这些见解，以查看：</p><p> Commonalities :   Understanding the queries that consistently have positive impacts on the algorithm performance, and finding the commonality among these queries, can help you determine the limitations on an algorithm.</p><p> 常见性：了解一致对算法性能产生积极影响的查询，并在这些查询中找到共性，可以帮助您确定算法的限制。</p><p>  We conducted a deep dive analysis on evaluating offline metrics using MAP and NDCG to assess the success of our new Query-specific Pagerank algorithm. We found that our new algorithm returned higher scored documents more frequently and had slightly better scores in both offline metrics.</p><p>  我们对使用地图和NDCG评估离线指标进行深度潜水分析，以评估我们新的查询特定PageRank算法的成功。我们发现，我们的新算法更频繁地返回了更高的分数文档，并且在离线指标中具有稍微更好的分数。</p><p>  Next, we wanted to see how our users interact with our algorithms by looking at online metrics. Online metrics use search logs to determine how real search events perform. They’re based on understanding users’ behaviour with the search product and are commonly used to evaluate A/B tests.</p><p>  接下来，我们希望通过查看在线指标，了解用户如何与我们的算法进行互动。在线指标使用搜索日志来确定真实搜索事件的执行方式。它们基于了解用户与搜索产品的行为，通常用于评估A / B测试。 </p><p> The metrics you choose to evaluate the success of your algorithms depends on the goals of your product. Since the Shopify Help Center aims to provide relevant information to users looking for assistance with our platform, metrics that determine success include:</p><p>您选择评估算法成功的指标取决于您的产品的目标。由于Shopify帮助中心旨在向用户提供相关信息，以寻求与我们的平台的帮助，确定成功的指标包括：</p><p>  When running an A/B test, we need to define these measures and determine how we expect the new algorithm to move the needle. Below are some common online metrics, as well as product success metrics, that we used to evaluate how search events performed:</p><p>  运行A / B测试时，我们需要定义这些措施并确定我们如何预期新算法移动针。以下是一些常见的在线指标以及产品成功指标，我们用于评估搜索事件的执行方式：</p><p> Click-through rate (CTR): The portion of users who clicked on a search result when surfaced. Relevant results should be clicked, so we want a  high CTR.</p><p> 点击率（CTR）：在浮出水面时单击搜索结果的用户部分。应单击相关结果，因此我们想要一个高CTR。</p><p> Average rank: The average rank of clicked results. Since we aim for the most relevant results to be surfaced first, and clicked, we want to have a  low average rank.</p><p> 平均排名：单击结果的平均等级。由于我们的目标是首先浮出水面的最相关结果，并点击，我们希望具有较低的平均排名。</p><p> Abandonment:When a user has intent to find help but they didn’t interact with search results, didn’t contact us, and wasn’t seen again. We always expect some level of abandonment due to bots and spam messages (yes, search products get spam too!), so we want this to be  moderately low.</p><p> 放弃：当用户意图找到帮助但他们没有与搜索结果互动时，没有联系我们，并没有再次见到。由于机器人和垃圾邮件，我们总是期望一些级别的放弃（是的，搜索产品也得到垃圾邮件！），所以我们希望这可以适度低。</p><p> Deflection: Our search is a success when users don’t have to contact support to find what they’re looking for. In other words, the user was deflected from contacting us. We want this metric to be  high, but in reality we want people to contact us when it’s what is best for their situation, so deflection is a bit of a nuanced metric.</p><p> 偏转：我们的搜索是成功的，当用户不必联系支持以找到他们正在寻找的东西。换句话说，用户从联系我们偏转。我们希望这个度量标准很高，但实际上我们希望人们在其情况最适合其情况下与我们联系，因此偏转是一个细微的公制。</p><p> We use the Kafka data collected in our first step to calculate these metrics and see how successful our search product is over time, across user segments, or between different search topics. For example, we study CTR and deflection rates between users in different languages. We also use A/B tests to assign users to different versions of our product to see if our new version significantly outperforms the old.</p><p> 我们使用我们的第一步中收集的Kafka数据来计算这些指标，并了解我们的搜索产品随着时间的推移，用户段或不同的搜索主题之间的成功。例如，我们在不同语言中研究用户之间的CTR和偏转率。我们还使用A / B测试将用户分配给我们产品的不同版本，以查看我们的新版本是否显着优于旧的。 </p><p>  A/B testing search is similar to other A/B tests you may be familiar with. When a user visits the Help Center, they are assigned to one of our experiment groups, which determines which algorithm their subsequent searches will be powered by. Over many visits, we can evaluate our metrics to determine which algorithm outperforms the other (for example, whether algorithm A has a significantly higher click-through rate with search results than algorithm B).</p><p>A / B测试搜索类似于您可能熟悉的其他A / B测试。当用户访问帮助中心时，它们被分配给我们的实验组之一，该组确定其后续搜索的哪些算法将由其供电。在许多访问中，我们可以评估我们的指标，以确定哪些算法优于另一算法（例如，算法A是否具有比算法B的搜索结果具有明显高的咔哒率。</p><p>  We conducted an online A/B test to see how our new Query-specific Pagerank algorithm measured against our existing algorithm, with half of our search users assigned to group A (powered by Vanilla Pagerank) and half assigned to group B (powered by Query-specific Pagerank). Our results showed that users in group B were:</p><p>  我们进行了在线A / B测试，看看我们的新查询特定的PageRank算法如何针对我们现有的算法测量，其中一半的搜索用户分配给A组（Vanilla PageRank）和分配给B组（由查询供电） - 特价Pagerank）。我们的结果表明，B组中的用户是：</p><p>  Essentially, group B users were able to find helpful articles with less effort when compared to group A users.</p><p>  基本上，B组用户能够在与用户组相比时找到有用的努力。</p><p>  After using our evaluation framework to measure our new algorithm against our existing algorithm, it was clear that our new algorithm outperformed the former in a way that was meaningful for our product. Our metrics showed our experiment was a success, and we were able to replace our Vanilla Pagerank algorithm with our new and improved Query-specific Pagerank algorithm.</p><p>  使用我们的评估框架来衡量我们现有算法的新算法，很明显，我们的新算法以对我们产品有意义的方式表现优于前者。我们的指标表明我们的实验取得了成功，我们能够用我们的新的和改进的查询特定PageRank算法取代我们的Vanilla PageRank算法。</p><p> If you’re using this framework to evaluate your search algorithms, it’s important to note that even a failed experiment can help you learn and identify new opportunities. Did your offline or online testing show a decrease in performance? Is a certain subset of queries driving the performance down? Are some users better served by the changes than other segments? However your analysis points, don’t be afraid to dive deeper to understand what’s happening. You’ll want to document your findings for future iterations.</p><p> 如果您使用此框架来评估搜索算法，重要的是要注意，即使是失败的实验也可以帮助您学习和识别新的机会。您的离线或在线测试是否显示了性能下降？是一个询问的某个子集，驱动性能下降吗？一些用户是否比其他部分更好地服务？然而，您的分析点，不要害怕深入了解发生的事情。您将想记录您的调查结果以获取未来的迭代。</p><p>  Algorithms are the secret sauce of any search product. The efficacy of a search algorithm has a huge impact on a users’ experience, so having the right process to evaluate a search algorithm’s performance ensures you’re making confident decisions with your users in mind.</p><p>  算法是任何搜索产品的秘密调味料。搜索算法的功效对用户的经验产生了巨大影响，因此具有评估搜索算法的性能的正确进程可确保您在与您的用户与用户提供自信的决策。</p><p>  A high-quality and reliable labelled dataset is key for a successful, unbiased evaluation of a search algorithm.</p><p>  高质量可靠的标记数据集是成功，对搜索算法评估的键。 </p><p> Online metrics provide valuable insights on user behaviours, in addition to algorithm evaluation, even if they’re resource-intensive and risky to implement.</p><p>在线指标，除了算法评估之外，还提供了对用户行为的有价值的见解，即使它们是资源密集型和实现的风险，还提供了有价值的洞察。</p><p> Offline metrics are helpful for iterating on new algorithms quickly and mitigating the risks of launching new algorithms into production.</p><p> 离线指标有助于快速迭代新算法并减轻将新算法推出生产的风险。</p><p> Jodi Sloan is a Senior Engineer on the Discovery Experiences team. Over the past 2 years, she has used her passion for data science and engineering to craft performant, intelligent search experiences across Shopify. If you’d like to connect with Jodi, reach out on  LinkedIn.</p><p> Jodi Sloan是Discovery体验团队的高级工程师。在过去的两年里，她利用她对数据科学和工程的热情来制作性能，智能化搜索体验。如果您想与Jodi联系，请在LinkedIn上伸出援手。</p><p> Selina Li is a Data Scientist on the Messaging team. She is currently working to build intelligence in conversation and improve merchant experiences in chat. Previously, she was with the Self Help team where she contributed to deliver better search experiences for users in Shopify Help Center and Concierge. If you would like to connect with Selina, reach out on  Linkedin.</p><p> Selina Li是消息传递团队的数据科学家。她目前正致力于在谈话中建立智慧，并改善聊天中的商家体验。以前，她与自助团队有贡献，为购物帮助中心和礼宾服务提供了更好的搜索体验。如果您想与Selina联系，请在LinkedIn上伸出援手。</p><p>  If you’re a data scientist or engineer who’s passionate about building great search products, we’re hiring! Reach out to us or apply on our  careers page.</p><p>  如果您是一个关于建立大型搜索产品的热情的数据科学家或工程师，我们都会招聘！联系我们或申请我们的职业页面。</p><p>  Stories from the teams who build and scale Shopify. The commerce platform powering more than 1,000,000 businesses worldwide.</p><p>  从建立和规模购物的团队中的故事。商业平台在全球推出超过1000,000名业务。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://shopify.engineering/evaluating-search-algorithms">https://shopify.engineering/evaluating-search-algorithms</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/评估/">#评估</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/search/">#search</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/搜索/">#搜索</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>