<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>人工智能如何学习识别有毒的在线内容 How AI Is Learning to Identify Toxic Online Content</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">How AI Is Learning to Identify Toxic Online Content<br/>人工智能如何学习识别有毒的在线内容 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-18 17:56:14</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/92bd92243a4778341c8c449e3934799d.jpg"><img src="http://img2.diglog.com/img/2021/2/92bd92243a4778341c8c449e3934799d.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Social platforms large and small are struggling to keep their communities safe from hate speech, extremist content, harassment and misinformation. Most recently, far-right agitators  posted openly about plans to storm the U.S. Capitol before doing just that on January 6. One solution might be AI: developing algorithms to detect and alert us to toxic and inflammatory comments and flag them for removal. But such systems face big challenges.</p><p>大小不一的社交平台都在努力使自己的社区免受仇恨言论，极端主义内容，骚扰和错误信息的伤害。最近，极右翼煽动者在1月6日这样做之前公开发布了关于冲进美国国会大厦的计划。一种解决方案可能是AI：开发算法以检测并警告我们有毒和煽动性言论，并将其标记为删除。但是，这样的系统面临着巨大的挑战。</p><p>  The prevalence of hateful or offensive language online has been growing rapidly in recent years, and the problem is  now rampant. In some cases, toxic comments online have even resulted in real life violence, from  religious nationalism in Myanmar to  neo-Nazi propaganda in the U.S. Social media platforms, relying on thousands of human reviewers, are  struggling to moderate the ever-increasing volume of harmful content. In 2019, it was reported that Facebook moderators are at risk of  suffering from PTSD as a result of repeated exposure to such distressing content. Outsourcing this work to machine learning can help manage the rising volumes of harmful content, while limiting human exposure to it. Indeed, many tech giants have been  incorporating algorithms into their content moderation for years.</p><p>  近年来，网上仇恨或令人反感的语言的普及迅速增长，现在这个问题十分普遍。在某些情况下，在线上的有毒评论甚至导致了现实生活中的暴力事件，从缅甸的宗教民族主义到美国的新纳粹宣传，社会媒体平台都在依靠成千上万的人类评论家，努力缓解日益增长的有害内容。据报道，在2019年，由于反复暴露于此类令人痛苦的内容中，Facebook版主有遭受PTSD的风险。将此工作外包给机器学习可以帮助管理不断增长的有害内容，同时限制人们对其的接触。确实，许多技术巨头多年来一直在将算法纳入其内容审核中。</p><p>  One such example is Google’s Jigsaw, a company focusing on making the internet safer. In 2017, it helped create  Conversation AI, a collaborative research project aiming to detect toxic comments online. However, a tool produced by that project,  called Perspective, faced substantial  criticism. One common complaint was that it created a general “toxicity score” that wasn’t flexible enough to serve the varying needs of different platforms. Some Web sites, for instance, might require detection of threats but not profanity, while others might have the opposite requirements.</p><p>  其中一个例子就是Google的Jigsaw，该公司致力于提高互联网的安全性。 2017年，它帮助创建了Conversation AI，这是一个旨在在线检测有毒评论的协作研究项目。但是，该项目开发的一种名为Perspective的工具遭到了广泛的批评。一个普遍的抱怨是它创建了一个通用的“毒性评分”，该评分不够灵活，无法满足不同平台的各种需求。例如，某些网站可能需要检测威胁而不是亵渎行为，而其他网站可能有相反的要求。</p><p>  Another issue was that the algorithm learned to conflate toxic comments with nontoxic comments that contained words related to  gender, sexual orientation, religion or disability. For example,  one user reported that simple neutral sentences such as “I am a gay black woman” or “I am a woman who is deaf” resulted in high toxicity scores, while “I am a man” resulted in a low score.</p><p>  另一个问题是该算法学会了将有害评论与包含与性别，性取向，宗教或残疾有关的词语的无毒评论混为一谈。例如，一个用户报告说，简单的中性句子，例如“我是同性恋黑人妇女”或“我是聋哑的妇女”，会导致较高的毒性评分，而“我是男性”则会导致较低的评分。</p><p>  Following these concerns, the Conversation AI team invited developers to train their own toxicity-detection algorithms and enter them into three competitions (one per year) hosted on Kaggle, a Google subsidiary known for its community of machine learning practitioners, public data sets and challenges. To help train the AI models, Conversation AI released two public data sets containing over one million toxic and non-toxic comments from Wikipedia and a service called Civil Comments. The comments were  rated on toxicity by annotators, with a “Very Toxic” label indicating “a very hateful, aggressive, or disrespectful comment that is very likely to make you leave a discussion or give up on sharing your perspective,” and a “Toxic” label meaning “a rude, disrespectful, or unreasonable comment that is somewhat likely to make you leave a discussion or give up on sharing your perspective.” Some comments were seen by many more than 10 annotators (up to thousands), due to sampling and strategies used to enforce rater accuracy.</p><p>  针对这些问题，Conversation AI团队邀请开发人员训练自己的毒性检测算法，并将其参加在Kaggle上举办的三项竞赛（每年一次），该竞赛是Google子公司，以其机器学习从业者，公共数据集和挑战社区而闻名。为了帮助训练AI模型，Conversation AI发布了两个公共数据集，其中包含来自Wikipedia的超过一百万条有毒和无毒评论以及一项名为Civil Comments的服务。评论者对评论的毒性进行了评分，“非常有毒”标签表示“非常可恶，激进或不尊重的评论，很可能使您离开讨论或放弃分享您的观点，”和“有毒” ”标签的意思是“粗鲁，无礼或不合理的评论，在某种程度上可能会让您离开讨论或放弃分享您的观点。”由于采用了用于提高评分者准确性的抽样方法和策略，许多注释者（多达数千个）看到了一些注释。</p><p>  The goal of the  first Jigsaw challenge was to build a multilabel toxic comment classification model with labels such as “toxic”, “severe toxic”, “threat”, “insult”, “obscene”, and “identity hate”. The  second and  third challenges focused on more specific limitations of their API: minimizing unintended bias towards pre-defined identity groups and training multilingual models on English-only data.</p><p>  拼图的第一个挑战是建立一个带有“有毒”，“严重有毒”，“威胁”，“侮辱”，“淫秽”和“身份仇恨”等标签的多标签有毒评论分类模型。第二个和第三个挑战集中在其API的更具体限制上：最大程度地减少对预定义身份组的意外偏见，并在仅英语数据上训练多语言模型。</p><p>  Although the challenges led to some clever ways of improving toxic language models, our team at  Unitary, a content-moderation AI company, found none of the trained models had been released publicly.</p><p>  尽管挑战带来了一些巧妙的方法来改进有害语言模型，但我们在内容审核AI公司Unitary的团队发现，没有经过训练的模型都没有公开发布。 </p><p>  For that reason, we decided to take inspiration from the best Kaggle solutions and train our own algorithms with the specific intent of releasing them publicly. To do so, we relied on existing “transformer” models for natural language processing, such as  Google’s BERT. Many such models are accessible in an open-source  transformers library.</p><p>因此，我们决定从最好的Kaggle解决方案中汲取灵感，并针对将其公开发布的特定目的训练我们自己的算法。为此，我们依靠现有的“变压器”模型进行自然语言处理，例如Google的BERT。在开放源代码转换器库中可以访问许多这样的模型。</p><p>  This is how our team built  Detoxify, an open-source, user-friendly comment detection library to identify inappropriate or harmful text online. Its intended use is to help researchers and practitioners identify potential toxic comments. As part of this library, we released three different models corresponding to each of the three Jigsaw challenges. While the top Kaggle solutions for each challenge use model ensembles, which average the scores of multiple trained models, we obtained a similar performance with only one model per challenge. Each model can be easily accessed in one line of code and all models and training code are publicly available on  GitHub. You can also try a demonstration in  Google Colab.</p><p>  这就是我们的团队构建Detoxify的方式，Detoxify是一个开放源代码，用户友好的注释检测库，用于在线识别不适当或有害的文本。它的预期用途是帮助研究人员和从业人员识别潜在的毒性评论。作为该库的一部分，我们发布了三种不同的模型，分别对应于三个拼图挑战。尽管针对每个挑战的顶级Kaggle解决方案都使用模型集合，这些模型将多个训练过的模型的得分平均，但我们获得的性能类似，每个挑战只有一个模型。可以通过一行代码轻松访问每种模型，并且所有模型和培训代码都可以在GitHub上公开获得。您也可以尝试在Google Colab中进行演示。</p><p>  While these models perform well in a lot of cases, it is important to also note their limitations. First, these models will work well on examples that are similar to the data they have been trained on. But they are likely to fail if faced with unfamiliar examples of toxic language. We encourage developers to fine-tune these models on data sets representative of their use case.</p><p>  尽管这些模型在很多情况下都能发挥良好的性能，但重要的是还要注意它们的局限性。首先，这些模型将在与已训练数据相似的示例上很好地工作。但是，如果面对陌生的有害语言示例，他们很可能会失败。我们鼓励开发人员在代表其用例的数据集上微调这些模型。</p><p>  Furthermore, we noticed that the inclusion of insults or profanity in a text comment will almost always result in a high toxicity score, regardless of the intent or tone of the author. As an example, the sentence “I am tired of writing this stupid essay” will give a toxicity score of 99.7 percent, while removing the word ‘stupid’ will change the score to 0.05 percent.</p><p>  此外，我们注意到在文本评论中包含侮辱或亵渎行为几乎总是会导致较高的毒性评分，无论作者的意图或语气如何。例如，句子“我厌倦了撰写这篇愚蠢的文章”将给出99.7％的毒性分数，而删除“愚蠢”一词将使该分数变为0.05％。</p><p>  Lastly, despite the fact that one of the released models has been specifically trained to limit  unintended bias, all three models are still likely to exhibit some bias, which can pose ethical concerns when used off-the-shelf to moderate content.</p><p>  最后，尽管已经对其中一个发布的模型进行了专门的培训以限制意外的偏见，但所有三个模型仍然可能表现出一定的偏见，当使用现成的适度内容时，这可能会引起道德问题。</p><p>  Although there has been considerable progress on automatic detection of toxic speech, we still have a long way to go until models can capture the actual, nuanced, meaning behind our language—beyond the simple memorization of particular words or phrases. Of course, investing in better and more representative datasets would yield incremental improvements, but we must go a step further and begin to interpret data in context, a crucial part of understanding online behavior. A seemingly benign text post on social media accompanied by racist symbolism in an image or video would be easily missed if we only looked at the text. We know that lack of context can often be the cause of our own human misjudgments. If AI is to stand a chance of replacing manual effort on a large scale, it is imperative that we give our models the full picture.</p><p>  尽管在自动检测有毒语音方面取得了长足的进步，但是除了简单地记住特定的单词或短语之外，我们还需要很长的路要走，模型才能捕捉到我们语言背后的实际，细微的含义。当然，对更好，更具代表性的数据集进行投资将产生渐进的改进，但是我们必须走得更远，开始在上下文中解释数据，这是理解在线行为的关键部分。如果只看文字，很容易会错过社交媒体上看似良性的文字帖子，并带有图像或视频中的种族主义象征意味。我们知道，缺乏背景往往会导致我们自己的错误判断。如果AI有机会取代大规模的人工工作，那么我们必须为模型提供全面的视图。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/">https://www.scientificamerican.com/article/can-ai-identify-toxic-online-content/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/人工智能/">#人工智能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>