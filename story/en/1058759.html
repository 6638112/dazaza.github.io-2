<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>统一CUDA Python生态系统 Unifying the CUDA Python Ecosystem</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Unifying the CUDA Python Ecosystem<br/>统一CUDA Python生态系统 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-16 23:40:19</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/1126e4097bc0f30edf3f00e181d5e762.jpg"><img src="http://img2.diglog.com/img/2021/4/1126e4097bc0f30edf3f00e181d5e762.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Python plays a key role within the science, engineering, data analytics, and deep learning application ecosystem. NVIDIA has long been committed to helping the Python ecosystem leverage the accelerated massively parallel performance of GPUs to deliver standardized libraries, tools, and applications. Today, we’re introducing another step towards simplification of the developer experience with improved Python code portability and compatibility.</p><p>Python在科学，工程，数据分析和深度学习应用生态系统中发挥着关键作用。 NVIDIA长期以来一直致力于帮助Python生态系统利用GPU的加速大规模平行性能，以提供标准化的库，工具和应用。如今，我们正在向简化开发人员体验简化改进的Python代码可移植性和兼容性。</p><p> Our goal is to help unify the Python CUDA ecosystem with a single standard set of low-level interfaces, providing full coverage of and access to the CUDA host APIs from Python. We want to provide an ecosystem foundation to allow interoperability among different accelerated libraries. Most importantly, it should be easy for Python developers to use NVIDIA GPUs.</p><p> 我们的目标是帮助统一Python CUDA生态系统，单一标准的低级接口，提供从Python的CUDA主机API的完全覆盖和访问。我们希望提供生态系统基础，以允许不同加速库之间的互操作性。最重要的是，Python开发人员应该容易使用NVIDIA GPU。</p><p>  To date, access to CUDA and NVIDIA GPUs through Python could only be accomplished by means of third-party software such as Numba, CuPy, Scikit-CUDA, RAPIDS, PyCUDA, PyTorch, or TensorFlow, just to name a few. Each wrote its own interoperability layer between the CUDA API and Python.</p><p>  迄今为止，可以通过Python访问CUDA和NVIDIA GPU，只能通过NumBA，Cupy，Scikit-Cuda，Rapids，Pycuda，Pytorch或Tensorflow等第三方软件来完成，只是为了命名几个。每个都在CUDA API和Python之间写了自己的互操作性层。</p><p> By releasing CUDA Python, NVIDIA is enabling these platform providers to focus on their own value-added products and services. NVIDIA also hopes to lower the barrier to entry for other Python developers to use NVIDIA GPUs. The initial release of CUDA Python includes Cython and Python wrappers for the CUDA Driver and runtime APIs.</p><p> 通过释放CUDA Python，NVIDIA使这些平台提供商能够专注于自己的增值产品和服务。 NVIDIA还希望降低其他Python开发人员使用NVIDIA GPU的障碍。 CUDA Python的初始版本包括CUDA驱动程序和运行时API的Cython和Python包装器。</p><p> In future releases, we may offer a Pythonic object model and wrappers for CUDA libraries (cuBLAS, cuFFT, cuDNN, nvJPEG, and so on). Upcoming releases may also be available with source code on GitHub or packaged through PIP and Conda.</p><p> 在未来的版本中，我们可以为CUDA库（Cublas，Cufft，Cudnn，NVJPEG等）提供Pythonic对象模型和包装器。即将推出的版本也可以在GitHub上的源代码或通过PIP和公共区域进行包装。</p><p>  Because Python is an interpreted language, you need a way to compile the device code into PTX and then extract the function to be called at a later point in the application. It’s not important for understanding CUDA Python, but Parallel Thread Execution ( PTX) is a low-level virtual machine and instruction set architecture (ISA). You construct your device code in the form of a string and compile it with  NVRTC, a runtime compilation library for CUDA C++. Using the NVIDIA  Driver API, manually create a CUDA context and all required resources on the GPU, then launch the compiled CUDA C++ code and retrieve the results from the GPU. Now that you have an overview, jump into a commonly used example for parallel programming:  SAXPY.</p><p>  由于Python是一种解释语言，所以您需要一种方法来将设备代码编译为PTX，然后提取要在应用程序的稍后点调用的函数。对于理解CUDA Python并不重要，但并行线程执行（PTX）是一个低级虚拟机和指令集架构（ISA）。您以字符串的形式构建您的设备代码，并使用NVRTC编译它，这是CUDA C ++的运行时编译库。使用NVIDIA驱动程序API，手动创建CUDA上下文以及GPU上的所有所需资源，然后启动编译的CUDA C ++代码并从GPU检索结果。既然您概述了，跳入一个常用的并行编程示例：saxpy。</p><p> The first thing to do is import the  Driver API and  NVRTC modules from the CUDA Python package. In this example, you copy data from the host to device. You need  NumPy to store data on the host.</p><p> 首先要做的是从CUDA Python包导入驱动程序API和NVRTC模块。在此示例中，您将数据从主机复制到设备。您需要numpy来存储主机上的数据。 </p><p> import cuda_driver as cuda # Subject to change before releaseimport nvrtc # Subject to change before releaseimport numpy as np</p><p>将CUDA_DRIVER导入CUDA＃主题在HeledimIMPORT NVRTC＃之前进行更改，以便在HeledimImport Numpy作为NP之前更改</p><p> Error checking is a fundamental best practice in code development and a code example is provided. For brevity, error checking within the example is omitted. In a future release, this may automatically raise exceptions using a Python object model.</p><p> 错误检查是代码开发中的基本最佳实践，并提供代码示例。对于简洁起见，省略了示例中的错误检查。在将来的释放中，这可能会使用Python对象模型自动提出异常。</p><p> def ASSERT_DRV(err): if isinstance(err, cuda.CUresult): if err != cuda.CUresult.CUDA_SUCCESS: raise RuntimeError(&#34;Cuda Error: {}&#34;.format(err)) elif isinstance(err, nvrtc.nvrtcResult): if err != nvrtc.nvrtcResult.NVRTC_SUCCESS: raise RuntimeError(&#34;Nvrtc Error: {}&#34;.format(err)) else: raise RuntimeError(&#34;Unknown error type: {}&#34;.format(err))</p><p> def assert_drv（err）：如果isinstance（err，cuda.curesult）：如果err！= cuda.curesult.cuda_success：raintrationError（＆＃34; cuda错误：{}＆＃34; .format（err））elif isinstance （err，nvrtc.nvrtcresult）：如果err！= nvrtc.nvrtcresult.nvrtc_success：ring timerror（＆＃34; nvrtc错误：{}＆＃34; .format（err））else：race timerror（＆＃34;未知错误类型：{}＆＃34; .format（err））</p><p> It’s common practice to write CUDA kernels near the top of a translation unit, so write it next. The entire kernel is wrapped in triple quotes to form a string. The string is compiled later using NVRTC. This is the only part of CUDA Python that requires some understanding of CUDA C++. For more information, see  An Even Easier Introduction to CUDA.</p><p> 在翻译单元顶部附近写CUDA内核是常见的做法，所以接下来写它。整个内核以三重引号包裹以形成一个字符串。稍后使用NVRTC编译字符串。这是CUDA Python的唯一一部分，需要一些对CUDA C ++的理解。有关更多信息，请参阅甚至更轻松地介绍CUDA。</p><p> saxpy = &#34;&#34;&#34;\extern &#34;C&#34; __global__void saxpy(float a, float *x, float *y, float *out, size_t n){ size_t tid = blockIdx.x * blockDim.x + threadIdx.x; if (tid &lt; n) { out[tid] = a * x[tid] + y[tid]; }}&#34;&#34;&#34;</p><p> Saxpy =＆＃34;＆＃34;＆＃34; \ extern＆＃34; c＆＃34; __Grobal__void saxpy（float a，float * x，float * y，float * out，size_t n）{size_t tid = blockidx.x * blockdim.x + threadidx.x; if（tid＆lt; n）{out [tid] = a * x [tid] + y [tid]; }}}}}＆＃34;＆＃34;＆＃34;</p><p> Go ahead and compile the kernel into PTX. Remember that this is executed at runtime using NVRTC. There are three basic steps to NVRTC:</p><p> 继续编译内核进入PTX。请记住，使用NVRTC在运行时执行。 NVRTC有三个基本步骤：</p><p>  In the following code example, compilation is targeting compute capability  75, or Turing architecture, with FMAD enabled. If compilation fails, use  nvrtcGetProgramLog to retrieve a compile log for additional information.</p><p>  在以下代码示例中，编译是针对Compute能力75或TING架构，使用FMAD启用。如果编译失败，请使用nvrtcgetproglog检索编译日志以获取其他信息。 </p><p> # Create programerr, prog = nvrtc.nvrtcCreateProgram(str.encode(saxpy), b&#34;saxpy.cu&#34;, 0, [], [])# Compile programopts = [b&#34;--fmad=false&#34;, b&#34;--gpu-architecture=compute_75&#34;]err, = nvrtc.nvrtcCompileProgram(prog, 2, opts)# Get PTX from compilationerr, ptxSize = nvrtc.nvrtcGetPTXSize(prog)ptx = b&#34; &#34; * ptxSizeerr, = nvrtc.nvrtcGetPTX(prog, ptx)</p><p>＃创建程序交弹，prog = nvrtc.nvrtccreateprogram（str.encode（saxpy），b＆＃34; saxpy.cu＆＃34 ;, 0，[]，[]）＃编译程序= [b＆＃34;  -  fmad = false ＃34;，B＆＃34;  -  gpu-architecture = compute_75＆＃34;] err，= nvrtc.nvrtccompileprogram（prog，2，opts）＃get ptx from compilationerr，ptxsize = nvrtc.nvrtcgettxsize（prog）ptx = b＆＃ 34; ＆＃34; * ptxsizeerr，= nvrtc.nvrtcgetptx（prog，ptx）</p><p> Before you can use the PTX or do any work on the GPU, you must create a CUDA context. CUDA contexts are analogous to host processes for the device. In the following code example, the Driver API is initialized so that the NVIDIA driver and GPU are accessible. Next, a handle for compute device 0 is passed to  cuCtxCreate to designate that GPU for context creation. With the context created, you can proceed in compiling the CUDA kernel using NVRTC.</p><p> 在使用PTX或在GPU上执行任何工作之前，必须创建CUDA上下文。 CUDA上下文类似于设备的主机进程。在以下代码示例中，初始化驱动程序API，以便可访问NVIDIA驱动程序和GPU。接下来，将计算设备0的句柄传递给CuctXCreate以指定用于上下文创建的GPU。在创建的上下文中，您可以使用NVRTC进行编译CUDA内核。</p><p> # Initialize CUDA Driver APIerr, = cuda.cuInit(0)# Retrieve handle for device 0err, cuDevice = cuda.cuDeviceGet(0)# Create contexterr, context = cuda.cuCtxCreate(0, cuDevice)</p><p> ＃initialize cuda driver apierr，= cuda.cuinit（0）＃检索设备0err，cudevice = cuda.cudeviceget（0）＃创建contexterr，context = cuda.cuctxcreate（0，cudevice）</p><p> With a CUDA context created on device 0, load the PTX generated earlier into a module. A module is analogous to dynamically loaded libraries for the device. After loading into the module, extract a specific kernel with  cuModuleGetFunction. It is not uncommon for multiple kernels to reside in PTX.</p><p> 使用在设备0上创建的CUDA上下文，将前面生成的PTX加载到模块中。模块类似于动态加载设备的库。加载到模块后，用CufoduleGetFunction提取特定内核。多个内核驻留在PTX中并不罕见。</p><p> # Load PTX as module data and retrieve functionptx = np.char.array(ptx)err, module = cuda.cuModuleLoadData(ptx.ctypes.get_data())err, kernel = cuda.cuModuleGetFunction(module, b&#34;saxpy&#34;)</p><p> ＃加载ptx作为模块数据和检索函数portsptx = np.char.array（ptx）err，module = cuda.cumoduleloaddata（ptx.ctypes.get_data（））err，kernel = cuda.cumodulegetFunction（模块，B＆＃34; Saxpy＆＃ 34;）</p><p> Next, get all your data prepared and transferred to the GPU. For increased application performance, you can input data on the device to eliminate data transfers. For completeness, this example shows how you would transfer data to and from the device.</p><p> 接下来，获取准备和转移到GPU的所有数据。为了提高应用程序性能，您可以输入设备上的数据以消除数据传输。为了完整性，此示例显示了如何将数据传输到设备。</p><p> NUM_THREADS = 512 # Threads per blockNUM_BLOCKS = 32768 # Blocks per grida = np.array([2.0], dtype=np.float32)n = np.array(NUM_THREADS * NUM_BLOCKS, dtype=np.uint32)bufferSize = n * a.itemsizehX = np.random.rand(n).astype(dtype=np.float32)hY = np.random.rand(n).astype(dtype=np.float32)hOut = np.zeros(n).astype(dtype=np.float32)</p><p> num_threads = 512＃每个blocknum_blocks的线程= 32768＃每个grida = np.array（[2.0]，dtype = np.float32）n = np.array（num_threads * num_blocks，dtype = np.uint32）buffersize = n * a。项目zhx = np.random.rand（n）.astype（dtype = np.float32）hy = np.random.rand（n）.astype（dtype = np.float32）hout = np.zeros（n）.astype（dtype = np.float32） </p><p> With the input data  a,  x, and  y created for the SAXPY transform device, resources must be allocated to store the data using  cuMemAlloc. To allow for more overlap between compute and data movement, use the asynchronous function  cuMemcpyHtoDAsync. It returns control to the CPU immediately following command execution.</p><p>对于为SAXPY变换设备创建的输入数据A，X和Y，必须分配资源以使用CUMEMALLOC存储数据。要允许在计算和数据移动之间进行更多重叠，请使用异步函数cumemcpyhtodasync。在命令执行之后，它立即将控件返回到CPU。</p><p> Python doesn’t have a natural concept of pointers, yet  cuMemcpyHtoDAsync expects  void*. Therefore,  XX.ctypes.get_data retrieves the pointer value associated with XX.</p><p> Python没有指针的自然概念，但CumemcPyHtodasync预计将失效*。因此，xx.ctypes.get_data检索与xx关联的指针值。</p><p> err, dXclass = cuda.cuMemAlloc(bufferSize)err, dYclass = cuda.cuMemAlloc(bufferSize)err, dOutclass = cuda.cuMemAlloc(bufferSize)err, stream = cuda.cuStreamCreate(0)err, = cuda.cuMemcpyHtoDAsync( dXclass, hX.ctypes.get_data(), bufferSize, stream)err, = cuda.cuMemcpyHtoDAsync( dYclass, hY.ctypes.get_data(), bufferSize, stream)</p><p> err，dxclass = cuda.cumalaloc（缓冲区大小）err，dyclass = cuda.cumemalloc（缓冲）err，doutclass = cuda.cumemalloc（缓冲）err，stream = cuda.custeamcreate（0）err，= cuda.cumemcyhtodasync（dxclass，hx .ctypes.get_data（），buffersize，stream）err，= cuda.cumemcpyhtodasync（dyclass，hy.ctypes.get_data（），缓冲，流）</p><p> With data prep and resources allocation finished, the kernel is ready to be launched. To pass the location of the data on the device to the kernel execution configuration, you must retrieve the device pointer. In the following code example,  int(dXclass) retries the pointer value of  dXclass, which is  CUdeviceptr, and assigns a memory size to store this value using  np.array.</p><p> 通过数据准备和资源分配完成，内核已准备就绪。要将数据的位置传递到内核执行配置，必须检索设备指针。在以下代码示例中，int（dxclass）重试dxclass的指针值，该指针值是cudeviceptr，并为存储器大小分配以使用np.array存储此值。</p><p> Like  cuMemcpyHtoDAsync,  cuLaunchKernel expects  void** in the argument list. In the earlier code example, it creates  void** by grabbing the void* value of each individual argument and placing them into its own contiguous memory.</p><p> 像CumemcPyhtodasync一样，Culaunchkernel期望Void **参数列表中。在早期的代码示例中，它通过抓住每个单独参数的void *值并将它们放入自己的连续内存来创建void **。</p><p> # The following code example is not intuitive # Subject to change in a future releasedX = np.array([int(dXclass)], dtype=np.uint64)dY = np.array([int(dYclass)], dtype=np.uint64)dOut = np.array([int(dOutclass)], dtype=np.uint64)args = [a, dX, dY, dOut, n]args = np.array([arg.ctypes.get_data() for arg in args], dtype=np.uint64)</p><p> ＃以下代码示例不是直观的＃，可以在未来的重新启动= np.Array中更改（[int（dxclass）]，dtype = np.uint64）dy = np.array（[int（dyclass）]，dtype = np .uint64）dout = np.array（[int（doutclass）]，dtype = np.uint64）args = [a，dx，dy，dout，n] args = np.array（[arg.ctypes.get_data（） args]，dtype = np.uint64）</p><p>  err, = cuda.cuLaunchKernel( kernel, NUM_BLOCKS, # grid x dim 1, # grid y dim 1, # grid z dim NUM_THREADS, # block x dim 1, # block y dim 1, # block z dim 0, # dynamic shared memory stream, # stream args.ctypes.get_data(), # kernel arguments 0, # extra (ignore))err, = cuda.cuMemcpyDtoHAsync( hOut.ctypes.get_data(), dOutclass, bufferSize, stream)err, = cuda.cuStreamSynchronize(stream)</p><p>  err，= cuda.culaunchkernel（内核，num_blocks，＃grid x dim 1，＃grid y dim 1，＃grid z昏暗num_threads，＃block x dim 1，＃block y dim 1，＃block z dim 0，＃动态共享内存流，＃stream args.ctypes.get_data（），＃内核参数0，＃extra（忽略））err，= cuda.cumemcpydtohasync（hout.ctypes.get_data（），doutclass，buffersize，stream）err，= cuda。 custreamsynchronize（Stream） </p><p> The  cuLaunchKernel function takes the compiled module  kernel and execution configuration parameters. The device code is launched in the same stream as the data transfers. That ensures that the kernel’s compute is performed only after the data has finished transfer, as all API calls and kernel launches within a stream are serialized. After the call to transfer data back to the host is executed,  cuStreamSynchronize is used to halt CPU execution until all operations in the designated stream are finished.</p><p>CULUUNNKERNEL函数采用已编译的模块内核和执行配置参数。设备代码在与数据传输相同的流中启动。这可确保仅在数据完成传输后执行内核的计算，因为所有API调用和在流中启动都会序列化。执行呼叫转回主机后，CuStreamSynchronize用于停止CPU执行，直到指定流中的所有操作完成。</p><p> # Assert values are same after running kernelhZ = a * hX + hYif not np.allclose(hOut, hZ): raise ValueError(&#34;Error outside tolerance for host-device vectors&#34;)</p><p> #sssert值在运行kernelhz = a * hx + hyif之后也是相同的，而不是np.allclose（hout，hz）：提升valuseerror（＆＃34;主机设备向量的公差超出误差;）</p><p> Perform verification of the data to ensure correctness and finish the code with memory clean up.</p><p> 执行数据验证以确保正确性，并使用内存清理完成代码。</p><p>   Performance is a primary driver in targeting GPUs in your application. So, how does the above code compare to its C++ version? Table 1 shows that the results are nearly identical.  NVIDIA NSight Systems was used to retrieve kernel performance and  CUDA Events was used for application performance.</p><p>   性能是针对您的应用程序中的GPU中的主要驱动程序。那么，上面的代码如何与其C ++版本进行比较？表1显示结果几乎相同。 NVIDIA NSIGHT系统用于检索内核性能，CUDA事件用于应用程序性能。</p><p>    CUDA Python is also compatible with  NVIDIA Nsight Compute, which is an interactive kernel profiler for CUDA applications. It allows you to have detailed insights into kernel performance. This is useful when you’re trying to maximize performance (Figure 1).</p><p>    CUDA Python也与NVIDIA NSIGHT Compute兼容，这是一个用于CUDA应用程序的交互式内核分析器。它允许您对内核性能进行详细的洞察。当您尝试最大化性能时，这很有用（图1）。</p><p>   CUDA Python coming soon, along with a detailed description of APIs, installation notes, new features, and examples. For more information, see the following posts:</p><p>   CUDA Python即将推出，以及API，安装说明，新功能和示例的详细说明。有关更多信息，请参阅以下帖子：</p><p>   Special thanks to Vladislav Zhurba, a CUDA Python developer, for his help on the examples provided in this post.</p><p>   特别感谢CUDA Python开发人员Vladislav Zhurba，他的帮助在这篇文章中提供的例子。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://developer.nvidia.com/blog/unifying-the-cuda-python-ecosystem/">https://developer.nvidia.com/blog/unifying-the-cuda-python-ecosystem/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/python/">#python</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cuda/">#cuda</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>