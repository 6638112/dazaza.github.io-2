<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>主动-主动Postgres联合会是Kubernetes Active-Active Postgres Federation on Kubernetes</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Active-Active Postgres Federation on Kubernetes<br/>主动-主动Postgres联合会是Kubernetes </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-20 12:16:49</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/cf851b5e92e995ded6ae3353dd1ce93b.png"><img src="http://img2.diglog.com/img/2020/12/cf851b5e92e995ded6ae3353dd1ce93b.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>What if I told you that you can create an out-of-the-box active-active, federated PostgreSQL cluster on Kubernetes?</p><p>如果我告诉您可以在Kubernetes上创建现成的主动-主动，联合PostgreSQL集群怎么办？</p><p> Since logical decoding was introduced in PostgreSQL 9.4, I have been fascinated by the various applications it has. In fact, I&#39;ve used this feature to apply the concepts of   change data capture both in   theory and practice to the benefit of both applications and users.   Logical replication and   native partitioning support, introduced in Postgres 10, offered even more possibilities on distributing application load, in particular for geographically distributed workloads.</p><p> 自从PostgreSQL 9.4引入逻辑解码以来，我对它所具有的各种应用程序着迷。实际上，我已经使用此功能将更改数据捕获的概念在理论和实践上都应用到应用程序和用户的利益上。 Postgres 10中引入的逻辑复制和本机分区支持为分配应用程序负载提供了更多可能性，尤其是对于地理上分散的工作负载。</p><p> While I&#39;ve seen the increased need for turnkey   high availability for applications (and thanks to the improvements in cloud technologies, something I&#39;ve embraced), I&#39;ve been seeing a less common use case crop up more where an application may require the availability of multiple write nodes. I&#39;ve seen the cases range from geographically distributed applications that have large amounts of writes within their region to embedded applications that may be disrupted from the Internet for some time and need to sync up later.</p><p> 尽管我已经看到了对应用程序高可用性的日益增长的需求（并且由于云技术的改进，我已经接受了这种东西），但我看到了一个不那么普遍的用例，但更多应用程序可能需要多个写入节点的可用性。我已经看到了各种情况，从在其区域内有大量写操作的地理分布的应用程序到可能在一段时间内被Internet中断并需要稍后同步的嵌入式应用程序。</p><p> Additionally, while these use cases may want to distribute writes, they want to ensure that a single node can view all of the data. This is a classic example of data federation, and a concept that PostgreSQL has supported for a long time via   foreign data wrappers. However, we can make a federated PostgreSQL cluster much more efficient at reading by using some features in modern PostgreSQL versions, such as logical replication and partitioning!</p><p> 此外，虽然这些用例可能要分发写入，但他们希望确保单个节点可以查看所有数据。这是数据联合的经典示例，也是PostgreSQL通过外部数据包装器长期支持的概念。但是，通过使用现代PostgreSQL版本中的某些功能（例如逻辑复制和分区），我们可以使联合的PostgreSQL集群的读取效率更高！</p><p> Let&#39;s walk through how we can set up a three primary node federated Postgres cluster that avoids conflicts using the   Crunchy Postgres Operator and Kubernetes.</p><p> 让我们逐步介绍如何使用Crunchy Postgres Operator和Kubernetes建立三个主要节点的联合Postgres集群，从而避免冲突。</p><p>  For convenience, I am going to create the environment using the   PostgreSQL Operator.   Setting up the Postgres Operator is out of scope for this blog, but if you are following along at home, I suggest that you use the   quickstart. By default, all PostgreSQL clusters will be deployed into the  pgo namespace.</p><p>  为了方便起见，我将使用PostgreSQL Operator创建环境。设置Postgres Operator超出了本博客的范围，但是如果您在家中跟随，建议您使用快速入门。默认情况下，所有PostgreSQL集群都将部署到pgo命名空间中。</p><p> Let&#39;s create three PostgreSQL clusters, specifically three instances that are running   PostgreSQL 13. I&#39;ve segmented the instances into &#34;east&#34;, &#34;central&#34;, and &#34;west&#34;, and on each instance, I am going to create a user named  hippo with a password of  datalake, as well as create a database name  hippo.</p><p> 让我们创建三个PostgreSQL集群，特别是三个正在运行PostgreSQL 13的实例。我已将这些实例分为“东部”，“中央”和“＃34”。 ; west＆＃34 ;，在每个实例上，我都将创建一个名为hippo的用户，其密码为datalake，并创建一个数据库名称hippo。 </p><p> pgo create cluster hippo-east \ --ccp-image-tag=centos8-13.1-4.6.0 \ --username=hippo \ --password=datalake \ --password-superuser=superdatalake \ --database=hippopgo create cluster hippo-central \ --ccp-image-tag=centos8-13.1-4.6.0 \ --username=hippo \ --password=datalake \ --password-superuser=superdatalake \ --database=hippopgo create cluster hippo-west \ --ccp-image-tag=centos8-13.1-4.6.0 \ --username=hippo \ --password=datalake \ --password-superuser=superdatalake \ --database=hippo</p><p>pgo创建集群hippo-east \ --ccp-image-tag = centos8-13.1-4.6.0 \ --username = hippo \ --password = datalake \ --password-superuser = superdatalake \ --database = hippopgo创建集群hippo-central \ --ccp-image-tag = centos8-13.1-4.6.0 \ --username = hippo \ --password = datalake \ --password-superuser = superdatalake \ --database = hippopgo创建集群hippo-west \ --ccp-image-tag = centos8-13.1-4.6.0 \-用户名=河马\-密码=数据湖\-密码超级用户=超级数据湖\-数据库=河马</p><p>  We are going to need a way to identify each node in our federated cluster. The   pgnodemx extension, included with the Postgres Operator, provides a convenient way to look up the name of the node. While the Postgres Operator by default enables the shared library for pgnodemx, you will still need to add the extension to the database to use it.</p><p>  我们将需要一种方法来标识联合集群中的每个节点。 Postgres Operator附带的pgnodemx扩展名提供了一种方便的方法来查找节点的名称。虽然Postgres Operator默认情况下为pgnodemx启用了共享库，但是您仍然需要将扩展​​名添加到数据库中才能使用它。</p><p> We need to add this extension as the Postgres superuser, which is named  postgres to the  hippo database. There are several ways to   connect to a PostgreSQL database running on Kubernetes, as   described in the Postgres Operator documentation. Additionally, we will need to run commands on all three Postgres instances, so please pay attention to where you are executing the below!</p><p> 我们需要将此扩展名添加为Postgres超级用户，该用户名为hipgre数据库的postgres。有多种连接到Kubernetes上运行的PostgreSQL数据库的方法，如Postgres Operator文档中所述。此外，我们将需要在所有三个Postgres实例上运行命令，因此请注意执行以下命令的位置！</p><p> In the example above, Recall when we provisioned our three clusters, we set a superuser password of  superdatalake. First, let&#39;s connect to the  hippo-east Postgres cluster and into the  hippo database. If you are using the   port-forward technique, your connection string may look like this:</p><p> 在上面的示例中，在设置三个群集时，请回想一下，我们将超级用户密码设置为superdatalake。首先，让我们连接到东面的河马Postgres集群和河马数据库。如果您使用端口转发技术，则连接字符串可能如下所示：</p><p>  Once you are logged in, create the following function that will get the node name and grant permission to the  hippo user to be able to execute it:</p><p>  登录后，创建以下函数，该函数将获取节点名称并将权限授予河马用户以执行该节点：</p><p> CREATE EXTENSION IF NOT EXISTS pgnodemx;CREATE OR REPLACE FUNCTION hippo.get_node_name()RETURNS textAS $$ SELECT val FROM kdapi_setof_kv(&#39;labels&#39;) WHERE key=&#39;pg-cluster&#39;;$$ LANGUAGE SQL SECURITY DEFINER IMMUTABLE;GRANT EXECUTE ON FUNCTION hippo.get_node_name() TO hippo;</p><p> 如果不存在，请创建扩展名pgnodemx;创建或替换功能hippo.get_node_name（）返回文本从kdapi_setof_kv（＆＃39; labels＆＃39;）中选择val，其中key =＆＃39; pg-cluster＆＃39 ;; $$语言SQL安全性定义不可变；在功能上执行GRIP hippo.get_node_name（）到hippo；</p><p>  Test out the function as the  hippo user! For example, try logging into the  hippo database as the  hippo user in the  hippo-east cluster following similar steps to how you logged in above. For instance using the port-forward method:</p><p>  以河马用户身份测试功能！例如，尝试以与上述登录方式类似的步骤，以hippo-east集群中的hippo用户身份登录到hippo数据库。例如，使用port-forward方法： </p><p>     Success! If you are not using Kubernetes, you can achieve a similar set up by setting a   PostgreSQL customized option, e.g.  node.node_name and referencing that in later parts of this example.</p><p>成功！如果您不使用Kubernetes，则可以通过设置PostgreSQL自定义选项来实现类似的设置，例如node.node_name，并在本示例的后面部分中进行引用。</p><p> The base configuration for setting up our three node writable PostgreSQL cluster is complete. Now we need to set up our actual data structure!</p><p> 设置我们的三节点可写PostgreSQL集群的基本配置已完成。现在我们需要建立实际的数据结构！</p><p>  As mentioned above, there are several ways to   connect to a PostgreSQL database running on Kubernetes, as   described in the Postgres Operator documentation. To set up our data structure, we will need to log in as the  hippo user using the password that we set up in the earlier step ( datalake). Additionally, we will need to run commands on all three Postgres instances, so please pay attention to where you are executing the below!</p><p>  如上所述，有多种方法可以连接到在Kubernetes上运行的PostgreSQL数据库，如Postgres Operator文档中所述。要设置我们的数据结构，我们将需要使用在先前步骤（datalake）中设置的密码以河马用户身份登录。此外，我们将需要在所有三个Postgres实例上运行命令，因此请注意执行以下命令的位置！</p><p> First, let&#39;s connect to the  hippo-east Postgres cluster and into the  hippo database. If you are using the   port-forward technique, your connection string may look like this:</p><p> 首先，让我们连接到东面的河马Postgres集群和河马数据库。如果您使用端口转发技术，则连接字符串可能如下所示：</p><p>  Our data structure is going to collect observed values and the time in which they were observed. Execute the following SQL commands while connected to &#34;hippo-east&#34;:</p><p>  我们的数据结构将收集观察值和观察时间。连接到＆＃34; hippo-east＆＃34;时，执行以下SQL命令：</p><p> CREATE TABLE hippos ( id uuid DEFAULT gen_random_uuid() NOT NULL, node_name text, value numeric, created_at timestamptz) PARTITION BY LIST (node_name);CREATE TABLE hippo_default PARTITION of hippos (PRIMARY KEY (id)) DEFAULT;CREATE TABLE hippo_east PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-east&#39;);CREATE TABLE hippo_central PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-central&#39;);CREATE TABLE hippo_west PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-west&#39;);CREATE OR REPLACE FUNCTION add_node_name()RETURNS trigger AS $$BEGIN UPDATE hippos SET node_name = hippo.get_node_name() WHERE node_name IS NULL; RETURN NULL;END$$ LANGUAGE plpgsql;CREATE TRIGGER add_node_nameAFTER INSERT ON hippos FOR EACH STATEMENT EXECUTE FUNCTION add_node_name();</p><p> CREATE TABLE hippos（id uuid DEFAULT gen_random_uuid（）NOT NULL，node_name文本，数值数值，created_at timestamptz）PARTITION BY LIST（node_name）; CREATE TABLE hippo_default分隔河马（PRIMARY KEY（id））DEFAULT; CREATE TABLE hippo_east PARTITION OF （PRIMARY KEY（ID））中的值（＆＃39; hippo-east＆＃39;）; CREATE TABLE hippo_centralPartition of hippos（PRIMARY KEY（ID））中的值（＆＃39; hippo-central＆＃39; ）;为（（＃hippo-west＆＃39;）中的值创建河马表（primary key（id））的hippo_west分区; .get_node_name（）WHERE node_name是NULL;返回NULL； END $$语言plpgsql；为每个语句执行功能创建触发add_node_nameAFTER在河马上插入add_node_name（）;</p><p> Notice that the value that will be used as the primary key uses the  gen_random_uuid() function that is   available in PostgreSQL 13 -- older versions of PostgreSQL need to run   CREATE EXTENSION pgcrypto; as a superuser to allow use of that function.</p><p> 请注意，将用作主键的值使用了PostgreSQL 13中可用的gen_random_uuid（）函数-较早版本的PostgreSQL需要运行CREATE EXTENSION pgcrypto；作为超级用户允许使用该功能。 </p><p>   First, we created a partitioned table with a partition key corresponding to our three node names:  hippo-east,  hippo-central, and  hippo-west. These will be used to segment the writes from the different nodes. Also notice that we created a   default partition: this is important for appropriately routing each inserted row into the correct partition.</p><p>首先，我们创建了一个分区表，该分区表的分区键对应于我们的三个节点名称：hippo-east，hippo-central和hippo-west。这些将用于分割来自不同节点的写入。还要注意，我们创建了一个默认分区：这对于将每个插入的行适当地路由到正确的分区中非常重要。</p><p> PostgreSQL 13 introduced the ability to use BEFORE row triggers on partitioned tables, though they do not allow you to modify the partition key. However, this is exactly what we need to do! Instead, after a row is inserted (and routed to the default partition), we run a statement trigger that moves any row in the default partition into the partition representing the current node (which we get from  get_node_name() function we created).</p><p> PostgreSQL 13引入了在分区表上使用BEFORE行触发器的功能，尽管它们不允许您修改分区键。但是，这正是我们需要做的！相反，在插入一行（并将其路由到默认分区）之后，我们运行语句触发器，将默认分区中的任何行移动到代表当前节点的分区中（这是从我们创建的get_node_name（）函数获得的）。</p><p> In other words, the above schema ensures that all inserted rows are routed to a partition representing that specific node!</p><p> 换句话说，以上架构确保将所有插入的行都路由到表示该特定节点的分区！</p><p>   For the next step, we need to set up logical replication between the nodes. To do this, we need to be a Postgres superuser, in this case  postgres. Recall when we provisioned our three clusters, we set a superuser password of  superdatalake. To set up logical replication, we will need to perform the following steps:</p><p>   下一步，我们需要在节点之间设置逻辑复制。为此，我们需要成为Postgres超级用户，在本例中为postgres。回想一下，当我们配置了三个集群时，我们将超级用户密码设置为superdatalake。要设置逻辑复制，我们将需要执行以下步骤：</p><p> Grant the  hippo user the  REPLICATION privilege. Note that this is a near-superuser privilege but we are doing this for the convenience of our demo.</p><p> 向河马用户授予REPLICATION特权。请注意，这是接近超级用户的特权，但是为了方便演示，我们正在这样做。</p><p> Create publishers on each of the clusters to advertise their specific partition collecting writes. For example, on  hippo-east this would be the  hippo_east table.</p><p> 在每个群集上创建发布者，以公布其特定分区收集的写操作。例如，在hippo-east上，这将是hippo_east表。</p><p> After all of the publishers are created, create subscriptions on each of the clusters to the other clusters to read in all of the changes.</p><p> 创建所有发布者之后，在每个群集上创建对其他群集的订阅以读取所有更改。 </p><p> This time, we will have to run distinct commands on each PostgreSQL cluster, so pay attention!</p><p>这次，我们将不得不在每个PostgreSQL集群上运行不同的命令，因此请注意！</p><p>     As described above, this provides the  hippo PostgreSQL user with the replication privilege and defines a   logical replication publication for the  hippo_east table.</p><p>     如上所述，这为河马PostgreSQL用户提供了复制特权，并为hippo_east表定义了逻辑复制发布。</p><p>     Now we are ready to set up the logical replication subscriptions. Each node needs to subscribe to the other nodes in order to receive all of the changes. For example,  hippo-east needs to subscribe to the publishers on  hippo-central and  hippo-west to receive changes. Log into  hippo-east as the PostgreSQL superuser (&#34;postgres&#34;) and execute the following commands:</p><p>     现在，我们准备设置逻辑复制预订。每个节点都需要订阅其他节点才能接收所有更改。例如，河马东方需要订阅河马中央和河马西部的发行商来接收更改。以PostgreSQL超级用户（＆＃34; postgres＆＃34;）登录到hippo-east并执行以下命令：</p><p> CREATE SUBSCRIPTION sub_hippo_east_hippo_central CONNECTION &#39;dbname=hippo host=hippo-central.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_central;CREATE SUBSCRIPTION sub_hippo_east_hippo_west CONNECTION &#39;dbname=hippo host=hippo-west.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_west;</p><p> 创建订阅sub_hippo_east_hippo_central连接＆＃39; dbname = hippo主机= hippo-central.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_central;创建订阅sub_hippo_east_hippo_west连接＆＃39; dbname = hippo主机= hippo-west.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_west;</p><p> (Note that I am deriving the  host name using the Kubernetes Services that the Postgres Operator creates.   Read more about Kubernetes network names).</p><p> （请注意，我正在使用Postgres Operator创建的Kubernetes Services导出主机名。了解有关Kubernetes网络名称的更多信息）。</p><p>  CREATE SUBSCRIPTION sub_hippo_central_hippo_east CONNECTION &#39;dbname=hippo host=hippo-east.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_east;CREATE SUBSCRIPTION sub_hippo_central_hippo_west CONNECTION &#39;dbname=hippo host=hippo-west.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_west;</p><p>  创建订阅sub_hippo_central_hippo_east连接＆＃39; dbname = hippo主机= hippo-east.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_east;创建订阅sub_hippo_central_hippo_west连接＆＃39; dbname = hippo主机= hippo-west.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_west;</p><p>  CREATE SUBSCRIPTION sub_hippo_west_hippo_east CONNECTION &#39;dbname=hippo host=hippo-east.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_east;CREATE SUBSCRIPTION sub_hippo_west_hippo_central CONNECTION &#39;dbname=hippo host=hippo-central.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_central;</p><p>  创建订阅sub_hippo_west_hippo_east连接＆＃39; dbname = hippo主机= hippo-east.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_east;创建订阅sub_hippo_west_hippo_central连接＆＃39; dbname = hippo主机= hippo-central.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_central; </p><p>   To test out the cluster, try writing some data to all three nodes. Here is an example to get you started: modify to the settings specific to your environment:</p><p>要测试集群，请尝试将一些数据写入所有三个节点。这是一个入门的示例：修改特定于您的环境的设置：</p><p> # hippo-eastPGPASSWORD=datalake psql -h hippo-east.pgo -U hippo hippo -c &#39;INSERT INTO hippos (value, created_at) VALUES (random(), CURRENT_TIMESTAMP);# hippo-centralPGPASSWORD=datalake psql -h hippo-central.pgo -U hippo hippo -c &#39;INSERT INTO hippos (value, created_at) VALUES (random(), CURRENT_TIMESTAMP);# hippo-westPGPASSWORD=datalake psql -h hippo-west.pgo -U hippo hippo -c &#39;INSERT INTO hippos (value, created_at) VALUES (random(), CURRENT_TIMESTAMP);</p><p> ＃hippo-eastPGPASSWORD = datalake psql -h hippo-east.pgo -U hippo hippo -c＆＃39; INSERT INTO hippos（value，created_at）VALUES（random（），CURRENT_TIMESTAMP）;＃hippo-centralPGPASSWORD = datalake psql -h hippo-central.pgo -U hippo hippo -c＆＃39; INSERT INTO hippos（value，created_at）VALUES（random（），CURRENT_TIMESTAMP）;＃hippo-westPGPASSWORD = datalake psql -h hippo-west.pgo -U hippo hippo -c＆＃39;插入河马（value，created_at）VALUES（random（），CURRENT_TIMESTAMP）;</p><p> Now, log into  hippo-east as the  hippo user and inspect the  hippos table. This is what I saw:</p><p> 现在，以河马用户身份登录河马东并检查河马表。这是我看到的：</p><p> hippo=&gt; TABLE hippos; id | node_name | value | created_at --------------------------------------+---------------+-------------------+------------------------------- 4608b3a8-0f34-4837-8456-5944a61d15de | hippo-central | 0.484604634620151 | 2020-12-19 16:41:08.707359+00 eefa9a61-cc7e-44bc-a427-4b26c2564d24 | hippo-east | 0.270977731568895 | 2020-12-19 16:41:16.837468+00 37cf93dd-c7a1-44be-9eab-a58c73a14740 | hippo-west | 0.509173376067992 | 2020-12-19 16:40:59.949198+00(3 rows)</p><p> 河马=＆gt;表河马; id | node_name |价值| created_at -------------------------------------- + ---------- ----- + ------------------- + ------------------------ ------- 4608b3a8-0f34-4837-8456-5944a61d15de |河马中心| 0.484604634620151 | 2020-12-19 16：41：08.707359 + 00 eefa9a61-cc7e-44bc-a427-4b26c2564d24 |东河马| 0.270977731568895 | 2020-12-19 16：41：16.837468 + 00 37cf93dd-c7a1-44be-9eab-a58c73a14740 |西河马| 0.509173376067992 | 2020-12-19 16：40：59.949198 + 00（3行）</p><p> Pretty cool, so everything replicated over! And to ensure that this was not all an illusion, I inspected each of the partition tables to see what rows were in them:</p><p> 很酷，所以一切都复制了！为了确保这不是一种幻想，我检查了每个分区表以查看其中有哪些行：</p><p> hippo=&gt; TABLE hippo_east; id | node_name | value | created_at --------------------------------------+------------+-------------------+------------------------------- eefa9a61-cc7e-44bc-a427-4b26c2564d24 | hippo-east | 0.270977731568895 | 2020-12-19 16:41:16.837468+00(1 row)hippo=&gt; TABLE hippo_central; id | node_name | value | created_at --------------------------------------+---------------+-------------------+------------------------------- 4608b3a8-0f34-4837-8456-5944a61d15de | hippo-central | 0.484604634620151 | 2020-12-19 16:41:08.707359+00(1 row)hippo=&gt; TABLE hippo_west; id | node_name | value | created_at --------------------------------------+------------+-------------------+------------------------------- 37cf93dd-c7a1-44be-9eab-a58c73a14740 | hippo-west | 0.509173376067992 | 2020-12-19 16:40:59.949198+00(1 row)</p><p> 河马=＆gt;表hippo_east; id | node_name |价值| created_at -------------------------------------- + ---------- -+ ------------------- + --------------------------- ---- eefa9a61-cc7e-44bc-a427-4b26c2564d24 |东河马| 0.270977731568895 | 2020-12-19 16：41：16.837468 + 00（1行）hippo =>表hippo_central; id | node_name |价值| created_at -------------------------------------- + ---------- ----- + ------------------- + ------------------------ ------- 4608b3a8-0f34-4837-8456-5944a61d15de |河马中心| 0.484604634620151 | 2020-12-19 16：41：08.707359 + 00（1行）hippo =>表hippo_west; id | node_name |价值| created_at -------------------------------------- + ---------- -+ ------------------- + --------------------------- ---- 37cf93dd-c7a1-44be-9eab-a58c73a14740 |西河马| 0.509173376067992 | 2020-12-19 16：40：59.949198 + 00（1行）</p><p> What about the other nodes? Log into  hippo-central or  hippo-west -- you should see something similar to this:</p><p> 那其他节点呢？登录到河马中心或河马西部-您应该看到类似以下内容： </p><p> hippo=&gt; TABLE hippos; id | node_name | value | created_at --------------------------------------+---------------+-------------------+------------------------------- 4608b3a8-0f34-4837-8456-5944a61d15de | hippo-central | 0.484604634620151 | 2020-12-19 16:41:08.707359+00 eefa9a61-cc7e-44bc-a427-4b26c2564d24 | hippo-east | 0.270977731568895 | 2020-12-19 16:41:16.837468+00 37cf93dd-c7a1-44be-9eab-a58c73a14740 | hippo-west | 0.509173376067992 | 2020-12-19 16:40:59.949198+00</p><p>河马=＆gt;表河马; id | node_name |价值| created_at -------------------------------------- + ---------- ----- + ------------------- + ------------------------ ------- 4608b3a8-0f34-4837-8456-5944a61d15de |河马中心| 0.484604634620151 | 2020-12-19 16：41：08.707359 + 00 eefa9a61-cc7e-44bc-a427-4b26c2564d24 |东河马| 0.270977731568895 | 2020-12-19 16：41：16.837468 + 00 37cf93dd-c7a1-44be-9eab-a58c73a14740 |西河马| 0.509173376067992 | 2020-12-19 16：40：59.949198 + 00</p><p> Success -- we create a three node Postgres cluster on Kubernetes where each node can safely accept writes!</p><p> 成功-我们在Kubernetes上创建了一个三节点Postgres集群，每个节点都可以安全地接受写入！</p><p>  It looks like our application is going to need a node in the south, so how do we add another node?</p><p>  看来我们的应用程序将需要在南部的一个节点，那么如何添加另一个节点？</p><p> This is actually not too complicated if we follow the above steps. Note that as you add more nodes, you may need to increase PostgreSQL parameters such as   max_wal_senders and   max_replication_slots.</p><p> 如果遵循上述步骤，实际上并不太复杂。请注意，随着添加更多节点，可能需要增加PostgreSQL参数，例如max_wal_senders和max_replication_slots。</p><p>   Similar to the beginning of the example, log in as the  postgres user into the  hippo database and create the  pgnodemx extension and the  get_node_name function. For convenience, here are those commands again:</p><p>   与示例开头类似，以postgres用户身份登录到hippo数据库，并创建pgnodemx扩展名和get_node_name函数。为了方便起见，以下是这些命令：</p><p> CREATE EXTENSION IF NOT EXISTS pgnodemx;CREATE OR REPLACE FUNCTION hippo.get_node_name()RETURNS textAS $$ SELECT val FROM kdapi_setof_kv(&#39;labels&#39;) WHERE key=&#39;pg-cluster&#39;;$$ LANGUAGE SQL SECURITY DEFINER IMMUTABLE;GRANT EXECUTE ON FUNCTION hippo.get_node_name() TO hippo;</p><p> 如果不存在，请创建扩展名pgnodemx;创建或替换功能hippo.get_node_name（）返回文本从kdapi_setof_kv（＆＃39; labels＆＃39;）中选择val，其中key =＆＃39; pg-cluster＆＃39 ;; $$语言SQL安全性定义不可变；在功能上执行GRIP hippo.get_node_name（）到hippo；</p><p> Now log into  hippo-south as the  hippo user and add the schema, now with an additional partition for  hippo-south:</p><p> 现在以河马用户身份登录到河马南，并添加架构，现在为河马南添加了一个额外的分区： </p><p> CREATE TABLE hippos ( id uuid DEFAULT gen_random_uuid() NOT NULL, node_name text, value numeric, created_at timestamptz) PARTITION BY LIST (node_name);CREATE TABLE hippo_default PARTITION of hippos (PRIMARY KEY (id)) DEFAULT;CREATE TABLE hippo_east PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-east&#39;);CREATE TABLE hippo_central PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-central&#39;);CREATE TABLE hippo_west PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-west&#39;);CREATE TABLE hippo_south PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (&#39;hippo-south&#39;);CREATE OR REPLACE FUNCTION add_node_name()RETURNS trigger AS $$BEGIN UPDATE hippos SET node_name = hippo.get_node_name() WHERE node_name IS NULL; RETURN NULL;END$$ LANGUAGE plpgsql;CREATE TRIGGER add_node_nameAFTER INSERT ON hippos FOR EACH STATEMENT EXECUTE FUNCTION add_node_name();</p><p>CREATE TABLE hippos（id uuid DEFAULT gen_random_uuid（）NOT NULL，node_name文本，数值数值，created_at timestamptz）PARTITION BY LIST（node_name）; CREATE TABLE hippo_default分隔河马（PRIMARY KEY（id））DEFAULT; CREATE TABLE分隔河马（PRIMARY KEY（ID））中的值（＆＃39; hippo-east＆＃39;）; CREATE TABLE hippo_centralPartition of hippos（PRIMARY KEY（ID）） ）;在（＆＃39; hippo-west＆＃39;）中创建河马（PRIMARY KEY（id））中的表hippo_west分区;在（＆## 39; hippo-south＆＃39;）;创建或替换功能add_node_name（）返回触发器为$$ BEGIN UPDATE hippos SET node_name = hippo.get_node_name（）WHERE node_name为NULL;返回NULL； END $$语言plpgsql；为每个语句执行功能创建触发add_node_nameAFTER在河马上插入add_node_name（）;</p><p> Log into  hippo-south as a Postgres superuser and grant the  REPLICATION privilege to hippo, create the publisher for the  hippo_south partition, and create the subscribers:</p><p> 以Postgres超级用户身份登录hippo-south，并向hippo授予REPLICATION特权，为hippo_south分区创建发布者，并创建订阅者：</p><p> ALTER ROLE hippo REPLICATION;CREATE PUBLICATION pub_hippo_south FOR TABLE hippo.hippo_south;CREATE SUBSCRIPTION sub_hippo_south_hippo_east CONNECTION &#39;dbname=hippo host=hippo-east.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_east;CREATE SUBSCRIPTION sub_hippo_south_hippo_central CONNECTION &#39;dbname=hippo host=hippo-central.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_central;CREATE SUBSCRIPTION sub_hippo_south_hippo_west CONNECTION &#39;dbname=hippo host=hippo-west.pgo user=hippo password=datalake&#39; PUBLICATION pub_hippo_west;</p><p> ALTER ROLE hippo复制；创建pub_hippo_south for table hippo.hippo_south；创建订阅sub_hippo_south_hippo_east连接＆＃39; dbname = hippo host = hippo-east.pgo user = hippo password = datalake＆＃39;出版物pub_hippo_east;创建订阅sub_hippo_south_hippo_central连接＆＃39; dbname = hippo主机= hippo-central.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_central;创建订阅sub_hippo_south_hippo_west连接＆＃39; dbname = hippo主机= hippo-west.pgo用户= hippo密码= datalake＆＃39;出版物pub_hippo_west;</p><p> As the  hippo user, log into  hippo-east,  hippo-central, and  hippo-west and add the  hippo_south partition:</p><p> 作为河马用户，登录到河马东，河马中央和河马西部，并添加hippo_south分区：</p><p>  Now as a Postgres superuser, log into each of the nodes below and execute the following, starting with  hippo-east:</p><p>  现在，以Postgres超级用户身份，登录到下面的每个节点，并从hippo-east开始执行以下操作：</p><p>      With that, you have added an additional node to your federated PostgreSQL cluster! Test it out and see the results.</p><p>      这样，您就为联邦PostgreSQL集群添加了一个额外的节点！测试一下，看看结果。</p><p>  One question that comes up with systems with multiple write nodes is how to handle conflicts. In the above example, the conflict is actually handled at each individual node: the primary key is set in each partition, so if a conflicting UUID is generated, it will not be inserted and therefore, not replicated. This does not prevent two nodes from generating the same UUID: we could leverage UUIDv5 or the like to help prevent this conflict, or ensure there is some other natural key we can use as a lookup for our source of truth.</p><p>  具有多个写节点的系统提出的一个问题是如何处理冲突。在上面的示例中，冲突实际上是在每个单独的节点上处理的：在每个分区中都设置了主键，因此，如果生成冲突的UUID，则不会插入该UUID，因此不会对其进行复制。这不会阻止两个节点生成相同的UUID：我们可以利用UUIDv5等来帮助防止这种冲突，或者确保可以使用其他一些自然键来查找真相。 </p><p> As a safety measure, we could further lock down the schema to prevent a node from inadvertently adding data to a partition that it is not allowed to write into.</p><p>作为安全措施，我们可以进一步锁定模式，以防止节点无意中将数据添加到不允许写入的分区中。</p><p> If we do not intend to create additional nodes, another solution for the primary key conflicts is to coordinate sequences. For example, using our three node cluster, we could design a schema that looks similar to this:</p><p> 如果我们不打算创建其他节点，则针对主键冲突的另一种解决方案是协调序列。例如，使用我们的三节点集群，我们可以设计一个类似于以下内容的架构：</p><p> # hippo-eastCREATE TABLE hippos ( id bigint GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 3) NOT NULL, value numeric, created_at timestamptz) PARTITION BY LIST (((id - 1) % 3));CREATE TABLE hippo_east PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (0);CREATE TABLE hippo_central PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (1);CREATE TABLE hippo_west PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (2);# hippo-centralCREATE TABLE hippos ( id bigint GENERATED BY DEFAULT AS IDENTITY (START WITH 2 INCREMENT BY 3) NOT NULL, value numeric, created_at timestamptz) PARTITION BY LIST (((id - 1) % 3));CREATE TABLE hippo_east PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (0);CREATE TABLE hippo_central PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (1);CREATE TABLE hippo_west PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (2);# hippo-westCREATE TABLE hippos ( id bigint GENERATED BY DEFAULT AS IDENTITY (START WITH 3 INCREMENT BY 3) NOT NULL, value numeric, created_at timestamptz) PARTITION BY LIST (((id - 1) % 3));CREATE TABLE hippo_east PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (0);CREATE TABLE hippo_central PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (1);CREATE TABLE hippo_west PARTITION OF hippos (PRIMARY KEY (id)) FOR VALUES IN (2);</p><p> ＃hippo-eastCREATE TABLE hippos（id bigint由DEFAULT AS IDENTITY生成（以1 INCREMENT BY 3开头）不为空，值是数值，在timestamptz上创建）PARTITION BY LIST（（（（id-1）％3））; CREATE TABLE hippo_east PARTITION （0）中的值的河马（PRIMARY KEY（id））；（1）中的值创建河马（PRIMARY KEY（id））的表；河马（PRIMARY KEY（id））中创建表的hippo_west分区（2）中的值;＃hippo-central创建表河马（id bigint由默认身份作为身份生成（以2 INCREMENT BY 3开头）不是NULL，值是数值，在timestamptz时created）按列表划分（（（（id-1）％3） ）;为（0）中的值创建河马表（主要键（id））;为（1）中的值创建河马表（主要键（id））;为河马（原始表）创建表hippo_west分区（2）中的值的键（id））;＃hippo-west创建表河马（id bigint由默认身份标识生成（以3递增3开头）不为空，值数值，created_at timestamptz）按列表划分（（（（id-1）％3））;为（0）中的值创建河马表hippo_eastPartition（主键（id））;为河马表创建hippo_centralPartition（PRIMARY KEY（ id））（1）中的值；为（2）中的值创建河马表hippo_west分区（主键（id））；</p><p> This will prevent the primary keys from conflicting and remove the need for the statement trigger, but adding additional nodes will take a nontrivial amount of effort.</p><p> 这将防止主键冲突并消除对语句触发器的需要，但是添加其他节点将花费大量的精力。</p><p> How about high availability? Each node needs to have its own HA. The Postgres Operator simplifies this as you can deploy   HA clusters by default, and convert clusters to using HA with the    pgo scale command. The PostgreSQL Operator also has the added benefit of   built-in backups with pgBackRest, though note that each node may have a different view of the world at the time of backup.</p><p> 高可用性如何？每个节点都需要有自己的HA。 Postgres Operator可以简化此操作，因为您可以默认部署HA群集，并通过pgo scale命令将群集转换为使用HA。 PostgreSQL Operator还具有使用pgBackRest进行内置备份的附加好处，尽管请注意，在备份时每个节点可能对世界有不同的看法。</p><p> While this solution presents a way to achieve write scaling with PostgreSQL, note that it is not a &#34;set and forget&#34; solution: you do need to monitor your nodes, in particular, for the situation when a node is   unable to rejoin the cluster for a prolonged period of time. PostgreSQL 13 also adds a configuration parameter that allows for a replication slot to be dropped if it has gone unacknowledged for too long, though note that means you would have to resync the partition on the affected node.</p><p> 尽管此解决方案提供了一种使用PostgreSQL实现写扩展的方法，但请注意，它不是“设置后忘记”的设置。解决方案：您确实需要监视您的节点，尤其是当节点长时间无法重新加入群集时。 PostgreSQL 13还添加了一个配置参数，如果它未被确认太长时间，则允许删除复制插槽，不过请注意，这意味着您必须在受影响的节点上重新同步分区。</p><p> Any system that requires multiple writable nodes has challenges. The recent advancements in PostgreSQL make it possible to build out more complex architectures that solve complex data access and distribution problems while providing the robust developer functionality that drew me into Postgres into the first place!</p><p> 任何需要多个可写节点的系统都面临挑战。 PostgreSQL的最新进展使构建更复杂的体系结构成为可能，该体系结构解决了复杂的数据访问和分发问题，同时提供了强大的开发人员功能，使我深深地融入了Postgres！ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://info.crunchydata.com/blog/active-active-postgres-federation-on-kubernetes">https://info.crunchydata.com/blog/active-active-postgres-federation-on-kubernetes</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/联合会/">#联合会</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/hippo/">#hippo</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>