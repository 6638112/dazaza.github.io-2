<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>ML超越曲线拟合：因果推理和DO-COMPULE的介绍（2018） ML Beyond Curve Fitting: An Intro to Causal Inference and Do-Calculus (2018)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">ML Beyond Curve Fitting: An Intro to Causal Inference and Do-Calculus (2018)<br/>ML超越曲线拟合：因果推理和DO-COMPULE的介绍（2018） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-23 00:18:21</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/2916bd6015af3e9e1e962504cf897b6a.png"><img src="http://img2.diglog.com/img/2021/6/2916bd6015af3e9e1e962504cf897b6a.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>You might have come across  Judea Pearl&#39;s new book, and a  related interview which was widely shared in my social bubble. In the interview, Pearl dismisses most of what we do in ML as curve fitting. While I believe that&#39;s an overstatement (conveniently ignores RL for example), it&#39;s a nice reminder that most productive debates are often triggered by controversial or outright arrogant comments. Calling machine learning alchemy was a great recent example. After reading the article, I decided to look into his famous do-calculus and the topic causal inference once  again.</p><p>您可能已经遇到了犹太珍珠＆＃39;新书籍，以及在我的社交泡沫中广泛分享的相关面试。在采访中，珍珠驳回了我们在ml中的大部分曲线适合。虽然我相信＆＃39;＆＃39;虽然夸大了（方便地忽略了rl），它是一个很好的提醒，最富有成效的辩论常常被争议或彻底的傲慢评论触发。呼叫机学习炼金术是最近的一个伟大的例子。阅读文章后，我决定调查他着名的Do-Calmulus，并再次进行主题。</p><p> Again, because this happened to me semi-periodically. I first learned do-calculus in a (very unpopular but advanced) undergraduate course Bayesian networks. Since then, I have re-encountered it every 2-3 years in various contexts, but somehow it never really struck a chord. I always just thought &#34;this stuff is difficult and/or impractical&#34; and eventually forgot about it and moved on. I never realized how fundamental this stuff was, until now.</p><p> 再次，因为这发生在我周期性上。我首先在（非常不受欢迎但高级）本科课程贝叶斯网络中学习了DO-COMPULA。从那时起，我在各种情况下每2  -  3年重新遇到它，但不知怎的，它从未真正击中了和弦。我总是想到和＃34;这种东西是困难和/或不切实际的＆＃34;最终忘了它并继续前进。直到现在，我从未意识到这种东西有多基础。</p><p> This time around, I think I fully grasped the significance of causal reasoning and I turned into a full-on believer. I know I&#39;m late to the game but I almost think it&#39;s basic hygiene for people working with data and conditional probabilities to understand the basics of this toolkit, and I feel embarrassed for completely ignoring this throughout my career.</p><p> 这一次，我想我完全掌握了因果推理的重要性，并且我变成了一个全面的信徒。我知道我迟到了比赛，但我几乎认为它＆＃39;对于使用数据和条件概率来了解这个工具包的基础知识的人，我觉得在整个职业生涯中完全无视这个令人尴尬的卫生。</p><p> In this post I&#39;ll try to explain the basics, and convince you why you should think about this, too. If you work on deep learning, that&#39;s an even better reason to understand this. Pearl&#39;s comments may be unhelpful if interpreted as contrasting deep learning with causal inference. Rather, you should interpret it as highlighting causal inference as a huge, relatively underexplored, application of deep learning. Don&#39;t get discouraged by causal diagrams looking a lot like Bayesian networks (not a coincidence seeing they were both pioneered by Pearl) they don&#39;t compete with, they complement deep learning.</p><p> 在这篇文章中，我试图解释基础知识，并说服你为什么也应该考虑这一点。如果你在深入学习，那个＆＃39;这是一个更好的理解理解这一点。如果用因果推断被解释为对比深度学习，珍珠＆＃39的评论可能会无益。相反，您应该将其解释为突出显示因果推断作为深度学习的巨大，相对望远镜的应用。不鼓励被呼吁地看待很多贝叶斯网络（看不到珍珠的巧合）来劝阻，他们不竞争，他们补充了深入学习。</p><p>  First of all, causal calculus differentiates between two types of conditional distributions one might want to estimate.  tldr: in ML we usually estimate only one of them, but in some applications we should actually try to or have to estimate the other one.</p><p>  首先，因果演数在两种类型的条件分布之间区分了可能想要估计的。 TLDR：在ml中，我们通常只估计其中一个，但在某些应用中，我们实际上应该尝试或必须估计另一个。</p><p> To set things up, let&#39;s say we have i.i.d. data sampled from some joint $p(x,y,z,\ldots)$. Let&#39;s assume we have lots of data and the best tools (say, deep networks) to fully estimate this joint distribution, or any property, conditional or marginal distribution thereof. In other words, let&#39;s assume $p$ is known and tractable. Say we are ultimately interested in how variable $y$ behaves given $x$. At a high level, one can ask this question in two ways:</p><p> 让事情设置，让＆＃39; s说我们有i.i.d.从某种关联$ p（x，y，z，\ ldots）上采样的数据。假设我们假设我们有大量的数据和最佳工具（例如，深度网络），以完全估计这种联合分布，或其任何财产，条件或边际分布。换句话说，假设$ P $＆＃39假设是已知的和易行的。假设我们最终对$ y $的变量达到$ x $。在高水平，人们可以用两种方式提出这个问题：</p><p> observational $p(y\vert x)$: What is the distribution of $Y$ given that I  observe variable $X$ takes value $x$. This is what we usually estimate in supervised machine learning. It is a conditional distribution which can be calculated from $p(x,y,z,\ldots)$ as a ratio of two of its marginals. $p(y\vert x) = \frac{p(x,y)}{p(x)}$. We&#39;re all very familiar with this object and also know how to estimate this from data.</p><p> 观察到$ p（y \ vert x）$：$ y $的分布是什么，我观察到变量$ x $ trave $ x $。这就是我们通常在监督机器学习中估算的。它是一种条件分布，可以从$ p（x，y，z，\ ldots）计算，作为其边缘两个的比率。 $ p（y \ vert x）= \ frac {p（x，y）} {p（x）} $。我们非常熟悉这个对象，也知道如何从数据估算这一点。 </p><p> interventional $p(y\vert do(x))$: What is the distribution of $Y$ if I were to  set the value of $X$ to $x$. This describes the distribution of $Y$ I would observe if I intervened in the data generating process by artificially forcing the variable $X$ to take value $x$, but otherwise simulating the rest of the variables according to the original process that generated the data. (note that the data generating procedure is NOT the same as the joint distribution $p(x,y,z,\ldots)$ and this is an important detail).</p><p>介入$ p（y \ vert do（x））$：$ y $的分布是什么，如果我将值设置为$ x $。这描述了$ y $ i将观察到我会观察到在数据生成过程中，人工迫使变量$ x $占据价值$ x $，但根据生成的原始过程模拟了变量的其余部分数据。 （请注意，数据生成过程与联合分配$ p（x，y，z，\ ldots）$，这是一个重要细节）。</p><p>  No. $p(y\vert do(x))$ and $p(y\vert x)$ are not generally the same, and you can verify this with several simple thought experiments. Say, $Y$ is the pressure in my espresso machine&#39;s boiler which ranges roughly between $0$ and $1.1$ bar depending on how long it&#39;s been turned on. Let $X$ be the reading of the built-in barometer. Let&#39;s say we jointly observe X and Y at random times. Assuming the barometer functions properly $p(y|x)$ should be a unimodal distribution centered around $x$, with randomness due to measurement noise. However, $p(y|do(x))$ won&#39;t actually depend on the value of $x$ and is generally the same as $p(y)$, the marginal distribution of boiler pressure. This is because artificially setting my barometer to a value (say, by moving the needle) won&#39;t actually cause the pressure in the tank to go up or down.</p><p>  没有。$ p（y \ vert do（x））$和$ p（y \ vert x）$通常是相同的，并且您可以用几个简单的思想实验验证。说，$ y $是我浓缩咖啡机的压力和＃39; s锅炉，大约在0美元和1.1美元的$ 1.1 $酒吧，具体取决于它＆＃39被打开了多长时间。让$ x $是读取内置晴雨表。让＆＃39;说我们在随机时间联合观察x和y。假设晴雨表功能正常$ P（y | x）$应该是一个左右的单向分布，x $左右，由于测量噪音，随机性。但是，$ p（y | do（x））$ won＆＃39; t实际上取决于$ x $的值，通常与$ p（y）$通常相同，锅炉压力的边缘分布。这是因为人工地将晴雨表设定为值（例如，通过移动针）Won＆＃39; T实际上导致坦克中的压力上升或下降。</p><p> In summary, $y$ and $x$ are correlated or statistically dependent and therefore seeing $x$ allows me to predict the value of $y$, but $y$ is not caused by $x$ so setting the value of $x$ won&#39;t effect the distribution of $y$. Hence, $p(y\vert x)$ and $p(y\vert do(x))$ behave very differently. This simple example is just the tip of the iceberg. The differences between interventional and observational conditionals can be a lot more nuanced and hard to characterize when there are lots of variables with complex interactions.</p><p> 总之，$ y $和$ x $是关联或统计上依赖的，因此看到$ x $允许我预测$ y $的值，但$ y $不是由$ x $造成的$ x $ sto设置$ x $ won＆＃39; t影响$ y $的分配。因此，$ p（y \ vert x）$和$ p（y \ vert do（x））$表现得非常不同。这个简单的例子只是冰山一角。介入和观察条件之间的差异可能更为细微，并且难以表征，当存在复杂的相互作用的许多变量时。</p><p>  Depending on the application you want to solve, you should seek to estimate one of these conditionals. If your ultimate goal is diagnosis or forecasting (i.e. observing a naturally occurring $x$ and inferring the probable values of $y$) you want the observational conditional $p(y\vert x)$. This is what we already do in supervised learning, this is what Judea Pearl called curve fitting. This is all good for a range of important applications such as classification, image segmentation, super-resolution, voice transcription, machine translation, and many more.</p><p>  根据您想要解决的应用程序，您应该寻求估计其中一个条件。如果您的最终目标是诊断或预测（即观察自然发生的$ x $并推断出$ y $的可能值），则需要观察条件$ p（y \ vert x）$。这就是我们在监督学习中所做的事情，这就是犹太珍珠叫曲线的珍珠。这对于一系列重要的应用程序都很好，如分类，图像分割，超分辨率，语音转录，机器翻译等等。</p><p> In applications where you ultimately want to control or choose $x$ based on the conditional you estimated, you should seek to estimate $p(y\vert do(x))$ instead. For example, if $x$ is a medical treatment and $y$ is the outcome, you are not merely interested in observing a naturally occurring treatment $x$ and predicting the outcome, we want to  proactively choose the treatment $x$ given our understanding of how it effects the outcome $y$. Similar situations occur in system identification, control and online recommender systems.</p><p> 在您最终想要控制或根据估计的条件选择$ x $的应用中，您应该寻求估计$ p（y \ vert do（x））$。例如，如果$ x $是一个医疗和$ y $是结果，你不仅对观察自然发生的治疗$ x $和预测结果，我们不仅感兴趣，我们希望主动选择治疗$ x $ ket了解它如何影响结果$ y $。在系统识别，控制和在线推荐系统中发生类似情况。</p><p>  This is perhaps the main concept I haven&#39;t grasped before. $p(y\vert do(x))$ is in fact a vanilla conditional distribution, but it&#39;s not computed based on $p(x,z,y,\ldots)$, but a different joint $p_{do(X=x)}(x,z,y,\ldots)$ instead. This $p_{do(X=x)}$ is the joint distribution of data which we would observe if we actually carried out the intervention in question. $p(y\vert do(x))$ is the conditional distribution we would learn from data collected in  randomized controlled trials or A/B tests where the experimenter controls $x$. Note that actually carrying out the intervention or randomized trials may be impossible or at least impractical or unethical in many situations. You can&#39;t do an A/B test forcing half your subjects to smoke weed and the other half to smoke placebo to understand the effect on marijuana on their health. Even if you can&#39;t directly estimate $p(y\vert do(x))$ from randomized experiments, the object still exists. The main point of causal inference and do-calculus is:</p><p>  这也许是我在之前的主要概念＆＃39; t之前。 $ p（y \ vert do（x））$实际上是vanilla条件分布，但它没有基于$ p（x，z，y，\ ldots）$，但不同的联合$ p_ {do（x = x）}（x，z，y，\ ldots）$。这是$ p_ {do（x = x）} $是我们将在实际执行有关干预的情况下观察的数据的联合分布。 $ p（y \ vert do（x））$是条件分布，我们将从随机控制试验中收集的数据或A / B测试中学习，其中实验者控制$ x $。请注意，在许多情况下，实际执行干预或随机试验可能是不可能的或至少是不切实际的或不道德的。你可以＆＃39; t做一个/ b试验强迫你的一半受试者吸烟，另一半到抽烟，以了解大麻的影响。即使您可以直接估计$ p（y \ vert do（x））$从随机实验，仍然存在。因果推断和DO-COMPULUS的主要点是：</p><p> If I cannot measure $p(y\vert do(x))$ directly in a randomized controlled trial, can I estimate it based on data I observed outside of a controlled experiment?</p><p> 如果我无法直接测量$ p（y \ vert do（x））$直接在随机对照试验中，我可以根据我在受控实验之外观察到的数据来估计它吗？ </p><p>  Let&#39;s start with a diagram that shows what&#39;s going on if we only care about $p(y\vert x)$, i.e. the simple supervised learning case:</p><p>让＆＃39开始使用一个图表，如果我们只关心$ p（y \ vert x）$，即简单的监督学习案例：</p><p>  Let&#39;s say we observe 3 variables, $x, z, y$, in this order. Data is sampled i.i.d. from some observable joint distribution over 3 variables, denoted by the blue factor graph labelled &#39;observable joint&#39;. If you don&#39;t know what a factor graph is, it&#39;s not important, the circles represent random variables, the little square represents a joint distribution of the variables it&#39;s connected to. We are interested in predicting $y$ from $x$, and say that $z$ is a third variable which we do not want to infer but we can also measure (I included this for completeness). The observational conditional $p(y\vert x)$ is calculated from this joint via simple conditioning. From the training data we can build a model $q(y\vert x;\theta)$ to approximate this conditional, for example using a deep net minimizing cross-entropy or whatever.</p><p>  让＆＃39; s表示我们观察到3个变量，$ x，z，y $，按此顺序。数据被采样I.i.d.从3个变量的一些可观察的关节分布，由标记为＆＃39的蓝色因子图表示;可观察的关节＆＃39;如果你不知道是什么因子图，它＆＃39;不重要，圆圈代表随机变量，小方块代表了变量的接头分布＆＃39; s连接的变量。我们有兴趣预测$ y $ x $，并说$ z $是我们不想推断的第三种变量，但我们也可以衡量（我包括完整性，我包含这个）。观察条件$ P（Y \ VERT X）$通过简单的调节计算。从训练数据来看，我们可以构建$ q（y \ vert x; \ theta）$以近似该条件，例如使用深净最小化跨熵或其他任何条件。</p><p> Now, what if we&#39;re actually interested in $p(y\vert do(x))$ rather than $p(y\vert x)$? This is what it looks like:</p><p> 现在，如果我们＆＃39;重新对$ p（y \ vert do（x））$而不是$ p（y \ vert x）$？这是它的样子：</p><p>  So, we still have the blue observed joint and data is still sampled from this joint. However, the object we wish to estimate is on the bottom right, the red intervention conditional $p(y\vert do(x))$. This is related to the intervention joint which is denoted by the red factor graph above it. It&#39;s a joint distribution over the same domain as $p$ but it&#39;s a different distribution. If we could sample from this red distribution (e.g. actually run a randomized controlled trial where we get to pick $x$), the problem would be solved by simple supervised learning. We could generate data from the red joint, and estimate a model directly from there. However, we assume this is not possible, and all we have is data sampled from the blue joint. We have to see if we can somehow estimate the red conditional $p(y\vert do(x))$ from the blue joint.</p><p>  因此，我们仍然具有蓝色观察的关节，数据仍然从该联合中取样。但是，我们希望估计的对象位于右下方，红色干预条件$ p（y \ vert do（x））$。这与干预接头有关，其由其上方的红色因子图表示。它＆＃39;在同一个域名的联合分布，但它与不同的分布不同。如果我们可以从这个红色分布中进行采样（例如，实际上运行随机对照试验我们得到$ x $），那么问题将通过简单的监督学习来解决。我们可以从红色关节生成数据，并直接从那里估算模型。但是，我们假设这是不可能的，我们所拥有的只是从蓝色关节采样的数据。我们必须看看我们是否可以以某种方式估计红色条件$ p（y \ vert do（x））$从蓝色关节。</p><p>  If we want to establish a connection between the blue and the red joints,  we must introduce additional assumptions about the causal structure of the data generating mechanism. The only way we can make predictions about how our distribution changes as a consequence of an interaction is if we know how the variables are causally related. This information about causal relationships is not captured in the joint distribution alone. We have to introduce something more expressive than that. Here is how what this looks like:</p><p>  如果我们想在蓝色和红色关节之间建立连接，我们必须引入关于数据生成机制的因果结构的额外假设。我们唯一可以做出关于我们如何由于交互而改变的预测的方法是，如果我们知道变量是有因果关系的。单独的联合分配没有捕获有关因果关系的信息。我们必须介绍比这更具表现力的东西。这是它看起来的样子如何：</p><p>  In addition to the observable joint we now also have a causal model of the world (top left) This causal model contains more detail than the joint distribution: it knows not only that pressure and barometer readings are dependent but also that pressure causes the barometer to go up and not the other way around. The arrows in this model correspond to the assumed direction of causation, and the absence of an arrow represents the absence of direct causal influence between variables. The mapping from causal diagrams to joint distributions is many-to-one: several causal diagrams are compatible with the same joint distribution. Thus, it is generally impossible to conclusively choose between different causal explanations by looking at observed data only.</p><p>  除了可观察的关节之外，我们现在还有世界的因果模型（左上角）这种因果模型含有比联合分布更多的细节：它不仅知道压力和晴雨表读数都是依赖性的，而且还要依赖于压力，而且压力导致晴雨表也是如此。上去，而不是其他方式。该模型中的箭头对应于假设的原因方向，并且不存在箭头表示变量之间的缺乏因果关系。从因果图到联合分布的映射是多对一：几个因果图与相同的联合分布兼容。因此，通常不可能通过仅查看观察到的数据来结论不同的因果解释之间。</p><p> Coming up with a causal model is a modeling step where we have to consider assumptions about how the world works, what causes what. Once we have a causal diagram, we can emulate the effect of intervention by mutilating the causal network: deleting all edges that lead into nodes in a $do$ operator. This is shown on the middle-top panel. The mutilated causal model then gives rise to a joint distribution denoted by the green factor graph. This joint has a corresponding conditional distribution $\tilde{p}(y\vert do(x))$, which we can use as our approximation of $p(y\vert do(x))$. If we got the causal structure qualitatively right (i.e. there are no missing nodes and we got the direction of arrows all correct), this approximation is exact and $\tilde{p}(y\vert do(x)) = p(y\vert do(x))$. If our causal assumptions are wrong, the approximation may be bogus.</p><p> 提出了因果模型是一个建模步骤，我们必须考虑对世界如何工作的假设，导致的原因。一旦我们有一个因果关系图，我们就可以通过难以使因果网络进行干预的效果：删除以$ Operator以$ Operator删除导致节点的所有边缘。这在中顶面板上显示。然后肢体的因果模型产生了由绿色因子图表示的关节分布。该关节具有相应的条件分配$ \ tilde {p}（y \ vert do（x））$，我们可以用作我们的$ p的近似值（y \ vert do（x））$。如果我们有定性正确的原因结构（即没有缺少的节点，我们得到箭头的方向，如果所有正确的箭头的方向），则这个近似是精确的，$ \ tilde {p}（y \ vert do（x））= p（y \ vert do（x））$。如果我们的因果假设是错误的，则近似可能是虚假的。 </p><p> Critically, to get to this green stuff, and thereby to establish the bridge between observational data and interventional distributions, we had to combine data with additional assumptions, prior knowledge if you wish. Data alone would not enable us to do this.</p><p>批判性地，要达到这种绿色的东西，从而建立观察数据和介入分布之间的桥梁，我们必须将数据与其他假设相结合，如果您愿意，先验知识。单独的数据不会让我们这样做。</p><p>  Now the question is, how can we say anything about the green conditional when we only have data from the blue distribution. We are in a better situation than before as we have the causal model relating the two. To cut a long story short, this is what the so-called  do-calculus is for. Do-calculus allows us to massage the green conditional distribution until we can express it in terms of various marginals, conditionals and expectations under the blue distribution. Do-calculus extends our toolkit of working with conditional probability distributions with four additional rules we can apply to conditional distributions with the $do$ operators in them. These rules take into account properties of the causal diagram. The details can&#39;t be compressed into a single blog post, but here is  an introductory paper on them..</p><p>  现在问题是，当我们只有来自蓝色分布的数据时，我们如何在绿色条件下说任何事情。我们处于更好的情况，而不是我们拥有与之相关的因果模型。为了缩短一个长话，这就是所谓的Do-Scalulus是为了。 DO-COMPULUS允许我们按摩绿色条件分布，直到我们在蓝色分布下的各种边缘，条件和期望方面表达它。 Do-Calculus扩展了我们使用四个附加规则的条件概率分布的工具包，我们可以应用于它们中的$ DO $运算符的条件分发。这些规则考虑了因果关系图的属性。细节可以＆＃39; t被压缩成一个博客文章，但这是它们的介绍文件..</p><p> Ideally, as a result of a do-calculus derivation you end up with an equivalent formula for $\tilde{p}(y\vert do(x))$ which no longer has any do operators in them, so you estimate it from observational data alone. If this is the case we say that the causal query $\tilde{p}(y\vert do(x))$ is  identifiable. Conversely, if this is not possible, no matter how hard we try applying do-calculus, we call the causal query  non-identifiable, which means that we won&#39;t be able to estimate it from the data we have. The diagram below summarizes this causal inference machinery in its full glory.</p><p> 理想情况下，由于DO-COMPULAS导出，您最终有一个等效的$ \ tilde {p}（y \ vert do（x））$，它们不再有任何操作员，所以您估计它单独观察数据。如果是这种情况，我们会说因果查询$ \ tilde {p}（y \ vert do（x））是可识别的。相反，如果这是不可能的，无论我们尝试申请Do-Calculus多么努力，我们都会致电因果解法不可识别，这意味着我们能够从我们拥有的数据中估计它。下图总结了这一因果推理机械的全部荣耀。</p><p>  The new panel called &#34;estimable formula&#34; shows the equivalent expression for $\tilde{p}(y\vert do(x))$ obtained as a result of the derivation including several do-calculus rules. Notice how the variable $z$ which is completely irrelevant if you only care about $p(y\vert x)$ is now needed to perform causal inference. If we can&#39;t observe $z$ we can still do supervised learning, but we won&#39;t be able to answer causal inference queries $p(y\vert do(x))$.</p><p>  新面板名为＆＃34;可评估公式＆＃34;显示作为$ \ tilde {p}（y \ vert do（x））$的等效表达式，因为包括多个DO-COMBULUS规则的导出。请注意，如果您只关心$ p（y \ vert x）$执行因果推断，则如何如何无关，这是如何完全无关紧要的。如果我们可以＆＃39; t观察$ z $我们仍然可以监督学习，但我们赢得了＆＃39; t能够回答因果推断查询$ p（y \ vert do（x））$。</p><p>  You can never fully verify the validity and completeness of your causal diagram based on observed data alone. However, there are certain aspects of the causal model which are empirically testable. In particular, the causal diagram implies certain conditional independence or dependence relationships between sets of variables. These dependencies or independencies can be empirically tested, and if they are not present in the data, that is an indication that your causal model is wrong. Taking this idea forward you can attempt to perform full causal discovery: attempting to infer the causal model or at least aspects of it, from empirical data.</p><p>  您可以基于观察到的数据，完全验证原因图的有效性和完整性。然而，存在经验验证的因果模型的某些方面。特别地，因果关系图暗示了一组变量之间的条件独立性或依赖关系。这些依赖项或独立性可以经验测试，如果它们不存在于数据中，则表示您的因果模型是错误的指示。采取这个想法前进您可以尝试从经验数据中尝试推断出原因模型或至少方面的完整因果区发现。</p><p> But the bottom line is: a full causal model is a form of prior knowledge that you have to add to your analysis in order to get answers to causal questions without actually carrying out interventions. Reasoning with data alone won&#39;t be able to give you this. Unlike priors in Bayesian analysis - which are a nice-to-have and can improve data-efficiency - causal diagrams in causal inference are a must-have. With a few exceptions, all you can do without them is running randomized controlled experiments.</p><p> 但是，底线是：一个完整​​的因果模型是一种先验知识的形式，您必须添加到分析中，以便在没有实际进行干预的情况下获得因果问题的答案。单独推理数据赢得＆＃39;能够给你这个。与贝叶斯分析中的前瞻不同 - 这是一个很好的，并且可以提高数据效率 - 因果推理中的因果图是必备的。有几个例外情况，您可以在没有它们的情况下运行随机控制实验。</p><p>  Causal inference is indeed something fundamental. It allows us to answer &#34;what-if-we-did-x&#34; type questions that would normally require controlled experiments and explicit interventions to answer. And I haven&#39;t even touched on counterfactuals which are even more powerful.</p><p>  因果推断确实是基本的。它允许我们回答＆＃34;什么 -  we-did-x＆＃34;通常需要受控实验和明确干预措施的键入问题。而且我甚至触及了更强大的反事实甚至触及了。 </p><p> You can live without this in some cases. Often, you really just want to do normal inference. In other applications such as model-free RL, the ability to explicitly control certain variables may allow you to sidestep answering causal questions explicitly. But there are several situations, and very important applications, where causal inference offers the only method to solve the problem in a principled way.</p><p>在某些情况下，您可以没有这种情况。通常，你真的只想做正常推断。在诸如无模型RL的其他应用程序中，明确控制某些变量的能力可能允许您明确地回答因果问题。但是存在几种情况和非常重要的应用程序，其中因果推断提供了以原则方式解决问题的唯一方法。</p><p> I wanted to emphasize again that this is not a question of whether you work on deep learning or causal inference. You can, and in many cases you should, do both. Causal inference and do-calculus allows you to understand a problem and establish what needs to be estimated from data based on your assumptions captured in a causal diagram. But once you&#39;ve done that, you still need powerful tools to actually estimate that thing from data. Here, you can still use deep learning, SGD, variational bounds, etc. It is this cross-section of deep learning applied to causal inference which the recent article with Pearl claimed was under-explored.</p><p> 我想再次强调，这不是您是否在深入学习或因果推断上工作的问题。您可以，在许多情况下，您应该，兼顾。因果推断和DO-COMPULA允许您了解问题并确定需要根据在因果图中捕获的假设来从数据估计的内容。但是一旦你完成了，你仍然需要强大的工具来实际估计来自数据的东西。在这里，您仍然可以使用深度学习，SGD，变分界等。它是应用于因因果推理的这种深度学习的横截面，最近的珍珠文章索赔是探讨的。</p><p> UPDATE: In the comments below people actually pointed out some relevant papers (thanks!). If you are aware of any work, please add them there.</p><p> 更新：在下面的评论中，人们实际指出了一些相关论文（谢谢！）。如果您了解任何工作，请在那里添加它们。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.inference.vc/untitled/">https://www.inference.vc/untitled/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/超越/">#超越</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/curve/">#curve</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>