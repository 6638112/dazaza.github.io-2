<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>2020年的新设置 New setup for 2020</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">New setup for 2020<br/>2020年的新设置 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-27 07:30:29</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/d120b123b23c0efe0814098e10b00187.png"><img src="http://img2.diglog.com/img/2020/12/d120b123b23c0efe0814098e10b00187.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>We talked about the changelog.com setup for 2019 in  episode #344, and also captured it last year  in this post. At the time, changelog.com was running on Docker Swarm, everything was managed by Terraform, and the end-result was simpler &amp; better because:</p><p>我们在第344集中讨论了2019年的changelog.com设置，并在去年的这篇文章中也进行了介绍。当时，changelog.com在Docker Swarm上运行，所有内容均由Terraform管理，最终结果更加简单更好是因为：</p><p>  This year, we simplified and improved the changelog.com setup further by replacing Docker Swarm and Terraform with Linode Kubernetes Engine (LKE). Not only is the new setup more cohesive, but deploys are 20% faster, changelog.com is more resilient with a mean time to recovery (MTTR) of just under 8 minutes, and interacting with the entire setup is done via a single pane of glass:</p><p>  今年，我们用Linode Kubernetes Engine（LKE）取代了Docker Swarm和Terraform，从而进一步简化和改进了changelog.com的设置。新设置不仅更具凝聚力，而且部署速度提高了20％，changelog.com的恢复能力更强，平均恢复时间（MTTR）不到8分钟，并且通过单个窗格即可完成整个设置的交互玻璃：</p><p>    We will answer all these questions, and also cover what worked well, what could have been better, and what comes next for changelog.com. Let’s start with the big one:</p><p>    我们将回答所有这些问题，并介绍有效的方法，可能会更好的方法以及changelog.com的后续内容。让我们从大的开始：</p><p>  Because managing infrastructure doesn’t bring any value to Changelog, the business. Kubernetes enables us to declare stateless &amp; stateful workloads, web traffic ingresses, certificates and other higher level concepts instead of servers, load balancers, and other similar lower-level concepts. At the end of the day, we don’t care how things happen, as long as the following happens: automatic zero-downtime deploys on every commit, daily backups, minimal disruption when an actual server goes away, etc. Kubernetes makes this easy.</p><p>  由于管理基础架构不会为Changelog带来任何价值，因此对企业而言。 Kubernetes使我们能够声明无状态＆amp;状态工作负载，Web流量入口，证书和其他较高级别的概念，而不是服务器，负载平衡器和其他类似的较低级别的概念。归根结底，只要发生以下情况，我们就不会在意事情的发生：在每次提交时自动部署零停机时间，每天备份，在实际服务器消失时将中断降到最低等。Kubernetes使此操作变得容易。</p><p> Let us expand with a specific example: what business value does renewing the TLS certificate bring? Yes, all web requests that we serve must be secured, and the SSL certificate needs to be valid, but why should we spend any effort renewing it?</p><p> 让我们以一个特定的示例进行扩展：续订TLS证书带来了哪些业务价值？是的，我们提供的所有Web请求都必须得到保护，并且SSL证书必须有效，但是为什么我们要花些精力来更新它？</p><p> Maybe your IaaS provider already manages SSL for you, but does that mean that you are forced to use the CDN service from the same provider? Or does your CDN provider manage one certificate, and your IaaS provider another certificate?</p><p> 也许您的IaaS提供商已经为您管理SSL，但这是否意味着您被迫使用同一提供商的CDN服务？还是您的CDN提供程序管理一个证书，而IaaS提供程序管理另一个证书？</p><p> In our case, even if Fastly, our CDN partner, can manage Let’s Encrypt (LE) certificates for us, we need to figure out how to keep them updated in our Linode NodeBalancer (a.k.a. load balancer). We have learned this the hard way, but if Fastly manages LE certificates for a domain, no other LE provisioner can be configured for that domain. Our load balancer needs TLS certificates as well, and if we use LE via Fastly, we can’t get LE certificates for the load balancer. While this is a very specific example from our experience, it shows the type of situations that can take away time &amp; effort from higher-value work.</p><p> 就我们而言，即使我们的CDN合作伙伴Fastly可以为我们管理Let's Encrypt（LE）证书，我们也需要弄清楚如何在我们的Linode NodeBalancer（又称负载均衡器）中对其进行更新。我们很难学到这一点，但是，如果快速管理某个域的LE证书，则无法为该域配置其他LE供应商。我们的负载均衡器也需要TLS证书，如果我们通过“快速”使用LE，则无法获得负载均衡器的LE证书。根据我们的经验，这是一个非常具体的示例，它显示了可能占用时间和时间的情况类型。高价值工作的努力。 </p><p> In our Linode Kubernetes Engine setup,  cert-manager is responsible for managing certificates for all our domains, and  a job keeps them synchronised in Fastly. If we need to check on the state of all our certificates, we can do it from the same pane of glass mentioned earlier:</p><p>在我们的Linode Kubernetes Engine设置中，cert-manager负责管理我们所有域的证书，并通过一项工作使它们保持快速同步。如果我们需要检查所有证书的状态，则可以从前面提到的同一窗格中进行检查：</p><p>   A single  Linode CLI command gets us a managed Kubernetes cluster, and a few commands later we have changelog.com and all its dependent services up and running, including DNS, TLS &amp; CDN integration.  We no longer provision load balancers, or configure DNS; we simply describe the resources that we need, and Kubernetes makes it happen.</p><p>   单个Linode CLI命令为我们提供了一个托管的Kubernetes集群，随后还有一些命令使我们可以运行changelog.com及其所有相关服务，包括DNS，TLS和Amp。 CDN集成。我们不再提供负载平衡器或配置DNS；我们只需描述我们需要的资源，Kubernetes即可实现。</p><p> Yes, we need to install additional components such as  ingress-nginx,  cert-manager,  postgres-operator and a few others, but once we do this, we get to use higher-level resources such as PostgreSQL clusters, Let’s Encrypt certificates and ingresses that handle load balancer provisioning.</p><p> 是的，我们需要安装其他组件，例如ingress-nginx，cert-manager，postgres-operator和其他一些组件，但是一旦完成，我们就可以使用更高级别的资源，例如PostgreSQL集群，加密证书和入口处理负载均衡器配置。</p><p> Did I mention that the Kubernetes is managed? That means updates are taken care of, and the requested number of nodes will be kept running at all times. If one of the VMs that runs a Kubernetes node gets deleted, it will be automatically re-created a few minutes later, and everything that should be running on it will be restored. This changes the conversation from   the IaaS provisioner needs to run and converge on the desired state to   Kubernetes will automatically fix everything within 10 minutes.</p><p> 我是否提到过Kubernetes是托管的？这意味着将处理更新，并且请求数量的节点将始终保持运行。如果删除了运行Kubernetes节点的其中一台虚拟机，则几分钟后将自动重新创建该虚拟机，并且应在该虚拟机上运行的所有内容都将被还原。这改变了从IaaS供应商运行所需的会话并收敛到所需状态到Kubernetes的对话，它将在10分钟内自动修复所有问题。</p><p>   We are convinced that you have heard about “production Kubernetes” many times in recent years. Maybe you have even tried some examples out, and had it running on your local machine via  Kind or  k3sup. For the vast majority, the beginning is easy, but running production on Kubernetes is hard. Many who start don’t fully complete the migration. Even with  Monzo and  Zalando sharing Kubernetes in production stories for years, it doesn’t make it any easier for you.</p><p>   我们坚信，近年来您已经多次听说过“生产Kubernetes”。也许您甚至尝试了一些示例，并通过Kind或k3sup在本地计算机上运行它。对于绝大多数人来说，开始很容易，但是在Kubernetes上运行生产却很困难。许多新手并没有完全完成迁移。即使Monzo和Zalando在制作故事中分享Kubernetes多年了，也并没有使您更轻松。</p><p> The best way that we can think of helping your production Kubernetes journey is  by making all our code available, including all the commit history that got us where we are today. If you spend time exploring past commits, you will even find links to public discussions with our technology partners that lead to specific improvements in Linode Kubernetes Engine (LKE), as well as other related areas such as  kube-prometheus &amp;  fastly-cli.</p><p> 我们可以考虑帮助您的Kubernetes生产过程的最佳方法是使所有代码可用，包括使我们今天处于今天的所有提交历史。如果您花时间探索过去的提交，您甚至会找到与我们的技术合作伙伴进行公开讨论的链接，从而导致Linode Kubernetes Engine（LKE）以及其他相关领域（例如kube-prometheus＆amp;快速cli。</p><p> Changelog.com is proof that running on production on Kubernetes can be straightforward, and it works well. Even if your team is small and time available for infrastructure-related work is limited, you can use our approach to get you going, then modify and adapt as and when needed. If changelog.com has been running on this setup with improved availability, responsiveness and resiliency, so can your production web app.</p><p> Changelog.com证明了在Kubernetes上的生产环境上运行非常简单，并且运行良好。即使您的团队很小，可用于基础架构相关工作的时间也很有限，您也可以使用我们的方法开始工作，然后在需要时进行修改和适应。如果changelog.com已在此设置上运行且具有更高的可用性，响应能力和弹性，那么您的生产Web应用程序也可以。 </p><p>  It all started at KubeCon + CloudNativeCon North America 2019 (remember  #374 &amp;  #375?), when I met with Hillary &amp; Mike from Linode. They gave me early access to Linode Kubernetes Engine (LKE) and one Linode CLI command later, we had our first three-node Kubernetes cluster running in Newark, New Jersey.</p><p>当我与希拉里（Hillary）和安德鲁（Hampary）见面时，一切始于KubeCon + CloudNativeCon North America 2019（还记得＃374和＃375？）来自Linode的Mike。他们让我可以尽早使用Linode Kubernetes引擎（LKE）和一个Linode CLI命令，之后，我们在新泽西州的纽瓦克运行了我们的第一个三节点Kubernetes集群。</p><p> By the way, if you want early access to new Linode new products before they hit the market and provide valuable feedback to influence product direction, you can join  Linode Green Light too.</p><p> 顺便说一句，如果您想在新的Linode新产品上市之前就尽早使用它们并提供有价值的反馈意见来影响产品发展方向，那么您也可以加入Linode Green Light。</p><p>   Monitoring was a bit trickier because we had to figure out how to do Prometheus &amp; Grafana in Kubernetes properly. There is some conflicting advice on how to get this up and running, and  the documentation didn’t work for us, so we have contributed our solution. Out of all the available approaches, we have settled on the  kube-prometheus operator which gives us plenty of insight into Kubernetes and system metrics out of the box. We are yet to integrate kube-prometheus with other services such as ingress-nginx, PostgreSQL, Phoenix etc.</p><p>   监控有些棘手，因为我们必须弄清楚Prometheus＆amp; amp;的方法。在Kubernetes中正确使用Grafana。关于如何启动和运行它，存在一些相互矛盾的建议，并且文档对我们不起作用，因此我们贡献了我们的解决方案。在所有可用的方法中，我们选择了kube-prometheus运算符，该运算符使我们可以开箱即用地洞悉Kubernetes和系统指标。我们尚未将kube-prometheus与其他服务集成，例如ingress-nginx，PostgreSQL，Phoenix等。</p><p> As soon as we had  grafana.changelog.com in place and all baseline components worked well together, we moved to setting up a static instance of changelog.com. This would give us a clear picture of how a simple changelog.com app would work in various conditions (upgrades, loss of nodes, etc.). The resulting artefact was temporary -  a stepping stone - which kept things simple and allowed us to observe web request latencies and error rates in various failure scenarios. Our findings gave us confidence in LKE and we gave it the green light by  sharing our Linode customer story.</p><p> 一旦我们安装了grafana.changelog.com并且所有基线组件都可以很好地协同工作，我们便着手建立了changelog.com的静态实例。这将使我们清楚地了解简单的changelog.com应用在各种情况下（升级，节点丢失等）如何工作。由此产生的伪像是暂时的-一块垫脚石-使事情变得简单，并允许我们观察各种失败情况下的Web请求延迟和错误率。我们的发现使我们对LKE充满信心，并通过分享我们的Linode客户故事为它开了绿灯。</p><p> We settled on  Crunchy Data PostgreSQL Operator for running our production PostgreSQL on K8S. Just as we have been running PostgreSQL on Docker for years with no issues, PostgreSQL on K8S via Crunchy is an even better experience from our perspective. We are happy with the outcome, even if we don’t leverage all of Crunchy PostgreSQL features just yet.</p><p> 我们选择了Crunchy Data PostgreSQL Operator在K8S上运行生产PostgreSQL。正如我们在Docker上运行PostgreSQL多年没有问题一样，从我们的角度来看，通过Crunchy在K8S上运行PostgreSQL是一个更好的体验。即使我们尚未充分利用Crunchy PostgreSQL的所有功能，我们也对结果感到满意。</p><p> For zero-downtime automatic app updates, we chose  Keel. CircleCI continues to build, test &amp; publish container images to DockerHub. The decision that we have made for our 2019 setup to keep a clean separation between CI &amp; production paid off: we didn’t have to change anything in this part of our workflow. We could build a new second half of the workflow based on Kubernetes, while continuing to run the existing Docker Swarm one. In the new workflow, when the changelog.com container image gets updated, DockerHub sends a webhook event to Keel, which is just another deployment running in LKE. In response to this event, Keel updates the changelog.com deployment because we always want to run the latest commit that passes the build &amp; test stages. Yes, we know that GitOps via  Flux or  ArgoCD is a more comprehensive and robust approach, but in this case we chose the simplest thing that works for us. Having said that,  this discussion with @fwiles makes us think that we should revisit Flux soon.</p><p> 对于零停机时间自动应用程序更新，我们选择了Keel。 CircleCI继续构建，测试和测试将容器映像发布到DockerHub。我们为2019年的安装方案做出的决定是保持CI与A生产得到回报：我们无需在工作流程的这一部分进行任何更改。我们可以在Kubernetes的基础上构建新的工作流程的后半部分，同时继续运行现有的Docker Swarm。在新的工作流程中，当changelog.com容器映像更新时，DockerHub将一个webhook事件发送给Keel，这只是LKE中运行的另一个部署。为响应此事件，Keel更新了changelog.com部署，因为我们始终希望运行通过构建和测试的最新提交。测试阶段。是的，我们知道通过Flux或ArgoCD的GitOps是一种更全面，更强大的方法，但是在这种情况下，我们选择了最适合我们的方法。话虽如此，与@fwiles的讨论使我们认为我们应该尽快重新访问Flux。</p><p> Throughout our entire changelog.com on Kubernetes journey, Andrew, a Linode engineer, was always there to help with a recommendation, fix or simply bounce some ideas off. We talked about Kubernetes upgrades and helped make them better (first implementation of  linode-cli lke pool recycle would update all nodes at once, meaning significant downtime), kicked off  PROXY protocol support for LoadBalancer Services, and touched on persistent volumes performance. If there was one thing that captures our interaction with Andrew best,  it is this highly detailed &amp; clear example on how to configure PROXY protocol support on LKE.</p><p> 在我们整个Kubernetes旅程的changelog.com上，Linode工程师Andrew总是在场以提供建议，修正或简单地提出一些想法。我们讨论了Kubernetes的升级并帮助他们提高了性能（首先实施linode-cli lke池回收将立即更新所有节点，这意味着大量的停机时间），启动了对LoadBalancer Services的PROXY协议支持，并谈到了持久卷性能。如果有一件事最能说明我们与安德鲁的互动，那就是这个高度详细的内容。关于如何在LKE上配置PROXY协议支持的清晰示例。 </p><p>   All put together, the time that it took us to migrate from Docker Swarm to Kubernetes was around three weeks. Most of this time was spent experimenting and getting comfortable with the baseline components such as ingress-nginx, external-dns, cert-manager etc. Taking into account  how complex the K8S ecosystem is regarded to be, I would say that our changelog.com migration to LKE worked well.</p><p>综上所述，我们从Docker Swarm迁移到Kubernetes所花费的时间大约是三周。这段时间中的大部分时间都花在试验和适应基线组件上，例如ingress-nginx，external-dns，cert-manager等。考虑到K8S生态系统被认为多么复杂，我想说一下我们的changelog.com迁移到LKE效果很好。</p><p> There is room for improving the performance of persistent volumes in LKE. When we disabled the CDN, disk utilisation for the persistent volume which serves all our media went up to 100% and stayed there for the entire duration of this test scenario. As unlikely as this is to happen, a cold cache would mean high latencies for all media assets. The worst part is that serving the same files from disks local to the VMs is 44x faster than from persistent volumes (267MB/s vs 6MB/s). Even with Fastly fronting all changelog.com media, this is what cache misses mean for disk IO utilisation &amp; saturation:</p><p> 在LKE中仍有改进持久卷性能的空间。当我们禁用CDN时，为我们所有媒体提供服务的持久卷的磁盘利用率上升到100％，并在此测试方案的整个过程中一直保持在那里。尽管这种情况不太可能发生，但冷缓存将意味着所有媒体资产的高延迟。最糟糕的部分是，从VM本地磁盘提供相同文件的速度比持久卷的速度快44倍（267MB / s和6MB / s）。即使快速放置所有changelog.com介质，这也意味着高速缓存未命中对磁盘IO利用率和饱和：</p><p>  During our migration, we had HTTP verification configured for Let’s Encrypt (LE) certificates. With the DNS TTL set to 3600 seconds, after the changelog.com DNS A record was updated to point to LKE, a new certificate could not be obtained because LE was still using the previous, cached changelog.com A record for verification. Having hit the LE certificate request limit, combined with the fact that we didn’t change the default DNS TTL of 3600 in external-dns, we ended with about 30 minutes of partial downtime until the A record revert propagated through the DNS network. We have since switched to DNS-based verification for LE, and are now using a wildcard certificate, but this aspect of the migration did not work as well as it could have.</p><p>  在迁移过程中，我们为“加密”（LE）证书配置了HTTP验证。将DNS TTL设置为3600秒，将changelog.com DNS A记录更新为指向LKE之后，由于LE仍在使用先前缓存的changelog.com A记录进行验证，因此无法获得新证书。达到LE证书请求限制后，再加上我们没有更改外部DNS中默认的3600的DNS TTL，我们结束了大约30分钟的部分停机，直到A记录还原通过DNS网络传播。此后，我们已切换到LE的基于DNS的验证，并且现在使用通配符证书，但是迁移的这一方面无法正常运行。</p><p> Linode Kubernetes Engine (LKE) worked as expected. The interaction with Linode was spot on, all improvements that were required for our migration have shipped in a timely manner, support was always responsive and progress was constant.  Thank you Linode!</p><p> Linode Kubernetes Engine（LKE）正常工作。与Linode的互动非常明显，我们迁移所需的所有改进均已及时交付，支持始终在响应之中，并且不断取得进步。谢谢Linode！</p><p>  To begin with, we are still playing the manual upgrade game. Now that we are on LKE, we want to automate K8S upgrades, baseline components upgrades (ingress-nginx, cert-manager etc.) as well as app dependency upgrades (Erlang &amp; Elixir, PostgreSQL, etc.). We know that it’s a tall order, but is now within our reach considering the primitives that the Kubernetes ecosystem unlocks. The first step will be to upgrade our existing K8S v1.17 to v1.18 so that we are running on a supported version, as well as all other components that had significant releases in the last 6 months ( cert-manager for example shipped 1.0 since we started using it).</p><p>  首先，我们仍在玩手动升级游戏。现在，我们使用LKE，我们希望自动化K8S升级，基准组件升级（ingress-nginx，cert-manager等）以及应用程序依赖项升级（Erlang＆amp; Elixir，PostgreSQL等）。我们知道这是一个艰巨的任务，但考虑到Kubernetes生态系统解锁的原始要素，现在已经可以实现。第一步是将现有的K8S v1.17升级到v1.18，以便我们在受支持的版本以及最近6个月中具有重要版本的所有其他组件上运行（例如cert-manager 1.0）因为我们开始使用它）。</p><p> We would very much like to address the persistent volume limited performance on LKE. Even if with some work we could migrate all media assets to S3, we would still have to contend with our PostgreSQL performance, as well as Prometheus which is highly dependent on disk performance.  The managed db service that Linode has on the 2020 roadmap might solve the PostgreSQL performance, and Grafana Cloud may be a solution to Prometheus, but all these approaches seem to be solving a potential LKE improvement by going outside of LKE, which doesn’t feel like a step in the right direction. As an aside, I am wondering if our friends at Upbound -  hi Jared &amp; Dan 👋🏻 - would recommend  Rook for pooling local SSD storage for persistent volumes on LKE? 🤔</p><p> 我们非常想解决LKE上持续的容量受限性能问题。即使可以进行一些工作，我们也可以将所有媒体资产迁移到S3，但我们仍然必须应对PostgreSQL的性能以及高度依赖于磁盘性能的Prometheus。 Linode在2020年路线图上提供的托管数据库服务可能会解决PostgreSQL的性能，而Grafana Cloud可能是Prometheus的解决方案，但是所有这些方法似乎都通过摆脱LKE来解决潜在的LKE改进，这并不令人感到就像朝正确方向迈出的一步。顺便说一句，我想知道我们在Upbound的朋友-Jared＆amp; Dan👋🏻-是否会建议Rook为LKE上的持久卷合并本地SSD存储？ 🤔</p><p> I am very much looking forward to integrating all our services with Grafana &amp; Prometheus, as well as Loki which is still on our TODO list from last year. We are missing many metrics that are likely to highlight areas of improvement, as well as problems in the making that we are simply not aware of. Centralised logging running on LKE, alongside metrics, would be very nice to have, especially since this would enable us to start deriving more business outside of the app, which is currently hand-rolled with PostgreSQL &amp; Elixir. While this approach works well, I know that we could do so much better, and take business insights into directions that right now are unfathomable.</p><p> 我非常期待将我们的所有服务与Grafana＆amp;普罗米修斯（Prometheus）以及从去年起仍在TODO名单中的Loki。我们缺少许多可能突出改进领域的指标，以及我们根本不知道的制造问题。在LKE上与指标一起运行的集中日志记录非常好，尤其是因为这将使我们能够开始在应用程序之外派生更多业务，而该应用程序目前由PostgreSQL＆amp;长生不老药。虽然这种方法行之有效，但我知道我们可以做得更好，并将业务洞察力带入目前无法企及的方向。 </p><p> Another thing on our radar is picking up  OpenFaaS for certain tasks that we still run outside of LKE, and for which we use Ruby scripting. While we could rewrite them to Elixir so that everything runs within the same app, it seems easier to lift and shift the Ruby code that has been working fine for years into one or more functions running on OpenFaaS. Well spotted, the hint was in the video above. 😉</p><p>我们要注意的另一件事是为仍在LKE之外运行的某些任务选择OpenFaaS，并且我们使用Ruby脚本。尽管我们可以将它们重写为Elixir，以便所有内容都可以在同一应用程序中运行，但将多年来运行良好的Ruby代码提升和转移到在OpenFaaS上运行的一个或多个函数中似乎更加容易。很好发现，提示在上面的视频中。 😉</p><p> As always, our focus is on steady improvement. I feel that what we delivered in 2020 is a significant improvement to what we had before. I am really looking forward to what we get to build for 2021, as well as sharing it with you all!</p><p> 与往常一样，我们的重点是稳步改善。我觉得我们在2020年交付的产品与以前相比有了很大的改进。我非常期待我们为2021年建立的基础，并与大家分享！</p><p>     changelog.com availability SLO is 4 nines, (99.99%) which translates to just over 50 minutes of downtime in one year. Our 2019 availability SLI was 3 nines and a 6 (99.96%) with over 220 minutes of downtime. A misconfigured Docker service was the main reason, but we also had 50 micro downtimes of a minute or two.</p><p>     changelog.com的可用性SLO为4个9（99.99％），这意味着一年内停机时间仅超过50分钟。我们的2019年可用性SLI为3个9和6（99.96％），停机时间超过220分钟。 Docker服务配置错误是主要原因，但我们也有50分钟或一两分钟的微停机时间。</p><p> With an ongoing LKE migration, our 2020 availability SLI stands at 4 nines, with 38 minutes of downtime currently. Most of it was due to my DNS &amp; LE certificate blunder hit during the migration to LKE, but with 2 months to go, our availability SLI is almost 6 times better this year compared to the previous one.</p><p> 随着LKE的持续迁移，我们的2020年可用性SLI为4个9，目前停机时间为38分钟。大部分是由于我的DNS＆amp;在迁移到LKE期间，LE证书出现了严重错误，但是还有2个月的时间，今年我们的可用性SLI比上一个要好6倍。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://changelog.com/posts/the-new-changelog-setup-for-2020">https://changelog.com/posts/the-new-changelog-setup-for-2020</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/2020/">#2020</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/设置/">#设置</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/setup/">#setup</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>