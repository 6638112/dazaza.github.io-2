<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>苹果M1协处理器的秘密 The Secret Apple M1 Coprocessor</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">The Secret Apple M1 Coprocessor<br/>苹果M1协处理器的秘密 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-16 19:14:55</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/1/5232fe26141c8ee44eb61670287c63fe.png"><img src="http://img2.diglog.com/img/2021/1/5232fe26141c8ee44eb61670287c63fe.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Developer Dougall Johnson has through reverse engineering, uncovered a secret powerful coprocessor dubbed AMX: Apple Matrix coprocessor inside the M1 chip.</p><p>开发人员道格·约翰逊（Dougall Johnson）通过反向工程发现了一个秘密的，功能强大的协处理器，称为AMX：M1芯片内部的Apple Matrix协处理器。</p><p>  Stories about the Apple Matrix coprocessor (AMX) are already out there. But not exactly discussed in a beginner friendly manner. And that is what I try to do here. Bring you the story buried under thick layers of technical jargon without treating you like an idiot.</p><p>  关于Apple Matrix协处理器（AMX）的故事已经在那里。但是并没有以初学者友好的方式进行精确讨论。这就是我在这里尝试做的。将故事带给您隐藏在厚厚的专业术语下，而不会像对待白痴一样对待您。</p><p> To tell this story we need to clarify the basics such as what is a  coprocessor? What is a  matrix? And why should you even care about any of this?</p><p> 为了讲这个故事，我们需要弄清基本知识，例如什么是协处理器？什么是矩阵？而且，您为什么还要关心这些？</p><p> More importantly  why does none of the Apple slides talk about this coprocessor? Why is it seemingly a secret? If you have read about the Neural Engine inside the M1 System-on-a-Chip (SoC) you may be confused about what makes Apple’s Matrix coprocessor (AMX) is different.</p><p> 更重要的是，为什么没有Apple幻灯片谈论此协处理器？为什么看似秘密？如果您已经了解了M1片上系统（SoC）中的神经引擎，那么您可能会对Apple的矩阵协处理器（AMX）与众不同的原因感到困惑。</p><p> Before we get to the big question, let me start with the basic concepts such as what a matrix and a coprocessor is.</p><p> 在提出一个大问题之前，让我从基本概念开始，例如什么是矩阵和协处理器。</p><p>  A matrix is basically just a table of numbers. If you have worked with spreadsheets such as Microsoft Excel, you have basically worked with something very similar to matricies. The key difference is that in math such tables of numbers have a laundry list of operations they support and specific behavior. A matrix can come in different flavors as you see here. A matrix with such a row, is usually called a  row vector. If one a column, we call it a  column vector.</p><p>  矩阵基本上只是一个数字表。如果您使用过Microsoft Excel等电子表格，则基本上可以使用与矩阵非常相似的东西。关键区别在于，在数学上，此类数字表具有其支持的特定操作和特定行为的清单。正如您在此处看到的，矩阵可以有不同的口味。具有这种行的矩阵通常称为行向量。如果一个列，我们称之为列向量。</p><p>  We can add, subtract, scale and multiple matrices. Addition is pretty easy. You just add every element separately. Multiplication is a bit more involved. I am just showing the simple case here.</p><p>  我们可以添加，减去，缩放和多个矩阵。加法非常容易。您只需分别添加每个元素。乘法要复杂得多。我只是在这里显示一个简单的案例。 </p><p>       In particular machine learning which has been hot these last years. Just adding more cores to the CPU will not make this run fast enough as it is really demanding. You really need specialized hardware. Regular tasks such browsing the internet, writing email, word processing and spreadsheets has been running fast enough for years. It is for specialized tasks which we really need to boost the processing power.</p><p>特别是最近几年一直很热门的机器学习。仅向CPU添加更多的内核并不能使它足够快地运行，因为这确实非常需要。您确实需要专用的硬件。多年来，浏览互联网，编写电子邮件，文字处理和电子表格等常规任务已经足够快地运行了。这是我们真正需要提高处理能力的特殊任务。</p><p>  On any given chip, Apple has a max number of transistors to spend building different kinds of hardware. They could add more CPU cores but that really just speeds up regular tasks, which already run fast enough. Thus they have chosen to spend transistors to make specialized hardware to tackle image processing, video decoding and machine learning. This specialized hardware is the coprocessor and accelerators.</p><p>  在任何给定的芯片上，Apple都有最多数量的晶体管用于构建不同种类的硬件。他们可以添加更多的CPU内核，但这实际上只是加快了已经足够快地运行的常规任务。因此，他们选择花费晶体管制造专门的硬件来处理图像处理，视频解码和机器学习。这种专用硬件是协处理器和加速器。</p><p>   If you have read about the Neural Engine, you will know that it also does matrix operations to help with machine learning tasks. So why do we need the Matrix coprocessor? Or are they actually just the same thing? Am I just confused? No, let me clarify how Apple’s Matrix Coprocessor differ from the Neural Engine and why we need both.</p><p>   如果您已经阅读了有关神经引擎的信息，您将知道它还会执行矩阵运算以帮助完成机器学习任务。那么为什么我们需要矩阵协处理器呢？还是它们实际上是同一件事？我只是感到困惑吗？不，让我澄清一下苹果的矩阵协处理器与神经引擎有何不同，以及为什么我们需要两者。</p><p>  I admit that in past stories I often use the term coprocessor and accelerator interchangeably but they are not the same. A GPU as found in your Nvidia graphics card and the Neural Engine are both a type of accelerator.</p><p>  我承认在过去的故事中，我经常互换使用术语协处理器和加速器，但是它们并不相同。 Nvidia图形卡中的GPU和神经引擎都是加速器的一种。</p><p> In both cases you have special areas of memory which the CPU has to fill up with data it wants processed as well as another part of memory which it fills up with a list of instructions that accelerator should perform. It is time consuming for a CPU to setup this kind of processing. There is a lot of coordination, filling in data, and then waiting to get results back.</p><p> 在这两种情况下，您都有特殊的内存区域，CPU必须填充要处理的数据，而内存的另一部分则填充加速器应执行的指令列表。 CPU设置此类处理非常耗时。需要进行大量协调，填写数据，然后等待获得结果。</p><p> Thus this only pays off for larger tasks. For smaller tasks the overhead will be too high.</p><p> 因此，这只会为更大的任务带来回报。对于较小的任务，开销将太高。</p><p>  This is where coprocessors are a benefit over accelerators. Coprocessors sit and spy on the stream of machine code instructions being fed from memory (or cache more specifically) into the CPU. Coprocessor are made to react to particular instructions they were made to process. The CPU meanwhile has been made to mostly ignore these instructions or help facilitate the handling of them by a coprocessor.</p><p>  这就是协处理器优于加速器的地方。协处理器坐在并监视从内存（或更具体地说，是缓存）馈送到CPU的机器代码指令流。使协处理器对它们要处理的特定指令作出反应。同时，已使CPU几乎忽略了这些指令或帮助协处理器处理它们。 </p><p> What we gain from this is that instructions carried out by the coprocessor can be placed inside your regular code. This is different from say a GPU. If you have done GPU programming you know that shader programs are placed into separate buffers of memory, and you have to explicitly transport these shader programs to the GPU. You cannot place GPU specific instruction inside your regular code. Thus for smaller workloads involving matrix processing AMX will be better than the Neural Engine.</p><p>我们从中得到的好处是，协处理器执行的指令可以放在常规代码中。这与GPU不同。如果您已完成GPU编程，则知道着色器程序已放置在单独的内存缓冲区中，并且您必须将这些着色器程序显式传输到GPU。您不能将GPU特定的指令放在常规代码中。因此，对于涉及矩阵处理的较小工作量，AMX将比神经引擎更好。</p><p> What is the catch? You need to actually define the instructions in the instruction-set architecture (ISA) of your microprocessor. Thus you need much tighter integration with the CPU when using a coprocessor than when using an accelerator.</p><p> 有什么收获？您实际上需要在微处理器的指令集体系结构（ISA）中定义指令。因此，与使用加速器相比，使用协处理器时需要与CPU紧密集成。</p><p> ARM Ltd. creators of the ARM instruction-set architecture (ISA) has long resisted adding custom instructions to their ISA. This is one of the advantages of RISC-V:  What Is Innovative About RISC-V?</p><p> ARM指令集体系结构（ISA）的创建者ARM公司长期以来一直拒绝向其ISA添加自定义指令。这是RISC-V的优点之一：RISC-V的创新之处是什么？</p><p> However due to pressure from customers ARM relented and announced in 2019 that they would allow extensions.  EE Times reports:</p><p> 但是，由于客户的压力，ARM在2019年做出了让步并宣布将允许扩展。 EE Times报告：</p><p> The new instructions are interleaved with standard Arm instructions. To avoid software fragmentation and maintain a coherent software development environment, Arm expects customers to use the custom instructions mostly in called library functions.</p><p> 新指令与标准Arm指令交错。为了避免软件碎片化并保持一致的软件开发环境，Arm希望客户主要在称为库函数的地方使用自定义指令。</p><p> This may help explain why AMX instructions are not described in official documentation. ARM Ltd. expects Apple to keep these kinds of instructions inside libraries provided by the customer (Apple in this case).</p><p> 这可能有助于解释为什么官方文档中没有描述AMX指令。 ARM Ltd.期望Apple将此类指令保存在客户提供的库中（在这种情况下为Apple）。</p><p>  It is easy to confuse something like a matrix coprocessor with a SIMD vector engine, which you find inside most modern processors today including ARM processors. SIMD stands for Single Instruction Multiple Data.</p><p>  将矩阵协处理器与SIMD向量引擎相混淆是很容易的，您可以在当今大多数现代处理器（包括ARM处理器）中找到它们。 SIMD代表单指令多数据。 </p><p>  SIMD is a way of getting higher performance when you need to perform the same operation on multiple elements. This is closely related to matrix operations. In fact SIMD instructions such as ARM’s Neon instructions or Intel x86 SSE or AVX are often used to speed up matrix multiplications.</p><p>当您需要对多个元素执行相同的操作时，SIMD是一种获得更高性能的方法。这与矩阵运算密切相关。实际上，SIMD指令（例如ARM的Neon指令或Intel x86 SSE或AVX）通常用于加速矩阵乘法。</p><p>  However a SIMD vector engine is part of a microprocessor core. Just like the ALU (Arithmetic Logic Unit) and FPU (Floating Point Unit) is part of the CPU. Inside the microprocessor there is an instruction decoder which will pick apart an instruction and decide what functional unit to activate (gray boxes).</p><p>  但是，SIMD向量引擎是微处理器核心的一部分。就像ALU（算术逻辑单元）和FPU（浮点单元）一样，它也是CPU的一部分。在微处理器内部有一个指令解码器，它将分解一条指令并确定要激活的功能单元（灰色框）。</p><p>  A coprocessor in contrast is external to a microprocessor core. In fact one of the early ones, Intel’s 8087 was a physically separate chip designed to speed up floating point calculations.</p><p>  相反，协处理器位于微处理器内核的外部。实际上，英特尔8087是最早的芯片之一，它是一种物理上独立的芯片，旨在加快浮点计算的速度。</p><p>  Now you may wonder why anyone would want to complicate CPU design by having a separate chip like this which has to sniff on the data flowing from memory to the CPU, to see if anything is a floating point instruction.</p><p>  现在，您可能会奇怪，为什么有人会想通过拥有一个像这样的单独芯片来使CPU设计复杂化，该芯片必须嗅探从内存到CPU的数据流，以查看是否有任何浮点指令。</p><p> The reason was simple, the original 8086 CPU in the first PCs contained 29,000 transistors. The 8087 in contrast was far more complex at 45,000 transistors. It was really hard to make anything with that many transistors. Combining these two chips into one would have been really hard and expensive.</p><p> 原因很简单，第一批PC中的原始8086 CPU包含29,000个晶体管。相反，8087在45,000个晶体管时要复杂得多。这么多晶体管真的很难制造任何东西。将这两种芯片合并为一个芯片确实非常困难且昂贵。</p><p> But as manufacturing technology improved, it was not a problem to put floating point units (FPUs) inside the CPU. Thus FPUs replaced the floating point coprocessors.</p><p> 但是，随着制造技术的改进，将浮点单元（FPU）放入CPU并不是问题。因此，FPU取代了浮点协处理器。</p><p> Why the AMX is not simply a part of the Firestorm cores on the M1 is not clear to me. They are all on the same silicon die anyway. I can only offer some speculations. By being a coprocessor, it may be easier for the CPU to continue running in parallel. Apple may also have liked to keep non-standard ARM stuff outside of their ARM CPU cores.</p><p> 对于我来说，为什么AMX不仅是M1上Firestorm核心的一部分还不清楚。无论如何，它们都在同一个硅芯片上。我只能提供一些推测。通过成为协处理器，CPU继续并行运行可能会更容易。苹果可能还喜欢将非标准的ARM产品保留在其ARM CPU内核之外。 </p><p>  If AMX is not described in official documentation, how do we even know about it? Thanks to developer Dougall Johnson, who has done an amazing job reverse engineering the M1 to discover this coprocessor. His efforts are described  here. For matrix related math operations Apple has special libraries or frameworks such as  Accelerate, which is made up of:</p><p>如果官方文档中没有描述AMX，我们怎么知道呢？感谢开发人员Dougall Johnson，他出色地完成了M1的逆向工程，以发现该协处理器。这里描述了他的努力。对于与矩阵有关的数学运算，Apple具有特殊的库或框架，例如Accelerate，其组成如下：</p><p> BLAS — a sort of industry standard for linear algebra (what we call the math dealing with matricies and vectors).</p><p> BLAS —一种线性代数的行业标准（我们称其为处理矩阵和向量的数学方法）。</p><p>  vDSP —  digital signal processing. Fourier transformations, convolution. These are mathematical operations important in image processing or any signal really including audio.</p><p>  vDSP —数字信号处理。傅立叶变换，卷积。这些是在图像处理或真正包括音频的任何信号中很重要的数学运算。</p><p> Dougall Johnson knew these libraries would use the AMX coprocessor to speed up their calculations. Thus he wrote special programs to analyze and observe what these programs did to discover the special undocumented AMX machine code instructions.</p><p> Dougall Johnson知道这些库将使用AMX协处理器来加快计算速度。因此，他编写了一些特殊程序来分析和观察这些程序做了什么，以发现未记录的特殊AMX机器代码指令。</p><p> But why doesn’t Apple document this and let us use these instructions directly? As mentioned earlier, this is something ARM Ltd. would like to avoid. If custom instructions are widely used it could fragment the ARM ecosystem.</p><p> 但是，Apple为什么不记录下来并让我们直接使用这些说明？如前所述，这是ARM Ltd.想要避免的事情。如果广泛使用自定义指令，则可能使ARM生态系统支离破碎。</p><p> However more importantly, this is an advantage to Apple. By  only letting  their libraries use these special instructions Apple retains the freedom to radically change how this hardware works later. They could remove or add AMX instructions. Or they could let the Neural Engine do the job. Either way they make the job easier for developers. Developers only need to use the  Accelerate framework and can ignore how Apple specifically speeds up matrix calculations.</p><p> 但是，更重要的是，这对Apple来说是一个优势。通过仅允许其库使用这些特殊说明，Apple保留了从根本上改变此硬件以后工作方式的自由。他们可以删除或添加AMX指令。或者，他们可以让神经引擎来完成这项工作。无论哪种方式，它们都使开发人员的工作更加轻松。开发人员只需要使用Accelerate框架，就可以忽略Apple专门如何加快矩阵计算的速度。</p><p> This is one of the big advantages Apple has by being vertically integrated. By controlling both the hardware and the software, they can pull these kinds of tricks. So the next question is how big a deal is this? What does this buy Apple in terms of performance and capabilities?</p><p> 这是苹果通过垂直整合获得的最大优势之一。通过控制硬件和软件，他们可以利用这些技巧。那么下一个问题是这有多大的意义？就性能和功能而言，这对苹果有什么好处？ </p><p>  Nod Labs is a company that does machine interaction, intelligence and perception. Fast matrix operations are naturally in their interest. They have written a highly technical blog post of doing performance tests of AMX:  Comparing Apple’s M1 matmul performance — AMX2 vs NEON.</p><p>Nod Labs是一家从事机器交互，智能和感知的公司。快速矩阵运算自然是他们的兴趣所在。他们撰写了一篇有关AMX性能测试的技术性很强的博客文章：比较Apple的M1 matmul性能-AMX2 vs NEON。</p><p> What they are doing is comparing performance of doing similar code using AMX with doing it using the Neon instructions, which are officially supported by ARM. Neon is a type of SIMD instructions.</p><p> 他们正在做的是比较使用AMX和使用Neon指令（由ARM正式支持）执行类似代码的性能。霓虹灯是一种SIMD指令。</p><p> What  Nod Labs found was that by using AMX they were able to get twice the performance of Neon instructions for matrix operations. It doesn’t mean AMX is better for everything, but at least for machine learning and high performance computing (HPC) type of work, we can expect that AMX gives an edge over the competition.</p><p> Nod Labs发现的是，通过使用AMX，他们可以在矩阵运算中获得两倍于Neon指令的性能。这并不意味着AMX可以在所有方面都做得更好，但是至少对于机器学习和高性能计算（HPC）类型的工作而言，我们可以期望AMX在竞争中占据优势。</p><p>  The Apple Matrix Coprocessor looks like some rather impressive piece of hardware giving Apple’s ARM processor an edge in machine learning and HPC related tasks. Further investigation will give us a more complete picture and I can update this story with more details.</p><p>  Apple Matrix协处理器看起来像一些相当令人印象深刻的硬件，这使Apple的ARM处理器在机器学习和HPC相关任务方面处于优势。进一步的调查将为我们提供更完整的信息，我可以更详细地更新此故事。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/swlh/apples-m1-secret-coprocessor-6599492fc1e1">https://medium.com/swlh/apples-m1-secret-coprocessor-6599492fc1e1</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/m1/">#m1</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/协处理器/">#协处理器</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>