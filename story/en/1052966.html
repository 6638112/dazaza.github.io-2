<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>医疗计算机视觉中的变压器 Transformers in Medical Computer Vision</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Transformers in Medical Computer Vision<br/>医疗计算机视觉中的变压器 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-17 19:02:25</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/ab7225a466be0f98f5e730320d47c305.png"><img src="http://img2.diglog.com/img/2021/3/ab7225a466be0f98f5e730320d47c305.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>If you’re new to transformers, it’s difficult to find a better explanation of the matrix mechanics powering the architecture than  this one written by Jay Alammar. Ezra’s Mary Fallah has also written  two excellent articles on transformer-based language models and their applications in clinical NLP. For this article, we’ll only dive deep enough into transformers to understand how they are poised to change the state of the art in computer vision.</p><p>如果您是最新的变形金刚，很难找到对矩阵力学的更好的解释，这些力学为架构提供了由Jay Alammar撰写的建筑。 EZRA的玛丽瑞拉还在临床NLP中编写了关于基于变压器的语言模型及其应用的一篇优秀的文章。对于本文，我们只能深入了解变形金刚，了解如何在计算机愿景中改变最新技术。</p><p> Transformers are a group of n eural network architectures that convert one sequence to another (sequence to sequence). The input and output sequence could be text, time series or any other data that can be represented as a sequence.</p><p> 变形金刚是一组N个E且网络架构，可将一个序列转换为另一个序列（序列序列）。输入和输出序列可以是文本，时间序列或可以表示为序列的任何其他数据。</p><p> Transformers, like all sequence to sequence architectures, have two components: an encoder and a decoder. The encoder transforms the input sequence into an embedding space. We can think of the embedding space as a vector representation of the input data. Next, the decoder takes data in the embedding space and converts this into an output.</p><p> 像序列架构的所有序列一样，变形金刚有两个组件：编码器和解码器。编码器将输入序列转换为嵌入空间。我们可以将嵌入空间视为输入数据的矢量表示。接下来，解码器在嵌入空间中取消数据，并将其转换为输出。</p><p> There are many examples of sequence to sequence algorithms, Seq2Seq is one example that uses a recurrent neural network (RNN), specifically an LSTM, for the encoder and decoder models. RNNs were the state of the art for natural language processing for almost a decade. So why are RNNs for natural language processing no longer in vogue and what makes Transformers special? Transformers have a special power: attention.</p><p> 序列算法有许多序列示例，SEQ2SEQ是使用反复性神经网络（RNN），特别是LSTM的一个示例，用于编码器和解码器模型。对于自然语言处理近十年来，RNN是最先进的。那么为什么RNN用于自然语言处理不再在流行音中，是什么让变压器特殊？变压器具有特殊的力量：注意。</p><p> To understand transformers and attention, we have to understand the limitations of previously popular recurrent neural networks. Let’s look at a common use case in NLP, machine translation. RNN encoders process one input token (word) at a time. At each processing step, an intermediate embedding or “hidden state” is produced. At the end of the sentence, the final hidden state represents the embedding for the entire sentence. This embedding is then passed to the decoder network. Likewise, the decoder network takes the embedding and sequentially produces the output sequence.</p><p> 要了解变压器和关注，我们必须了解以前流行的经常性神经网络的局限性。让我们在NLP，机器翻译中查看一个常用用例。 RNN编码器一次处理一个输入令牌（Word）。在每个处理步骤中，产生中间嵌入或“隐藏状态”。在句子的末尾，最终隐藏状态代表整个句子的嵌入。然后将此嵌入陷入解码器网络。同样地，解码器网络采用嵌入并顺序地产生输出序列。</p><p>  There is an issue with this approach. In long passages of text, context from a word at the beginning of the sentence or passage must pass through the encoder many times. At each processing step, the new hidden state must capture the new word as well as context from all previous words. In our example above, the word “Early” passes through the RNN twice before the encoder produces the embedding. Each encoding step must preserve the context within the resulting intermediate embedding vector. In theory, this is possible, however, in practise this results in accumulated error and loss of context. Both in training and in use, RNNs show difficulty in allowing long-range information to reach the decoder.</p><p>  这种方法存在一个问题。在长通话中，句子或段落开始时的单词的上下文必须多次通过编码器。在每个处理步骤中，新的隐藏状态必须捕获新单词以及从前一词中的上下文。在我们上面的示例中，在编码器产生嵌入之前，“早期”一词通过RNN两次通过。每个编码步骤必须在所得到的中间嵌入向量中保留上下文。理论上，这是可能的，但是，在实践中，这导致积累的误差和上下文丢失。无论是在培训和使用中，RNN都难以允许远程信息到达解码器。</p><p>  In contrast, transformers tackle this issue with two conceptually simple new ideas. Firstly, encoders in transformers process the entire sentence as a whole. This removes the constraint of requiring intermediate embeddings to capture long range information. Since sequential computation of the input sequence is no longer required, transformers are more computationally efficient as we are able to better parallelize operations. Secondly, transformers utilize the attention mechanism. We won’t dive into the mathematics here, but suffice to say that the attention mechanism gives transformers the ability to score the importance of words within a sentence relative to each other. As a result of the attention mechanism, transformers are better able to capture the relationship between different words.</p><p>  相比之下，变形金刚用两个概念上简单的新想法来解决这个问题。首先，变形金刚中的编码器将整个句子处理整个句子。这消除了需要中间嵌入物以捕获远程信息的约束。由于不再需要输入序列的顺序计算，因此变压器更加计算效率，因为我们能够更好地并行化操作。其次，变压器利用注意机制。我们不会潜入这里的数学，但足以说注意力机制使变形例能够将单词在句子中的重要性相对于彼此进行比分。由于注意机制，变压器更好地捕捉不同词之间的关系。 </p><p>  Transformers have reached state of the art for almost every NLP task. Much of this success can be attributed to their ability to learn arbitrary structures and relationships for sequential data. After seeing their success in the NLP domain, researchers from computer vision began to ask: can the success of transformers generalise to images?</p><p>几乎每个NLP任务都达到了最先进的技术。这一成功的大部分都可以归因于他们学习序列数据的任意结构和关系的能力。看到他们在NLP领域的成功后，来自计算机愿景的研究人员开始问：变形金刚的成功是否可以推广到图像？</p><p> Part of the desire to adapt transformers to images is to remove the need for image-specific inductive biases. Let’s remind ourselves: an inductive bias is a design choice when creating a learning algorithm that relies on assumption about the data being processed. In computer vision, convolutional neural networks with layer aggregations (e.g. max pooling) dominate due to their property of translational invariance, a quality that means CNNs are able to recognise image features regardless of position or angle within an image. Convolution is a powerful concept and one of the inductive biases manually “baked” into state of the art computer vision architectures.</p><p> 将变压器调整为图像的一部分是为了消除对特定图像的感应偏差的需求。让我们提醒自己：诱导偏差是创建依赖于正在处理的数据的学习算法时的设计选择。在计算机视觉中，具有层聚合的卷积神经网络（例如，最大池）由于其平移不变性的性质而占主导地位，这是一种质量，意味着CNNS能够识别图像特征，无论图像内的位置还是角度。卷积是一个强大的概念，一个感应偏差之一手动“烘焙”到艺术计算机视觉架构的状态。</p><p> Despite their power, manually choosing these inductive biases also introduces limitations. Translation invariance, the very property that makes CNNs effective, also results in undesirable behaviours. Take for example, a network tasked with classifying whether an image contains a face. CNNs are highly effective at recognising facial features: eyes, ears, lips, a nose, and eyebrows. However, CNNs struggle to learn the spatial relationships between these subcomponents and other higher level logic such as the number of each of these components that should be present. As long as the subcomponents are present, CNNs are likely to classify the image as containing a face. However, a face that contains multiple lips and 20 eyes will also be classified as a face. CNNs have no concept of the number or relative position of these facial features.</p><p> 尽管他们的力量，手动选择这些归纳偏差也引入了局限性。翻译不变性，使CNNS有效的属性也导致不良行为。例如，一个网络任务，用于分类图像是否包含面部。 CNNS在识别面部特征方面非常有效：眼睛，耳朵，嘴唇，鼻子和眉毛。然而，CNNS努力学习这些子组件与其他更高级别逻辑之间的空间关系，例如应该存在的每个组件的数量。只要存在子组件，即CNN可能会将图像分类为包含面部的图像。然而，含有多个嘴唇和20只眼睛的脸也将被归类为面部。 CNN没有这些面部特征的数量或相对位置的概念。</p><p>  Transformers bring two advantages to the table. First, when applied to images, it has been shown that the attention mechanism is a  generalised case of convolution. That means transformers are able to learn to mimic convolutions. Secondly, transformers are able to learn the complex spatial relationships between high level image features. In our face example, counting the number of eyes and relative position of facial features is not a problem for transformer architectures. Together, these advantages allow transformers to address the limitations of translation invariance whilst still being flexible enough to learn operations that can produce state of the art performance.</p><p>  变压器向桌面带来两种优点。首先，当应用于图像时，已经表明注意机制是卷积的广义情况。这意味着变形金刚能够学会模仿卷曲。其次，变形金刚能够学习高级图像特征之间的复杂空间关系。在我们的脸部示例中，计数面部特征的眼数和相对位置不是变压器架构的问题。这些优势在一起允许变压器解决翻译不变性的局限性，同时仍然灵活地学习可以产生最新性能的业务。</p><p>  There are two approaches to transformers in computer vision, hybrid architectures and transformer-only architectures. In language applications, the input to a transformer is a series of token embeddings (a linguistic unit akin to a word). For transformers applied to images, hybrid and transformer-only approaches are distinguished by their input:</p><p>  计算机视觉，混合架构和唯一的变压器架构中有两种转型机。在语言应用程序中，变压器的输入是一系列令牌嵌入式（类似于单词的语言单位）。对于应用于图像的变压器，仅通过其输入来区分混合动力和变压器的方法：</p><p> Hybrid: A CNN is used to produce an embedding for an image or sub-region of the image (patch). The encoding or embedding is used as the input for a subsequent transformer. The CNN is used to process lower level features in the image.</p><p> 混合：CNN用于产生图像的图像或子区域的嵌入（贴片）。编码或嵌入用作后续变压器的输入。 CNN用于处理图像中的较低级别特征。</p><p> Transformer-only: A trainable part of the architecture is responsible for projecting patches to an embedding space. No hand-coded convolutional architecture is used. The transformer architecture is solely responsible for learning both lower level and higher level features.</p><p> 仅变压器：架构的培训部分负责将修补程序投影到嵌入空间。没有使用手工编码的卷积架构。变压器架构仅负责学习较低级别和更高级别的功能。 </p><p> Both approaches result in the mapping of a patch P to an embedding of dimension D. In the hybrid case, we are able to reuse pre-trained CNNs (e.g. ResNet) to extract lower level feature vectors. One downside of a hybrid approach is that we do not rid ourselves of the inductive bias problem we previously mentioned, since we are still using pre-trained convolutional neural networks to perform the initial image embedding. An upside to the inductive bias is that hybrid approaches are more computationally efficient (relative to transformer only approaches) as they generally start with a pretrained CNN and fewer total model parameters. In contrast, transformer-only approaches have no inductive bias and therefore must learn to extract both low and high level features. To the best of my knowledge, there are no current examples (as of March 2021) of transformer-only approaches in medical computer vision. However, in the wider field, transformer-only works such as Image GPT have demonstrated that a hybrid approach is not essential. Given sufficient data and computation, transformers alone can be trained to produce state of the art results in computer vision.</p><p>两种方法导致贴片P映射到尺寸D的嵌入。在混合案件中，我们能够重用预先训练的CNN（例如RESET）来提取较低级别的特征向量。混合方法的一个缺点是我们不会摆脱我们之前提到的归纳偏见问题，因为我们仍然使用预先训练的卷积神经网络来执行初始图像嵌入。归纳偏差的良好方法是混合方法更加计算地高效（相对于变压器仅接近），因为它们通常以掠过的CNN和更少的总模型参数开始。相比之下，唯一的变换器的方法没有归纳偏差，因此必须学习提取低级和高级功能。据我所知，在医疗计算机视觉中，没有当前的例子（截至3月2021年3月）。但是，在更广泛的领域中，诸如图像GPT的变换器的工作已经证明了混合方法不是必需的。鉴于足够的数据和计算，可以训练单独的变压器以产生技术的状态，导致计算机视觉。</p><p>   The intersection of transformers, computer vision and medicine is still in the earliest stages. Indeed, there are few papers in the subfield of transformers in computer vision altogether. When we specialise further into the niche of medical computer vision, there are only a handful of public works. With so few publications on this combination of topics, much of the potential of applying transformers to medical images has yet to be demonstrated. Here are two papers that explore the future promise of transformers. We will focus on the approaches of the papers rather than the results.</p><p>   变压器，计算机视觉和医学的交叉仍处于最早的阶段。实际上，计算机愿景的变压器子场中有很少的论文。当我们专注于医疗计算机愿景的利基时，只有少数公共工程。在这种主题的组合上有了这么少，尚未证明将变压器应用于医学图像的大部分潜力。这是两篇论文，探索变形金刚的未来承诺。我们将专注于论文的方法而不是结果。</p><p>    GANBERT is our first example of a hybrid transformer approach. In order to understand GANBERT, we need some background on the problem domain it is attempting to solve: removing the need for multiple medical scans. PET imaging, among other applications, is routinely used in clinical practice to diagnose Alzheimer’s disease. PET image acquisition involves injection of a radioactive tracer followed by a scan. Injection of the tracer adds additional discomfort and risks to the imaging procedure. On top of this, PET scans do not provide great anatomical detail so they are often accompanied by an MRI or CT scan to provide anatomy detail. These multiple scans add additional cost and burden to both patients and hospitals. Hence, there is demand for an alternative procedure that could provide the diagnostic power of PET with the anatomical detail of an MRI.</p><p>    Ganbert是一个混合变压器方法的第一个例子。为了理解Ganbert，我们需要一些关于问题域的背景，它试图解决：删除对多种医疗扫描的需求。除其他应用中，宠物成像通常用于临床实践以诊断阿尔茨海默病。宠物图像采集涉及注入放射性示踪剂，然后注射扫描。注射示踪剂增加了成像过程的额外的不适和风险。在此之上，宠物扫描不提供很大的解剖细节，因此它们通常伴随着MRI或CT扫描来提供解剖细节。这些多次扫描增加了患者和医院的额外成本和负担。因此，需要一种替代程序，可以提供PET的诊断能力与MRI的解剖细节。</p><p> GANBERT explores the possibility of synthesising PET images from MRI T1-weighted images, removing the need for multiple scans. The authors’ approach is a type of Generative Adversarial Network where the Generator is a CNN (3D U-Net) and the Discriminator is a transformer (BERT).</p><p> Ganbert探讨了从MRI T1加权图像综合宠物图像的可能性，从而消除了多个扫描的需要。作者的方法是一种生成的对抗网络，其中发电机是CNN（3D U-Net），并且鉴别器是变压器（伯特）。</p><p> BERT was originally created for natural language applications and is designed to take tokenized and encoded words as input. These are sentences where the words are split into tokens and then encoded by converting the tokens to IDs. Token encoding a sentence results in representing the text as a series of integers. In order to be compatible with BERT, the MRI and PET images are quantized and represented as sequences of image intensity values. Each quantized image intensity value is analogous to a word or token in the NLP application of BERT. Paired MRI and PET sequences are concatenated and the transformer model is trained as a masked language model (MLM). For natural language applications, a masked language model is where a portion of the text is masked and the algorithm is tasked with using the remaining text to predict the masked text. For GANBERT, a portion of the PET sequence is masked and the transformer is tasked with using the MRI data to predict the missing PET data.</p><p> 最初是为自然语言应用而创建的BERT，旨在将标记化和编码的单词作为输入。这些是句子，其中单词被分成令牌，然后通过将令牌转换为ID来编码。编码句子的令牌导致将文本表示为一系列整数。为了与BERT兼容，MRI和PET图像被量化并表示为图像强度值的序列。每个量化的图像强度值类似于NLP应用中的NLP应用中的单词或令牌。配对的MRI和PET序列被连接，变压器模型被培训为屏蔽语言模型（MLM）。对于自然语言应用，屏蔽语言模型是屏蔽文本的一部分，并且使用剩余的文本来预测屏蔽文本的算法是任务的。对于Ganbert，屏蔽了一部分PET序列，并且使用MRI数据任务是任务的，以预测丢失的PET数据。</p><p> The approach of synthesising one image modality from another is promising. The main limitation of the work is that only quantitative metrics such as SSIM, PSNR and RMSE are provided without a baseline for what is an acceptable level of performance for each of these metrics. Performing a follow-up study with radiologist evaluation of the images would establish a baseline of the interpretability of the AI-generated PET images.</p><p> 合成另一个图像模型的方法是有前途的。该工作的主要限制是，只有仅提供SSIM，PSNR和RMSE等定量度量，而没有基线对于每个度量的每个度量是可接受的性能水平。用放射科医师进行后续研究的图像将建立AI生成的PET图像的可解释性的基线。</p><p>    Histopathology images are startlingly large, often in the gigapixel range. Training whole-image networks is not feasible for these images. A conventional approach for deep learning applied to histopathology images is to use patch-level CNNs to split the image up into many smaller patches and process each patch independently.</p><p>    组织病理学图像始于大，通常在千兆像素范围内。培训整个图像网络对于这些图像来说是不可行的。应用于组织病理学图像的深度学习的常规方法是使用补丁电平CNN，将图像分成许多较小的贴片并独立处理每个贴片。 </p><p> When processed independently, patch-level algorithms lack context from other distant patches. The authors of HATNet use a hybrid approach of self-attention and CNNs to approach the problem. HATNet tackles this by hierarchically processing an image at three levels: words, bags and images (words → bags → image).</p><p>独立处理时，补丁级算法缺少其他遥远补丁的上下文。 Hatnet的作者使用混合方法的自我关注和CNN来接近问题。 Hatnet通过在三个级别分层处理图像来解决这个问题：单词，袋子和图像（单词→袋→图片）。</p><p> The lowest level, words, are image patches that are encoded using an off-the-shelf pre-trained CNN into a 256-dimensional embedding vector. These “word”-level encodings are the lowest level representation in the architecture. Word-level encodings are then processed by a transformer architecture at the word, bag, and image level. This three-level processing allows the architecture to incorporate image-wide context from clinically relevant tissue structures.</p><p> 最低级别，单词是使用从架子预先训练的CNN中编码的图像修补程序，进入256维嵌入向量。这些“Word”-Level编码是架构中最低的级别表示。然后由单词，袋子和图像级别的变压器架构处理字级编码。这三级处理允许架构从临床相关的组织结构中包含图像范围的背景。</p><p> HATNet’s results speak for themselves, beating the next best method, Y-Net, a derivative of U-Net (by +8% in F1-score, a balanced measure of both precision and recall). These results are impressive, not because HATNet beats the state of the art, but because they represent the promise of future progress applying transformers and the attention mechanism to similar data.</p><p> Hatnet的结果为自己说话，击败了下一个最佳方法，y-net，U-net的衍生物（F1分数+ 8％，均衡和召回的平衡措施）。这些结果令人印象深刻，而不是因为Hatnet击败了最先进的国家，而是因为它们代表了将来进步的承诺将变形金刚和注意机制应用于类似的数据。</p><p>  You may have noticed that the two examples in the previous section were both hybrid approaches, where a transformer is used in tandem with a convolutional neural network. Why are there so few examples of transformer-only approaches, even in the wider computer vision field? On top of this, transformers have been used in NLP for 3+ years and are only starting to take off in computer vision. What took the field so long?</p><p>  您可能已经注意到上一节中的两个示例都是混合方法，其中变压器与卷积神经网络串联使用。为什么甚至在更广泛的计算机视觉领域中的变形金属近似的少数例子？在此之上，变压器已在NLP中使用3年以上，并且仅在计算机视觉中开始起飞。这么长时间拍了什么？</p><p> To answer both of these questions, we need to revisit the key difference between transformers and CNNs. Transformers’ greatest advantage, less inductive bias, is also their weakness. As mentioned earlier in the article, transformers do not come “pre-baked” with the same inductive bias as convolutional neural networks. That is, convolutional neural networks systematically reduce small-scale information to global information via convolution and aggregation steps that are hand coded. For transformers, there is no such hand coding. In order to achieve results that surpass CNNs, transformers must learn this process during training. Learning this process from scratch requires  a lot of data and computation, and that has been the limiting step over the last three years.</p><p> 要回答这两个问题，我们需要重新审视变压器和CNN之间的关键差异。变形金刚的最大优势，较少的归纳偏见，也是它们的弱点。如前所述，变压器不会以与卷积神经网络相同的归纳偏差，不随意“预烘焙”。也就是说，卷积神经网络通过手工编码的卷积和聚合步骤系统地将小规模信息减少到全局信息。对于变压器，没有这样的手编码。为了实现超越CNN的结果，变压器必须在培训期间学习此过程。从头开始学习此过程需要大量的数据和计算，这是过去三年的限制步骤。</p><p> Two recent high-profile applications of transformers in computer vision demonstrate this well: Vision Transformer (ViT) by Google Brain and Image GPT (iGPT) by OpenAI. Both models obtain results by training or pre-training on large proprietary datasets. ViT uses the 300 million image Google JFT dataset, available only internally at Google. Computation is the second limiting factor that has delayed the emergence of transformers in computer vision. The authors of iGPT highlight that iGPT-L (the second largest iGPT model trained) took 2500 V100-days to train while a similarly performing CNN-based Momentum Contrast model took only 70 V100-days of training to reach equivalent performance, over 35x difference in computation required. Together, these factors limit research to groups that have access to large medical image datasets and equivalent access to accelerated computing (GPUs and TPUs).</p><p> 最近电脑愿景中变压器的两个高调应用展示了这篇：谷歌大脑和图像GPT（IGPT）的视觉变压器（VIT）。两种模型通过培训或在大型专有数据集上进行培训来获得结果。 VIT使用了300 000张图片Google JFT数据集，仅在谷歌内部提供。计算是第二限制因素，其延迟了计算机视觉中变形金刚的出现。 IGPT的作者突出显示IGPT-L（第二大IGPT模型培训）训练2500 v100天训练，而同样执行的基于CNN的动量造影模型只花了70 v100天的训练，以达到相同的性能，但差异超过35倍在需要计算中。这些因素将这些因素限制了对具有大型医疗图像数据集的组的组和等效访问加速计算（GPU和TPU）。</p><p>  I hope you have found this to be a useful introduction to transformers in a clinical imaging context. As access to large datasets and computational resources improves, expect to see similar sized advances in medical computer vision. We are still in the early stages of progress in this field and many exciting milestones are still to come.</p><p>  我希望您已发现这是对临床成像语境中变形金刚的有用介绍。由于访问大型数据集和计算资源，期望在医疗计算机视觉中看到类似的大小进步。我们仍然处于这一领域的早期进展的阶段，许多令人兴奋的里程碑仍然是。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://techblog.ezra.com/transformers-in-medical-computer-vision-643b0af8fc41">https://techblog.ezra.com/transformers-in-medical-computer-vision-643b0af8fc41</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/计算机/">#计算机</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/medical/">#medical</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/图像/">#图像</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012 - 2021 diglog.com </div></div></body></html>