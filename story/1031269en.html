<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>从事语音技术25年了，但我仍然不和我的电脑说话</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">从事语音技术25年了，但我仍然不和我的电脑说话</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-27 00:26:22</div><div class="page_narrow text-break page_content"><p>These are personal reflections and observations, some of which might seem opinionated or simply be mistaken. Feel free to comment, I probably won’t be offended.</p><p>这些都是个人的思考和观察，其中一些可能看起来很固执己见，或者根本就是错的。请随意评论，我可能不会被冒犯。</p><p> While I was a post-grad at Cambridge in 1994, I was taught by  Steve  Young and  Tony Robinson, who had created some of the best speech recognition systems in the world. However, the most important thing I learned in my first few days, was something I would never have guessed from seeing early versions of  DragonDictate.  They had already cracked it. A modestly powerful computer could convert continuous natural speech into text, as it was being spoken, with about 95% accuracy, i.e. the technology was already better than all but the best-trained professionals.</p><p>1994年我在剑桥读研究生时，师从史蒂夫·杨(Steve Young)和托尼·罗宾逊(Tony Robinson)，他们创造了一些世界上最好的语音识别系统。然而，我在最初的几天里学到的最重要的东西，是我从看过早期版本的“龙霸天下”中永远猜不到的东西。他们已经破解了它。一台功能适中的计算机可以将连续的自然语音转换成人们所说的文本，准确率约为95%，也就是说，这项技术已经比除了训练有素的专业人员之外的所有人都要好。</p><p> Why has it taken until the last few years for speech recognition to be adopted in day-to-day use? The technology has many hidden industrial applications, but as a real-time user interface for day-to-day use, i.e. talking to your computer, adoption has been unbelievably slow. When I was studying in the 90s, I read about a sort of reverse Turing test, which demonstrated one reason why. Volunteers believed they were talking to a computer, but responses were actually provided by a human being typing “behind the curtain”. The observations and subsequent interviews showed that,  back then, people simply didn’t like it.</p><p>为什么直到最近几年语音识别才在日常使用中被采用？这项技术有许多隐藏的工业应用，但作为日常使用的实时用户界面，即与计算机对话，采用速度慢得令人难以置信。当我在90年代学习的时候，我读到过一种反向图灵测试，它证明了其中一个原因。志愿者们以为他们在对着电脑说话，但实际上是由一个人在“窗帘后面”打字来提供回应的。观察和随后的采访显示，当时人们根本不喜欢它。</p><p>  I’m sure that in part, it is just unfamiliarity, and so there is a generational effect. My kids talk to computers more than I do. However, there really are serious problems with speech as a primary user interface:</p><p>我敢肯定，在某种程度上，这只是一种陌生，因此存在代际效应。我的孩子比我更多地与计算机交谈。然而，语音作为主要用户界面确实存在严重问题：</p><p> Immature tech: it isn’t quite there yet - go into a crowded coffee shop and say “Hey Siri …”</p><p>不成熟的技术：它还没有完全成熟--走进拥挤的咖啡店，说“嘿，siri…”。</p><p> However, when I left college, to work in the real world, I learned a couple things from working with speech recognition, which (luckily for me) were not obvious to all those people demonstrating and discussing speech recognition, who were still focusing on dictation.</p><p>然而，当我离开大学，在现实世界中工作时，我从语音识别工作中学到了一些东西，这些东西(对我来说幸运的是)对所有那些展示和讨论语音识别的人来说并不明显，他们仍然专注于听写。</p><p>  I fell into a career, developing scalable digital media products. Among other things, I led the development of  BBC News Online. Then in 2000, I made a decision to apply my knowledge of speech technology, to solve problems in the media industry, and also to respect my presentiment that people still didn’t like talking to computers.</p><p>我投身于开发可伸缩数字媒体产品的职业。在其他方面，我领导了BBC新闻在线的发展。然后在2000年，我决定应用我的语音技术知识，解决媒体行业的问题，同时也尊重我的预感，即人们仍然不喜欢与计算机交谈。</p><p> So, with funding from a big software company, I developed some products which revolved around applying speech recognition to recorded speech. This was surprisingly easy, because another company in the same group was SoftSound, founded by my old teacher from Cambridge, Tony Robinson.</p><p>因此，在一家大型软件公司的资助下，我开发了一些产品，这些产品围绕着将语音识别应用于录制的语音。这出奇地简单，因为同一组中的另一家公司是SoftSound，由我在剑桥的老老师托尼·罗宾逊(Tony Robinson)创立。</p><p> I had been particularly interested in Tony’s lectures, and jumped at the chance of working with him on product development. He had succeeded in competing with the world’s best systems, but using much less memory and processing power, by using  neural networks. In that sense, we were decades ahead of the crowd, most of whom moved towards neural nets in the mid-2010s.</p><p>我对托尼的演讲特别感兴趣，并欣然接受了与他在产品开发方面合作的机会。他已经成功地与世界上最好的系统竞争，但通过使用神经网络，他使用的内存和处理能力要少得多。从这个意义上说，我们领先了人群几十年，他们中的大多数人在2010年代中期转向了神经网络。</p><p> My team took SoftSound’s speech recognition algorithms, and devised ways to combine them with video, text and image recognition, to create search engines for TV, film and radio archives. We created all kinds of cool things, like editing software allowing a video to be edited just by cutting and pasting the script. This was a little too far ahead of its time to be a hot seller, but we won some awards and got a lot of good press.</p><p>我的团队采用了SoftSound的语音识别算法，并设计了将它们与视频、文本和图像识别相结合的方法，为电视、电影和广播档案创建搜索引擎。我们创造了各种各样很酷的东西，比如编辑软件，只需剪切和粘贴脚本就可以编辑视频。对于热销来说，这有点超前了，但我们赢得了一些奖项，得到了很多好的新闻。</p><p>  Seeing people use our speech-based search engines was a revelation. It taught me that people love to spot mistakes, and use them as a reason to dismiss even the most obviously useful of innovations. This was a similar phenomenon to the YouTube clips of Scottish people talking to early versions of Siri.</p><p>看到人们使用我们基于语音的搜索引擎是一种启示。它教会了我，人们喜欢发现错误，并以此为理由，即使是最明显有用的创新也会不屑一顾。这与YouTube上的苏格兰人与早期版本的Siri对话的片段类似。</p><p> The TV archives we worked on had all kinds of background noise and music, and recognition accuracy fell from the laboratory rate of 95% to about 65–70%. Interestingly, this still allowed the search engine to find the right clips.</p><p>我们工作的电视档案有各种背景噪音和音乐，识别准确率从实验室的95%降到了65%-70%左右。有趣的是，这仍然允许搜索引擎找到正确的剪辑。</p><p> The problem was that if we showed users the transcripts in the results list, despite the fact that these contained their search terms, their eyes would be drawn to the errors (one or two in almost every line). However, the tech  was working, and it didn’t take long to come up with a solution: instead of showing the full text, we showed a still image from each clip and a list of the matching words.</p><p>问题是，如果我们向用户显示结果列表中的文本，尽管这些文本包含他们的搜索词，但他们的眼睛会被错误吸引(几乎每行都有一两个错误)。然而，这项技术正在发挥作用，没过多久就想出了一个解决方案：我们没有显示全文，而是显示了每个剪辑的静止图像和匹配的单词列表。</p><p>  For me, it was a great use of the technology, compared to the dictation packages seen at every trade show. It was genuinely useful, and it didn’t rely on changing anyone’s behaviour too much. It extended what was becoming a ubiquitous human skill — searching for stuff by typing keywords — and applied it to even more stuff: videos as well as web-pages. Our standard demo involved searching for a key word within hundreds of hours of video, and hitting  next over and over again, seeing the video jump to another and another and another person saying the entered word in different contexts.</p><p>对我来说，与每个商展上看到的听写包相比，这是对这项技术的一次很大的利用。它真的很有用，而且不太依赖于改变任何人的行为。它扩展了已经变得无处不在的人类技能-通过键入关键字来搜索东西-并将其应用到更多的东西上：视频和网页。我们的标准演示包括在数百小时的视频中搜索一个关键词，然后反复点击下一步，看到视频跳到另一个，另一个人在不同的上下文中说出输入的单词。</p><p> Now, to give Nuance, and DragonDictate credit: by the late 1990s, they had created Dragon NaturallySpeaking, which no longer required the user to speak with gaps between the words, and quite soon after, they were selling their technology, like us, as a toolkit to integrate into any application.</p><p>现在，值得称赞的是Nuance和DragonDictate：到20世纪90年代末，他们已经创建了Dragon NaturallySpeaging，不再需要用户说话时词与词之间的间隙，很快，他们就像我们一样，将他们的技术作为工具包出售，以集成到任何应用程序中。</p><p> Also, despite not being of interest to me, there were of course, all sorts of people who used speech recognition for dictation — professionals for whom dictation was already the norm, and a wide variety of people for whom keyboards are difficult to use.</p><p>此外，尽管我不感兴趣，但当然也有各种各样的人使用语音识别进行听写-对专业人士来说，听写已经是一种规范，还有各种各样的人很难使用键盘。</p><p>  Whether at SoftSound, Entropic or Nuance, from the mid-1990s on, we used to joke year after year after year, that “  Next year will be the big year for speech recognition”. Somehow that has finally crept up on us.</p><p>无论是SoftSound、Entrotic还是Nuance，从上世纪90年代中期开始，我们年复一年地开玩笑说，“明年将是语音识别的重要一年”。不知何故，这终于悄悄地降临到了我们身上。</p><p>  The lessons I learned building real-world applications, are pertinent to the behaviour I have seen in the last few years. Many people still don’t like talking to Siri when they have enough fingers free to type instead. However, just like our success in extending search into new media types, Siri and its cohort have succeeded in extending search into new situations: driving, cooking, bathing the kids etc:</p><p>我在构建真实世界的应用程序时学到的经验教训，与我在过去几年中看到的行为相关。许多人仍然不喜欢与Siri交谈，因为他们有足够的手指空闲来打字。然而，就像我们成功地将搜索扩展到新媒体类型一样，Siri和它的同龄人也成功地将搜索扩展到了新的情况：开车、做饭、给孩子洗澡等：</p><p>    That said, it is a full ten years on from the launch of Siri, and it is still not that easy to reroute a map, or to correct Alexa quickly, when Audible starts reading “50 Shades” to your kids.</p><p>这就是说，Siri推出已经整整十年了，当Audible开始给你的孩子读“50度”时，改变地图的路线，或者快速纠正Alexa仍然不是那么容易。</p><p> Audio feedback doesn’t give the user the same reassuring sense of certainty as a graphical user interface. One glance will confirm that I have typed my card number correctly, but you don’t have to be unusually impatient for your heart to sink, when you hear the inhumanly calm words, “I heard 4659 1234 1234 1234. Is that correct? Say yes or press one to confirm”.</p><p>音频反馈不会给用户带来与图形用户界面一样令人放心的确定感。扫一眼就会确认我输入的卡号是正确的，但当你听到这些异常平静的话时，你不必异常地不耐烦，因为你的心会沉下去，“我听到了4659 1234 1234 1234。对吗？回答是或按一确认“。</p><p> And as for errors, as well as Scottish accented YouTube clips, by 2016, there were much less jokey news stories, making claims that this was inherently racist technology. If Microsoft Office only worked for 90% of people, there would be uproar. Did this mean that speech recognition was just a novelty, rather than real product, empowering business etc?</p><p>至于错误，以及带有苏格兰口音的YouTube剪辑，到2016年，开玩笑的新闻故事要少得多，声称这是天生的种族主义技术。如果微软Office只为90%的人工作，那将会引起轩然大波。这是否意味着语音识别只是一种新奇的东西，而不是真正的产品、增强商业能力等？</p><p> However, neural nets really have come to the rescue, especially for this kind of problem. Having enough of the right training data turns out to be much more important than knowledge of the phonetic differences between accents — the network will work out what those differences are.</p><p>然而，神经网络真的起到了拯救作用，特别是对于这类问题。事实证明，拥有足够的正确训练数据比了解口音之间的语音差异要重要得多-网络将找出这些差异是什么。</p><p> Even five years ago, we needed to train systems for each regional accent, but nowadays, Siri copes with Scottish accents, just by training its networks on Scottish people reading known texts, i.e. teaching the networks the various ways in which a word can be pronounced.</p><p>甚至在五年前，我们还需要为每个地区的口音培训系统，但现在，Siri要应对苏格兰口音，只需培训它的网络，让苏格兰人阅读已知的文本，即教网络一个单词的各种发音方式。</p><p>  Computers have made multi-taskers of us all, and sometimes I think that, as an interface, even for interpersonal communications, speech can sometimes set us back:  I can be in several text chats at once, but I can’t be on two voice calls. Text and screen interactions have some real advantages, with which speech shouldn’t even try to compete.</p><p>计算机让我们所有人都成为一心多用的人，有时我认为，作为一种界面，即使是用于人际交流，语音有时也会让我们倒退：我可以同时进行几次短信聊天，但我不能同时进行两次语音通话。文字和屏幕互动有一些真正的优势，语音甚至不应该试图与之竞争。</p><p> However, for speech technology to reach the potential of what it, uniquely, can do well, it still has much further to go. This is good news for the industry, as more and more startups are funded to solve real-world problems, not dealt with by the big players.</p><p>然而，语音技术要发挥其独一无二的潜力，还有很长的路要走。这对该行业来说是个好消息，因为越来越多的初创企业被资助来解决现实世界的问题，而不是由大公司来处理。</p><p> Technology has to get as good at listening, and at speaking, as human beings are, and then — in some contexts — get better than we are. Here are a few examples from projects, which I and others have been working on lately.</p><p>技术必须像人类一样善于倾听和说话，然后-在某些情况下-变得比我们更好。这里有几个我和其他人最近一直在做的项目的例子。</p><p> Away from our headsets, speech isn’t really as linear as I have made out. In close proximity to someone speaking, I might whisper a comment to another listener, and still go unheard by anyone else. At a dinner party, I might be involved in more than one conversation at a time, because it is easy, in the 3D space of the real-world, to keep track of who has said what, and to control the volume and direction of my speech to target a specific listener.</p><p>离开我们的耳机，语音并不像我所说的那样是线性的。在靠近说话的人时，我可能会对另一位听众低声说一句话，但仍然不会被任何人听到。在晚宴上，我可能会同时参与多个对话，因为在现实世界的3D空间中，很容易跟踪谁说了什么，并控制我的演讲的音量和方向，以针对特定的听众。</p><p> Technology to separate speech from different speakers is coming on in leaps and bounds. This is achieved both by analysing the speech more deeply, and by combining the audio data with other sources, like using multiple microphones to measure relative volume and direction, or by using input from cameras to add lip movements and facial expressions to the mix.</p><p>将语音从不同的说话者中分离出来的技术正在突飞猛进地出现。这既可以通过更深入地分析语音来实现，也可以通过将音频数据与其他来源相结合来实现，比如使用多个麦克风来测量相对音量和方向，或者通过使用摄像头的输入将嘴唇运动和面部表情添加到混合中。</p><p> In 2016, Google came up with a new approach to speech synthesis, using WaveNet, a neural network, which can be trained to generate almost any kind of sound, and then training it with real human speech. Once trained, it can be fed with quite robotic synthesized speech, and then make it sound human.</p><p>2016年，谷歌提出了一种新的语音合成方法，使用神经网络WaveNet，它可以被训练成几乎产生任何类型的声音，然后用真实的人类语音进行训练。一旦经过训练，它就可以接受相当机器人合成的语音，然后让它听起来像人类。</p><p> Nowadays, the latest developments are routinely shared, and the whole industry takes the latest ideas from Google, NVIDIA, Microsoft and a global community of university researchers, and with their blessing, extends them and applies them in new contexts, adding expertise from their own niche professions.</p><p>如今，最新的发展被例行公事地分享，整个行业从谷歌、NVIDIA、微软和全球大学研究人员社区吸收最新想法，并在他们的支持下，将其扩展并应用于新的环境，增加来自自己利基专业的专业知识。</p><p> I have spent a lot of time working on systems which analyse accents, mispronunciations and speech impediments. Some people are difficult to understand because they have an unfamiliar accent, or are only just learning a language. We can make it easier to master pronunciation by giving them real-time feedback, but maybe we needn’t bother: morphing accents, and correcting errors in real time are both becoming a reality.</p><p>我花了很多时间在分析口音、发音错误和说话障碍的系统上。有些人很难理解，因为他们有不熟悉的口音，或者只是刚刚学习一门语言。我们可以通过给他们实时反馈来让他们更容易掌握发音，但也许我们不需要费心：变形口音和实时纠正错误都正在成为现实。</p><p>  Speech does not only vary by accent, but also by emotional and physical state. When a condition makes someone unintelligible, it is feasible not only to improve intelligibility but to identify what is wrong, perhaps categorising emergency calls, where the speaker is affected by stroke, sedation, drunkenness, concussion, or merely identify that the caller is a child, or speaks a particular language.</p><p>讲话不仅因口音不同而不同，还因情感和身体状态不同而不同。当一种情况让人听不懂时，不仅可以提高清晰度，还可以识别出哪里出了问题，也许可以对紧急呼叫进行分类，比如说话者受到中风、镇静、醉酒、脑震荡的影响，或者只是识别呼叫者是孩子，或者说一种特定的语言。</p><p> Finally, early identification of certain serious long-term neurological conditions is possible by monitoring subtle changes to speech. This can be done without hospital visits or even without targeting those who are at risk. Conveniently for all concerned, we all speak into our phones and computers all the time, so it would only be necessary to opt in, and give permission for your voice to be analysed, without compromising confidentiality by being recorded or listened to.</p><p>最后，通过监测语言的细微变化，早期识别某些严重的长期神经疾病是可能的。这可以在不去医院就诊的情况下完成，甚至不需要针对那些处于风险中的人。对于所有相关的人来说，方便的是，我们都在对着我们的手机和电脑说话，所以只需选择加入，并允许您的声音被分析，而不会因为被录音或被监听而危及机密性。</p><p> With the right training data, perhaps the same technology could be trained to recognise whether your cough, is in fact a  new persistent dry cough.</p><p>有了正确的训练数据，也许同样的技术可以被训练来识别你的咳嗽，实际上是否是一种新的持续性干咳。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://matthewkaras.medium.com/25-years-in-speech-technology-d5f9dfd98429">https://matthewkaras.medium.com/25-years-in-speech-technology-d5f9dfd98429</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/电脑/">#电脑</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/语音/">#语音</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/speech/">#speech</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1030651.html"><img src="http://img2.diglog.com/img/2020/10/thumb_686ffeb23c432d1aec644a54eddfa580.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030651.html">IPad 12.9“2020年回顾(个人电脑用户视角)</a></div><span class="my_story_list_date">2020-10-23 3:46</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030237.html"><img src="http://img2.diglog.com/img/2020/10/thumb_24e3cbefaf2f59753986bcf650aade74.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030237.html">WarnerMedia首次推出WarnerMedia Ride，这是一项免费视频服务，适用于通过AT&T的无限数据联网汽车计划连接到车载WiFi的智能手机和平板电脑</a></div><span class="my_story_list_date">2020-10-21 10:59</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030136.html"><img src="http://img2.diglog.com/img/2020/10/thumb_5bd26e532561a6eef12529fc36e1f249.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030136.html">预览版：MSI Summit Tiger Lake商用笔记本电脑</a></div><span class="my_story_list_date">2020-10-21 2:26</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030061.html"><img src="http://img2.diglog.com/img/2020/10/thumb_4b7d9ecba1881eda1176c27aaa7da297.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030061.html">Azure笔记本电脑的生命周期结束</a></div><span class="my_story_list_date">2020-10-20 22:46</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>