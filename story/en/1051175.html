<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>关于整数的概念 On Summing Integers</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">On Summing Integers<br/>关于整数的概念 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-08 00:59:46</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/220e16717a8220e9cb7536807d792e9f.png"><img src="http://img2.diglog.com/img/2021/3/220e16717a8220e9cb7536807d792e9f.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Recently there has been considerable discussion regarding (and questioning) the utility of AVX512. Though there have been many academic papers and commercial systems showing performance improvements through the use of vector instructions in general, here I will present a more pedestrian and simple to analyze use case. This article is partly inspired by the excellent ACM article  Benchmarking Hello World by Richard Sites. Only by deeply understanding simple programs do we have any hope of tackling larger ones.</p><p>最近关于（和质疑）AVX512的效用有相当大的讨论。虽然已经有许多学术论文和商业系统通过使用传染媒介指令，但在这里，我将在这里展示一个更多的行人和简单的分析用例。本文部分受到理查德网站的优秀ACM文章的精彩ACM文章。只有通过深入了解简单的程序，我们希望解决更大的程序。</p><p> Suppose that you want   to sum a sequence of 32 bit integers. If the data was stored as a std::vector&lt;int&gt; , the naive loop summing the elements might even get auto-vectorized and you would get sensible performance out of straight forward code.</p><p> 假设您希望总结32位整数的序列。如果数据存储为STD :: Vector＆lt; int＆gt; ，Naive循环求和元素甚至可能会自动矢量化，并且您将获得直接转发代码的明智性能。</p><p> Suppose now that you knew something more about your data, namely that the majority of the values could actually fit within a single byte. Using 4 bytes to store each of these values is wasteful, both in terms of RAM capacity as well as memory bandwidth (&amp; energy, etc) consumed when summing the data.</p><p> 假设您现在知道更多关于您的数据的内容，即大多数值实际上可以适合单个字节。使用4个字节将每个值存储在ram容量和存储器带宽（＆amp;能量等）方面浪费，在求解数据时消耗的内存带宽（＆amp;能量等）。</p><p> A possible encoding (certainly not the best) of the data, taking advantage of our knowledge of the distribution of values, is the following ( a godbolt link ; medium has terrible formatting):</p><p> 可以使用我们对价值分布的知识的数据的一种可能的编码（肯定不是最佳的）是以下（越怪诞链接;媒体有可怕的格式）：</p><p> struct data {  std::vector&lt;char&gt; byteVals; //byteVals[i] == -128 means look in intVals  std::vector&lt;int&gt; intVals; //length is number of -128 values in byteVals void push_back(int v) {  char bv = v;  if (bv == -128 || bv != v) {  byteVals.push_back(-128);  intVals.push_back(v);  }  else {  byteVals.push_back(bv);  }  } };</p><p> 结构数据{std :: vector＆lt; char＆gt; bytevals; // bytevals [i] == -128意味着看看INTVALS STD :: Vector＆lt; Int＆gt; Intvals; //长度为bytevals void push_back（int v）{char bv = v; if（bv == -128 || bv！= v）{bytevals.push_back（-128）; Intvals.push_back（v）; } else {bytevals.push_back（bv）; }};</p><p> Here values in the range [-127,127] take only one byte to store, every other int value effectively takes 5 bytes to store. If a sizeable fraction of all values are in the range [-127, 127], then this will take much less space (&amp; bandwidth, etc) when storing and summing the values. Notice that this encoding preserves the original order of the values. We will not make use of this property below, but it might be useful for example if this were used as a light-weight compression strategy in a columnar database.</p><p> 这里的值[-127,127]只需要一个字节来存储，每个其他int值有效地存储5个字节才能存储。如果所有值的大小分数都在[-127,127]中，则在存储和求解值时，这将取得更少的空间（＆amp;带宽等）。请注意，此编码保留了值的原始顺序。我们不会在下面使用此属性，但它可能有用，例如，如果将其用作柱状数据库中的轻重压缩策略。</p><p> For the rest of the blog post we will assume that ~75% of the values in the data are in the range [-127,127] and the other ~25% are not. The data will consist of 100 million integers. We will also focus here solely on single thread performance. We will also ignore undefined behavior in the C++ code below, as this is just toy code.</p><p> 对于博客文章的其余部分，我们将假设数据中的75％的值在范围内[-127,127]，另一个〜25％不是。数据将包括1亿整数。我们还将仅关注单线性能。我们还将忽略下面的C ++代码中的未定义行为，因为这只是玩具代码。 </p><p> Consider now the task of summing the values represented in this data structure. A naive scalar loop might look something like this:</p><p>现在考虑求解在此数据结构中表示的值的任务。一个天真的标量循环可能看起来像这样：</p><p> int sum_scalar(data&amp; d) {  int intValIndex = 0;  int sum = 0; //ignore overflow UB  for (auto b : d.byteVals) {  if (b == -128) {  sum += d.intVals[intValIndex];  intValIndex++;  }  else {  sum += b;  }  }  return sum; } ------------------------------------------------------------------ Benchmark Time CPU Iterations ------------------------------------------------------------------ BM_scalar/iterations:10 269462740 ns 270312500 ns 10</p><p> int sum_scalar（数据＆amp; d）{int intvalindex = 0; int sum = 0; //忽略overflow UB for（自动b：d.bytevals）{if（b == -128）{sum + = d.intvals [IntvalIndex]; IntvalIndex ++; } else {sum + = b; }}返回总和; } ------------------------------------------- -----------------基准时间CPU迭代--------------------------- -------------------------------- BM_SCALAR /迭代：10 269462740 NS 270312500 NS 10</p><p> If you profile this with VTune, you will find that a considerable number of execution slots (~31.4%) are wasted on “Bad Speculation”. If you are not familiar with this, please see [1].</p><p> 如果您使用VTUNE进行了配置文件，您会发现相当数量的执行插槽（〜31.4％）浪费在“糟糕的猜测”上。如果您不熟悉此，请参阅[1]。</p><p>  If we think back to our data distribution is it easy to see why this might be the case: the processor has no hope of latching onto a predictable pattern from the branch history it observes when checking for the sentinel value (-128). This will result in numerous branch mispredictions, leading to wasted work and stalls due to pipeline flushes.</p><p>  如果我们认为回到我们的数据分发，那么易于看出为什么这可能是这种情况：处理器在检查Sentinel值时，处理器没有希望从分支历史中锁定到可预测的模式上。这将导致众多分支机构错误预测，导致由于管道冲洗而浪费的工作和摊位。</p><p> It is possible to remedy this and convert this to a branch-less summation in the following way:</p><p> 可以通过以下方式对此进行补救并将其转换为分支的求和：</p><p> int sum_scalar_branchless(data&amp; d) {  int intValIndex = 0;  int sum = 0; //ignore overflow UB  for (auto b : d.byteVals) {  int cond = b == -128;  sum += (1 - cond) * b;  // possible OOB access, then multiplied by 0; ignore UB   sum += cond * d.intVals[intValIndex];  intValIndex += cond;  }  return sum; } --------------------------------------------------------------- Benchmark Time CPU Iterations --------------------------------------------------------------- BM_scalar_branchless 125660283 ns 125000000 ns 6</p><p> int sum_scalar_branchless（数据＆amp; d）{int intvalindex = 0; int sum = 0; //忽略溢出UB for（自动b：d.bytevals）{int cond = b == -128; SUM + =（1  -  COND）* B; //可能的OOB访问权限，然后乘以0;忽略UB Sum + = Cond * D.Intvals [IntvalIndex]; Intvalindex + = Cond;返回金额; } ------------------------------------------- -------------基准时间CPU迭代----------------------------------- ---------------------------  -  bm_scalar_branchless 125660283 ns 125000000 ns 6</p><p> This turns out to be slightly more than 2x faster. VTune shows that ~84% of the execution slots are spent retiring uops.</p><p> 这结果略高于2倍。 VTUNE显示〜84％的执行插槽是退休的UOPS。 </p><p>   To improve this considerably (not claiming that this is the absolute fastest of course), we will make use of a variety of AVX512 features/instructions (on an Icelake i5–1035G4 laptop): byte granular blends, byte granular comparision, and VNNI (outside of neural networks!):</p><p>要大大提高（未声称这是绝对最快的），我们将利用各种AVX512功能/说明（在ICELAKE I5-1035G4笔记本电脑上）：字节颗粒混合物，字节粒度比较和VNNI（在神经网络之外！）：</p><p> UPDATE: see  https://www.realworldtech.com/forum/?threadid=200693&amp;curpostid=200694 for a dramatic simplification. Not catching this is an oversight on my part. This post will be updated to include numbers with the mentioned strategy.</p><p> 更新：请参阅https://www.realworldtech.com/forum/?threadid=200693 &amp; zhostid=200694为戏剧性的简化。没有抓起这是我的疏忽。此帖子将被更新以包含所提到的策略的数字。</p><p> UPDATE: To my surprise and after much fiddling, I did not manage to write  a version that was measurably faster (indeed they were at least a percent slower) than the hand written sum_avx512 shown below. There is almost certainly something that I am doing wrong but I can’t seem to figure out what it is. I will take this opportunity to leave this as an exercise for the reader :).</p><p> 更新：到我的惊喜，经过大量摆弄，我没有设法编写一个可测量的版本，比下面所示的手写Sum_avx512更快（确实它们至少是较慢的百分比）。几乎肯定是我做错了，但我似乎无法弄清楚它是什么。我将借此机会将此作为读者的锻炼:)。</p><p> int sum_avx512(data&amp; d) {  int n = d.byteVals.size();  assert(n % 64 == 0); int intValIndex = 0;  //ignore overflow UB  auto vsum = _mm512_set1_epi32(0);  auto vsum2 = _mm512_set1_epi32(0); auto numIntVals = d.intVals.size();  auto numIntsProcessed = 0;  for (int i = 0; i &lt; n; i += 64) {  __m512i v = _mm512_loadu_epi8(&amp;d.byteVals[i]); // 1 where the value is -128, 0 otherwise  __mmask64 mask = _mm512_cmp_epi8_mask(  v,  _mm512_set1_epi8(-128),  _MM_CMPINT_EQ  ); // turn -128 values into 0, so that we avoid summing them  v = _mm512_mask_blend_epi8(mask, v, _mm512_set1_epi8(0)); /*  dst = _mm512_dpbusd_epi32(src, a, b)  Multiply groups of 4 adjacent pairs of unsigned 8-bit integers in a with  corresponding signed 8-bit integers in b, producing 4 intermediate signed 16-bit  results. Sum these 4 results with the corresponding 32-bit integer in src, and  store the packed 32-bit results in dst. Use of VNNI outside of DNN&#39;s!!!  */  vsum = _mm512_dpbusd_epi32(  vsum,  _mm512_set1_epi8(1), //multiply by 1 to preserve the values  v  ); // Interleave processing of ints to fill execution slots  if (numIntVals - numIntsProcessed &gt;= 16) {  vsum2 = _mm512_add_epi32(vsum2, _mm512_loadu_epi32(&amp;d.intVals[numIntsProcessed]));  numIntsProcessed += 16;  }  } auto ret = _mm512_reduce_add_epi32(_mm512_add_epi32(vsum, vsum2)); // no need to try to be optimal here; should only have a few remaining elements  int remainingSum = 0;  for (; numIntsProcessed &lt; numIntVals; numIntsProcessed++)  remainingSum += d.intVals[numIntsProcessed]; return ret + remainingSum; } ----------------------------------------------------- Benchmark Time CPU Iterations ----------------------------------------------------- BM_avx512 9435973 ns 9375000 ns 75</p><p> int sum_avx512（数据＆amp; d）{int n = d.bytevals.size（）;断言（n％64 == 0）; int intvalindex = 0; //忽略溢出UB自动vsum = _mm512_set1_epi32（0）;自动vsum2 = _mm512_set1_epi32（0）;自动numintvals = d.intvals.size（）;自动numintsprocessed = 0; for（int i = 0; i＆lt; n; i + = 64）{__m512i v = _mm512_loadu_epi8（＆amp; d.bytevals [i]）; // 1在其中值为-128,0否则__mmask64 mask = _mm512_cmp_epi8_mask（v，_mm512_set1_epi8（-128），_mm_cmpint_eq）; //将-128值转换为0，因此我们避免求和它们v = _mm512_mask_blend_epi8（mask，v，_mm512_set1_epi8（0））; / * DST = _MM512_DPBUSD_EPI32（SRC，A，B）将4个相邻的A相对的无符号8位整数组乘以B中的相应符号的8位整数，产生4个中间符号16位结果。使用SRC中的相应的32位整数以及将Packed 32位导致DST存储的32位整数。在DNN＆＃39之外的vnni使用!!! * / vsum = _mm512_dpbusd_epi32（vsum，_mm512_set1_epi8（1），//乘以1保留值v）; // Interleave处理INTS填充执行插槽（NumintVals  -  NumintsProcessed＆gt; = 16）{vsum2 = _mm512_add_epi32（vsum2，_mm512_loadu_epi32（＆amp; d.intvals [numintsprocessed]）; numintsprocessed + = 16;自动RET = _mm512_reduce_add_epi32（_mm512_add_epi32（vsum，vsum2））; //无需在这里尝试最佳;只有剩下的剩余元素int，剩余时间= 0; for（; numintsprocessed＆lt; numintvals; numintsprocessed ++）剩下+ = d.intvals [numintsprocessed];返回RET +余量; } ------------------------------------------- ----基准时间CPU迭代------------------------------------ ----------- bm_avx512 9435973 ns 9375000 ns 75</p><p>   It is difficult to do too much better than this (at least with this data representation) because we are nearly at the limit of single core memory bandwidth:</p><p>   很难做到的那么好（至少使用此数据表示），因为我们几乎处于单核存储带宽的极限：</p><p>  You might be wondering what the performance is of the auto vectorized loop (verified by looking at assembly) over data stored simply as std::vector&lt;int&gt; (no compression tricks).</p><p>  您可能会想知道在仅仅存储为STD :: Vector＆lt; Int＆gt的数据时，可以想知道性能是自动矢量化循环（通过查看组装验证）。 （没有压缩技巧）。</p><p> int sumVec(std::vector&lt;int&gt;&amp; vec) {  auto v = vec.data();  int sum = 0;  for (int i = 0; i &lt; n; i++)  sum += v[i];  return sum; } ----------------------------------------------------- Benchmark Time CPU Iterations ----------------------------------------------------- BM_vec 20682647 ns 21139706 ns 34</p><p> int sumvec（std ::: vector＆lt; int＆amp; vec）{auto v = vec.data（）; int sum = 0; for（int i = 0; i＆lt; n; i ++）sum + = v [i];退货总和; } ------------------------------------------- ----基准时间CPU迭代------------------------------------ ----------  -  BM_VEC 20682647 NS 21139706 NS 34 </p><p> It turns out to be ~2.19x slower than the hand vectorized sum over the compressed data representation. VTune data:</p><p>事实证明，比压缩数据表示的手矢量化总和慢〜2.19x。 VTune数据：</p><p>  shows ~82.4% execution slots stalled on memory. For both this and the previous hand vectorized avx512 code, we are essentially near/at the single core memory bandwidth. The sum over the compressed representation is faster simply because fewer bytes are transferred from DRAM. Indeed the bandwidth improvement is roughly 4 / (1*0.75 + 5 * 0.25) = 2, which is fairly close to the observed improvement in performance.</p><p>  显示〜82.4％的执行插槽停止在内存上。对于这两个和之前的手矢量化AVX512代码，我们基本上接近/在单核内存带宽附近。压缩表示的总和仅仅是因为从DRAM转移了更少的字节。实际上，带宽改善大约为4 /（1 * 0.75 + 5 * 0.25）= 2，这与观察到的性能改善相当接近。</p><p> While data volumes are exploding, improvements in performance/$ for CPUs continue to slow and DRAM prices continue to stagnate. I believe this will eventually force the software industry to move away from wasteful and “hardware agnostic code”. Put less charitably, I would consider it hardware ignorant code. As Amin Vahdat mentions in this excellent  keynote about the future of hardware and software, we will no longer be able to ignore integer factor potential improvements. AVX512, and hardware aware/hardware specific code more broadly, will be a key part of this future.</p><p> 虽然数据量爆炸，但CPU的性能改进继续缓慢，DRAM价格继续停滞不前。我相信这最终将强迫软件行业远离浪费和“硬件不可知论码”。少收获，我会考虑它的硬件无知代码。正如Amin Vahdat在这个优秀的关于硬件和软件的未来的主题演示中，我们将无法再忽略整数因素潜在改进。 AVX512和硬件感知/硬件特定代码更广泛地，将成为本未来的关键部分。</p><p>   [1] Yasin, Ahmad. “A top-down method for performance analysis and counters architecture.”  2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS). IEEE, 2014.</p><p>   [1]亚辛，艾哈迈德。 “绩效分析和柜台架构的自上而下方法。” 2014 IEEE系统和软件性能分析国际研讨会（ispass）。 IEEE，2014。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://unomerite.medium.com/on-summing-integers-608b2063583">https://unomerite.medium.com/on-summing-integers-608b2063583</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/整数/">#整数</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/summing/">#summing</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>