<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>类型理论和类型检查器的入门指南 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">类型理论和类型检查器的入门指南 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-06 04:31:33</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/a75d0dadef02cd93a35dd0af826da56e.png"><img src="http://img2.diglog.com/img/2020/12/a75d0dadef02cd93a35dd0af826da56e.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>到目前为止，在本系列文章中，我们已经研究了梯度下降的一般原理，以及如何计算前馈神经网络中每一层的反向传播，然后概括了一下CNN中不同类型层的反向传播。</p><p> 现在，我们将退后一步，从更一般的意义上来看反向传播-通过计算图。通过这一过程，我们将大致了解框架如何计算其</p><p> 我们将使用LSTM单元作为激励示例-继续对IMDB评论数据集进行情感分析-您可以在随附的Jupyter笔记本中找到代码</p><p>  让我们回顾一下本系列中使用的原理：</p><p> 偏导数的直觉：宽松地思考∂y x \ frac {\ partial {y}} {\ partial {x}}∂x y∂量化如果给xxxa一点“轻推”的值，yyy会改变多少。在那时候。</p><p> 分解计算-我们可以使用链式规则来帮助我们进行计算-而不是一口气尝试计算导数，而是将计算分解为较小的中间步骤。</p><p> 计算链式规则-当考虑要在链式规则表达式中包含哪些中间值时，请考虑涉及x x x的方程的直接输出-当我们稍微推拉x x x时，其他哪些值会直接受到影响？ </p><p>一次只包含一个元素-我们不用担心整个矩阵A A A，而是关注元素A i j A_ {ij} A i j。我们将一遍又一遍地引用一个方程式：</p><p> C i j = ∑ k A i k B k j C = A。 B C_ {ij} = \ sum_k A_ {ik} B_ {kj} \ iff C = A.B C i j = k A i k B k j C = A。乙</p><p> 尝试从一个元素到矩阵时，一个有用的技巧是在重复索引（此处为k）上求和-这表明矩阵乘法。</p><p>  C i j = A i j B i j C = A * B C_ {ij} = A_ {ij} B_ {ij} \ iff C = A * B C i j = A i j B i j⟺C = A * B</p><p> 完好无损地检查尺寸-检查所有匹配的矩阵的尺寸（派生矩阵应具有与原始矩阵相同的尺寸，并且所有相乘在一起的矩阵应具有对齐的尺寸。</p><p> 计算图使我们可以清楚地分解计算，并在计算链式规则时查看即时输出。</p><p> 我们将使用LSTM的计算图表示形式（如上所示），通过时间的反向传播来计算梯度。 </p><p>从上一篇文章开始，LSTM中一个时间步的正向传播方程为：</p><p> Γi =σ（W i。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + bi）\ Gamma_i = \ sigma（W_i。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] + b_i）Γi =σ（W i。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + bi）</p><p> Γf =σ（W f。[a＆lt; t-1＆gt;，x＆lt; t＆gt;] + bf）\ Gamma_f = \ sigma（W_f。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] + b_f）Γf =σ（W f。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + bf）</p><p> Γo =σ（W o。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + bo）\ Gamma_o​​ = \ sigma（W_o。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] + b_o）Γo =σ（W o。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + bo）</p><p> c〜＆lt;吨= tanh⁡（W c。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + b c）\ tilde {c} ^ {＆lt; t＆gt;} = \ tanh（W_c。[a ^ {＆lt; t-1>}，x ^ {＆lt; t＆gt;}] + b_c）c〜＆lt;吨= tanh（W c。[a＆lt; t − 1＆gt;，x＆lt; t＆gt;] + b c）</p><p> c ＜吨=Γi ∗ c〜＆lt;吨+Γf ∗ c＆lt; t − 1＆gt; {c} ^ {＆lt; t＆gt;} = \ Gamma_i * \ tilde {c} ^ {＆lt; t＆gt;} + \ Gamma_f * {c} ^ {＆lt; t-1＆gt;} c＆lt;吨=Γi ∗ c〜＆lt;吨+Γf ∗ c＆lt; t − 1＆gt;</p><p> ＆lt;吨=Γo tanh⁡c＆lt;吨a ^ {＆lt; t＆gt;} = \ Gamma_o​​ * \ tanh {c} ^ {＆lt; t＆gt;} a＆lt;吨=Γo ∗ tanh c＆lt;吨 </p><p>[a＆lt; t − 1＆gt; ，x ＜吨] [a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] [a＆lt; t − 1＆gt; ，x ＜吨]表示两个矩阵的级联以形成（n a + n x）（n_a + n_x）（n a + n x）x m m m矩阵。一种 。 B A.B A。 B表示A A A和B B B的矩阵乘法，而A * B A * B A * B表示元素乘法。 Γ\ GammaΓ是指门-有关所使用符号的完整分类，请参见上一篇文章定义LSTM。</p><p> 给定相对于＆lt; 2的梯度，以反向传播通过该单元。吨a ^ {＆lt; t＆gt;} a＆lt;吨和c＆lt;吨c ^ {＆lt; t＆gt;} c＆lt;吨从下一步的反向传播中，我们需要计算每个权重W i，W f，W o，W c W_i，W_f，W_o，W_c W i，W f，W o，W的梯度c并对bi，bf，bo，bc b_i，b_f，b_o，b_c bi，bf，bo，bc进行偏置，最后我们需要向后传播到上一个时间步长并计算关于a＆lt; ; t − 1＆gt; a ^ {＆lt; t-1＆gt;} a＆lt; t − 1＆gt;和c＆lt; t − 1＆gt; c ^ {＆lt; t-1＆gt;} c＆lt; t-1＞。</p><p> 这些是很多要计算的偏导数-实际上，随着我们的神经网络变得越来越复杂，将会有更多的偏导数要计算。</p><p>  首先，由于门方程是相同的，我们可以将它们组合起来，所以我们有一个3 na 3n_a 3 na xmmm矩阵Γ\ GammaΓ包含三个门的输出，我们可以引用矩阵Γi的前三分之一\ Gamma_iΓi和另外三分之二Γf \ Gamma_fΓf和Γo \ Gamma_o​​Γo。那么我们有一个3 na 3n_a 3 na x（na + nx）（n_a + n_x）（na + nx）权重矩阵W g W_g W g和一个3 na 3n_a 3 na x 1 1 1偏置向量bg b_g bg。</p><p> 接下来，我们要记录计算中的所有中间阶段（图中的每个节点）。</p><p>  我们将＆lt; t − 1＆gt; a ^ {＆lt; t-1＆gt;} a＆lt; t − 1＆gt;并且x ＜吨x ^ {＆lt; t＆gt;} x＆lt;吨形成（n a + n x）（n_a + n_x）（n a + n x）x m m m级联的输入矩阵[a＆lt; t − 1＆gt; ，x ＜吨] [a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] [a＆lt; t − 1＆gt; ，x ＜吨]。</p><p> 我们使用权重W g W_g W g和偏差b g b_g b g来计算门Z g Z_g Z g的加权输入矩阵。 </p><p>同样，我们计算候选存储器c〜＆lt;的加权输入Z c Z_c Z c。吨\ tilde {c} ^ {＆lt; t＆gt;} c〜＆lt;吨使用权重矩阵W c W_c W c和偏差b c b_c b c。</p><p> （注意：该图使用一个权重矩阵W，但由于使用的激活函数不同，因此有助于分别考虑这些权重）</p><p> 我们将S型激活函数应用于Z g Z_g Z g以获得门矩阵Γ\ GammaΓ（在图中用f，i，o表示），tanh激活函数应用于Z c Z_c Z c以获得c〜 ＆lt;吨\ tilde {c} ^ {＆lt; t＆gt;} c〜＆lt;吨（在图中用g表示）。</p><p> 由于逐元素的乘法和加法是简单的运算，为简便起见，我们将不给出中间输出Γi * c〜＆lt;吨\ Gamma_i * \ tilde {c} ^ {＆lt; t＆gt;}Γi ∗ c〜＆lt;吨并且Γf ∗ c＆lt; t − 1＆gt; \ Gamma_f * {c} ^ {＆lt; t-1＆gt;}Γf ∗ c＆lt; t − 1＆gt;自己的符号。</p><p> 现在，我们将计算图分解为步骤，并添加了中间变量，我们得到了以下等式：</p><p> Z g ＝ W g。 [a＆lt; t − 1＆gt; ，x ＜吨] + b g Z_g = W_g。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] + b_g Z g = W g。 [a＆lt; t − 1＆gt; ，x ＜吨] + b g</p><p> Z c = W c。 [a＆lt; t − 1＆gt; ，x ＜吨] + b c Z_c = W_c。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] + b_c Z c = W c。 [a＆lt; t − 1＆gt; ，x ＜吨] + b c </p><p>c ＜吨=Γi ∗ c〜＆lt;吨+Γf ∗ c＆lt; t − 1＆gt; {c} ^ {＆lt; t＆gt;} = \ Gamma_i * \ tilde {c} ^ {＆lt; t＆gt;} + \ Gamma_f * {c} ^ {＆lt; t-1＆gt;} c＆lt;吨=Γi ∗ c〜＆lt;吨+Γf ∗ c＆lt; t − 1＆gt;</p><p> 这些方程式对应于图中的节点-左侧变量是节点的输出边，右侧变量是节点的输入边。</p><p>  这些方程使我们在计算链式规则时可以清楚地看到变量的即时输出-例如对于[＆lt; t − 1＆gt; ，x ＜吨] [a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] [a＆lt; t − 1＆gt; ，x ＜吨]立即输出是Z g Z_g Z g和Z c Z_c Z c。</p><p> 如果我们看方程式/计算图，我们通常可以看一下运算的类型，然后使用相同的标识：</p><p> 加法：如果C = A + BC = A + BC = A + B，则∂C∂A = 1 \ frac {\ partial {C}} {\ partial {A}} = 1∂A∂C = 1和∂ C∂B = 1 \ frac {\ partial {C}} {\ partial {B}} = 1∂B∂C = 1</p><p> 元素乘法：如果C = A * BC = A * BC = A ∗ B，则∂C∂A = B \ frac {\ partial {C}} {\ partial {A}} = B∂A∂C = = B ∂C∂B = A \ frac {\ partial {C}} {\ partial {B}} = A∂B∂C = A</p><p> tanh：如果C = tanh⁡AC = \ tanh AC = tanh A，则∂C∂A = 1-tanh⁡2 A = 1-C 2 \ frac {\ partial {C}} {\ partial {A}} = 1 -\ tanh ^ 2 A = 1-C ^ 2∂A∂C = 1 = tanh 2 A = 1 − C 2 </p><p>乙状结肠：如果C =σ（A）C = \ sigma（A）C =σ（A），则∂C∂A =σ（A）−σ2（A）= C ∗（1 − C）\ frac {\部分{C}} {\部分{A}} = \ sigma（A）-\ sigma ^ 2（A）= C *（1-C）∂A∂C = =σ（A）−σ2（A） = C ∗（1 − C）</p><p> 加权输入：这与前馈神经网络相同。如果Z = W。 X + b Z = W.X + b Z = W。 X + b然后：</p><p> ∂J∂W = 1 m∂J∂Z。 XT \ frac {\ partial {J}} {\ partial {W}} = \ frac {1} {m} \ frac {\ partial {J}} {\ partial {Z}}。X ^ {T}∂W ∂J = m 1∂Z∂J。 T</p><p> ∂J∂b = 1 m ∑ i = 1 m∂J f Z \ frac {\ partial {J}} {\ partial {b}} = \ frac {1} {m} \ sum_ {i = 1} ^ { m} \ frac {\ partial {J}} {\ partial {Z}}∂b∂J = m 1 i = 1 m∂Z∂J</p><p> ∂X = W T。 ∂J∂Z \ frac {\ partial {J}} {\ partial {X}} = W ^ {T}。\ frac {\ partial {J}} {\ partial {Z}}∂X∂J = WT 。 ∂Z∂J</p><p> 有了这些通用的计算图原理，我们就可以应用链式规则。我们对偏导数进行元素乘（∗ * ∗），即</p><p> ∂J∂A =∂J∂B ∗∂B∂A \ frac {\ partial {J}} {\ partial {A}} = \ frac {\ partial {J}} {\ partial {B}} * \ frac {\ partial {B}} {\ partial {A}}∂A∂J = =∂B∂J ∗∂A∂B </p><p>因此，如果B = f（A）B = f（A）B = f（A）且C = g（A）C = g（A）C = g（A），即BBB和CCC是AAA中的立即输出计算图，然后将偏导数求和：</p><p> ∂J∂A =∂J∂B ∗ B∂A +∂J∂C ∗∂C∂A \ frac {\ partial {J}} {\ partial {A}} = \ frac {\ partial {J}} {\ partial {B}} * \ frac {\ partial {B}} {\ partial {A}} + \ frac {\ partial {J}} {\ partial {C}} * \ frac {\ partial {C} } {\ partial {A}}∂A∂J = = B B∂J ∗∂A∂B +∂C∂J ∗∂A∂C</p><p> 在TensorFlow或Keras之类的深度学习框架中，每个可区分的操作都将具有这样的身份。</p><p>  尝试计算∂J∂A \ frac {\ partial {J}} {\ partial {A}}∂A∂J时，我们将使用以下一般公式：</p><p> ∂J∂A =∂J∂B ∗∂B∂A \ frac {\ partial {J}} {\ partial {A}} = \ frac {\ partial {J}} {\ partial {B}} * \ frac {\ partial {B}} {\ partial {A}}∂A∂J = =∂B∂J ∗∂A∂B</p><p> 为简便起见，我们将使用上述操作的标识替换∂B∂A \ frac {\ partial {B}} {\ partial {A}}∂A∂B的值。</p><p>   ∂J〜a〜＆lt;吨=∂J＆lt;吨∗Γo \ frac {\ partial {J}} {\ partial {\ tilde {a} ^ {＆lt; t＆gt;}}} = \ frac {\ partial {J}} {\ partial {a ^ {＆lt; t＆gt;}}} * \ Gamma_o​​∂a〜＆lt;吨∂J = a＆lt;吨∂J ∗Γo </p><p>∂JΓo =∂J∂a＆lt;吨* a〜＆lt;吨\ frac {\ partial {J}} {\ partial {\ Gamma_o​​}} = \ frac {\ partial {J}} {\ partial {a ^ {＆lt; t＆gt;}}} * \波浪号{a} ^ {＆lt; t＆gt;}∂Γo∂J = = a a＆lt;吨∂J ∗ a〜＆lt;吨</p><p> 使用方程式6，并将方程式5写为c ＜c的方程式。 t + 1＆gt; c ^ {＆lt; t + 1＆gt;} c＆lt; t + 1＆gt;而不是c＆lt;吨c ^ {＆lt; t＆gt;} c＆lt;吨（即在时间步长上加1）：</p><p> ∂c＜吨=∂c＆lt; t + 1＆gt; *Γf +∂J∂a〜＆lt;吨∗（1 − a〜＆lt; t＆gt; 2）\ frac {\ partial {J}} {\ partial {c ^ {＆lt; t＆gt;}}} = \ frac {\ partial {J}} {\ partial {c ^ {＆lt; t + 1＆gt;}}} * \ Gamma_f + \ frac {\ partial {J}} {\ partial {\ tilde {a} ^ {＆lt; t（＆gt;}}} *（1- \ tilde {a} ^ {＆lt; t＆gt; 2}）∂c＆lt;吨∂J = c＆lt; t + 1＆gt; ∂J ∗Γf + + a a＆lt;吨∂J ∗（1 − a〜＆lt; t＆gt; 2）</p><p>  ∂J c〜＆lt;吨=∂c＆lt;吨∗ i i \ frac {\ partial {J}} {\ partial {\ tilde {c} ^ {＆lt; t＆gt;}}} = \ frac {\ partial {J}} {\ partial {c ^ {＆lt; t＆gt;}}} * \ Gamma_i∂c〜＆lt;吨∂J = c＆lt;吨∂J ∗Γi</p><p> ∂J∂i =∂C∂＆lt;吨* c〜＆lt;吨\ frac {\ partial {J}} {\ partial {\ Gamma_i}} = \ frac {\ partial {J}} {\ partial {c ^ {＆lt; t＆gt;}}} * \ tilde {c} ^ {＆lt; t＆gt;}∂i =∂c＆lt;吨∂J ∗ c〜＆lt;吨</p><p> ∂JΓf =∂J∂c＆lt;吨* c＆lt; t − 1＆gt; \ frac {\ partial {J}} {\ partial {\ Gamma_f}} = \ frac {\ partial {J}} {\ partial {c ^ {＆lt; t＆gt;}}} * c ^ {＆lt; t-1＆gt;}∂Γf J = c＆lt;吨∂J ∗ c＆lt; t − 1＆gt;</p><p>  ∂J∂Z g =∂J∂ΓΓ∗（1 −Γ）\ frac {\ partial {J}} {\ partial {Z_g}} = \ frac {\ partial {J}} {\ partial {\ Gamma }} * \ Gamma *（1- \ Gamma）∂Z g∂J = =ΓΓJ ∗Γ∗（1 −Γ） </p><p>∂Z c = c c〜＆lt;吨∗（1 − c〜＆lt; t＆gt; 2）\ frac {\ partial {J}} {\ partial {Z_c}} = \ frac {\ partial {J}} {\ partial {\ tilde {c} ^^ { ＆lt; t＆gt;}}} *（1- \ tilde {c} ^ {＆lt; t＆gt; ^ 2}）∂Z c∂J =∂c〜＆lt;吨∂J ∗（1 − c〜＆lt; t＆gt; 2）</p><p> 公式1和2相同，偏导数也相同，只是下标不同。</p><p> ∂J∂W g = 1 m∂J∂Z g。 [a＆lt; t − 1＆gt; ，x ＜吨] T \ frac {\ partial {J}} {\ partial {W_g}} = \ frac {1} {m} \ frac {\ partial {J}} {\ partial {Z_g}}。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] ^ T∂W g∂J = m 1∂Z g∂J。 [a＆lt; t − 1＆gt; ，x ＜吨] T</p><p> ∂J bg = 1 m ∑ i = 1 m∂Z g（i）\ frac {\ partial {J}} {\ partial {b_g}} = \ frac {1} {m} \ sum_ {i = 1} ^ {m} \ frac {\ partial {J}} {\ partial {Z_g ^ {{i}}}}}∂bg∂J = m 1 i = 1 m∂Z g（i） ∂J</p><p> ∂J∂Wc ＝1m∂J∂Zc。 [a＆lt; t − 1＆gt; ，x ＜吨] T \ frac {\ partial {J}} {\ partial {W_c}} = \ frac {1} {m} \ frac {\ partial {J}} {\ partial {Z_c}}。[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}] ^ T∂W c∂J = m 1∂Z c∂J。 [a＆lt; t − 1＆gt; ，x ＜吨] T</p><p> ∂J bc = 1 m ∑ i = 1 m∂Z c（i）\ frac {\ partial {J}} {\ partial {b_c}} = \ frac {1} {m} \ sum_ {i = 1} ^ {m} \ frac {\ partial {J}} {\ partial {Z_c ^ {（i）}}}}∂bc∂J = m 1 i = 1 m∂Z c（i） ∂J</p><p> ∂J∂[a＆lt; t − 1＆gt; ，x ＜吨] ＝ W g T。 ∂J∂Z g + W c T。 ∂J∂Z c \ frac {\ partial {J}} {\ partial {[a ^ {＆lt; t-1＆gt;}，x ^ {＆lt; t＆gt;}]}} = W_g ^ T。\ frac {\ partial {J}} {\ partial {Z_g}} + W_c ^ T。\ frac {\ partial {J}} {\ partial {Z_c}}∂[ ＆lt; t − 1＆gt; ，x ＜吨]∂J = W g T。 ∂Z g∂J + W c T。 ∂Z c∂J </p><p>因此，通过将计算图分解为多个步骤，我们可以将计算分解为更小的更简单的步骤，而仅使用上述操作的派生身份。  我们看到的激励性示例使用LSTM网络对IMDB评论数据集进行情感分析  ＃我们计算dC＆lt; t＆gt; 因为我们都需要dC＆lt; t + 1＆gt; 和dA ＜ t＆gt;  ＃关于IFO门的输出的导数-滥用表示法称为dIFO  ＃关于门的未激活输出的导数（在应用S型信号之前）  今年夏天，我正在用自己的博客讲授我今年学到的主题。 这是双赢的局面-您可以获得计算机科学教程，并且可以与您分享！  在检查反向支撑方程式时，有一个数字检查器会很有帮助-我在随附的Jupyter笔记本中写了一个数字检查器。 </p><p>我们开始研究该系列中最常用的术语，然后研究林中的简单机器学习算法。  ...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://mukulrathi.co.uk/demystifying-deep-learning/backpropagation-computation-graph-lstm/">https://mukulrathi.co.uk/demystifying-deep-learning/backpropagation-computation-graph-lstm/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/类型/">#类型</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/type/">#type</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/计算/">#计算</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1037915.html"><img src="http://img2.diglog.com/img/2020/12/thumb_50a96ec38224a65e065993b6f8c5b389.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1037915.html">std :: visit是现代C ++的所有错误（2017） </a></div><span class="my_story_list_date">2020-12-5 22:9</span></div><div class="col-sm"><div><a target="_blank" href="/story/1037447.html"><img src="http://img2.diglog.com/img/2020/12/thumb_9cb818c3665e45455227ccec195377ac.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1037447.html">四种类型的汉字（2019） </a></div><span class="my_story_list_date">2020-12-4 20:32</span></div><div class="col-sm"><div><a target="_blank" href="/story/1037116.html"><img src="http://img2.diglog.com/img/2020/11/thumb_025b67dc2053e70abacf6317da9484ff.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1037116.html">Nim编译器— Pascal源代码</a></div><span class="my_story_list_date">2020-11-29 16:5</span></div><div class="col-sm"><div><a target="_blank" href="/story/1036720.html"><img src="http://img2.diglog.com/img/2020/11/thumb_7cf484ceeae6bc7fc2161eddc603cd15.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1036720.html">解析，不进行类型检查</a></div><span class="my_story_list_date">2020-11-26 23:13</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>