<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>用TCP包做大Going Big with TCP Packets</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Going Big with TCP Packets<br/>用TCP包做大</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-17 13:33:22</div><div class="page_narrow text-break page_content"><p>The following subscription-only content has been made available to you by an LWN subscriber. Thousands of subscribers depend on LWN for the best news from the Linux and free software communities. If you enjoy this article, please consider  subscribing to LWN. Thank youfor visiting LWN.net!</p><p>LWN订户已向您提供以下仅限订阅的内容。成千上万的订阅者依靠LWN从Linux和自由软件社区获得最好的消息。如果您喜欢这篇文章，请考虑订阅LWN。感谢您访问LWN。网</p><p> Like most components in the computing landscape, networking hardware hasgrown steadily faster over time. Indeed, today&#39;s high-end networkinterfaces can often move data more quickly than the systems they areattached to can handle. The networking developers have been working foryears to increase the scalability of their subsystem; one of the currentprojects is  theBIG TCP patch set from Eric Dumazet and Coco Li. BIG TCP isn&#39;t foreverybody, but it has the potential to significantly improve networkingperformance in some settings. Imagine, for a second, that you are trying to keep up with a 100Gb/snetwork adapter. As networking developer Jesper Brouer  described back in 2015, if one is using the longstandingmaximum packet size of 1,538 bytes, running the interface at full speedmeans coping with over eight-million packets per second. At that rate, CPUhas all of about 120ns to do whatever is required to handle each packet,which is not a lot of time; a single cache miss can ruinthe entire processing-time budget. The situation gets better, though, if the number of packets is reduced, andthat can be achieved by making packets larger. So it is unsurprising thathigh-performance networking installations, especially local-area networkswhere everything is managed as a single unit, use larger packet sizes.With proper configuration, packet sizes up to 64KB can be used, improving the situationconsiderably. But, in settings where data is being moved in units ofmegabytes or gigabytes (or more — cat videos are getting larger all thetime), that still leaves the system with a lot of packets to handle. Packet counts hurt in a number of ways. There is a significant fixedoverhead associated with every packet transiting a system. Each packetmust find its way through the network stack, from the upper protocol layersdown to the device driver for the interface (or back). More packets means moreinterrupts from the network adapter. The  sk_buffstructure (&#34;SKB&#34;) used to represent packets within the kernel is a largebeast, since it must be able to support just about any networking feature that may be in use; that leads to significant per-packetmemory use and memory-management costs. So there are good reasons to wishfor the ability to move data in fewer, larger packets, at least forsome types of applications. The length of an IP packet is stored in the IP header; for both IPv4 andIPv6, that length lives in a 16-bit field, limiting the maximumpacket size to 64KB. At the time these protocols were designed, a 64KBpacket could take multiple seconds to transmit on the backbone Internetlinks that were available, so it must have seemed like a wildly large number;surely 64KB would be more than anybody would ever rationally want to put intoa single packet. But times change, and 64KB can now seem like acripplingly low limit. Awareness of this problem is not especially recent: there is a solution(for IPv6, at least) to be found in  RFC 2675,which was adopted in 1999. The IPv6 specification allows the placement of &#34;hop-by-hop&#34;headers with additional information; as the name suggests, a hop-by-hopheader is used to communicate options between two directly connectedsystems. RFC 2675 enables larger packets with a couple of tweaks tothe protocol. To send a &#34;jumbo&#34; packet, a system must set the (16-bit) IPpayload length field to zero and add a hop-by-hop header containing thereal payload length. The length field in that header is 32 bits,meaning that jumbo packets can contain up to 4GB of data; that shouldsurely be enough for everybody. The BIG TCP patch set adds the logic necessary to generate and accept jumbopackets when the maximum transmission unit (MTU) of a connection is setsufficiently high. Unsurprisingly, there were a number of details tomanage to make this actually work. One of the more significant issues isthat packets of any size are rarely stored in physically contiguous memory,which tends to be hard to come by in general. For zero-copy operations,where the buffers live in user space, packets are guaranteed to bescattered through physical memory. So packets are representedas a set of &#34;fragments&#34;, which can be as short as one (4KB) page each;network interfaces handle the task of assembling packets from fragments ontransmission (or fragmenting them on receipt). Current kernels limit the number of fragments stored in an SKB to 17,which is sufficient to store a 64KB packet in single-page chunks. Thatlimit will clearly interfere with the creation of larger packets, so thepatch set raises the maximum number of fragments (to 45). But, asAlexander Duyck  pointedout, many interface drivers encode assumptions about the maximum numberof fragments that a packet may be split into. Increasing that limitwithout fixing the drivers could lead to performance regressions or evenlocked-up hardware, he said. After some discussion, Dumazet  proposedworking around the problem by adding a configuration option controlling themaximum number of allowed fragments for any given packet. That is fine forsites that build their own kernels, which prospective users of this feature are relatively likely to do. It offers little help fordistributors, though, who must pick a value for this option for all oftheir users. In any case, many drivers will need to be updated to handle jumbo packets.Modern network interfaces perform segmentation offloading, meaning thatmuch of the work of creating individual packets is done within theinterface itself. Making segmentation offloading work with jumbo packetstends to involve a small number of tweaks; a few drivers are updated in thepatch set. One other minor problem has to do with the placement of the RFC 2675hop-by-hop header. These headers, per the IPv6 standard, are placedimmediately after the IP header; that can confuse software that &#34;knows&#34;that the TCP header can be found immediately after the IP header in apacket. The  tcpdump utility has some problems in this regard; italso seems that there are a fair number of BPF programs in the wild thatcontain this assumption. For this reason, jumbo-packet handling isdisabled by default, even if the underlying hardware and link could handlethose packets. Dumazet included some brief benchmark results with the patch posting.Enabling a packet size of 185,000 bytes increased network throughput bynearly 50% while also reducing round-trip latency significantly. So BIGTCP seems like an option worth having, at least in the sort of environments(data centers, for example) that use high-speed links and can reliablydeliver large packets. If tomorrow&#39;s cat videos arrive a little morequickly, BIG TCP may be part of the reason. See  Dumazet&#39;s2021 Netdev talk on this topic for more details.         ( Log in to post comments)</p><p>与计算领域的大多数组件一样，网络硬件随着时间的推移稳步增长。事实上，今天&#39；s的高端网络接口通常比其所连接的系统处理数据的速度更快。网络开发人员多年来一直致力于提高其子系统的可扩展性；当前的项目之一是Eric Dumazet和Coco Li开发的大型TCP补丁集。大TCP不是&#39；但在某些情况下，它有可能显著提高网络性能。想象一下，你正在努力跟上100Gb/s网络适配器的步伐。正如网络开发者Jesper Brouer在2015年所描述的那样，如果一个人使用的是1538字节的长期最大数据包大小，全速运行接口意味着每秒要处理800多万个数据包。在这个速率下，CPUH需要大约120N的时间来处理每个数据包，这并不需要很多时间；一次缓存未命中可能会破坏整个处理时间预算。不过，如果数据包的数量减少，情况会变得更好，而这可以通过使数据包变大来实现。因此，毫不奇怪，高性能网络安装，尤其是局域网，在局域网中，所有东西都作为一个单元进行管理，使用更大的数据包大小。通过适当的配置，可以使用最大为64KB的数据包，从而大大改善了这种情况。但是，在以兆字节或千兆字节为单位移动数据的环境中（或者更多——cat视频一直在变大），系统仍然需要处理大量数据包。数据包计数在很多方面都是有害的。在系统中传输的每个数据包都有一个重要的固定溢出。每个包必须在网络堆栈中找到自己的路径，从上层协议层到接口的设备驱动程序（或背面）。更多的数据包意味着来自网络适配器的中断更多。sk#buffstructure（&#34；SKB&#34；）用于在内核中表示数据包的是一个大型Beast，因为它必须能够支持可能正在使用的任何网络功能；这会导致每个包的内存使用和内存管理成本显著增加。因此，至少对于某些类型的应用程序，我们有充分的理由希望能够以更少、更大的数据包移动数据。IP分组的长度存储在IP报头中；对于IPv4和PV6，该长度都存在于16位字段中，将最大数据包大小限制为64KB。在设计这些协议时，一个64kb的数据包可能需要几秒钟才能在可用的主干网线上传输，因此它看起来一定是一个非常大的数字；当然，64KB比任何人理性地想要放入一个数据包的容量都要多。但时代变了，64KB现在似乎是一个非常低的限制。对这个问题的认识并不是最近才开始的：1999年采用的RFC 2675中有一个解决方案（至少是IPv6）。IPv6规范允许放置&#34；一步一步地&#34；带有附加信息的标题；顾名思义，逐跳标头用于在两个直接连接的系统之间传递选项。RFC 2675通过对协议进行几处调整来实现更大的数据包。发送&#34；jumbo和#34；数据包时，系统必须将（16位）IPpayload length字段设置为零，并添加包含实际有效负载长度的逐跳标头。该报头中的长度字段是32位，这意味着巨型数据包最多可以包含4GB的数据；这对每个人来说都应该足够了。当连接的最大传输单元（MTU）设置得非常高时，大型TCP补丁集增加了生成和接受巨型数据包所需的逻辑。不出所料，要想让它真正发挥作用，有很多细节需要管理。更重要的问题之一是，任何大小的数据包都很少存储在物理上连续的内存中，这通常很难获得。对于零拷贝操作，缓冲区位于用户空间，数据包保证通过物理内存分散。因此，数据包被表示为一组&#34；碎片&#34；，每个页面可以短至一（4KB）；网络接口处理从传输中收集数据包的任务（或在收到数据包时将其分割）。当前内核将存储在SKB中的片段数量限制为17，这足以将64KB的数据包存储在单页块中。这一限制显然会干扰更大数据包的创建，因此patch set将最大片段数提高到45个。但是，asAlexander Duyck指出，许多接口驱动程序对一个数据包可能被拆分成的最大片段数的假设进行编码。他说，在不修复驱动程序的情况下提高这一限制可能会导致性能下降，甚至导致硬件锁定。经过一些讨论，Dumazet建议通过添加一个配置选项来解决这个问题，该选项控制任何给定数据包允许的最大片段数。对于构建自己内核的站点来说，这是很好的，而这一功能的潜在用户相对来说很可能会这样做。不过，它对分销商几乎并没有帮助，他们必须为所有用户的这个选项选择一个值。在任何情况下，许多驱动程序都需要更新以处理巨型数据包。现代网络接口执行分段卸载，这意味着创建单个数据包的大部分工作是在接口本身内完成的。使用大型打包机进行分段卸载，只需进行少量调整；一些驱动程序在批处理集中更新。另一个小问题与RFC 2675逐跳标头的放置有关。根据IPv6标准，这些报头位于IP报头的中间位置；这可能会让那些&#34；知道&#34；可以在apacket中的IP头之后立即找到TCP头。tcpdump实用程序在这方面存在一些问题；似乎还有相当多的BPF项目包含了这种假设。因此，即使底层硬件和链路可以处理这些数据包，在默认情况下也会禁用巨型数据包处理。Dumazet在发布补丁时提供了一些简短的基准测试结果。实现185000字节的数据包大小可以将网络吞吐量提高近50%，同时还可以显著减少往返延迟。因此，BIGTCP似乎是一个值得拥有的选项，至少在使用高速链路并能可靠传输大数据包的环境（例如数据中心）中是这样。如果明天&#39；美国的猫视频来得更快一些，大TCP可能是原因之一。见杜马泽&#39；s2021 Netdev将就此主题进行讨论，了解更多详细信息。（登录发表评论）</p><p>  This is not too different from the problems that modern file systems have using the full bandwidth of modern storage. Isn&#39;t the root problem the same? CPUs are not getting faster, but other hardware pieces are, so abstraction layers have to do as little as possible and algorithms have to be extremely efficient. Very interesting times for operating system development.</p><p>这与现代文件系统在使用现代存储的全部带宽时遇到的问题没有太大区别。不是&#39；根本问题不一样吗？CPU的速度并没有加快，但其他硬件的速度却在加快，因此抽象层必须尽可能少地工作，算法必须非常高效。操作系统开发非常有趣的时刻。</p><p> HDD I/O speeds are not getting significantly faster, either. It&#39;s actually starting to become a bit of a problem, because the ratio of storage space to I/O bandwidth has gotten extremely biased in favor of the former as HDDs get bigger but not faster, so a large fleet of HDDs doesn&#39;t have enough overall bandwidth to be useful at scale. You could work around this by buying a lot of small HDDs, but that&#39;s so expensive (per gigabyte) that you&#39;re probably better off just buying SSDs instead. As a result, we&#39;re starting to see increased use of SSDs even in domains where HDD seek time would otherwise be acceptable, and HDDs are sometimes getting relegated to &#34;cool&#34; storage and batch processing.  (The above describes my experience with extremely large-scale services. For &#34;normal-sized&#34; services, you&#39;re probably fine with an HDD or ten, but if you&#39;ve suddenly decided to build your own private YouTube, you should at least run the numbers, just in case.)</p><p>硬盘的I/O速度也没有明显提高。它&#39；它实际上开始成为一个问题，因为随着HDD变得更大而不是更快，存储空间与I/O带宽的比率变得非常偏向前者，因此大量HDD不会&#39；我们没有足够的总带宽来大规模使用。你可以通过购买大量小型硬盘来解决这个问题，但那&#39；它是如此昂贵（每GB）以至于你&#39；我们最好还是买SSD吧。因此，我们&#39；重新开始看到SSD的使用增加，即使在硬盘寻找时间本来可以接受的领域，硬盘有时会降级到&#34；酷&#34；存储和批处理。（以上描述了我在超大规模服务方面的经验。对于&#34；普通规模的&#34；服务，你&#39；可能可以使用一个或十个硬盘，但如果你突然决定建立自己的私人YouTube，你至少应该运行这些数字，以防万一。）</p><p> I believe &#34;modern storage&#34; in 2022 are NVMe drives capable of 5 million IOPS each.</p><p>我相信&#34；现代化的存储&#34；2022年，每台NVMe驱动器的IOPS均达到500万次。</p><p> For many use-cases thats still just way too expensive (for the moment); and there is plenty of development still happening in HDDs (even tapes). For example, some vendors starting to deploy multi-actuator HDDs (we recently got support for that in Linux), so you can have multiple reads/writes concurrently - obviously that&#39;s still slower than flash.</p><p>对于许多用例来说，这仍然是非常昂贵的（目前）；HDD（甚至磁带）仍有大量发展。例如，一些供应商开始部署多驱动器硬盘（我们最近在Linux中得到了支持），因此您可以同时进行多个读/写操作——显然是&#39；它仍然比flash慢。</p><p> Well so are 100Gb/s NICs and switches so neither one is consumer level driven at the moment.</p><p>100Gb/s的NIC和交换机也是如此，所以目前它们都不是消费者级的。</p><p> Well, admittedly my home system isn&#39;t configured like a lot of them ...  But I reorganised (as in archived last year&#39;s) loads of mailing lists, and it was noticeable that even after Thunderbird came back and said &#34;finished&#34;, that the disk subsystem - a raid-5 array - was desperately struggling to catch up. With 32GB of ram, provided it caches everything fine I&#39;m not worried, but it&#39;s still a little scary knowing there&#39;s loads of stuff in the disk queue flushing as fast as possible ...</p><p>嗯，无可否认，我的家庭系统不是&#39；我不像很多人。。。但我重新组织了（就像去年存档的那样）大量邮件列表，值得注意的是，即使在雷鸟回来说&#34；完成&#34；，磁盘子系统——raid-5阵列——正在拼命追赶。有32GB的ram，只要它能缓存所有优秀的I&#39；我不担心，但它&#39；知道这一点还是有点吓人&#39；尽可能快地刷新磁盘队列中的大量内容。。。</p><p>  This was a problem even when I started in Google in 2007; disks were getting so large that anything like recovery or the likes was getting problematic. So the problem has been there all along, it&#39;s just moving slowly into more “normal” problem domains as the problem gets worse and worse.</p><p>即使在我2007年开始在谷歌工作时，这也是一个问题；磁盘变得如此之大，以至于像恢复之类的东西都成了问题。所以问题一直存在，它&#39；随着问题越来越严重，它只是慢慢地进入更“正常”的问题领域。</p><p>  Posted Feb 15, 2022 17:45 UTC (Tue) by  rgmoore ( ✭ supporter ✭, #75) [ Link]</p><p>2022年2月15日17:45 UTC（星期二）由rgmoore发布（✭ 支持者✭, #[链接]</p><p>  I assume that with something like YouTube, the final solution involves multiple levels of caching with different tradeoffs between latency and cost. With a site that is accessed mostly by browsing, you can get advance notice of what items are likely to be looked at soon based on what&#39;s on the users&#39; screens and can pre-populate the cache accordingly. I&#39;m sure there are engineers whose whole job is to optimize this behavior. It also makes me wonder if some of the dreaded YouTube algorithm isn&#39;t built around trying to steer viewers into stuff that&#39;s popular right now because it&#39;s sure to be available in cache.</p><p>我假设，对于YouTube这样的东西，最终的解决方案涉及多个级别的缓存，在延迟和成本之间进行不同的权衡。对于一个主要通过浏览来访问的网站，你可以根据&#39；关于用户&#39；屏幕，并可以相应地预填充缓存。我&#39；我确信有工程师的全部工作就是优化这种行为。这也让我想知道，一些可怕的YouTube算法是不是&#39；t试图引导观众进入&#39；现在很流行，因为它&#39；它肯定在缓存中可用。</p><p> Speaking with my Google hat on: Saying that the final solution involves &#34;multiple levels of caching&#34; is like saying that a game of Magic: The Gathering involves multiple rules.[1] Beyond that I&#39;m not really at liberty to comment, but I can point you at [2] for the official company line on how the recommendation system works.[3]  [1] See  https://media.wizards.com/2022/downloads/MagicCompRules%2... for the MTG rules. But don&#39;t actually try to read them, because they&#39;re intended to be used for resolving specific issues, not as a &#34;here&#39;s how to play the game&#34; starting point. [2]:  https://blog.youtube/inside-youtube/on-youtubes-recommend... [3]: The linked blog post is Google/YouTube&#39;s official position on the matter, and may not be reflective of my own personal views (which I have no interest in discussing). It&#39;s also from September 2021, so things may have changed since then.</p><p>与我的谷歌帽对话：说最终解决方案涉及&#34；多级缓存&#34；就像说一场魔法游戏：聚会涉及多个规则。[1] 除此之外，我&#39；I’我真的没有评论的自由，但我可以在[2]向您介绍公司关于推荐系统工作原理的官方热线。[3] [1]见https://media.wizards.com/2022/downloads/MagicCompRules%2...对于MTG规则。但是唐&#39；我不会真的去读它们，因为它们&#39；re用于解决特定问题，而不是作为&#34；这里&#39；这就是如何玩这个游戏&#34；起点。[2]:  https://blog.youtube/inside-youtube/on-youtubes-recommend...[3]：链接的博客帖子是Google/YouTube&#39；他在这件事上的官方立场，可能并不反映我个人的观点（我对讨论这个问题没有兴趣）。它&#39；S也从2021年9月开始，所以事情从那时开始可能已经改变了。</p><p>  &gt; Enabling a packet size of 185,000 bytes increased network throughput by nearly 50%  Presumably &#34;throughput&#34; involves getting the BIG TCP packet off the NIC via DMA, probably of the scatter-gather variety for so much data. It&#39;s remarkable that the speed of these transfers is sufficient for a 50% speed-up.</p><p>&gt；实现185000字节的数据包大小可以将网络吞吐量提高近50%&#34；吞吐量#34；涉及通过DMA从NIC上获取大的TCP数据包，对于如此多的数据，可能是分散-聚集类型。它&#39；值得注意的是，这些传输的速度足以使速度提高50%。</p><p>  &gt; Current kernels limit the number of fragments stored in an SKB to 17, which is  &gt; sufficient to store a 64KB packet in single-page chunks. That limit will  &gt; clearly interfere with the creation of larger packets, so the patch set raises  &gt; the maximum number of fragments (to 45). But, as Alexander Duyck pointed out,  &gt; many interface drivers encode assumptions about the maximum number of fragments  &gt; that a packet may be split into. Increasing that limit without fixing the  &gt; drivers could lead to performance regressions or even locked-up hardware, he  &gt; said.  &gt;   &gt; After some discussion, Dumazet proposed working around the problem by adding a  &gt; configuration option controlling the maximum number of allowed fragments for  &gt; any given packet. That is fine for sites that build their own kernels, which  &gt; prospective users of this feature are relatively likely to do. It offers little  &gt; help for distributors, though, who must pick a value for this option for all of  &gt; their users.   Hmm, sounds a lot like what we have in the block layer with &#34;block queue limits&#34;; where each low level driver that implements an interface to the block layer also provides a set of limits for this specific queue, also - among other things - including how the scatter-gather list of this queue must look like so the underlying device can actually deal with it. For example some devices can&#39;t deal with multi-page scatter-elements, while other can; some have a upper limit of scatter-elements in a single list, and so on. This way there doesn&#39;t have to be a single config switch and/or a kernel wide knob.</p><p>&gt；当前内核将存储在SKB中的片段数量限制为17个，即&gt；足以在单页块中存储64KB的数据包。该限制将&gt；显然会干扰更大数据包的创建，因此补丁集会引发&gt；碎片的最大数量（到45）。但是，正如亚历山大·杜伊克指出的那样，&gt；许多接口驱动程序对最大片段数的假设进行编码&gt；一个数据包可以被分成几个部分。在不修正&gt；他说，驱动程序可能会导致性能下降，甚至导致硬件锁定；她说&gt&gt；经过一些讨论后，Dumazet建议通过添加一个&gt；配置选项，用于控制&gt；任何给定的包。对于构建自己内核的网站来说，这很好，因为&gt；这项功能的潜在用户相对来说更可能这样做。它提供的服务很少&gt；不过，对于分销商，他们必须为所有&gt；他们的用户。嗯，听起来很像块层中的&#34；阻塞队列限制&#34；；其中，每个实现块层接口的低级驱动程序也为该特定队列提供了一组限制，除其他外，还包括该队列的分散-聚集列表的外观，以便底层设备能够实际处理它。例如，一些设备可以&#39；t处理多页分散元素，而其他元素可以；有些在单个列表中有分散元素的上限，以此类推。这样就没有&#39；不必是单个配置开关和/或整个内核的旋钮。</p><p>  &gt; software that &#34;knows&#34; that the TCP header can be found immediately after the IP header in a packet.</p><p>&gt；软件&#34；知道&#34；TCP报头可以在数据包的IP报头之后立即找到。</p><p>  I hereby confirm that 64KB packets seemed insanely large to me in the 1980s. ;-)</p><p>我在此确认，在20世纪80年代，64KB的数据包对我来说似乎太大了</p><p>   RFC2675 adds 32bit field for 4GB packets in IPv6, what about IPv4, how can IPv4 ever do more that 16-bit length of packages? I still don&#39;t understand how that can work.</p><p>RFC2675在IPv6中为4GB数据包添加了32位字段，那么IPv4呢，IPv4怎么能处理超过16位长度的数据包呢？我仍然不知道&#39；我不明白这是怎么回事。</p><p> Legacy technology can&#39;t do more than 16 bit packet lengths; you will have to upgrade to IPv6 if you need jumbo packets on the wire.</p><p>传统技术可以&#39；不能做超过16位的数据包长度；如果你需要网络上的巨型数据包，你必须升级到IPv6。</p><p>  One of the problems with implementing changes like this is the use of global settings which control too much for one setting. Take, for example, the problem of increasing MTU. It&#39;s hard to change the MTU because it&#39;s used both to control receive packet size and transmit packet size. Everyone on a LAN segment has to agree on the MTU because switches don&#39;t operate in the ICMP or (for IPv4) fragmentation domain. So, if one&#39;s corporate backbone has evolved to significantly higher bandwidth, but still drops a small fraction of packets, TCP flow control algorithms limit transmission based on latency. Larger MTUs would help, but implementing that requires a flag day in every subnet participating in a conversation. If one could change systems to accept larger MTUs, but not send larger MTUs, a flag day isn&#39;t required, as every system can be configured to accept larger MTUs, but not transmit them. Once every system has been changed, then the router port and endpoint systems can be changed to larger MTUs.  I hope that BIG TCP does something similar, that one can first configure systems to accept BIG TCP before they are sent, which would avoid flag-day requirements to implement it. Except for point-to-point communication, it will be years before every system can operate with BIG TCP. Ideally, it could be implemented on a connection-pair basis rather than switch-controlled Ethernet frame size limitations.</p><p>实现这样的更改的一个问题是使用全局设置，这对一个设置来说控制太多。例如，增加MTU的问题。它&#39；很难改变MTU，因为它&#39；s用于控制接收数据包大小和发送数据包大小。局域网上的每个人都必须同意MTU，因为交换机不&#39；t在ICMP或（对于IPv4）碎片域中运行。那么，如果一个&#39；s公司的主干网已发展到显著更高的带宽，但仍会丢弃一小部分数据包，TCP流量控制算法基于延迟限制传输。更大的MTU会有所帮助，但实现这一点需要在参与对话的每个子网中设立一个卖旗日。如果一个人可以改变系统，接受更大的MTU，但不发送更大的MTU，那么卖旗日就不是&#39；t必需，因为每个系统都可以配置为接受较大的MTU，但不能传输它们。一旦每个系统都被更改，那么路由器端口和端点系统就可以更改为更大的MTU。我希望BIG TCP也能做类似的事情，首先可以将系统配置为在发送之前接受BIG TCP，这样可以避免在卖旗日要求实现它。除了点对点通信，每个系统都需要数年才能使用大型TCP。理想情况下，它可以基于连接对而不是交换机控制的以太网帧大小限制来实现。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/包做/">#包做</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/tcp/">#tcp</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据包/">#数据包</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>