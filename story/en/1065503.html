<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Prometheus，但更大 Prometheus, but Bigger</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Prometheus, but Bigger<br/>Prometheus，但更大 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-13 23:03:18</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/ba9df2c3bcb72841376c52d850211715.png"><img src="http://img2.diglog.com/img/2021/6/ba9df2c3bcb72841376c52d850211715.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Up until January 2021, I have been using an enterprise monitoring solution to monitor Kubernetes clusters, the same one used for APM.</p><p>到2021年1月，我一直在使用企业监控解决方案来监控Kubernetes集群，同一个用于APM的群集。</p><p> It felt natural, the integration with Kubernetes was quite easy, only minor tweaks needed, and APM and Infrastructure metrics could be integrated, really nice and  magical.</p><p> 它感到自然，与Kubernetes的集成很容易，只有需要微小的调整，APM和基础设施指标可以集成，真的很好且神奇。</p><p> But despite the ease of collecting and storing data,   creating alerts using metrics had huge query limitations, we would often end up with alerts that differed from what our dashboards were showing. Not to mention that with 6 clusters, the number of metrics being collected and stored was pretty big, adding a huge cost to our monthly expenses.</p><p> 但尽管易于收集和存储数据，但使用指标创建警报具有巨大的查询限制，我们通常会最终得到与我们仪表板显示的内容不同的警报。更不用说用6个集群，收集和存储的指标数量非常大，增加了我们的月度费用的巨大成本。</p><p> After some considerations, the downsides were bigger than the upsides. It was time to change our monitoring solution!</p><p> 经过一些考虑因素，缺点比上游更大。是时候改变监控解决方案了！</p><p> But, what to use? Grafana was the obvious choice for the visual part, but what could we use for the &#34;backend&#34; that had the resilience and availability we needed?</p><p> 但是，用什么？ Grafana是视觉部分的明显选择，但我们可以为＆＃34使用什么;后端＆＃34;这是我们需要的弹性和可用性吗？</p><p> &#34;Pure&#34; OpenTSDB installations demanded too much work and attention, Standalone Prometheus did not offer replication and I would end up with multiple databases, TimeScaleDB looked nice, but I am no Postgres administrator.</p><p> ＆＃34;纯＆＃34; OpentsDB安装要求太多的工作和关注，独立的Prometheus没有提供复制，我最终会有多个数据库，时间尺度看起来很好，但我不是Postgres管理员。</p><p> After some experimentation with the above solutions, I looked to the CNCF website for more options and found   Thanos! It ticked all the right boxes:  Longtime retention,  Replication,  High Availability,  microservice approach, and  global view for all my clusters using the same database!</p><p> 经过一些关于上述解决方案的实验，我向CNCF网站寻找更多选项和发现！它勾选了所有正确的框：长时间保留，复制，高可用性，微服务方法以及使用同一数据库的所有群集的全局视图！ </p><p>  There is no persistent storage available on the clusters (a choice made to keep everything stateless), so the default Prometheus + Thanos sidecar approach was not an option, the metric storage must reside outside the clusters. Also, binding any Thanos component to a specific set of clusters is not possible, each cluster is isolated from the others, everything has to be monitored from &#34;outside&#34;.</p><p>群集中没有可用的持久存储（一个选择保留无状态的选择），因此默认的Prometheus + Thanos Sidecar方法不是一个选项，必须驻留在群集中的外部。此外，不可能将每个簇绑定到特定集群集中的任何大约组分，每个集群都与其他集群隔离，必须从＆＃34监测所有集群;外部＆＃34;</p><p> With the above said, high availability in mind, and Thanos possibility to run on virtual machines, the final architecture came to this:</p><p> 通过上述说法，高可用性，以及在虚拟机上运行的可能性，最终架构来到这一点：</p><p>  As shown in the diagram, everything is running in a multi-datacenter architecture, where each site has its own set of  Grafana +  Query servers bundled together, its own set of  store servers, and  three receive servers (half of the cluster).</p><p>  如图所示，一切都在多数据中心体系结构中运行，其中每个站点都有自己的一组Grafana +查询服务器，它自己的一组商店服务器和三个接收服务器（群集的一半）。</p><p> There is also an AWS RDS for our Grafana database. It doesn&#39;t need to be a huge database (low cost), and managing MySQL is out of our team&#39;s scope.</p><p> 我们的Grafana数据库也有一个AWS RDS。它不需要成为一个巨大的数据库（低成本），管理MySQL是我们的团队和＃39; S范围。</p><p>  Receive: This component is responsible for the TSDB, it also manages replication between all the servers running receive and TSBD block uploads to S3.</p><p>  接收：此组件负责TSDB，它还管理运行接收的所有服务器和TSBD块上传到S3之间的复制。</p><p>  Store: This component reads S3 for long-term metrics that are no longer stored in the  receive.</p><p>  存储：此组件读取S3，用于不再存储在接收中的长期度量标准。</p><p> Compactor: This component manages data downsampling and compaction for the TSDB blocks stored in S3</p><p> 压缩机：此组件管理数据下采样并对S3中存储的TSDB块进行压缩 </p><p>  All the cluster data ingestion is managed by a dedicated Prometheus POD running inside the clusters. It collects metrics from the Control Plate (API Server, Controller, and Scheduler), from the ETCD clusters, and from PODs inside the clusters that have metrics relevant to the infrastructure and Kubernetes itself (Kube-proxy, Kubelet, Node Exporter, State Metrics, Metrics Server, and other PODs that have the scraping annotation)</p><p>所有群集数据摄取是由群集内部运行的专用Prometheus Pod管理。它从etfd集群中收集来自控制板（API服务器，控制器和调度器）的指标，以及从与基础架构和Kubernetes自身相关的指标的群集内部（Kube-Proxy，Kubote，Node Exporter，状态指标） ，指标服务器和具有刮擦注释的其他豆荚）</p><p> The Prometheus POD then sends the information to one of the receive servers managing the TSDB using the remote storage configuration.</p><p> 然后，Prometheus POD将信息发送到使用远程存储配置管理TSDB的一个接收服务器的信息。</p><p>  All data is sent to a single server and then replicated to the other servers. The DNS address that Prometheus uses is a DNS GSLB that probes each receive server and balances the DNS resolution between the healthy ones, sharing the load across all servers, since the DNS resolution only gives one IP per DNS query.</p><p>  所有数据都被发送到单个服务器，然后复制到其他服务器。 Prometheus使用的DNS地址是DNS GSLB，该DNS GSLB探测每个接收服务器并余额余额在健康的服务器之间，因为DNS分辨率仅为每个DNS查询提供一个IP。</p><p> It is also important to mention that the data must be sent to a single receive instance and let it manage the replication, sending the same metric leads to replication failures and misbehaving.</p><p> 同样重要的是要提到必须将数据发送到单个接收实例并让它管理复制，发送相同的度量标准导致复制失败和行为不端。</p><p> At this level, metrics are also uploaded to the S3 Bucket for long time retention. Receive uploads the blocks every 2 hours (when each TSDB block is closed) and these metrics are available for querying using the  Store component.</p><p> 在此级别，度量标准也将上传到S3存储桶，长时间保留。接收每2小时（当每个TSDB块关闭时）上载块，并且这些度量标准可用于使用商店组件查询。</p><p> Here we can also set the retention for local data. In this case, all local data is kept for 30 days for daily use and troubleshooting, this allows faster querying.</p><p> 在这里，我们还可以设置本地数据的保留。在这种情况下，所有本地数据都保留30天进行日常使用和故障排除，这允许更快查询。</p><p> Data older than 30 days is available exclusively on S3 for up to 1 year for long period evaluations and comparisons.</p><p> 长期评估和比较的S3超过30天的数据可供选择。 </p><p>  With data being collected and stored in the receivers, everything is ready to be queried. This part is also set for multi-datacenter availability.</p><p>通过收集数据并存储在接收器中，一切都已准备好查询。此部分也设置为多数据中心可用性。</p><p> Each server is running both  Grafana and  Query, this makes it easier for us to identify and remove a server from the load balancer if one of them starts to malfunction (or both). In Grafana, the data source is configured as  localhost, so it always uses the local Query for data.</p><p> 每个服务器都在运行Grafana和查询，这使我们更容易从负载均衡器识别和删除服务器，如果其中一个开始故障（或两者）。在Grafana中，数据源配置为localhost，因此它始终使用本地查询进行数据。</p><p> For the query configuration, it has to know all the servers that have metrics stored (Receiver and Store). The query knows which server is online and is able to gather metrics from them.</p><p> 对于查询配置，它必须知道存储的所有服务器（接收器和存储）。查询知道哪个服务器在线，并且能够收集它们的指标。</p><p>  It also manages data deduplication, since it is querying all servers and with replication configured, all metrics have multiple copies. This is can be done using labels assigned to the metrics and a parameter for the query ( --query.replica-label=QUERY.REPLICA-LABEL). With these configured, the query component knows if the metrics gathered from the receivers and stores are duplicated and uses only one data point.</p><p>  它还管理数据重复数据删除，因为它是查询所有服务器和配置复制，所有指标都有多个副本。这可以使用分配给度量标准的标签和查询的参数（--query.replica-label = query.replica-label）来完成。使用这些配置，查询组件知道从接收器和存储中收集的度量是否重复并仅使用一个数据点。</p><p>  As mentioned, data is kept locally for a maximum period of 30 days, everything else is stored on S3. This allows for a reduced amount of space needed on the receivers and reduced costs, as block storage is more expensive than object storage. Not to mention that querying data older than 30 days is not very usual and is mostly used for resource usage history and projections.</p><p>  如上所述，数据在本地保持最长30天，其他一切都存储在S3上。这允许在接收器上减少所需的空间和降低成本，因为块存储比对象存储更昂贵。更不用说询问超过30天的查询数据不是通常的，并且主要用于资源使用历史记录和预测。</p><p> The data stored on the S3 bucket can be queried using the  store component. It is statically configured to only serve data older than 30 days (using the current time as reference) since the  query component sends the queries to all data components (store and receiver).</p><p> 可以使用商店组件查询存储在S3存储桶上的数据。它静态配置为仅服务于30天的数据（使用当前时间作为参考），因为查询组件将查询发送到所有数据组件（存储和接收器）。</p><p>  The store also keeps a local copy of the indexes of each TSDB block stored on the S3 bucket, so if a query needs data older than 30 days, it knows which blocks to download and use for serving the data.</p><p>  该商店还将每个TSDB块的索引的本地副本保存在S3存储桶上，因此如果查询需要数据超过30天，则会知道要下载和用于服务数据的块。 </p><p>  ~7.3 GB of data being ingested each day, or ~ 226.3 GB of data per month;</p><p>每天摄取〜7.3 GB数据，或每月数据约为226.3 GB;</p><p> For the monthly expenses, with most of the components running on-premises, there was a  90.61% cost reduction, going from  US$ 38,421.25 monthly to  US$ 3,608.99, including the AWS services cost.</p><p> 对于每月的费用，大多数组成部分运行本地，降低了90.61％的成本，从每月38,421.25美元到3,608.99美元，包括AWS服务费用。</p><p>  Configuring and setting up everything mentioned above took around a month or so, including testing some other solutions, validating the architecture, implementation, enabling all data collection from our clusters, and creating all dashboards.</p><p>  配置和设置上面提到的所有内容需要一个月左右，包括测试一些其他解决方案，验证架构，实现，从我们的集群中启用所有数据集合，并创建所有仪表板。</p><p> In the first week, the benefits were obvious. Monitoring the clusters became much easier, dashboards could be built and customized in a fast manner, and collecting metrics is almost plug &amp; play, with most applications exporting metrics in the Prometheus format and collection being automatic based on annotations.</p><p> 在第一周，好处是显而易见的。监控集群变得更加容易，仪表板可以以快速的方式构建和定制，并且收集指标几乎是插头＆amp;播放，大多数应用程序以Prometheus格式导出​​指标并基于注释自动。</p><p> Also, using Grafana&#39;s LDAP integration allowed access to different teams in a granular way. Developers and SREs have access to a huge set of dashboards, with relevant metrics about their namespaces, ingresses, and more.</p><p> 此外，使用Grafana＆＃39; LDAP集成允许以粒度的方式访问不同的团队。开发人员和SRE可以访问一组大量的仪表板，其中有关其名称空间，入口等相关指标。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://luizrojo.medium.com/prometheus-but-bigger-b678d8d6b29d">https://luizrojo.medium.com/prometheus-but-bigger-b678d8d6b29d</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>