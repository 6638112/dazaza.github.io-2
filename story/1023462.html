<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>培训LSTM的魔法师学徒指南</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">培训LSTM的魔法师学徒指南</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-09-12 04:43:27</div><div class="page_narrow text-break page_content"><p>那个面目可憎的老魔术师这一次不见了；我现在是主人，我是战术家，他所有的鬼魂都必须听从我的命令。也知道他的咒语、咒语和手势；凭着我的头脑创造的奇迹，我应该这样做。-约翰·沃尔夫冈·冯·歌德(魔法师的学徒)。</p><p>学者们一边仔细研读旧论文，一边钻研他们的电脑，对他们的研究主题有了深入的了解。当他们在想法空间中规划自己的道路时，他们会发展出一种深刻的直觉，哪些技术在实践中效果很好。不幸的是，这些来之不易的见解中有许多没有发表，仍然模糊不清。</p><p>去年，我在奥地利林茨的约翰尼斯·开普勒大学(Johannes Kepler University)上了一门课程，主题是递归神经网络和长短期记忆网络。在那里，赛普·霍克赖特(Sepp Hochreiter)分享了他和他的团队在培训LSTM时使用的一些“魔术”。这篇博文是我一些笔记的积累。</p><p>对于这篇文章，我假设您已经熟悉了LSTM。如果没有，我建议您从Chris Olah对LSTM网络的理解开始，然后继续阅读LSTM的原著(Hochreiter和Schmidhuber，1997)。</p><p>在我们开始之前，我还想强调一些在动机上有相似之处的资源：</p><p>LSTM：搜索空间奥德赛(Greff等人)。2017)来自IDSIA(戴尔摩尔人工智能研究所)。</p><p>如果您想要试验一些介绍的技术，并且需要一个灵活的基于Pytorch的LSTM实现，我推荐Michael Widrich的LSTM工具库。</p><p>所展示的技术的所有功劳都归功于作者。他们陈述中的所有错误都是我的。我总是渴望得到反馈。</p><p>Vanilla LSTM是最流行的变体之一，通常是流行软件库中的默认LSTM架构。它的特点是有三个门和一个存储状态-这些门为模型提供容量，保护存储单元不受信息和噪声的干扰；它们使LSTM的动态高度非线性，并允许它学习执行复杂的操作。</p><p>让我们简要介绍一下Vanilla LSTM的机制，以介绍本文中使用的符号和术语。在给定时间步长流入LSTM细胞的感觉输入\(\boldSymbol{x}(T)\)被转换为细胞输入激活\(\boldSymbol{z}(T)\)-\(\boldSymbol{z}(T)\)的元素由非线性函数\(g(\cdot)\)激活，该函数在实践中通常被定义为双曲正切或tanh。通过按元素将\(\boldSymbol{z}(T)\)乘以S型激活的输入门\(\boldSymbol{i}(T)\)，删除与当前时间步长无关的信息。类似地，使用S型激活的遗忘门\(\boldSymbol{f}(T)\)部分擦除前一时间点\(\boldSymbol{c}(t-1)\)的单元状态。新存储单元状态\(\boldSymbol{c}(T)\)是通过将当前单元状态更新\(\boldSymbol{i}(T)\ODOT\boldSymbol{z}(T)\)添加到过滤后的旧状态\(\boldSymbol{f}(T)\ODOT\boldSymbol{c}(t-1)\)来计算的。最后，LSTM使用存储单元激活函数\(h(\cdot)\)将存储内容压缩到特定的数字范围内，并通过输出门\(\boldSymbol{o}(T)\)过滤结果。这导致最终存储单元状态激活\(\boldSymbol{y}(T)\)。</p><p>\[\BEGIN{ALIGN}\boldSymbol{i}(T)&amp；=\sigma\Left(\boldSymbol{W}_{i}^{\top}\boldSymbol{x}(T)+\boldSymbol{R}_{i}^{\top}\boldSymbol{y}(t-1)\右)\boldSymbol{o}(T)&amp；=\sigma\Left(\boldbol{W}_{o}^{\top}\boldbol{x}(T)+\boldbol{R}_{o}^{\top}\boldbol{y}(t-1)\right)\boldbol{f}(T)&amp；=\sigma\Left(\boldSymbol{W}_{f}^{\top}\boldSymbol{x}(T)+\boldSymbol{R}_{f}^{\top}\boldSymbol{y}(t-1)\right)\boldSymbol{z}(T)&amp；=g\Left(\boldSymbol{W}_{z}^{\top}\boldSymbol{x}(T)+\boldSymbol{R}_{z}^{\top}\boldSymbol{y}(t-1)\right)\boldSymbol{c}(T)&amp；=\boldSymbol{f}(T)\ODOT\boldSymbol{c}(t-1)+\boldSymbol{i}(T)\ODOT\boldSymbol{c}(t-1)+\boldSymbol{i}(T)\ODOT\boldSymbol{c}(t-1)+\boldSymbol{i}(T)。=\boldSymbol{o}(T)\odot h(\boldSymbol{c}(T))\end{align}\]。</p><p>其中\(\sigma\)表示Sigmoid激活函数，\(\ODOT\)表示元素或Hadamard乘积。请注意，每个门都可以访问当前输入\(\boldSymbol{x}(T)\)和先前的单元状态激活\(\boldSymbol{y}(t-1)\)。</p><p>还要记住，权重\(W\)和重复权\(R\)在时间步长之间是共享的。</p><p>在实践中，通常选择输入激活函数\(g\)为tanh。但这一选择并不明显，事实上，在最初的LSTM文件中，Sigmoid被用来激活\(\boldSymbol{z}\)。由于存储单元的目的是随时间学习和记忆模式，因此S形激活是指示输入中实体的存在(激活值接近\(1\))或不存在(激活值接近\(0\))的自然选择。另一方面，Tanh的下限为\(-1\)似乎没有直观的意义。什么是否定模式？值为\(-1\)的激活是否表示输入中不存在某些内容？</p><p>采用TANH首先需要两次精神上的转变。首先，Tanh在元学习环境中具有直觉意义。我们现在用记忆单元来存储另一个神经网络的权重，而不是模式。要指示权重值是应该增加还是应该减少，我们需要正值和负值。</p><p>第二个直观的解释是暗示的存储。在此上下文中的暗示是支持或反对某事的证据。考虑一个来自文本分析的例子。假设一个模型在一个段落中遇到单词“团队”、“球员”和“目标”。这些都是强烈暗示正文是关于体育的，但如果下一段包括“经理人”、“收入”和“股东”等词，就是一个强烈的迹象，表明该段实际上描述了一个商业背景。正值可以被视为有利于某些类别的暗示，负值可以被视为不利于某些类别的暗示。</p><p>但是，有一个简单的数学原因，为什么tanh是激活单元格输入的首选。与常用的激活函数(如RELU和Sigmoid)相比，单元输入激活的期望值对于tanh为零(在零均值高斯预激活的假设下)：</p><p>要了解为什么需要此属性，请记住单元格状态是如何在给定时间步骤1更新的：</p><p>在每个时间步，我们将单元格输入激活\(\boldSymbol{z}(T)\)(由输入门过滤)添加到前一个单元格状态\(\boldSymbol{c}(t-1)\)。如果我们为\(\boldSymbol{z}\)选择一个激活函数，使得每个激活都有一个值\(\geq 0\)(例如，Sigmoid、RELU等)，则\(\boldSymbol{c}\)将很快具有非常大的值。即使激活相对较小，对于足够长的序列，细胞状态也会变大。这个问题被称为漂移效应。</p><p>但是，大的记忆细胞状态怎么会成为学习的障碍呢？要回答这个问题，我们需要看看LSTM的回传：</p><p>\[\BEGIN{ALIGNED}\FRAC{\PARTIAL L}{\PARTIAL\boldSymbol{c}(T)}}&amp；=\frac{\Partial L}{\Partial\boldSymbol{y}(T)}\color{#9900ff}{\FRAC{\Partial\boldSymbol{y}(T)}{\Partial\boldSymbol{c}(T)}}+\frac{\Partial L}{\Partial\boldSymbol{c}(t+1)}\FRAC{\Partial\boldSymbol{c}(t+1)}{\Partial\boldSymbol{c}(t。=\frac{\Partial L}{\Partial\boldSymbol{y}(T)}\OperatorName{diag}\Left(\boldSymbol{o}(T)\odot\color{#9900ff}{h^{\prime}(\boldsymbol{c}(t))}\right)+\frac{\partial L}{\Partial\boldSymbol{c}(t+1)}\End{Alignment}\]。</p><p>该方程递归地汇总来自未来的所有误差信号，并在时间上向后携带它们。直观地描述了时间t的细胞状态对损耗L的不同影响方式。我们仔细看一下突出显示的术语\(\Partial\boldSymbol{y}(T)/\Partial\boldSymbol{c}(T)\)，它描述了存储单元状态激活\(\boldSymbol{y}(T)\)如何随着\(\boldSymbol{c}(T)\)的改变而改变。可以通过计算\(\\运算符名{diag}\Left(\boldSymbol{o}(T)\odoth^{\Prime}(\boldSymbol{c}(T))\Right)\)来获得偏导数。现在我们有麻烦了。请记住，我们使用tanh作为存储单元激活函数\(h\)，以将存储单元状态压缩到数值范围\((-1，1)\)。其导数定义如下：</p><p>现在，如果由于漂移效应\(\boldSymbol{c}(T)\)变得非常大，则对于\(\boldSymbol{c}(T)\)中的每个元素，第二个等式中突出显示的项将计算为\(h^{\Prime}(\boldSymbol{c}(T))=1-1=0\)。结果，\(FRAC{\PARTIAL L}{\PARTIAL y(T)}\FRAC{\PARTIAL Y(T)}{\PARTIAL C(T)}\)将为零，并且单元状态失去其影响存储单元状态激活\(\boldSymbol{y}(T)\)的能力，并因此损失\(L\)。</p><p>我们可以看到，选择tanh作为我们的输入激活函数\(g\)要优于其他常用的函数，因为它消除了漂移效应。由于其范围集中在零附近，因此可以防止单元状态随时间累积。这反过来又稳定了学习信号，这是学习长期依赖的关键。</p><p>为了防止细胞状态积累，Felix Gehrs和Jürgen Schmidhuber提出了遗忘门(Gers，Schmidhuber和Cummins，1999)。其想法是允许网络在每次更新之前学习擦除小区状态。</p><p>然而，遗忘门的主要问题是它可能会重新引入消失的梯度问题-矛盾的是，LSTM架构的构建首先就是为了消除这个问题。让我们来看看忘记门是如何影响渐变的：</p><p>如果没有遗忘门，梯度等于单位矩阵。消除了渐变消失问题：</p><p>尽管遗忘门的使用可能有争议，但在训练序列很短的情况下，忘记门通常工作得非常好。为了减少有问题的梯度的影响，建议用较大的正值初始化遗忘门的偏置单元。这使得栅极最初是打开的(激活接近1)，并将梯度推向单位矩阵。</p><p>遗忘门的一种常见变体是将其绑定到输入门。旧信息从小区状态被擦除到与允许新信息流入的程度相同的程度。结果是简化了架构，减少了可训练参数2。小区状态更新如下：</p><p>聚焦LSTM是一种简化的LSTM变体，没有忘记门。它的主要动机是将单元输入激活\(\boldSymbol{z}(T)\)和门之间的关注点分离。在Vanilla LSTM中，\(\boldSymbol{z}\)和门都取决于当前外部输入\(\boldSymbol{x}(T)\)和先前的存储单元状态激活\(\boldSymbol{y}(t-1)\)。这可能会导致\(\boldSymbol{i}\)和\(\boldSymbol{z}\)之间的冗余。对于要被激活的\(\boldSymbol{i}(T)\odt\boldSymbol{z}(T)\)，\(\boldSymbol{i}\)和\(\boldSymbol{z}\)都需要激活，并且因为两者都访问相同的信息，所以它们通常学习相似的权重(即，当它们检测到相同的图案时)，或者它们中的任何一个收敛到常数值3。</p><p>对于聚焦的LSTM，\(\boldSymbol{z}\)仅看到当前输入\(\boldSymbol{x}(T)\)，而门的决定完全基于先前的存储单元状态激活\(\boldSymbol{y}(t-1)\)。首先，\(\boldSymbol{z}\)检测输入序列中的模式，然后输入门根据内部状态缩放\(\boldSymbol{z}\)。</p><p>这种信息访问划分比普通的LSTM有一些优势。首先，将可训练参数的个数减少到\(2i^{2}+i_D\)，没有偏置单位-其中\(D\)和\(i\)分别是输入维数和隐维数。其次，新信息进入存储单元的唯一方式现在是通过\(\boldSymbol{z}\)。这使得对LSTM的动态进行分析和推理变得容易得多，因此有助于提高可解释性。</p><p>我们继续拆除香草LSTM的机器。通过从聚焦的LSTM中移除输出门，我们得到了轻量级LSTM体系结构：</p><p>我们为什么要这么做呢？同样，我们用网络的自由度换取了更少的参数、更高的速度和可解释性。另一个原因是冗余。在Vanilla/Focked LSTM中，信息首先由当前单元的输出门过滤，然后通过管道输送到下一个单元的输入门。我们可以通过允许输入门访问未过滤的信息并丢弃输出门来简化这两步过程。输出门的另一个可能的缺点是它可能导致振荡-输出门在交替的时间步骤4打开和关闭。</p><p>想象一下读一本书。我们不是线性阅读的。我们跳来跳去，读完一段难懂的文章，偶尔会停下来沉思。Recurn Networks不能做到这一点。他们没有一种内部机制允许他们暂停并仔细考虑，例如，序列中的某个标记。RNN在每个时间步长花费相等的计算量。</p><p>用算法模拟思考的一种简单方法是在给定的时间步重复向网络馈送先前的输入或中性元素(例如，零张量)-我们让输入自动收报机特定次数。这些额外的自动收报机或思考步骤背后的直觉是提高网络的抽象级别。递归神经网络可以被视为前馈神经网络，其中层数与输入序列的长度相同，并且各层之间共享权重。通过重复某些元素，我们可以创建更深、更具表现力的网络，其深度超出了输入序列自然定义的深度。</p><p>然而，这个概念带来了一些问题。网络是否应该在这些深思熟虑的步骤中发出输出？我们应该让输入自动收报机走几步？应该重复哪些时间步骤？</p><p>Alex Graves(Graves 2016)的自适应计算时间算法解决了这些问题。所呈现的机制允许RNN了解在接收输入和发出输出之间要采取多少考虑步骤：</p><p>在每个时间步长，RNN使用以下方程式集计算中间状态序列\(\Left(s_{t}^{1}，\ldots，s_{t}^{N(T)}\right)\)和中间输出序列\(\Left(y_{t}^{1}，\ldots，y_{t}^{N(T)}\right)\)：</p><p>\[\begin{array}{l}s_{t}^{n}=\left\{\begin{array}{l}\mathcal{S}\left(s_{t-1}，x_{t}^{1}\Right)\Text{if}n=1\数学{S}\Left(s_{t}^{n-1}，x_{t}^{n}\Right)\Text{否则}\End{数组}\Right。\\y_{t}^{n}=W_{y}s_{t}^{n}+b_{y}\end{array}\]</p><p>但是，网络如何确定在任何给定的时间步长应该采取多少中间报价器步骤呢？为此，作者引入了由中间态\(s_{t}^{n}\)导出的附加S型停机单元：</p><p>在每个中间步骤之后，我们检查直到当前中间步骤的所有先前停止单元的总和是否大于或等于\(1-\ε\)，其中\(\ε\)是确保计算也可以在单个中间步骤之后停止的小的正常数，例如，ε\(0.1\)。在每个中间步骤之后，我们检查所有先前停止单元的总和是否大于或等于\(1-\ε\)，其中\(\ε\)是确保计算也可以在单个中间步骤之后停止的小的正常数。如果满足条件，则停止计算；如果不满足，则再添加一个中间步骤，并重复该过程。</p><p>一旦确定了中间步数\(N(T)\)，我们就可以产生当前时间步长的最终状态\(s_t\)和最终输出\(y_t\)。为此，我们首先使用停止单位来确定中间时间点的停止概率\(p_{t}^{n}\)：</p><p>最后，我们分别计算了停机概率与中间状态和输出之间的内积的状态和输出在\(t\)处的值：</p><p>我们现在已经看到了如何增强任何递归神经网络，以便它能够学习将计算资源自适应地分配到序列中的不同时间步，或者换句话说，是暂停和思考。</p><p>我们遇到的一个常见问题是，对于长序列，存储单元状态可能会变得非常大。我们已经看到了如何使用tanh作为输入激活函数，通过将预期中的内存增量集中在零附近来消除这种漂移效应。</p><p>另一种补充方法是使用负偏置单元初始化输入门-以及可选的单元输入激活\(\boldSymbol{z}\)。因此，门对大多数输入进行强过滤，网络应该学会只有在检测到相关模式时才打开门。我们应该为偏移单位选择哪些值？这是对记忆单元的小输入和足够大的梯度以稳定学习之间的权衡。固体偏置初始化在\((-1，-10)\)范围内，但根据任务的不同，该值可能会有所不同。</p><p>另一种有趣的偏置初始化策略是用负值递减序列初始化输入和输出门的偏置单元；例如，\(1-i)/2)_{i=1}^{i}\)，其中\(i\)枚举存储单元。此方案背后的直觉是，即使是非常简单的任务，LSTM通常也会使用其全部容量。这可能会在学习的后期阶段导致困难，因为网络的大部分内存容量已经因为冗余地存储简单的模式而耗尽。使用负级联来初始化偏置引入了存储单元之间的排序，并鼓励该模型用尽可能少的单位来解决该任务。这是因为具有较高(即较少负)偏置单元的存储单元单元在反向传递期间携带较强的学习信号。随着学习的进行，时间反向传播将一个接一个地“打开”剩余的单元。</p><p>该方案已成功应用于Sepp Hochreiter等人的同源性检测工作中。Al(Hochreiter，Heusel和Obermayer，2007)。</p><p>挤压函数\(g\)和\(h\)是促成LSTM单元内数值稳定环境的两个重要机制-将输入和单元状态都保持在固定的数值范围内。在输入序列中只有几个元素与任务相关的情况下，使用标量\(\alpha&gt；1\)(通常设置为\(4\))放大激活会很有帮助。</p><p>这是为什么？想象一下LSTM扫描一个输入序列。它检测一个模式，用\(g\)重新调整它的比例，通过将其与输入门相乘来过滤结果激活，然后将其存储。到目前为止，指示存在图案的信号趋于衰减。然后，当我们使用\(h\)重新调整存储单元状态时，信号可能不够强，无法激活单元输出。结果，存储的图案失去了影响其他单元激活的能力。当栅极打开时，定标\(g\)提供更强的信号-该信号强到足以使存储单元激活函数\(h\)有所不同。</p><p>缩放\(h\)具有类似的效果，并且可用于使存储单元状态中的微小变化可识别。</p><p>这里要说明的是，存储模式携带的信号应该被放大到足以使单元输出有所不同。</p><p>使用\(g\)的恒等式可以加快学习过程，因为激活函数的导数不会缩放学习信号。当然，主要缺点是单元状态增量可能很大，并且存储单元可能开始漂移。</p><p>线性存储单元状态激活函数\(h\)在存储单元用于计数或累积信息的情况下特别有用。</p><p>.</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.niklasschmidinger.com/posts/2020-09-09-lstm-tricks/">https://www.niklasschmidinger.com/posts/2020-09-09-lstm-tricks/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/lstm/">#lstm</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>