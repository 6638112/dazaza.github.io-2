<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>容器联网很简单（2020） Container networking is simple (2020)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Container networking is simple (2020)<br/>容器联网很简单（2020） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-20 00:16:29</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/1/feac9d6ff480a2766e92f8183b1499f8.png"><img src="http://img2.diglog.com/img/2021/1/feac9d6ff480a2766e92f8183b1499f8.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Working with containers always feels like magic. In a good way for those who understand the internals and in a terrifying - for those who don&#39;t. Luckily, we&#39;ve been looking under the hood of the containerization technology for quite some time already and even managed to uncover that  containers are just isolated and restricted Linux processes, that  images aren&#39;t really needed to run containers, and on the contrary -  to build an image we need to run some containers.</p><p>使用容器总是感觉很神奇。对于那些了解内部原理和恐怖的人来说，这是一个好方法-对于那些不了解这些信息的人。幸运的是，我们已经在容器化技术的框架下寻找了很长一段时间，甚至设法发现容器只是隔离和受限制的Linux进程，运行容器并不需要真正的映像，并且相反，要生成映像，我们需要运行一些容器。</p><p> Now comes a time to tackle the container networking problem. Or, more precisely, a single-host container networking problem. In this article, we are going to answer the following questions:</p><p> 现在是时候解决容器联网问题了。或更确切地说，是单主机容器网络问题。在本文中，我们将回答以下问题：</p><p> How to virtualize network resources to make containers think each of them has a dedicated network stack?</p><p> 如何虚拟化网络资源以使容器认为每个容器都有专用的网络堆栈？</p><p> How to turn containers into friendly neighbors, prevent them from interfering, and teach to communicate well?</p><p> 如何将容器变成友好的邻居，防止它们相互干扰，并教好沟通？</p><p>  How to reach containers running on a machine from the outside world ( aka port publishing)?</p><p>  如何从外部访问运行在计算机上的容器（又名端口发布）？</p><p> As a result, it&#39;ll become apparent that the single-host container networking is nothing more than a simple combination of the well-known Linux facilities:</p><p> 结果，很明显，单主机容器网络不过是众所周知的Linux设施的简单组合：</p><p>  And for better or worse, no code is required to make the networking magic happen...</p><p>  不管是好是坏，不需要任何代码即可实现网络魔术…… </p><p>  Any decent Linux distribution would probably suffice. All the examples in the article have been made on a fresh  vagrant CentOS 8 virtual machine:</p><p>任何体面的Linux发行版可能就足够了。本文中的所有示例都是在新的无所事事的CentOS 8虚拟机上制作的：</p><p>  For the sake of simplicity of the examples, in this article, we are not going to rely on any fully-fledged containerization solution (e.g.  docker or  podman). Instead, we&#39;ll focus on the basic concepts and use the bare minimum tooling to achieve our learning goals.</p><p>  为了简化示例，在本文中，我们将不依赖任何成熟的容器化解决方案（例如docker或podman）。相反，我们将专注于基本概念，并使用最少的工具来实现我们的学习目标。</p><p>  What constitutes a Linux network stack? Well, obviously, the set of network devices. What else? Probably, the set of routing rules. And not to forget, the set of netfilter hooks, including defined by iptables rules.</p><p>  什么是Linux网络堆栈？好吧，显然，这是网络设备的集合。还有什么？可能是路由规则集。别忘了，netfilter钩子集（包括iptables规则定义的）。</p><p>  #!/usr/bin/env bashecho &#34;&gt; Network devices&#34;ip linkecho -e &#34;\n&gt; Route table&#34;ip routeecho -e &#34;\n&gt; Iptables rules&#34;iptables --list-rules</p><p>  ＃！/ usr / bin / env bashecho＆＃34;＆gt;网络设备ip linkecho -e＆n>路由表＆＃34; ip routeecho -e＆＃34; \ n＆gt; iptables规则＆iptables --list-rules</p><p>    $ sudo ./inspect-net-stack.sh&gt; Network devices1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 52:54:00:e3:27:77 brd ff:ff:ff:ff:ff:ff&gt; Route tabledefault via 10.0.2.2 dev eth0 proto dhcp metric 10010.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100&gt; Iptables rules-P INPUT ACCEPT-P FORWARD ACCEPT-P OUTPUT ACCEPT-N ROOT_NS</p><p>    $ sudo ./inspect-net-stack.sh&gt;网络设备1：lo：＆lt; LOOPBACK，UP，LOWER_UP＆gt; mtu 65536 qdisc noqueue状态未知模式默认组默认qlen 1000链接/环回00：00：00：00：00：00 brd 00：00：00：00：00：002：eth0：＆lt; BROADCAST，MULTICAST，UP，LOWER_UP＆gt ; mtu 1500 qdisc fq_codel状态UP模式默认组默认qlen 1000链接/以太52：54：00：e3：27：77 brd ff：ff：ff：ff：ff：ff：ff＆gt;通过10.0.2.2 dev eth0 proto dhcp metric 10010.0.2.0/24 dev eth0 proto内核作用域链接src 10.0.2.15 metric 100的路由表默认值> iptables规则-P INPUT ACCEPT -P FORWARD ACCEPT -P OUTPUT ACCEPT -N ROOT_NS</p><p> We are interested in that output because we want to make sure that each of the containers we are going to create soon will get a separate network stack.</p><p> 我们对该输出感兴趣，因为我们想确保即将创建的每个容器都将获得一个单独的网络堆栈。</p><p> Well, you might have heard already, that one of the Linux namespaces used for containers isolation is called  network namespace. From   man ip-netns,  &#34;network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.&#34; For the sake of simplicity, this is the only namespace we&#39;re going to use in this article. Instead of creating fully-isolated containers, we&#39;d rather restrict the scope to only the network stack.</p><p> 好吧，您可能已经听说过，用于容器隔离的Linux名称空间之一称为网络名称空间。从man ip-netns来看，＆＃34;网络命名空间在逻辑上是网络堆栈的另一个副本，具有自己的路由，防火墙规则和网络设备。为了简单起见，这是我们将在本文中使用的唯一名称空间。我们没有创建完全隔离的容器，而是将范围限制为仅网络堆栈。 </p><p> One of the ways to create a network namespace is the  ip tool - part of the de facto standard  iproute2 collection:</p><p>ip工具是创建网络名称空间的一种方法，它是事实上的标准iproute2集合的一部分：</p><p>  How to start using the just created namespace? There is a lovely Linux command called  nsenter. It enters one or more of the specified namespaces and then executes the given program:</p><p>  如何开始使用刚创建的名称空间？有一个可爱的Linux命令，称为nsenter。它输入一个或多个指定的名称空间，然后执行给定的程序：</p><p> $ sudo nsenter --net=/var/run/netns/netns0 bash# The newly created bash process lives in netns0$ sudo ./inspect-net-stack.sh&gt; Network devices1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00&gt; Route table&gt; Iptables rules-P INPUT ACCEPT-P FORWARD ACCEPT-P OUTPUT ACCEPT</p><p> $ sudo nsenter --net = / var / run / netns / netns0 bash＃新创建的bash进程位于netns0 $ sudo ./inspect-net-stack.sh&gt;网络设备1：lo：＆lt; LOOPBACK＆gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link / loopback 00：00：00：00：00：00 brd 00：00：00：00：00：00＆gt;路由表＆gt; iptables规则-P输入接受-P转发接受-P输出接受</p><p> From the output above it&#39;s clear that the  bash process running inside  netns0 namespace sees a totally different network stack. There is no routing rules at all, no custom iptables chain, and only one loopback network device. So far, so good...</p><p> 从上面的输出中可以明显看出，在netns0名称空间内运行的bash进程看到了完全不同的网络堆栈。根本没有路由规则，没有自定义iptables链，只有一个环回网络设备。到现在为止还挺好...</p><p>   A dedicated network stack would be not so useful if we could not communicate with it. Luckily, Linux provides a suitable facility for that - a virtual Ethernet device! From   man veth,  &#34;veth devices are virtual Ethernet devices. They can act as tunnels between network namespaces to create a bridge to a physical network device in another namespace, but can also be used as standalone network devices.&#34;</p><p>   如果我们无法与专用网络堆栈通信，它就不会那么有用。幸运的是，Linux为此提供了合适的工具-虚拟以太网设备！从总体上讲，第九个设备是虚拟以太网设备。它们可以充当网络名称空间之间的隧道，以创建到另一个名称空间中的物理网络设备的桥，但是也可以用作独立的网络设备。</p><p> Virtual Ethernet devices always go in pairs. No worries, it&#39;ll be clear when we take a look at the creation command:</p><p> 虚拟以太网设备始终成对使用。不用担心，当我们看一下创建命令时，它将很清楚：</p><p>  With this single command, we just created a pair of  interconnected virtual Ethernet devices. The names  veth0 and  ceth0 have been chosen arbitrarily:</p><p>  使用此命令，我们仅创建了一对互连的虚拟以太网设备。名称veth0和ceth0是任意选择的： </p><p> $ ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 52:54:00:e3:27:77 brd ff:ff:ff:ff:ff:ff5: ceth0@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 66:2d:24:e3:49:3f brd ff:ff:ff:ff:ff:ff6: veth0@ceth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 96:e8:de:1d:22:e0 brd ff:ff:ff:ff:ff:ff</p><p>$ ip link1：lo：＆lt; LOOPBACK，UP，LOWER_UP＆gt; mtu 65536 qdisc noqueue状态未知模式默认组默认qlen 1000链接/环回00：00：00：00：00：00 brd 00：00：00：00：00：002：eth0：＆lt; BROADCAST，MULTICAST，UP，LOWER_UP＆gt ; mtu 1500 qdisc fq_codel状态UP模式默认组默认qlen 1000 link / ether 52：54：00：e3：27：77 brd ff：ff：ff：ff：ff：ff：ff5：ceth0 @ veth0：＆lt; BROADCAST，MULTICAST，M -DOWN＆gt; mtu 1500 qdisc noop状态DOWN模式默认组默认qlen 1000 link / ether 66：2d：24：e3：49：3f brd ff：ff：ff：ff：ff：ff：ff6：veth0 @ ceth0：＆lt; BROADCAST，MULTICAST，M -DOWN＆gt; mtu 1500 qdisc noop状态DOWN模式默认组默认qlen 1000 link / ether 96：e8：de：1d：22：e0 brd ff：ff：ff：ff：ff：ff：ff</p><p> Both  veth0 and  ceth0 after creation resides on the host&#39;s network stack (also called root network namespace). To connect the root namespace with the  netns0 namespace, we need to keep one of the devices in the root namespace and move another one into the  netns0:</p><p> 创建后的veth0和ceth0都位于主机的网络堆栈（也称为根网络名称空间）上。要将根名称空间与netns0名称空间连接，我们需要将其中一台设备保留在根名称空间中，并将另一台设备移至netns0中：</p><p> $ sudo ip link set ceth0 netns netns0# List all the devices to make sure one of them disappeared from the root stack$ ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:002: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 52:54:00:e3:27:77 brd ff:ff:ff:ff:ff:ff6: veth0@if5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 96:e8:de:1d:22:e0 brd ff:ff:ff:ff:ff:ff link-netns netns0</p><p> $ sudo ip link set ceth0 netns netns0＃列出所有设备，以确保其中之一从根堆栈中消失。$ ip link1：lo：＆lt; LOOPBACK，UP，LOWER_UP＆gt; mtu 65536 qdisc noqueue状态未知模式默认组默认qlen 1000链接/环回00：00：00：00：00：00 brd 00：00：00：00：00：002：eth0：＆lt; BROADCAST，MULTICAST，UP，LOWER_UP＆gt ; mtu 1500 qdisc fq_codel状态UP模式默认组默认qlen 1000链接/以太52：54：00：e3：27：77 brd ff：ff：ff：ff：ff：ff6：veth0 @ if5：＆lt; BROADCAST，MULTICAST＆gt; mtu 1500 qdisc noop状态DOWN模式默认组默认qlen 1000 link / ether 96：e8：de：1d：22：e0 brd ff：ff：ff：ff：ff：ff：ff link-netns netns0</p><p> Once we turn the devices on and assign proper IP addresses, any packet occurring on one of the devices will immediately pop up on its peer device connecting two namespaces. Let&#39;s start from the root namespace:</p><p> 一旦打开设备并分配了正确的IP地址，其中一个设备上发生的任何数据包都会立即在连接两个名称空间的对等设备上弹出。让我们从根名称空间开始：</p><p>   $ sudo nsenter --net=/var/run/netns/netns0$ ip link set lo up # whoops$ ip link set ceth0 up$ ip addr add 172.18.0.10/16 dev ceth0$ ip link1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:005: ceth0@if6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 66:2d:24:e3:49:3f brd ff:ff:ff:ff:ff:ff link-netnsid 0</p><p>   $ sudo nsenter --net = / var / run / netns / netns0 $ ip link set lo up＃哎呀$ ip link set ceth0 up $ ip addr add 172.18.0.10/16 dev ceth0 $ ip link1：lo：＆lt; LOOPBACK， UP，LOWER_UP＆gt; mtu 65536 qdisc noqueue状态未知模式默认组默认qlen 1000链接/回送00：00：00：00：00：00 brd 00：00：00：00：00：005：ceth0 @ if6：＆lt; BROADCAST，MULTICAST，UP ，LOWER_UP＆gt; mtu 1500 qdisc无队列状态UP模式默认组默认qlen 1000 link / ether 66：2d：24：e3：49：3f brd ff：ff：ff：ff：ff：ff：ff link-netnsid 0</p><p>   # From `netns0`, ping root&#39;s veth0$ ping -c 2 172.18.0.11PING 172.18.0.11 (172.18.0.11) 56(84) bytes of data.64 bytes from 172.18.0.11: icmp_seq=1 ttl=64 time=0.038 ms64 bytes from 172.18.0.11: icmp_seq=2 ttl=64 time=0.040 ms--- 172.18.0.11 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 58msrtt min/avg/max/mdev = 0.038/0.039/0.040/0.001 ms# Leave `netns0`$ exit# From root namespace, ping ceth0$ ping -c 2 172.18.0.10PING 172.18.0.10 (172.18.0.10) 56(84) bytes of data.64 bytes from 172.18.0.10: icmp_seq=1 ttl=64 time=0.073 ms64 bytes from 172.18.0.10: icmp_seq=2 ttl=64 time=0.046 ms--- 172.18.0.10 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 3msrtt min/avg/max/mdev = 0.046/0.059/0.073/0.015 ms</p><p>   ＃从`netns0`，ping root veth0 $ ping -c 2 172.18.0.11PING 172.18.0.11（172.18.0.11）56（84）字节数据。来自172.18.0.11的64字节：icmp_seq = 1 ttl = 64个时间= 0.038毫秒从172.18.0.11起的64个字节：icmp_seq = 2 ttl = 64个时间= 0.040毫秒--- 172.18.0.11 ping统计信息--- 2传输数据包，2接收到，0％数据包丢失，时间58msrtt分钟/平均/ max / mdev = 0.038 / 0.039 / 0.040 / 0.001 ms＃离开`netns0` $ exit＃从根名称空间ping ceth0 $ ping -c 2 172.18.0.10PING 172.18.0.10（172.18.0.10）56（84）字节数据来自172.18.0.10的.64字节：icmp_seq = 1 ttl = 64时间= 0.073毫秒ms来自172.18.0.10的64字节：icmp_seq = 2 ttl = 64时间= 0.046 ms --- 172.18.0.10 ping统计信息--- 2传输的数据包，2收到，0％封包丢失，时间3msrtt最小/平均/最大/ mdev = 0.046 / 0.059 / 0.073 / 0.015 ms</p><p> At the same time, if we try to reach any other addresses from the  netns0 namespace, we are not going to succeed:</p><p> 同时，如果我们尝试从netns0命名空间访问任何其他地址，我们将不会成功： </p><p> # Inside root namespace$ ip addr show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 52:54:00:e3:27:77 brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0 valid_lft 84057sec preferred_lft 84057sec inet6 fe80::5054:ff:fee3:2777/64 scope link valid_lft forever preferred_lft forever# Remember this 10.0.2.15$ sudo nsenter --net=/var/run/netns/netns0# Try host&#39;s eth0$ ping 10.0.2.15connect: Network is unreachable# Try something from the Internet$ ping 8.8.8.8connect: Network is unreachable</p><p>＃在根名称空间中$ ip addr show dev eth02：eth0：＆lt; BROADCAST，MULTICAST，UP，LOWER_UP＆gt; mtu 1500 qdisc fq_codel状态UP组默认qlen 1000 link / ether 52：54：00：e3：27：77 brd ff：ff：ff：ff：ff：ff：ff inet 10.0.2.15/24 brd 10.0.2.255作用域全局动态noprefixroute eth0 valid_lft 84057sec preferred_lft 84057sec inet6 fe80 :: 5054 ; s eth0 $ ping 10.0.2.15connect：网络不可达＃尝试从Internet $ ping 8.8.8.8connect：网络不可达</p><p> That&#39;s easy to explain, though. There is simply no route in the  netns0 routing table for such packets. The only entry there shows how to reach  172.18.0.0/16 network:</p><p> 不过，这很容易解释。 netns0路由表中根本没有此类数据包的路由。那里唯一的条目显示了如何到达172.18.0.0/16网络：</p><p>  Linux has a bunch of ways to populate the routing table. One of them is to extract routes from the directly attached network interfaces. Remember, the routing table in  netns0 was empty right after the namespace creation. But then we added the  ceth0 device there and assigned it an IP address  172.18.0.10/16. Since we were using not a simple IP address, but a combination of the address and the netmask, the network stack managed to extract the routing information from it. Every packet destined to  172.18.0.0/16 network will be sent through  ceth0 device. But any other packets will be discarded. Similarly, there is a new route in the root namespace:</p><p>  Linux有很多填充路由表的方法。其中之一是从直接连接的网络接口提取路由。记住，创建名称空间后，netns0中的路由表为空。但是随后我们在此处添加了ceth0设备，并为其分配了IP地址172.18.0.10/16。由于我们使用的不是简单的IP地址，而是地址和网络掩码的组合，因此网络堆栈设法从中提取路由信息。每个发往172.18.0.0/16网络的数据包都将通过ceth0设备发送。但是其他任何数据包将被丢弃。同样，根名称空间中有一条新路由：</p><p> # From `root` namespace:$ ip route# ... omited lines ...172.18.0.0/16 dev veth0 proto kernel scope link src 172.18.0.11</p><p> ＃从`root`名称空间开始：$ ip route＃...省略的行... 172.18.0.0/16 dev veth0 proto内核作用域链接src 172.18.0.11</p><p> At this point, we are ready to mark our very first question answered.  We know now how to isolate, virtualize, and connect Linux network stacks.</p><p> 至此，我们准备标记我们的第一个问题。现在我们知道如何隔离，虚拟化和连接Linux网络堆栈。</p><p>  The whole idea of containerization boils down to efficient resource sharing. I.e. it&#39;s uncommon to have a single container per machine. Instead, the goal is to run as many isolated processes in the shared environment as possible. So, what&#39;d happen if we were to place multiple containers on the same host following the  veth approach from above? Let&#39;s add the second  container:</p><p>  容器化的整个思想归结为有效的资源共享。即每台机器只有一个容器的情况很少见。相反，目标是在共享环境中运行尽可能多的隔离进程。因此，如果按照上面第7种方法将多个容器放置在同一主机上，会发生什么情况？让我们添加第二个容器：</p><p> # From root namespace$ sudo ip netns add netns1$ sudo ip link add veth1 type veth peer name ceth1$ sudo ip link set ceth1 netns netns1$ sudo ip link set veth1 up$ sudo ip addr add 172.18.0.21/16 dev veth1$ sudo nsenter --net=/var/run/netns/netns1$ ip link set lo up$ ip link set ceth1 up$ ip addr add 172.18.0.20/16 dev ceth1</p><p> ＃从根名称空间$ sudo ip netns添加netns1 $ sudo ip link add veth1类型veth对等名称ceth1 $ sudo ip link set ceth1 netns netns1 $ sudo ip link set veth1 up $ sudo ip addr add 172.18.0.21/16 dev veth1 $ sudo nsenter --net = / var / run / netns / netns1 $ ip链接设置为lo up $ ip链接设置为ceth1 up $ ip addr add 172.18.0.20/16 dev ceth1 </p><p>  # From `netns1` we cannot reach the root namespace!$ ping -c 2 172.18.0.21PING 172.18.0.21 (172.18.0.21) 56(84) bytes of data.From 172.18.0.20 icmp_seq=1 Destination Host UnreachableFrom 172.18.0.20 icmp_seq=2 Destination Host Unreachable--- 172.18.0.21 ping statistics ---2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 55mspipe 2# But there is a route!$ ip route172.18.0.0/16 dev ceth1 proto kernel scope link src 172.18.0.20# Leaving `netns1`$ exit# From root namespace we cannot reach the `netns1`$ ping -c 2 172.18.0.20PING 172.18.0.20 (172.18.0.20) 56(84) bytes of data.From 172.18.0.11 icmp_seq=1 Destination Host UnreachableFrom 172.18.0.11 icmp_seq=2 Destination Host Unreachable--- 172.18.0.20 ping statistics ---2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 23mspipe 2# From `netns0` we CAN reach `veth1`$ sudo nsenter --net=/var/run/netns/netns0$ ping -c 2 172.18.0.21PING 172.18.0.21 (172.18.0.21) 56(84) bytes of data.64 bytes from 172.18.0.21: icmp_seq=1 ttl=64 time=0.037 ms64 bytes from 172.18.0.21: icmp_seq=2 ttl=64 time=0.046 ms--- 172.18.0.21 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 33msrtt min/avg/max/mdev = 0.037/0.041/0.046/0.007 ms# But we still cannot reach `netns1`$ ping -c 2 172.18.0.20PING 172.18.0.20 (172.18.0.20) 56(84) bytes of data.From 172.18.0.10 icmp_seq=1 Destination Host UnreachableFrom 172.18.0.10 icmp_seq=2 Destination Host Unreachable--- 172.18.0.20 ping statistics ---2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 63mspipe 2</p><p>＃从`netns1`我们无法到达根名称空间！$ ping -c 2 172.18.0.21PING 172.18.0.21（172.18.0.21）56（84）字节数据从172.18.0.20 icmp_seq = 1目标主机UnreachableFrom 172.18.0.20 icmp_seq = 2无法到达目标主机--- 172.18.0.21 ping统计信息--- 2传输的数据包，接收到0个数据包，+ 2个错误，数据包丢失100％，时间55mspipe 2＃但是有一条路由！$ ip route172.18.0.0 / 16 dev ceth1原型内核作用域链接src 172.18.0.20＃离开`netns1` $ exit＃从根名称空间我们无法到达`netns1` $ ping -c 2 172.18.0.20PING 172.18.0.20（172.18.0.20）56（84） ）数据字节。从172.18.0.11 icmp_seq = 1目标主机不可达从172.18.0.11 icmp_seq = 2目标主机不可达--- 172.18.0.20 ping统计信息--- 2传输数据包，收到0个错误，+ 2错误，丢失100％ ，time 23mspipe 2＃从`netns0`我们可以到达`veth1` $ sudo nsenter --net = / var / run / netns / netns0 $ ping -c 2 172.18.0.21PING 172.18.0.21（172.18.0.21）56（84） ）数据字节。从172.18.0.2开始为64字节1：icmp_seq = 1 ttl = 64时间= 0.037 ms从172.18.0.21开始的64个字节：icmp_seq = 2 ttl = 64 time = 0.046 ms --- 172.18.0.21 ping统计信息--- 2数据包被发送，2被接收，0％数据包丢失，时间33msrtt min / avg / max / mdev = 0.037 / 0.041 / 0.046 / 0.007 ms＃但我们仍然无法达到`netns1` $ ping -c 2 172.18.0.20PING 172.18.0.20（172.18.0.20）56（84）字节从172.18.0.10 icmp_seq = 1目标主机不可达从172.18.0.10 icmp_seq = 2目标主机不可达-172.18.0.20 ping统计信息--- 2数据包传输，0接收，+ 2错误，100％数据包丢失，时间63mspipe 2</p><p> Whoops! Something is wrong...  netns1 is stuck in limbo. For some reason, it cannot talk to the root and from the root namespace we cannot reach it out too. However, since both containers reside in the same IP network  172.18.0.0/16, we now can talk to the host&#39;s  veth1 from the  netns0 container. Interesting...</p><p> 哎呀！出了点问题... netns1陷入了困境。由于某种原因，它无法与根对话，并且从根名称空间我们也无法实现。但是，由于两个容器都位于同一个IP网络172.18.0.0/16中，因此我们现在可以从netns0容器与主机veth1进行通信。有趣...</p><p> Well, it took me some time to figure it out, but apparently we are facing the clash of routes. Let&#39;s inspect the routing table in the root namespace:</p><p> 好吧，我花了一些时间才弄清楚，但显然我们正面临路线冲突。让我们检查根名称空间中的路由表：</p><p> $ ip route# ... omited lines ... #172.18.0.0/16 dev veth0 proto kernel scope link src 172.18.0.11172.18.0.0/16 dev veth1 proto kernel scope link src 172.18.0.21</p><p> $ ip route＃...省略的行...＃172.18.0.0 / 16 dev veth0原型内核作用域链接src 172.18.0.11172.18.0.0 / 16 dev veth1原型内核作用域链接src 172.18.0.21</p><p> Even though after adding the second  veth pair, root&#39;s network stack learned the new route  172.18.0.0/16 dev veth1 proto kernel scope link src 172.18.0.21, there already was an existing route for exactly the same network. When the second container tries to ping  veth1 device, the first route is being selected breaking the connectivity. If we were to delete the first route  sudo ip route delete 172.18.0.0/16 dev veth0 proto kernel scope link src 172.18.0.11 and recheck the connectivity, the situation would turn into a mirrored case. I.e. the connectivity of the  netns1 would be restored, but  netns0 would be in limbo.</p><p> 即使在添加第二对第五根之后，根的网络堆栈也了解到新的路由172.18.0.0/16 dev veth1原型内核作用域链接src 172.18.0.21，但已经存在用于同一网络的路由。当第二个容器尝试ping veth1设备时，正在选择第一个路由以断开连接。如果我们要删除第一个路由sudo ip route delete 172.18.0.0/16 dev veth0 proto内核作用域链接src 172.18.0.11并重新检查连接性，则情况将变成镜像情况。即netns1的连接性将恢复，但netns0将处于困境。</p><p>  Well, I believe if we selected another IP network for  netns1, everything would work. However, multiple containers sitting in one IP network is a legitimate use case. Thus, we need to adjust the  veth approach somehow...</p><p>  好吧，我相信如果我们为netns1选择另一个IP网络，一切都会正常。但是，一个IP网络中有多个容器是合法的用例。因此，我们需要以某种方式调整第五种方法。</p><p> Behold the Linux bridge - yet another virtualized network facility! The Linux bridge behaves like a network switch. It forwards packets between interfaces that are connected to it. And since it&#39;s a switch, it does it on the L2 (i.e. Ethernet) level.</p><p> 看一下Linux桥-另一个虚拟化网络工具！ Linux网桥的行为类似于网络交换机。它在与其连接的接口之间转发数据包。而且由于它是交换机，所以它是在L2（即以太网）级别执行的。 </p><p> Let&#39;s try to play with our new toy. But first, we need to clean up the existing setup because some of the configurational changes we&#39;ve made so far aren&#39;t really needed anymore. Removing network namespaces would suffice:</p><p>让我们尝试玩我们的新玩具。但是首先，我们需要清理现有的设置，因为到目前为止我们确实不再需要进行某些配置更改。删除网络名称空间就足够了：</p><p> $ sudo ip netns delete netns0$ sudo ip netns delete netns1# But if you still have some leftovers...$ sudo ip link delete veth0$ sudo ip link delete ceth0$ sudo ip link delete veth1$ sudo ip link delete ceth1</p><p> $ sudo ip netns删除netns0 $ sudo ip netns删除netns1＃但是如果您还有剩余的钱... $ sudo ip link delete veth0 $ sudo ip link delete ceth0 $ sudo ip link delete veth1 $ sudo ip link delete ceth1</p><p> Quickly re-create two containers. Notice, we don&#39;t assign any IP address to the new  veth0 and  veth1 devices:</p><p> 快速重新创建两个容器。请注意，我们不会为新的veth0和veth1设备分配任何IP地址：</p><p> $ sudo ip netns add netns0$ sudo ip link add veth0 type veth peer name ceth0$ sudo ip link set veth0 up$ sudo ip link set ceth0 netns netns0$ sudo nsenter --net=/var/run/netns/netns0$ ip link set lo up$ ip link set ceth0 up$ ip addr add 172.18.0.10/16 dev ceth0$ exit$ sudo ip netns add netns1$ sudo ip link add veth1 type veth peer name ceth1$ sudo ip link set veth1 up$ sudo ip link set ceth1 netns netns1$ sudo nsenter --net=/var/run/netns/netns1$ ip link set lo up$ ip link set ceth1 up$ ip addr add 172.18.0.20/16 dev ceth1$ exit</p><p> $ sudo ip netns添加netns0 $ sudo ip链接添加veth0类型veth对等名称ceth0 $ sudo ip链接集veth0 up $ sudo ip链接集ceth0 netns netns0 $ sudo nsenter --net = / var / run / netns / netns0 $ ip链接设置lo up $ ip链接设置ceth0 up $ ip addr添加172.18.0.10/16 dev ceth0 $ exit $ sudo ip netns添加netns1 $ sudo ip link添加veth1类型veth对等名称ceth1 $ sudo ip链接set veth1 up $ sudo ip链接设置ceth1 netns netns1 $ sudo nsenter --net = / var / run / netns / netns1 $ ip链接set lo up $ ip链接set ceth1 up $ ip addr add 172.18.0.20/16 dev ceth1 $ exit</p><p>  $ ip routedefault via 10.0.2.2 dev eth0 proto dhcp metric 10010.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100</p><p>  $ ip routedefault via 10.0.2.2 dev eth0 proto dhcp metric 10010.0.2.0/24 dev eth0 proto内核作用域链接src 10.0.2.15 metric 100</p><p>    $ sudo ip link set veth0 master br0$ sudo ip link set veth1 master br0</p><p>    $ sudo ip链接设置veth0主br0 $ sudo ip链接设置veth1主br0</p><p>   $ sudo nsenter --net=/var/run/netns/netns0$ ping -c 2 172.18.0.20PING 172.18.0.20 (172.18.0.20) 56(84) bytes of data.64 bytes from 172.18.0.20: icmp_seq=1 ttl=64 time=0.259 ms64 bytes from 172.18.0.20: icmp_seq=2 ttl=64 time=0.051 ms--- 172.18.0.20 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 2msrtt min/avg/max/mdev = 0.051/0.155/0.259/0.104 ms</p><p>   $ sudo nsenter --net = / var / run / netns / netns0 $ ping -c 2 172.18.0.20PING 172.18.0.20（172.18.0.20）56（84）字节数据。来自172.18.0.20的64字节：icmp_seq = 1 ttl = 64时间= 0.259 ms64字节，来自172.18.0.20：icmp_seq = 2 ttl = 64时间= 0.051 ms-- 172.18.0.20 ping统计信息--- 2数据包传输，2接收，0％数据包丢失，时间2msrtt分钟/平均/最大/ mdev = 0.051 / 0.155 / 0.259 / 0.104毫秒 </p><p> $ sudo nsenter --net=/var/run/netns/netns1$ ping -c 2 172.18.0.10PING 172.18.0.10 (172.18.0.10) 56(84) bytes of data.64 bytes from 172.18.0.10: icmp_seq=1 ttl=64 time=0.037 ms64 bytes from 172.18.0.10: icmp_seq=2 ttl=64 time=0.089 ms--- 172.18.0.10 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 36msrtt min/avg/max/mdev = 0.037/0.063/0.089/0.026 ms</p><p>$ sudo nsenter --net = / var / run / netns / netns1 $ ping -c 2 172.18.0.10 PING 172.18.0.10（172.18.0.10）数据的56（84）字节来自172.18.0.10的64字节：icmp_seq = 1 ttl = 64时间= 0.037 ms64从172.18.0.10开始的64字节：icmp_seq = 2 ttl = 64时间= 0.089 ms-- 172.18.0.10 ping统计信息--- 2传输的数据包，2接收到，0％数据包丢失，时间36msrtt分钟/平均/最大/ mdev = 0.037 / 0.063 / 0.089 / 0.026 ms</p><p> Lovely! Everything works great. With this new approach, we haven&#39;t been configuring  veth0 and  veth1 at all. The only two IP addresses we assigned were on the  ceth0 and  ceth1 ends. But since both of them are on the same Ethernet segment (remember, we connected them to the virtual switch), there is connectivity on the L2 level:</p><p> 可爱！一切正常。使用这种新方法，我们根本不需要配置veth0和veth1。我们分配的唯一两个IP地址分别在ceth0和ceth1末端。但是由于它们都在同一以太网段上（请记住，我们已将它们连接到虚拟交换机），因此在L2级别具有连通性：</p><p> $ sudo nsenter --net=/var/run/netns/netns0$ ip neigh172.18.0.20 dev ceth0 lladdr 6e:9c:ae:02:60:de STALE$ exit$ sudo nsenter --net=/var/run/netns/netns1$ ip neigh172.18.0.10 dev ceth1 lladdr 66:f3:8c:75:09:29 STALE$ exit</p><p> $ sudo nsenter --net = / var / run / netns / netns0 $ ip neigh172.18.0.20 dev ceth0 lladdr 6e：9c：ae：02：60：de STALE $ exit $ sudo nsenter --net = / var / run / netns / netns1 $ ip neigh172.18.0.10 dev ceth1 lladdr 66：f3：8c：75：09：29 STALE $退出</p><p> Congratulations, we learned how to  turn containers into friendly neighbors, prevent them from interfering, but keep the connectivity.</p><p> 恭喜，我们学会了如何将容器变成友好的邻居，防止它们相互干扰，但保持连接。</p><p>  Our containers can talk to each other. But can they talk to the host, i.e. the root namespace?</p><p>  我们的容器可以互相交谈。但是它们可以与主机（即根名称空间）对话吗？</p><p>     # Use exit to leave `netns0` first:$ ping -c 2 172.18.0.10PING 172.18.0.10 (172.18.0.10) 56(84) bytes of data.From 213.51.1.123 icmp_seq=1 Destination Net UnreachableFrom 213.51.1.123 icmp_seq=2 Destination Net Unreachable--- 172.18.0.10 ping statistics ---2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 3ms$ ping -c 2 172.18.0.20PING 172.18.0.20 (172.18.0.20) 56(84) bytes of data.From 213.51.1.123 icmp_seq=1 Destination Net UnreachableFrom 213.51.1.123 icmp_seq=2 Destination Net Unreachable--- 172.18.0.20 ping statistics ---2 packets transmitted, 0 received, +2 errors, 100% packet loss, time 3ms</p><p>     ＃首先使用exit离开netnet0：$ ping -c 2 172.18.0.10PING 172.18.0.10（172.18.0.10）56（84）字节数据从213.51.1.123 icmp_seq = 1目标网络不可达自213.51.1.123 icmp_seq = 2目标网络不可达--172.18.0.10 ping统计信息--- 2数据包传输，0接收，+ 2错误，100％数据包丢失，时间3ms $ ping -c 2 172.18.0.20PING 172.18.0.20（172.18.0.20） 56（84）字节数据。从213.51.1.123 icmp_seq = 1从目标网络不可达从213.51.1.123 icmp_seq = 2目标网络不可达--- 172.18.0.20 ping统计信息--- 2数据包传输，已接收0，+ 2错误，100丢包百分比，时间3ms</p><p> To establish the connectivity between the root and container namespaces, we need to assign the IP address to the bridge network interface:</p><p> 为了在根和容器名称空间之间建立连接，我们需要将IP地址分配给网桥网络接口： </p><p>  Once we assigned the IP address to the bridge interface, we got a route on the host routing table:</p><p>将IP地址分配给网桥接口后，便在主机路由表上获得了一条路由：</p><p> $ ip route# ... omited lines ...172.18.0.0/16 dev br0 proto kernel scope link src 172.18.0.1$ ping -c 2 172.18.0.10PING 172.18.0.10 (172.18.0.10) 56(84) bytes of data.64 bytes from 172.18.0.10: icmp_seq=1 ttl=64 time=0.036 ms64 bytes from 172.18.0.10: icmp_seq=2 ttl=64 time=0.049 ms--- 172.18.0.10 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 11msrtt min/avg/max/mdev = 0.036/0.042/0.049/0.009 ms$ ping -c 2 172.18.0.20PING 172.18.0.20 (172.18.0.20) 56(84) bytes of data.64 bytes from 172.18.0.20: icmp_seq=1 ttl=64 time=0.059 ms64 bytes from 172.18.0.20: icmp_seq=2 ttl=64 time=0.056 ms--- 172.18.0.20 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 4msrtt min/avg/max/mdev = 0.056/0.057/0.059/0.007 ms</p><p> $ ip route＃...省略的行... 172.18.0.0/16 dev br0 proto内核作用域链接src 172.18.0.1 $ ping -c 2 172.18.0.10PING 172.18.0.10（172.18.0.10）56（84）个字节来自172.18.0.10的data.64字节：icmp_seq = 1 ttl = 64时间= 0.036 ms来自172.18.0.10的data.64字节：icmp_seq = 2 ttl = 64时间= 0.049 ms --- 172.18.0.10 ping统计信息--- 2传输的数据包， 2收到，0％数据包丢失，时间11msrtt min / avg / max / mdev = 0.036 / 0.042 / 0.049 / 0.009 ms $ ping -c 2 172.18.0.20PING 172.18.0.20（172.18.0.20）56（84）字节数据来自172.18.0.20的.64字节：icmp_seq = 1 ttl = 64时间= 0.059毫秒来自172.18.0.20的.64字节：icmp_seq = 2 ttl = 64时间= 0.056 ms --- 172.18.0.20 ping统计信息--- 2传输的数据包，2收到，0％数据包丢失，时间4msrtt最小/平均/最大/ mdev = 0.056 / 0.057 / 0.059 / 0.007 ms</p><p> The container probably also got an ability to ping the bridge interface, but they still cannot reach out to host&#39;s  eth0. We need to add the default route for containers:</p><p> 容器也可能具有ping桥接接口的功能，但是它们仍然无法与主机eth0保持联系。我们需要为容器添加默认路由：</p><p> $ sudo nsenter --net=/var/run/netns/netns0$ ip route add default via 172.18.0.1$ ping -c 2 10.0.2.15PING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.64 bytes from 10.0.2.15: icmp_seq=1 ttl=64 time=0.036 ms64 bytes from 10.0.2.15: icmp_seq=2 ttl=64 time=0.053 ms--- 10.0.2.15 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 14msrtt min/avg/max/mdev = 0.036/0.044/0.053/0.010 ms# And repeat the change for `netns1`</p><p> $ sudo nsenter --net = / var / run / netns / netns0 $ ip路由通过172.18.0.1添加默认值ping -c 2 10.0.2.15 PING 10.0.2.15（10.0.2.15）56（84）个字节的data.64来自10.0.2.15的字节：icmp_seq = 1 ttl = 64时间= 0.036 ms ms来自10.0.2.15的字节：icmp_seq = 2 ttl = 64时间= 0.053 ms --- 10.0.2.15 ping统计信息--- 2数据包发送，2接收， 0％数据包丢失，时间14msrtt min / avg / max / mdev = 0.036 / 0.044 / 0.053 / 0.010 ms＃并重复更改netns1</p><p> This change basically turned the host machine into a router and the bridge interface became the default gateway for the containers.</p><p> 这项更改基本上将主机变成了路由器，并且网桥接口成为了容器的默认网关。</p><p>  Perfect, we connected containers with the root namespace. Now, let&#39;s try to connect them to the outside world. By default, the packet forwarding (i.e. the router functionality) is disabled in Linux. We need to turn it on:</p><p>  完美的是，我们将容器与根名称空间相连。现在，让我们尝试将它们连接到外部世界。默认情况下，在Linux中禁用数据包转发（即路由器功能）。我们需要打开它：</p><p>    Well, still doesn&#39;t work. What have we missed? If the container were to sends packets to the outside world, the destination server would not be able to send packets back to the container because the container&#39;s IP address is private. I.e. the routing rules for that particular IP are known only to the local network. And lots of the containers in the world share exactly the same private IP address  172.18.0.10. The solution to this problem is called the  Network address translation (NAT). Before going to the external network, packets originated by the containers will get their source IP addresses replaced with the host&#39;s external interface address. The host also will track </p><p>    好吧，仍然行不通。我们错过了什么？如果容器要将数据包发送到外界，则目标服务器将无法将数据包发送回容器，因为容器的IP地址是私有的。即该特定IP的路由规则仅本地网络知道。世界上许多容器共享完全相同的专用IP地址172.18.0.10。解决此问题的方法称为网络地址转换（NAT）。在进入外部网络之前，由容器产生的数据包将其源IP地址替换为主机的外部接口地址。主持人还将跟踪 </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://iximiuz.com/en/posts/container-networking-is-simple/">https://iximiuz.com/en/posts/container-networking-is-simple/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/2020/">#2020</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/容器/">#容器</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>