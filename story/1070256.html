<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>神经数据分析中的降维</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">神经数据分析中的降维</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-07-29 21:54:27</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/7/99b8da0172616d7527839828723da956.png"><img src="http://img2.diglog.com/img/2021/7/99b8da0172616d7527839828723da956.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>同时记录数百个神经元已变得司空见惯。如果根据过去的趋势推断，到 2030 年我们通常可能会记录 1 万个神经元。我们将如何处理所有这些数据？要处理 14 维空间，请想象一个 3D 空间并大声对自己说 14。每个人都这样做。 — Geoffrey Hinton 神经动力学的维度往往低于记录的神经元数量所暗示的维度 – Humphries (2020) 报告了 10 倍到 100 倍的压缩，具体取决于区域和任务。 Humphries 建议，借助神经降维的强大原理，降维可以向我们展示神经回路所体现的真实潜在信号。即使我们不完全相信这个强大的原理——它在视觉研究人员中肯定存在争议——许多人赞同弱原理：更小、时间更平滑、噪声更少的子空间比原始数据更容易理解。在这里，我介绍了神经数据分析中无监督方法的分类，确定了文献中一些令人兴奋的趋势，并展示了一些关键差距——未来几年的研究路线图。为什么我们关心神经科学中的降维？让我们数一数方式（大致按野心排序，或从弱原则到强原则）： 压缩：将数据压缩到几十个维度而不是几百个维度有实际的计算原因——即节省内存和处理。可视化：人类不擅长可视化超过 2 或 3 个维度。将信息压缩到少数有意义的维度可以更容易地绘制它们，并直观地了解数据变化的因素。</p><p>去噪：如果我们想了解单次试验期间发生了什么，我们需要对数据进行去噪。如果 100 个神经元是同一个潜在变量的 100 个独立表现，我们可以通过平均来将信噪比提高一个因子√100 = 10。降维灵活地平均同一数据的多个重叠实现，以实现信噪比的提升。生成假数据：许多降维方法都是有效的生成模型，我们可以使用它们生成假神经数据，例如对尖峰排序和钙成像信号处理管道进行基准测试。与行为相关：嘈杂的高维神经数据很难与动物的行为相关联，而这往往是我们最终关心的。将信息压缩到低维子空间可以更容易地与行为相关联。理解计算：潜在空间可能代表原始数据中不可见的神经计算的有趣方面，例如，线吸引子或旋转动力学。降维的潜在空间可以帮助发现那些隐藏的计算。解开潜在因素：也许最雄心勃勃的是，我们想要确定导致数据变化的潜在因素。大多数降维方法不直接捕获因果机制——这意味着每个逻辑维度可能捕获几个因果机制。但是一些降维方法渴望解开独立的原因——线性 ICA（独立成分分析）可能是最著名的例子。标准免责声明：因果推断很难。对因果论断持怀疑态度。这是一个活跃的研究领域。鉴于这些不同的分析目标，我们可以编译一个规范模型，该模型涵盖了广泛的降维技术。从概念上讲，我们有一个生成模型，它通过映射函数 f 和噪声模型将潜在因子 z(t) 映射到神经观察 x(t)。潜在因素根据它们的动态 D 演化。降维技术的中心目标是学习推理函数 φ，它将神经观察 x(t) 映射到潜在因素 z(t)。该推理函数可以像矩阵乘法一样简单，也可以像由多步优化程序实现的隐式函数一样复杂。</p><p>为了在地面上放置一些赌注，让我们看看这些不同的组件如何在您可能听说过的一些流行模型中相互作用： PCA：在主成分分析 (PCA) 中，映射函数 f 是瞬时线性函数，噪声是高斯函数（在 PCA 的概率扩展中）。动态没有明确建模。推理函数是映射函数 ICA 的逆矩阵：在独立分量分析 (ICA) 中，映射函数的类别与 PCA 中的相同，但在学习过程中，映射函数 φ 被约束为潜在空间 z(t ) 被分解为独立的来源。 GPFA：在高斯过程因子分析 (GPFA) 中，映射函数再次是线性函数，后跟高斯噪声，但现在使用独立的高斯过程 (GP) 先验明确地建模动态。您可以将其视为强制潜在变量随时间平滑变化。推理是通过具有期望最大化的封闭形式 GP 推理来完成的，以学习超参数 LFADS：在通过动态系统的潜在因子分析 (LFADS) 中，动态由 RNN 给出，而映射函数是线性的，噪声可以是高斯或泊松。 RNN 隐含地强加了平滑的动态。推理是通过变分自动编码器 (VAE) 框架中的变分推理完成的。也许最重要的动人部分是动态。为了在单个试验级别推断神经活动的潜在结构，使用当前现有的记录技术，没有足够的神经元和 SNR 来推断单个试验级别的潜在结构：我们必须灵活地平滑时间。复杂的动态先验需要复杂的、概率性的推理方法——每种方法在速度、实现的简单性和通用性方面都有其权衡——这就是为什么我们在过去 5 年中看到了方法的爆炸式增长。现在我们已经看到了一些降维方法的例子，让我们看看一些已发表的分类法，它们试图为混乱带来一些秩序。我已经用上面的规范模型（红色）的组件注释了这些分类法以供参考。 Cunningham and Yu (2014) 的经典评论中有一个早期的分类法：</p><p>我们看到了这项早期工作中所代表的目标的多样性。接下来几年的大部分研究都集中在构建复杂的新推理方法上，这些方法假定显式生成和动态模型。动力学允许模型随着时间的推移整合信息，这使得进行单次试验推理成为可能。约翰·坎宁安 (John Cunningham) 在 2018 年左右的一次演讲中提出了这些模型的新分类法：为低质量道歉，它是从关键幻灯片的照片中提取的 这个更新的表格有点难以解析，但我发现它提供了很多历史上下文和谱系：我们在左列中看到有关易处理模型的早期工作（隐藏马尔可夫模型、线性动力系统、切换 LDS）在中列和右列中演变为更复杂的模型。复杂的模型带来了复杂的推理技术； Memming Park 在 2020 年 CoSyNe 演讲中提供了一个很好的总结：其中许多方法最近被编译成 Hurwitz 等人的另一个分类法。 (2021)，用尖端方法综合了该领域的一些早期工作。首字母缩略词有点难以描述，但您通常可以通过查看首字母缩略词中的字母组来猜测每种方法的含义：GP：高斯过程。高斯过程通常用于对潜在变量施加时间平滑，很少用于对非线性调谐曲线进行建模。</p><p>LDS：线性动力系统。卡尔曼滤波器是具有基础线性动力系统的模型的示例。 SLDS：切换 LDS。可以根据离散变量在 2 个或更多潜在线性动态之间切换的模型。 P*：泊松。首字母缩略词中的起始 P 通常指的是泊松噪声模型。 VAE：变分自动编码器。一种潜在变量模型，它使用编码器将观察结果压缩为潜在变量，使其成为易处理的分布（通常是多元高斯分布），并通过变分推理进行拟合。您会注意到，这些分类法并未明确说明如何将行为和实验条件集成到每个模型中。该领域一些最有趣的发展与将行为显式地整合到降维中有关。让我们考虑一个更通用的降维模型，它集成了行为以及对应于实验条件的外部协变量。在这里，我们有与神经时间序列 x(t) 平行的行为时间序列 u(t)。例如，u(t) 可以由到达任务中手臂位置的时间序列、鼠标面部视频的 PC 或实验条件组成。映射函数和生成模型现在对这些时间序列的串联进行操作——潜在空间 z(t) 在行为和神经数据之间共享。在这里，行为回归到大脑数据（反之亦然）和降维之间的区别是模糊的：它介于两者之间。这是我在考虑整合行为和神经数据时使用的心理模型，但请注意，整合行为和神经数据的方法有很多，但表面上看起来不同。 LDA：线性判别分析。经典技术。查找可以最大程度区分一组类的一组维度。没有动态。</p><p>CCA：典型相关分析。经典技术。查找两组不同矩阵之间的公共线性子空间。通过线性函数进行推理和映射。无动力学 Demixed component analysis：根据不同级别的条件及其相互作用，对数据执行类似方差分析或类似张量分解的分解。没有动态，只有平均试验。 LFADS：通过动态系统进行的潜在因素分析。可以将行为和条件信息作为辅助信息集成到潜在空间。单次试验、RNN 动力学、显式噪声模型、变分推理 (VAE)。 PSID：优先子空间识别。卡尔曼滤波器启发模型将潜在空间明确划分为仅行为、仅神经和神经和行为子集。单次试验、线性动力学、高斯噪声模型、基于卡尔曼滤波器的推理（动态规划）。 pi-VAE：（其他评论中未涵盖，于 2020 年末发布）。泊松可解释变分自动编码器。嵌入有关条件和行为的潜在信息，作为类似 VAE 的分层生成模型的一部分。受近期融合非线性 ICA 和 VAE 工作的启发（Khemakhem 等人，2019 年），使用灵活的单射函数（GIN，RealNVP 的变体）作为生成模型来解开潜在原因（直至排列）。没有动态。正如您可能从上一段推断出的那样，我很高兴看到明确整合行为的模型取得了进展。降维算法通常在已知的低维系统上进行完整性检查——例如范德波尔振荡器、洛伦兹系统、平衡 EI 网络。然而，当它们在野外部署时，对它们的评估通常归结为将它们与已知的行为和实验条件相匹配。从本质上讲，行为是基本事实。在这种情况下，错失了不明确模拟行为的机会。您可能会问自己：集成行为和神经数据的潜变量模型与简单地对神经数据进行回归行为有何不同？一个答案是存在一个共享的低维瓶颈，而这个低维瓶颈的结构可以揭示一些关于神经计算和行为的有趣之处。例如考虑 Wiltschko 等人。 (2015)，他使用自回归隐马尔可夫模型 (AR-HMM) 对小鼠的自发行为进行建模：假设小鼠遵循由离散状态（例如步行、暂停、低后方等）确定的不同线性动力学。这是切换线性动力系统 (SLDS) 的一个例子，它也经常应用于神经数据。</p><p>现在想象一下相同的模型，但这次适用于神经和行为数据。推断的状态不仅会揭示通过头顶摄像头可见的行为状态，还会揭示神经状态，如唤醒和注意力。实际上，PSID 的目标是将低维瓶颈划分为仅行为维度、仅神经维度和行为与神经维度。理想的模型会向 PSID 中制定的基本计划添加更多的间接层（例如离散状态、行为和神经组件的不同非线性设备、切换动力学）。将其视为具有动力学的可解释非线性 CCA（典型相关分析）。我们希望从这些模型中得到什么？过去几年小鼠视觉中最令人惊讶的发现之一是发现了视觉皮层面部行为的众多编码维度（Stringer et al. (2019)）。这一发现的核心是降阶回归，它发现面部行为和视觉反应之间存在大量共享差异（排名 16！）。这种研究重新定义了我们对神经活动和行为之间关系的理解，我预计随着工具的成熟，我们会看到很多这样的研究。当我第一次研究降维技术时，令我震惊的是，大多数技术明智地使用简单的线性函数 f 来映射潜在变量 z(t) 到 x(t)。我对某些方法使用强大的非线性解码函数 f 感到困惑。如果目标是找到一个易于解释的潜在子空间，那么从潜在到数据的简单映射——类似于可信的 LNP（线性非线性泊松）模型不是更好吗？ P-GPLVM 论文（Anqi Wu 等人，2017 年）让我信服。要使用线性解码函数从海马体的线性轨迹上解码动物的位置，您需要一个高维的潜在空间。如果您的轨道长 1 米，并且每个场地的宽度约为 20 厘米，那么您可能需要 5 个维度 (=100 厘米 / 20 厘米) 来表示所有场地。但是只有一个逻辑维度（轨道上的线性位置）！在解码中添加非线性（以广义加性模型的风格）修复了这个缺陷。我们还看到了融合信号处理管道和降维的努力。巴蒂等人。 (2019) 提出了一个分层模型 (BehaveNet) 来直接从鼠标的图像推断潜在的。相关地，ValPACa（钙成像数据并行自动编码的变分阶梯）应用 LFADS 的分层变体直接从钙图像中提取潜在变量。将信号处理组件与潜在变量模型融合通常意味着使用丰富的非线性解码模型。因此，我们可能会看到更多具有丰富解码模型的论文，无论它们是 GP、RNN 还是可逆神经网络。这篇文章是在几个月前开始的，当时我想拥有工具来理解 Memming 的分类法和强调的方法——所以我参加了深度无监督学习的课程以获得正确的背景。事实证明，现代无监督学习以几种可见的方式影响了神经科学中的降维：使用 RNN 表示复杂动态，以及使用 BBVI 和 VAE 技术进行推理。然而，在无监督学习方面还有许多其他进步（参见深度无监督学习课程），但尚未对神经科学的降维产生重大影响。</p><p>自监督学习可以学习良好的低维表示，而无需明确学习生成模型，例如使用对比技术。在 Apple 研究人员的这篇论文 (Cheng et al. 2020) 中，他们使用对比物镜找到了 EEG 的出色表示。这种方法有助于找到杂乱信号的良好低维表示，例如 ECoG 和局部场电位。对抗性网络是另一个包含许多富有成果的想法的领域。同一篇论文 (Cheng et al. 2020) 使用对抗性评论家来强制表示从主题转移到主题。事实上，对抗性方法可用于划分潜在空间。最后，我将提到规范化流模型的使用。标准化流模型非常巧妙：它们使用可逆神经网络学习潜在空间。学习非常简单：你直接最大化数据的可能性，加上对神经网络雅可比的校正——在某些情况下，你甚至可以通过只使用体积保持变换来摆脱雅可比。在我看来，它们比 VAE 更不挑剔。它们非常灵活，我很乐意看到它们被更多地使用——据我所知，pi-VAE 是第一篇在神经科学背景下使用流模型的论文（尽管以有限的方式）。标准化流模型确实有一些需要首先解决的重大限制（不能直接模拟不连续分布，潜在空间与观察大小相同）。哪种方法最适合用于您的数据？很难说。大多数论文都是围绕深入了解特定数据集而构建的，并且几乎没有系统地评估哪种方法最有效以及何时最有效（更不用说运行时间和计算需要调整多少个旋钮）。 Chethan Pandarinath 在今年的 CoSyNe 上公布了 Neural Latents 基准。我是这个想法的忠实粉丝，一旦它发布，我就会很高兴地做出贡献。您应该尝试什么方法来处理您的数据？上面分类中列出的大多数方法都不是即插即用的事务。在实践中，如果您是一名实验者并且您不想浪费大量时间来做某事，您可以选择以下之一：线性动力系统 (LDS) 和导数，包括切换、泊松、循环、和 Linderman 实验室 ssm 包中的非线性映射变体。优秀的教程。 LFADS，它在 Tensorflow、PyTorch 和 jax 中有实现。另请参阅 AutoLFADS，了解可以在云中自动搜索超参数的变体。</p><p>这是核心问题之一：2018 年可能是新模型最大的一年（基于我看过的分类法）。这也是机器学习管道发生巨大变化的一年：Theano 即将推出，TF2.0 尚未推出，PyTorch 发展迅速。这里讨论的许多方法都在 github 存储库中，星数不到十，并且仅适用于旧版本的 CUDA 上的 TF、PyTorch 或 Theano 的过时版本。让它们运行可能意味着在 nvidia-docker 内部运行。进口反重力这不是。这是一条前进的道路：应该使用 nvidia-docker、捆包线和口香糖对现有方法进行基准测试（上图），然后可以选择最有前途的方法来保存在一个有据可查的包装内。该包可以使用与 sklearn 相同的约定（Elephant 有一个围绕此抽象构建的 GPFA 的优雅实现）。它应该选择一个框架（可能是 PyTorch）并承诺使用持续集成等方法来维护这些方法。此外，它应该可以通过云平台使用，例如通过 NeuroCAAS。这将大大增加更复杂的分析方法的范围。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://xcorr.net/2021/07/26/dimensionality-reduction-in-neural-data-analysis/">https://xcorr.net/2021/07/26/dimensionality-reduction-in-neural-data-analysis/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经/">#神经</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/reduction/">#reduction</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>