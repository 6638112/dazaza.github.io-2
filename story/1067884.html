<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>AGQA：组成，时空推理的基准 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">AGQA：组成，时空推理的基准 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-24 10:00:30</div><div class="page_narrow text-break page_content"><p>看看上面的视频和相关问题 - 在打开壁橱之前他们持有什么？在查看视频后，您可以轻松回答该人持有手机。人们在新视频中理解视觉事件并回答有关该视频的问题。我们可以将视觉事件和行动分解为个人和其他对象之间的各个交互。例如，该人最初握住手机，然后打开壁橱并取出图片。为了回答这个问题，我们需要识别“打开壁橱”的动作，然后了解如何“之前”应该限制我们在此操作之前对事件的答案进行搜索。接下来，我们需要检测“控股”的交互，并将对象识别为“手机”，以最终到达答案。我们了解各个推理步骤和视频的构成的问题，作为各个单独的相互作用的组成。</p><p> 设计机器可以类似地表现出对视觉事件的构成理解一直是计算机视觉社区的核心目标。为了衡量实现这一目标的进展，社区已发布众多视频问题应答基准（TGIF-QA，MSVD / MSRVTT，CLEVRER，ActivityNET-QA）。这些基准测试通过询问有关视频的问题并测量模型的答案准确性来评估模型。在过去几年中，在此类基准上的模型表现一直在鼓励：</p><p>  但是，目前尚不清楚为什么模特正在改善。简单的问题，如“在打开壁橱之前持有什么？”需要许多不同推理能力的组成。模型是否在识别行动方面改善？了解互动？或者他们只是在进行数据集中利用语言和视觉偏见的改善？由于这些基准主要提供单一的“整体准确性”度量作为评估措施，因此我们对每个模型的优势和劣势的宽度有有限的观点。</p><p> 为了更好地回答这些问题，我们介绍了基准动作基因组问题应答（AGQA）。 AGQA通过近二亿质量答案对来测量空间，颞率和组成推理。 AGQA的问题是复杂的，组成和注释，以允许明确的测试，该测试能够找到模型可以和无法回答的问题的类型。</p><p>  以这种规模创建基准测试是对人类注释器缩放的昂贵昂贵。相反，我们使用基于规则的问题模板设计了一个合成生成过程，以从场景信息生成问题，这代表了使用符号中的视频中发生的内容（图3：来自动作基因组的时空场景图）。合成代允许我们控制回答每个生成问题所需的内容，结构和组成推理步骤。</p><p> 我们在我们的基准测试中运行了艺术模型的状态，发现他们表现不佳，严重依赖语言偏见，并努力推广更复杂的任务。实际上，所有模型都几乎没有以上的消融，其中视频没有作为输入的输入。</p><p>  行动基因组问题回答有19200万复杂和组成问题答案对。我们还示出了390万质询答案对，使得该子集具有更均匀的答案分配和更广泛的问题。每个问题都有关于问题内容和结构的详细诠释。这些注释包括回答问题所需的推理步骤的程序，并将问题的项目映射到视频的相关部分（图4）。 AGQA还提供详细的指标，包括测试分配，以测量不同问题类型的性能和旨在测量组成推理的三个新度量。 </p><p>为了综合生成问题，我们首先通过场景图代表视频（图3）。我们从每个帧注释在该帧中发生的操作，对象和关系的视频中拍摄帧样。其次，我们建造了28个模板。这些模板包括引用场景图中的类型的类型的自然语言帧。在图4中，模板提供了一般的自然语言帧，询问主题是否在指定时间段内对对象的关系。每个模板还有一个计划概述了一系列步骤以便回答问题。图4中的示例在时间段迭代，查找它们具有该关系的所有对象，然后确定指定的对象是否存在该列表中。</p><p>  第三，我们组合场景图和模板来生成自然语言问题答案对。例如，上面的模板可以使用图3的场景图来生成自然语言问题“他们在躺下之前观看手机吗？”。然后，关联的程序通过在躺下之前迭代的时间自动生成答案，找到他们正在观看的所有物品，并确定它们在此期间不会观看手机。结合场景图和模板创建各种自然语言问题答案对。我们的基准测试中的每对都包括对用于生成答案的推理步骤的程序的引用，以及将问题中的单词的映射映射到场景图注释。最后，我们采取生成的对并平衡答案和问题类型的分布。我们对不同类别的顺畅答案分布，然后采样问题，以便数据集具有多样性问题结构。</p><p>  人体评估。我们通过人力验证验证我们的问题答案对，并发现注释者同意我们答案的86.02％。要在上下文中提出这个数字，GQA和Clevr，最近的两个自动化基准，分别报告了89.30％和92.60％的人类准确性。一些场景图在传播到不正确的问题中的场景图中具有不一致的，不正确或缺少的信息。场景图和人类理解定义的本体也可能存在差异。例如，场景图中有36个对象，但人类可以考虑视频中出现的对象，但不在模型的pureview中。</p><p> 我们对人类任务，每个错误来源以及未来视频表示的建议提供更多详细信息，以及我们论文的补充部分中的未来视频表示。</p><p> 模型性能取决于语言偏见。我们在我们的基准（HCRN，HME和PSAC）上运行了三个艺术模型的状态，并发现模型在我们的基准上挣扎。如果模型仅选择最可能的答案（“否”），则可以获得10.35％的准确性。最高评分模型HME，精度达到47.74％，乍一看似乎是一个很大的改善。然而，进一步的调查发现，即大部分增益来自仅利用语言偏见而不是视觉推理。虽然HCRN总体上实现了47.42％的精度，但它仍然在没有看到视频的情况下实现了47％的准确性。该模型如此依赖语言偏见而不是视觉推理的事实降低了我们的其他测试分裂的能力，以有效地测量这些特定模型的视觉推理。</p><p> 测量不同问题属性。我们提供测试集中的分裂，以测量不同类型的推理技能，语义类别和问题结构的模型性能。</p><p> 要了解不同类型的问题的模型性能，我们将通过回答问题所需的推理技能分割测试。例如，一些问题测试了第一概念，如第一个和最后一次（他们先拿起了什么，一道菜或图片？），一些比较多种动作的持续时间（是吃一些食物或坐在地板上的人坐在地板上的持续时间），其他人需要活动识别（他们持续的是什么？）。不同的模型在每个类别中实现了最高的准确性。模型性能也在这些类别中广泛变化，所有三种模型在活动识别上表现最差。 </p><p>AGQA还通过如果它们的语义重点在对象，关系或行动上拆分问题。只选择最常见的答案，分别有关对象，关系和行动的问题的准确性为9.38％，50％和32.91％。表现最高的模型实现了面向对象问题的42.48％，而盲模具的准确性精度为40.74％。盲模型表现出所有其他型号，精度为面向关系的问题，对行动导向问题的准确性为60.95％。</p><p> 最后，我们通过其结构诠释每个问题。查询问题是开放的答案（他们持有什么？）。验证问题验证是否存在问题（他们持有一道菜）。逻辑问题使用逻辑运算符（他们持有一道菜，但不是毯子？）。选择问题提供两个选项之间的选择（他们是否持有菜肴或毯子？）。比较问题比较两个选项的属性（与持有盘相比，他们坐着更长吗？）。每个模型在开放回答的问题上表现最糟糕，并最佳验证和逻辑问题。</p><p> 新的合成度量。我们还提供三项专门测量组成推理的新度量。这些拆分培训和测试集以测试模型概括到先前看到的想法的新组成，间接参考以及更具成分步骤的能力。</p><p> 首先，我们测量模型概括为新型组成的能力。我们认为构图是两个离散的想法，组合成一个例子。例如，“之前”和“站起来”是问题中的一个组成“他们在站起来之前采取了什么？”。为了确保这些组合物在测试集中是新颖的，我们包括在与其他物品组成时的培训套件之前和站立的想法。但是，我们不包括培训集中的疑问，其中出现前站的构成。模型致力于将它们在测试集中第一次看到的组合物概括。最佳表演模型几乎没有于两个答案的二进制问题的50％以上的准确性。在开放答案问题上有两个以上答案的问题，最高性能的模型精度达到23.72％。</p><p>  我们的第二次指标衡量间接参考的概括。直接引用说明他们指的是（电话），而间接参考由其属性或其他关系（他们持有的第一件事）引用。我们使用间接引用来提高我们问题的复杂性。此度量标准比较了模型如何应答问题，如果他们可以通过直接引用回答它，则如何使用间接引用来回答问题。如果可以通过直接引用回答它，模型可以使用间接参考应答大约80％的问题。</p><p> 第三个组成度量指标衡量更复杂的问题的概括。培训和测试拆分划分问题，使得培训集包含更简单的问题，其中组成步骤较少，而测试集包括具有更多组成步骤的问题。模型在这项任务上挣扎，因为它们都不是二进制问题的50％，只有两个答案。</p><p> 问题复杂性和准确性。最后，我们注释回答每个问题所需的组成步骤的数量。我们发现，尽管人类保持一致，因为问题变得更加复杂，但模型的准确性降低。 </p><p>AGQA在几个方向上打开了进步的途径。 神经象征性和元学习建模方法可以改善组成推理。 关于问题的编程细分也可以为努力提供发行的解释。 我们还邀请探索使用和生成视频的不同象征性表示。  我们的基准测试突出了现有型号的薄弱点，包括对语言偏见的过度增长，难以推广到新颖和更复杂的任务。 然而，其问题答案对和详细指标的平衡数据集提供了一种探索多个令人兴奋的新方向的基准。  通过通过电子邮件或电子邮件保持最新的帆博客帖子之上： </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="http://ai.stanford.edu/blog/agqa/">http://ai.stanford.edu/blog/agqa/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/组成/">#组成</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/benchmark/">#benchmark</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>