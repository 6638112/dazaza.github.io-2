<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>来自下面的MySQL MySQL from Below</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">MySQL from Below<br/>来自下面的MySQL </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-02 17:32:13</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/412bbb96f78fb2ae2ef68782d094adf3.jpg"><img src="http://img2.diglog.com/img/2021/3/412bbb96f78fb2ae2ef68782d094adf3.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>When you insert data into a database and run COMMIT you expect things to be there:  Atomically, Consistent, Isolated and Durable, like Codd commanded us 40 years ago, but also quickly. There is a surprising amount of sophistication being poured into this, but since I do not want to shame MongoDB and Redis developers in this post, I am not going to talk about that much in this place.</p><p>当您将数据插入数据库并运行COMMIT时，您会期望它存在：从原子上讲，一致，隔离和持久，就像40年前Codd命令我们一样，而且很快。令人惊讶的是，它的复杂性令人注目，但是由于我不想在本文中羞辱MongoDB和Redis开发人员，因此在这里我不再赘述。</p><p> We are instead trying to understand what our databases are doing all day, from the point of view of the storage stack.</p><p> 相反，我们正尝试从存储堆栈的角度了解数据库整天在做什么。</p><p>   Here is your performance tooling (sans EBPF) as shown by Brendan Gregg in various iterations. We are interested into the various blues in this graphic, VFS and downwards. The most detailed information of what goes on in the blue stack is given by blktrace. It’s an I/O request recorder.</p><p>   这是Brendan Gregg在各种迭代中展示的性能工具（没有EBPF）。我们对该图形中的各种蓝色，VFS和更低版本感兴趣。 blktrace提供了有关蓝色堆栈中发生的情况的最详细信息。这是一个I / O请求记录器。</p><p> Unfortunately, blktrace tooling sucks big time. You have the choice between blkparse, iowatcher and seekwatcher, and they either give you unappealing and hard to read ASCII dumps, or are hard to use and abandoned to boot. A colleague threw  Workload Analytics around. And with that you can actually see what goes on.</p><p> 不幸的是，blktrace工具占用大量时间。您可以在blkparse，iowatcher和seekwatcher之间进行选择，它们要么给您带来吸引力，并且很难读取ASCII转储，要么难以使用，并且无法启动。一位同事扔了Workload Analytics。这样您就可以实际看到发生了什么。</p><p>   When you go to our Grafana, and ask nicely, you can basically divide our databases into three load classes: “200 commit/s”, “2000 commit/s”, and “20.000 commit/s”, so “low write load”, “busy write load” and “should be thinking about what they are doing really soon” write load.</p><p>   当您访问我们的Grafana并提出很好的要求时，您基本上可以将我们的数据库分为三个负载类别：“ 200 commit / s”，“ 2000 commit / s”和“ 20.000 commit / s”，因此“低写入负载” ，“繁忙的写负载”和“应该很快考虑他们正在做什么”的写负载。</p><p>    We build our databases, especially customer facing databases, in a way that we try to hold the working set of the database in memory.</p><p>    我们以尝试将数据库的工作集保存在内存中的方式来构建数据库，尤其是面向客户的数据库。</p><p> The “working set” of anything is the set of things that you will be needing in the near future. Unfortunately, predicting the future is hard, so what we define as the working set instead is “the set of things that we have most recently used and are therefore hoping to use in the near future again”, which works reasonably well until there is a change in workload pattern.</p><p> 任何事物的“工作集”都是您不久的将来需要的事物集。不幸的是，很难预测未来，因此我们定义为工作集的是“我们最近使用过的一组东西，因此希望在不久的将来再次使用”。工作量模式的变化。 </p><p> In our databases, the “anything” that matters is a 16 KB database page as it resides in the InnoDB Buffer Pool. So what we do is simple: We spend money on memory, warm up the InnoDB Buffer Pool and then serve data from memory.We can prove that works: InnoDB can do point reads from memory at memcache speeds, which is why we don’t use much of memcache and Redis in the first place – they don’t help.</p><p>在我们的数据库中，重要的是位于InnoDB缓冲池中的16 KB数据库页面。所以我们做的很简单：我们在内存上花钱，预热InnoDB缓冲池，然后从内存中提供数据。我们可以证明是可行的：InnoDB可以以memcache的速度从内存中进行点读取，这就是为什么我们不这样做首先使用大量的Memcache和Redis –它们无济于事。</p><p> We can also capacity test our databases, and we will find that as we increase the load the number of disk reads does not go up. Data is served from memory, and not from disk.</p><p> 我们还可以对数据库进行容量测试，我们会发现随着负载的增加，磁盘读取次数不会增加。数据是从内存而不是磁盘提供的。</p><p> So what we care about is disk writes. They are the same on every instance of a replication chain, because of the shared nothing architecture of replication: Each chain member must apply the same set of changes from the primary down to all leaf replicas.</p><p> 因此，我们关心的是磁盘写入。由于没有共享的复制体系结构，因此在复制链的每个实例上它们都是相同的：每个链成员必须应用相同的更改集（从主副本到所有叶副本）。</p><p>  On Commit, we write out 1 KB (or more) of Redo Log. The write happens with  O_DIRECT to XFS, and there is a lot of thought that goes into these few words.</p><p>  在提交时，我们写出1 KB（或更多）的Redo Log。 O_DIRECT写入XFS时会发生这种情况，这几句话引起了很多思考。</p><p> Because it is  O_DIRECT, the write already writes to disk. There is no need to  fsync() or  fdatasync() or anything else.</p><p> 因为它是O_DIRECT，所以写入已写入磁盘。不需要fsync（）或fdatasync（）或其他任何东西。</p><p> O_DIRECT writes also bypass the file system buffer cache, which is good, because then there is less memory pressure and less risk of paging out the database server. Usually when that happens (memory pressure forces paging on mysqld), you die a horrible and hard to debug death in latency problems and SLO violations.</p><p> O_DIRECT写入还绕过了文件系统缓冲区高速缓存，这很好，因为这样可以减少内存压力，并降低分页数据库服务器的风险。通常，当这种情况发生时（内存压力迫使在mysqld上进行页面调度），您将死于可怕的问题，并且很难在延迟问题和SLO违反中调试死亡。</p><p> We use XFS. XFS on the average takes twice as long as ext4 to write data to disk, but XFS always takes the same amount of time to write data – it has been specifically designed in a way that there is very low jitter. ext4, on the other hand, is known to have downspikes and large commit wait times every few seconds, and that is really, really bad when you run at 10.000 queries/s.</p><p> 我们使用XFS。平均而言，XFS将ext4写入磁盘所需的时间是ext4的两倍，但是XFS写入数据的时间总是相同的-它是专门为降低抖动而设计的。另一方面，已知ext4每秒钟都有下降的尖峰和大量的提交等待时间，而当您以10.000个查询/秒的速度运行时，这确实非常糟糕。 </p><p> XFS also is capable of having multiple concurrent writers on a file when the file is open with O_DIRECT. Unlike all other Linux File Systems,  it does not lock the file’s in-memory inode globally to guarantee atomic writes in this case. Such a feature is very dear to database people.</p><p>当使用O_DIRECT打开文件时，XFS还能够在文件上具有多个并发写入器。与所有其他Linux文件系统不同，在这种情况下，它不会全局锁定文件的内存inode以保证原子写入。这样的功能对于数据库人员来说是非常宝贵的。</p><p> The Commit is what users have to wait for. When it is done, the database can do the real data write later, often much later, and in the background. So what we really care for is the actual commit latency.</p><p> 提交是用户必须等待的。完成后，数据库可以稍后（通常要晚得多）在后台执行实际数据写入。因此，我们真正关心的是实际的提交延迟。</p><p> The real write happens later, in the Checkpoint.  What happens is complicated enough to justify a writeup of its own. But, to make it short, we write data to the Doublewrite Buffer in a few very large linear writes as protection against torn pages, and then we writev() a scatter I/O of 16K pages to update in place.</p><p> 真正的写入稍后在Checkpoint中发生。所发生的事情非常复杂，足以证明自己编写了自己的文章。但是，简而言之，我们通过一些非常大的线性写入将数据写入Doublewrite缓冲区，以防止页面被撕裂，然后我们将v（）分散的I / O写入16K页面以进行适当的更新。</p><p> This is different from Postgres, and enables us to avoid costly vacuum runs. We make an assumption here, and that is: Rollbacks are rare (and expensive in MySQL).</p><p> 这与Postgres不同，这使我们避免了昂贵的真空运行。我们在这里做一个假设，那就是：回滚是很少的（在MySQL中是昂贵的）。</p><p>   If you want to know if a thing can run MySQL, and how well, you simply randwrite 16KB pages in  O_DIRECT and see what happens. That’s a gross simplification compared to above, but holds up surprisingly well as a model for predicting MySQL speed and capacity.</p><p>   如果您想知道某个事物是否可以运行MySQL，以及运行的如何，您只需在O_DIRECT中随机写入16KB页面，然后看看会发生什么。与上述相比，这是一个很大的简化，但是令人惊讶的是，它可以作为预测MySQL速度和容量的模型。</p><p>  In order to see what real databases do to their disks, I went to the chosen representative for the 200-class and  blktrace’ed it.</p><p>  为了查看实际的数据库对其磁盘有何作用，我去了200级的选定代表，并对其进行了blktrace记录。</p><p> It turns out that you can’t  blktrace a single disk database machine, because the disk writes from the  blktrace will mess up the metrics from the database. It also turns out that  systemd mounts a 12GB ramdisk as  /run/user/&lt;uid&gt; that can hold a 10 minute  blktrace with no problems.</p><p> 事实证明，您无法blktrace一台磁盘数据库计算机，因为从blktrace写入磁盘会破坏数据库中的指标。还发现systemd将12GB ramdisk挂载为/ run / user /＆lt; uid＆gt;。可以保留10分钟的blktrace，没有问题。 </p><p>   The low red line is the 200 IOPS baseline load. Turns out, there are spikes, up and past 2000 IOPS.</p><p>红色低线是200 IOPS基准负载。事实证明，存在峰值，超过2000 IOPS。</p><p>  The second thing we learn: 200 IO Operations per Second it may be, but there are spikes, up to and past 2000 IOPS even. They are not long, but they happen often enough.</p><p>  我们要学习的第二件事：每秒可能有200个IO操作，但是甚至有峰值，甚至高达2000 IOPS。它们虽然不长，但经常发生。</p><p> So how long does it take to write to disk? Well, there is no disk. There is Flash Storage, and it does not even have a disk interface any more – it sits directly on the PCI bus. A Flash drive that is directly on the PCI bus is called NVME instead of SSD (it is otherwise the same drive).</p><p> 那么写入磁盘需要多长时间？好吧，没有磁盘。有闪存，甚至没有磁盘接口，它直接位于PCI总线上。直接在PCI总线上的闪存驱动器称为NVME而不是SSD（否则为同一驱动器）。</p><p>   Flash is not fast, but NVME is: The drive not only has flash, it also has (large capacitor buffered) non-volatile memory it uses to buffer write and rearrange things in the background. It also has its own ARM processor with its own operating system.</p><p>   闪存不是很快，但是NVME是：该驱动器不仅具有闪存，还具有（缓冲大电容器）非易失性存储器，用于在后台缓冲写入和重新排列内容。它还具有自己的ARM处理器和自己的操作系统。</p><p> Sometimes we have to wait a bit, but most of the time it’s really, really fast. How fast? We need to go deeper:</p><p> 有时我们需要稍等片刻，但是大多数时候它确实非常非常快。多快？我们需要更深入地研究：</p><p>  30µs or 0.03ms is the time it takes to write to memory over the PCI bus.</p><p>  30µs或0.03ms是通过PCI总线写入内存所花费的时间。</p><p> When we talk to the remote NVRAM on the other side of the PCI bus, it takes 30µs, or 0.03ms. As long as we do not overwhelm the NVRAM buffers of the drive, we have a 30µs commit latency on local memory.</p><p> 当我们与PCI总线另一侧的远程NVRAM进行通信时，这需要30µs或0.03ms。只要不使驱动器的NVRAM缓冲区不堪重负，我们在本地内存上的提交延迟就为30µs。 </p><p> Another number, which will come in handy later, is the time to cross our entire network stack within the same data center: That’s around 6 switches and routers, and it will add another 60µs to any round trip across the network in the best of all cases. So a remote flash drive never can be faster than around 100µs or 0.1ms.</p><p>另一个数字，将在以后派上用场，是时候在同一个数据中心内跨我们的整个网络堆栈：大约有6个交换机和路由器，这将使整个网络中的任何往返行程再增加60µs，这是最好的案件。因此，远程闪存驱动器永远不会比100µs或0.1ms快。</p><p>    When you plot “addresses written to over time”, you will see that there is a bunch of blocks that get written to again and again. These are the various logs and write buffers the database uses.</p><p>    当绘制“随着时间的推移写入的地址”时，您会看到有很多块被一次又一次地写入。这些是数据库使用的各种日志和写缓冲区。</p><p> The drive internally has a flash translation layer (FTL), which makes sure that these block rewrites are actually distributed across all the positions in the flash you have. This kind of Wear Leveling makes sure the drive actually can live 7-10 years before the flash wears out.</p><p> 该驱动器内部具有闪存转换层（FTL），可确保这些块重写实际上分布在您所拥有的闪存的所有位置上。这种磨损均衡技术可确保驱动器在闪光灯耗尽之前可以实际使用7-10年。</p><p> The SD card or USB memory stick you are using in your local Raspi does not have that, and neither has the on board flash in the entertainment module of your Tesla. That is why they fail rapidly as actual fixed blocks of flash are being rewritten a few thousand times. Use a proper drive with a proper FTL and that does not happen.</p><p> 您在本地Raspi中使用的SD卡或USB记忆棒没有此功能，并且Tesla的娱乐模块中也没有板载闪存。这就是为什么它们会快速失败的原因，因为实际的固定闪存块已被重写了数千次。请使用具有适当FTL的适当驱动器，但不会发生这种情况。</p><p> In the graphics above you can also see the checkpoints scatter writes happen, and you can see how they are spaced apart widely, many tens of thousands of transactions. Checkpointing allows the database to aggregate changes in in-memory pages (and the log, for persistence). All that complexity is actually saving you disk writes!</p><p> 在上面的图形中，您还可以看到检查点分散写入的发生，并且可以看到它们之间的间隔很大，成千上万的事务。通过检查点，数据库可以聚合内存页面（和日志，以保持持久性）中的更改。所有这些复杂性实际上可以节省您的磁盘写操作！</p><p> Flash does not have a single write head as disk drives do. They are not forced to do things sequentially, but in fact all the various chips in your drive can do things concurrently. Deep disk write queues are necessary to keep all these channels fed for NVME, while they are poison of SSD and HDD.</p><p> 闪存没有磁盘驱动器那样的单个写头。他们没有被迫按顺序执行事务，但实际上，驱动器中的所有各种芯片都可以同时执行事务。深盘写入队列对于将所有这些通道馈送给NVME是必不可少的，而它们却是SSD和HDD的毒药。</p><p>   A low concurrency hierarchy such as this one is not even touching the true write potential of a single NVME: A single drive can do 20.000 disk writes per second, sequentially in a single thread, if you are pushing really hard, but it can do 800.000 disk writes per second if you are feeding it with sufficient concurrency.</p><p>   诸如此类的低并发层次结构甚至没有达到单个NVME的真正写入潜力：如果您非常努力，单个驱动器每秒可以在单个线程中顺序执行每秒20.000磁盘写入操作，但是它可以完成800.000如果您以足够的并发速率进行写入，则每秒写入磁盘的次数。 </p><p> These are two other important numbers to keep in mind. They are the reason why “IOPS Quotas” should be a thing of the past – it is impossible for any reasonable workload to actually exhaust a single NVME drive, and distributed storages usually have hundreds of them.</p><p>这是另外两个需要牢记的重要数字。这就是“ IOPS配额”已成为过去的原因-任何合理的工作量都不可能真正耗尽单个NVME驱动器，而分布式存储通常有数百个。</p><p> Let’s have a look at the latency diagram from above, again, and check the distribution of completion times in a Latency Histogram. It tells us how prevalent slow writes are over the observation window:</p><p> 让我们再次从上方查看延迟图，并检查延迟直方图中的完成时间分布。它告诉我们观察窗口中普遍存在缓慢写入：</p><p>   So even with these curtains in the latency over time diagram, we can see that 90% of all writes happen in the 30µs window, and there are noteworthy amounts of writes in the &gt;100µs class (they do exist, but they are so rare, they do not show up in the histogram).</p><p>   因此，即使在时间延迟图上有这些限制，我们也可以看到90％的写入发生在30µs的窗口中，并且在> 100µs类中有值得注意的写入数量（它们确实存在，但它们如此罕见，它们不会显示在直方图中）。</p><p>    We can see that the grossly simplified 16 KB Randwrite model is actually not wrong, even if it is an oversimplification. Modelling more precisely does not significantly impact predictive performance, though, and this way it’s only a few lines of fio to get data.</p><p>    我们可以看到，经过简化的16 KB Randwrite模型实际上并没有错，即使它过于简化也是如此。不过，更精确地建模不会对预测性能产生重大影响，因此，仅几行即可获取数据。</p><p> So that’s our tour of the 200-class from the underside – this is how your storage sees your commits.</p><p> 这就是我们从底面进行的200班之旅–这就是您的存储空间如何查看提交的内容。</p><p>  I chose this particular database as a member of the 2000 Commit/s Class, also because it is the oldest and wildest of the database replication hierarchies that we have. At a time it was our performance bottleneck and was running over 20.000 queries/s with spikes into the 32.000, and with the hardware of that time that was not stable at all.</p><p>  我选择此特定数据库作为2000 Commit / s类的成员，也是因为它是我们拥有的最古老和最狂野的数据库复制层次结构。一次，这是我们的性能瓶颈，运行速度超过20.000次/秒，峰值达到32.000，并且当时的硬件根本不稳定。</p><p>     True, but see, even a big and old monster such as this one can live with the working set in memory, completely. You, too, can be a successful database performance consultant: Say “Buy more memory!” and “There is an index missing” as needed (add “That’s going to be expensive”, if you work for SAP or Oracle).</p><p>     的确如此，但请注意，即使是像这样的大怪兽，也可以完全将工作集保存在内存中。您也可以成为成功的数据库性能顾问：说“买更多的内存！”然后根据需要添加“缺少索引”（如果您在SAP或Oracle工作，则添加“这将非常昂贵”）。 </p><p>         We can see horizontal latency bands at fixed latencies. They expose internal structure of the Flash storage that I have not enough knowledge of. Grumble!</p><p>我们可以看到固定延迟的水平延迟带。它们公开了我所不了解的Flash存储的内部结构。叽！</p><p> We do see dark green, fixed bands of latency layers at 30µs, 120µs and 220µs. The higher layers show a repeating pattern over time. These are shenanigans of the FTL that are visible as delays to external users, but they are way below any critical level. I do not know enough about flash internals to explain these things, and that bothers me. A lot.</p><p> 我们的确看到深绿色固定的延迟层频带，分别为30µs，120µs和220µs。较高的层随着时间的推移显示出重复的模式。这些是FTL的恶作剧，对于外部用户来说是可见的延迟，但它们远低于任何关键级别。我对Flash内部知识了解不足，无法解释这些事情，这使我感到困扰。很多。</p><p>   Our 30µs, 120µs and 220µs bands as bumps in the histogram. Anything below 500µs is probably immaterial in terms of worry.</p><p>   我们的30µs，120µs和220µs频带作为直方图中的凸起。就担忧而言，任何低于500µs的时间都可能无关紧要。</p><p>  The chosen example is a  database as a queue, that logs changes from one data set for processing and creation of a materialized view in another data set. It is an example of  CQRS in Fowlerspeak.</p><p>  所选示例是一个数据库队列，该数据库记录来自一个数据集的更改，以便在另一个数据集中处理和创建实例化视图。这是Fowlerspeak中CQRS的一个示例。</p><p>    During the sampling, they ran at 2500 commit/s, with spikes up to 7500. The hardware will do up to 20.000 with them, and at times the queue will do as well.</p><p>    在采样过程中，它们以2500 commit / s的速度运行，峰值达到7500。硬件将对它们进行高达20.000的处理，有时队列也将执行。</p><p>    Turns out, writes to a Queue Databases keep the Queues to the NVME busy to their fullest potential.</p><p>    事实证明，写入队列数据库使使NVME的队列保持最大的忙碌状态。</p><p>    Well, if you kept up to this point you are probably not surprised to see write latency curtains, and you will probably also be seeing that they mirror the queue depth changes without me pointing that out specifically.</p><p>    好吧，如果您一直坚持到这一点，您可能不会对写入延迟的延迟感到惊讶，并且可能还会看到它们反映了队列深度的变化，而无需我特别指出。 </p><p>   We do see the curtains under the microscope, true, but look at this dark green band of fast writes below all of this.</p><p>我们确实看到了显微镜下的窗帘，这是真的，但是请看一下所有这些下面的快速写入的深绿色带。</p><p> As you can see in the dark green band of fast writes below all of the curtains, the FTL of this particular drive is kept well busy. It is also worth it’s money - no gross delays due to internal reorg is what you get when you pay for enterprise flash instead of home equipment.</p><p> 如您所见，在所有帘幕下方的快速写入的深绿色带中，该特定驱动器的FTL一直很忙。物有所值-当您为企业级闪存而非家用设备付费时，不会因内部重组而造成总延迟。</p><p>   By far the most writes are still to NVRAM drive-side. Above 0.25ms we are basically clean.</p><p>   到目前为止，大多数写入仍在NVRAM驱动器端。 0.25ms以上，我们基本上是干净的。</p><p> Smooth customer experience, even with an abusive customer such as this queue-database – bring it on, we can take your writes.</p><p> 平稳的客户体验，即使遇到像这样的队列数据库这样的滥用客户，也要启用它，我们可以接听您的意见。</p><p> Histograms are fun, by the way. We can for example count disk writes per block address:</p><p> 直方图很有趣。例如，我们可以计算每个块地址的磁盘写操作：</p><p>   In a database as a queue, if all works well, the tables change a lot, but never grow. There are very few actual data pages that make up the small queue table, but very many inserts and deletes. We have to persist them to disk to be ACID, in the log. But there is hardly any need to checkpoint, and even then it’s very few full pages to write out.</p><p>   在作为队列的数据库中，如果一切正常，则表会发生很大变化，但永远不会增长。组成小型队列表的实际数据页面很少，但是插入和删除却很多。我们必须在日志中将它们持久保存到磁盘上以成为ACID。但是几乎不需要检查点，即使那样，也几乎没有整页要写出来。</p><p>  If you made it this far, welcome and get your storage nerd badge at the counter.</p><p>  如果您走到了这一步，欢迎光临柜台并获得书呆子存储徽章。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.koehntopp.info/2021/02/25/mysql-from-below.html">https://blog.koehntopp.info/2021/02/25/mysql-from-below.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/mysql/">#mysql</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/来自/">#来自</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>