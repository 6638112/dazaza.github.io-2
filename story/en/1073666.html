<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>2022年使用JAXUsing JAX in 2022</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Using JAX in 2022<br/>2022年使用JAX</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-17 02:09:21</div><div class="page_narrow text-break page_content"><p>Since  JAX hit the scene in late 2018, it has been steadily growing in popularity, and for good reason. DeepMind  announced in 2020 that it is using JAX to accelerate its research, and a growing number of publications and projects from Google Brain and others are using JAX.  With all of this buzz, it seems like JAX is the next big Deep Learning framework, right?</p><p>自JAX于2018年底问世以来，它的受欢迎程度一直在稳步增长，这是有充分理由的。DeepMind在2020年宣布，它正在使用JAX来加速其研究，来自Google Brain和其他公司的越来越多的出版物和项目正在使用JAX。有了这么多的热议，JAX似乎是下一个大型深度学习框架，对吧？</p><p> Wrong. In this article we’ll clarify what JAX is (and isn’t), why you should care (or shouldn&#39;t, but you probably should), and whether you should (or shouldn’t) use it.</p><p>错误的在本文中，我们将阐明JAX是什么（不是），为什么应该关心（或不应该，但可能应该），以及是否应该（或不应该）使用它。</p><p>  If you&#39;re already familiar with JAX and want to skip the benchmarks, you can jump ahead to our recommendations on when to use it  here</p><p>如果你&#39；如果您已经熟悉JAX，并且想跳过基准测试，那么您可以在这里跳转到我们关于何时使用它的建议</p><p>  It may be best to start off with what JAX is  not. JAX is  not a Deep Learning framework or library, and it is not designed to ever be a Deep Learning framework or library in and of itself. In a sentence,  JAX is a numerical computing library which incorporates composable function transformations [ 1]. As we can see, Deep Learning is just a small subset of what JAX can do:</p><p>最好从JAX不是的东西开始。JAX不是一个深度学习框架或库，它本身也不是一个深度学习框架或库。总之，JAX是一个数字计算库，它包含了可组合函数转换[1]。正如我们所见，深度学习只是JAX所能做的一小部分：</p><p>   In short -   speed. This is the universal aspect of JAX that is relevant for any use case.</p><p>简而言之，就是速度。这是JAX的通用方面，与任何用例都相关。</p><p> Let&#39;s sum the first three powers of a matrix (element-wise) with both NumPy and JAX. First up is our NumPy implementation:</p><p>让&#39；s用NumPy和JAX求矩阵的前三次幂的和（按元素）。首先是我们的NumPy实现：</p><p>   We find that this calculation takes about  478 ms. Next, we implement this calculation with JAX:</p><p>我们发现这个计算大约需要478毫秒。接下来，我们用JAX实现这个计算：</p><p>   JAX performs this calculation in only  5.54 ms - over  86 times faster than NumPy.</p><p>JAX只需5.54毫秒即可完成此计算，比NumPy快86倍多。</p><p>    Things are not quite as simple as &#34;use JAX and your programs will be 86 times faster&#34;, but there are still a ton of reasons to use JAX. Since JAX provides a general foundation for scientific computing, it will be useful to different people in different fields for different reasons. Fundamentally,  if you are in  any field relating to scientific computing,  you should care about JAX.</p><p>事情并不像&#34那么简单；使用JAX，你的程序会快86倍；，但使用JAX仍然有很多理由。由于JAX为科学计算提供了一个通用的基础，对于不同的领域，不同的人有不同的原因。基本上，如果你在任何与科学计算相关的领域，你都应该关心JAX。</p><p>  1. NumPy on Accelerators - NumPy is one of the fundamental packages for scientific computing with Python, but it is compatible only with CPU. JAX provides an implementation of NumPy (with a near-identical API) that works on  both GPU  and TPU extremely easily. For many users, this  alone is sufficient to justify the use of JAX.</p><p>1.加速器上的NumPy——NumPy是使用Python进行科学计算的基本软件包之一，但它只与CPU兼容。JAX提供了NumPy的一个实现（具有几乎相同的API），可以非常轻松地在GPU和TPU上工作。对于许多用户来说，仅此一点就足以证明使用JAX的合理性。</p><p> 2. XLA - XLA, or Accelerated Linear Algebra, is a whole-program optimizing compiler, designed specifically for linear algebra. JAX is built on XLA, raising the computational-speed ceiling significantly [ 1].</p><p>2、XLA—XLA，或加速线性代数，是一个专门为线性代数设计的完整的程序优化编译器。Jax是基于XLA构建的，极大地提高了计算速度上限[1 ]。</p><p> 3. JIT - JAX allows you to transform your  own functions into just-in-time (JIT) compiled versions using XLA [ 7]. This means that you can increase computation speed by potentially  orders of magnitude by adding a simple function decorator to your computational functions.</p><p>3、JIT JAX允许您使用XLA（7）将您自己的函数转换成即时（JIT）编译版本。这意味着您可以通过在计算函数中添加一个简单的函数装饰器，将计算速度潜在地提高几个数量级。</p><p> 4. Auto-differentiation - The JAX documentation refers to JAX as &#34;Autograd and XLA, brought together&#34; [ 1]. The ability to automatically differentiate is crucial in many areas of scientific computing, and JAX provides several powerful auto-differentiation tools.</p><p>4.自动区分——JAX文档将JAX称为&#34；AutoGrad和XLA，汇集了34个；[ 1]. 自动区分的能力在科学计算的许多领域都至关重要，JAX提供了几种强大的自动区分工具。</p><p> 5. Deep Learning - While not a Deep Learning framework itself, JAX certainly provides a more-than-sufficient foundation for Deep Learning purposes. There are many libraries built on top of JAX that seek to build out Deep Learning capabilities, including  Flax,  Haiku, and  Elegy. We even highlighted JAX as a “framework” to watch in our recent  PyTorch vs TensorFlow article,  recommending its use for TPU-based Deep Learning research. JAX&#39;s highly efficient computations of Hessians are also relevant for Deep Learning, given that they make higher-order optimization techniques much more feasible.</p><p>5、深度学习——虽然不是一个深刻的学习框架本身，但JAX当然为深入学习的目的提供了足够多的基础。有许多基于JAX构建的库寻求构建深度学习能力，包括Flax、俳句和挽歌。在最近的PyTorch vs TensorFlow文章中，我们甚至强调JAX是一个值得关注的“框架”，建议将其用于基于TPU的深度学习研究。JAX&#39；Hessians的高效计算也与深度学习相关，因为它们使高阶优化技术更加可行。</p><p> 6. General Differentiable Programming Paradigm - While it is certainly possible to use JAX in order to build and train Deep Learning models, it also provides a framework for  general Differentiable Programming. This means that JAX can exploit  prior knowledge in a given field, built up through decades of research, by using a model-based Machine Learning approach to solving a problem.</p><p>6.通用可微编程范式——虽然使用JAX来构建和训练深度学习模型当然是可能的，但它也为通用可微编程提供了一个框架。这意味着，通过使用基于模型的机器学习方法来解决问题，JAX可以利用经过几十年研究积累起来的特定领域的先验知识。</p><p>     XLA, or Accelerated Linear Algebra, lies at the foundation of what makes JAX so powerful. Developed by Google, XLA is a domain-specific, graph-based, just-in-time compiler [ 2] for linear algebra that can significantly increase computation speed through a variety of whole-program optimizations [ 3].</p><p>XLA，或加速线性代数，正是JAX强大的基础。由谷歌开发的XLA是一种基于领域的、基于图形的、即时的线性代数编译器[2 ]，它可以通过各种全局程序优化显著地提高计算速度[3 ]。</p><p> In one example [ 2], XLA boosts BERT training speed by almost  7.3 times from a  computational standpoint alone, but lowered memory usage as a result of using XLA  also enables gradient accumulation, resulting in a staggering  12 times increase to computational throughput.</p><p>在一个例子（2）中，XLA单独从计算角度提高了伯特训练速度几乎7.3倍，但是由于使用XLA也使得存储器使用率降低，从而使得梯度累加，导致计算吞吐量的惊人增长12倍。</p><p>  XLA is baked into the very DNA of JAX - from their logos alone you can see how much the successes of JAX rely on XLA.</p><p>XLA被烘烤成JAX的DNA，从他们的标志中你可以看到JAX的成功依赖XLA。</p><p>   Answering exactly why XLA is such a big deal can yield a very technical (and long) discussion. For our purposes, it suffices to say that XLA is important because it  significantly increases execution speed and lowers memory usage by  fusing low-level operations.</p><p>正确回答为什么XLA是如此大的交易可以产生一个非常技术性的（和长期的）讨论。对于我们的目的，足够的XLA是重要的，因为它大大提高了执行速度，并通过融合低级操作降低内存使用。</p><p> XLA doesn’t precompile individual operations into compute kernels, but instead compiles the  entire graph into a sequence of compute kernels generated  specifically for that graph.</p><p>XLA不预先将单个操作编译成计算核，而是将整个图编译成一个专门为该图生成的计算内核序列。</p><p>  This approach increases speed by not performing needless kernel launches, as well as taking advantage of local information for optimization  [3]. Since XLA doesn’t materialize intermediate arrays in an operation sequence (instead keeping values in GPU registers and streaming them  [3], using XLA also reduces memory consumption.</p><p>这种方法通过不执行不必要的内核启动以及利用局部信息进行优化来提高速度[3]。由于XLA不在操作序列中实现中间数组（而是在GPU寄存器中保持值并将它们流到3），使用XLA也减少了内存消耗。</p><p>  This lowered memory consumption yields a  further speed boost given that (i) memory is often the limiting factor in computing with GPUs, and (ii) XLA does not waste time performing extraneous data movement.</p><p>这种降低的内存消耗会产生进一步的速度提升，因为（i）内存通常是用GPU计算的限制因素，并且（ii）XLA不会浪费执行无关数据移动的时间。</p><p> While operation fusion (or  kernel fusion) is the flagship feature of XLA, it should be noted that XLA also performs a  ton of other whole-program optimizations, like specializing to known tensor shapes (allowing for more aggressive constant propagation), analyzing and scheduling memory usage to eliminate intermediate storage buffers [ 4], performing memory layout operations, and only computing subsets of requested values if not all of them are being returned [ 5].</p><p>虽然操作融合（或核融合）是XLA的旗舰特征，但应该注意到XLA还执行了大量其他的整体程序优化，例如专门针对已知张量形状（允许更积极的恒定传播），分析和调度内存使用以消除中间存储缓冲器[4 ]，执行内存布局操作，并仅计算请求值的子集（如果不是全部返回的话）[5]。</p><p> Since all JAX operations are implemented in terms of operations in XLA, JAX has a unified language for computation that allows it to run seamlessly across CPU, TPU, and GPU, with library calls getting just-in-time compiled and executed [ 1].</p><p>由于所有Jax操作都是在XLA的操作中实现的，JAX有一个统一的计算语言，允许它在CPU、TPU和GPU之间无缝运行，而库调用及时编译和执行（1）。</p><p>  If none of the jargon above makes sense to you, don’t worry - just know that XLA is a very fast compiler which lies at the foundation of what makes JAX uniquely powerful and simple to use on a diverse range of hardware.</p><p>如果上面的术语没有一个对你有意义，不要担心——只知道XLA是一个非常快的编译器，它是JAX在各种各样的硬件上使用的唯一强大和简单的基础。</p><p>    So far, we’ve talked about XLA and how it allows JAX to implement NumPy on accelerators; but recall that this was only one half of our definition of JAX. JAX provides tools not only for powerful scientific computing, but also for  composable function transformations.</p><p>到目前为止，我们已经谈到了XLA，以及它如何允许JAX在加速器上实现NoMPY；但请记住，这只是我们对JAX定义的一半。JAX不仅为强大的科学计算提供了工具，还为可组合的函数转换提供了工具。</p><p> Quite simply, a function transformation is an  operator on a function whose output is  another function. If we use the gradient function transformation on a scalar-valued function  f(x), then we get a vector-valued function  f &#39;(x) which gives the gradient of the function at any point in the domain of  f(x).</p><p>简单地说，函数变换是一个函数上的运算符，其输出是另一个函数。如果我们对标量值函数f（x）使用梯度函数变换，那么我们得到一个向量值函数f&#39；（x） 它给出了函数在f（x）域中任意点的梯度。</p><p>  JAX incorporates an extensible system for such function transformations, and has four main transformations of interest to the typical user:</p><p>JAX为此类功能转换整合了一个可扩展系统，并有四个典型用户感兴趣的主要转换：</p><p>  Let’s take a look at each of these transformations in turn and talk about why they&#39;re so exciting.</p><p>让我们依次看看这些转变，并讨论它们的原因&#39；你太激动人心了。</p><p>  To be able to train Machine Learning models, one needs to be able to perform backpropagation. Rather than compute the gradient of the loss function at a certain  point as TensorFlow or PyTorch does by backpropagating through the computation graph, the JAX  grad() function transformation outputs the gradient  function, which can then be evaluated at any point in its domain.</p><p>为了能够训练机器学习模型，需要能够执行反向传播。与TensorFlow或Pytork通过在计算图中反向传播来计算损失函数在某一点的梯度不同，JAX grad（）函数变换输出梯度函数，然后可以在其域中的任何点对其进行计算。</p><p>  Automatic differentiation in JAX is  extremely powerful, which stems partially from JAX’s flexibility in “where” you can compute gradients. With  grad(), you can  differentiate through native Python and NumPy functions [ 6], such as loops, branches, recursion, closures, and “ PyTrees” (e.g. dictionaries).</p><p>JAX中的自动区分功能非常强大，这部分源于JAX在“何处”可以计算梯度方面的灵活性。使用grad（），您可以通过本机Python和NumPy函数[6]进行区分，例如循环、分支、递归、闭包和“PyTrees”（例如字典）。</p><p> Let’s look at an example - we’ll define a rectified cube function  f(x) = abs(x 3) with Python control flow. This implementation is obviously not the most computationally efficient approach, but it helps us highlight how  grad() works through native Python control flow and loops nested in conditionals.</p><p>让我们看一个例子——我们将用Python控制流定义一个经过修正的立方体函数f（x）=abs（x3）。这个实现显然不是计算效率最高的方法，但它帮助我们强调grad（）如何通过原生Python控制流和嵌套在条件中的循环工作。</p><p> def rectified_cube(x): r = 1 if x &lt; 0.: for i in range(3): r *= x r = -r else: for i in range(3): r *= x return rgradient_function = grad(rectified_cube)print(f&#34;x = 2 f(x) = {rectified_cube(2.)} f&#39;(x) = 3*x^2 = {gradient_function(2.)}&#34;)print(f&#34;x = -3 f(x) = {rectified_cube(-3.)} f&#39;(x) = -3*x^2 = {gradient_function(-3.)}&#34;)</p><p>def_立方体（x）：如果x&lt；0.：对于范围（3）中的i:r*=xr=-r其他：对于范围（3）中的i:r*=x返回rgradient_函数=grad（矫正的_立方体）打印（f&#34；x=2f（x）={矫正的_立方体（2.）}f&#39；（x） =3*x^2={gradient_函数（2.）}&#34;)打印（f&#34；x=-3F（x）={正方体（-3.）}f&#39；（x） =-3*x^2={gradient_函数（-3.）}&#34;)</p><p> x = 2 f(x) = 8.0 f&#39;(x) = 3*x^2 = 12.0x = -3 f(x) = 27.0 f&#39;(x) = -3*x^2 = -27.0</p><p>x=2f（x）=8.0f&#39；（x） =3*x^2=12.0x=-3f（x）=27.0f&#39；（x） =-3*x^2=-27.0</p><p> We can see that we get the expected results when evaluating the function and its derivative at  x=2 and   x=-3 .</p><p>我们可以看到，在x=2和x=-3时计算函数及其导数时，我们得到了预期的结果。</p><p>  JAX makes it easy to differentiate to  any order by the repeated application of  grad().</p><p>通过重复应用grad（），JAX可以轻松区分任何顺序。</p><p> # for x &gt;= 0: f(x)=x^3 =&gt; f&#39;(x)=3*x^2 =&gt; f&#39;&#39;(x)=3*2*x =&gt; f&#39;&#39;&#39;(x)=6third_deriv = grad(grad(grad(rectified_cube)))for i in range(5): print(third_deriv(float(i)))</p><p>#对于x&gt；=0:f（x）=x^3=&gt；f&#39；（x） =3*x^2=&gt；f&#39&#39;（x） =3*2*x=&gt；f&#39&#39;&#39;（x） =6third_deriv=grad（grad（grad（grad（rectived_cube）））表示范围（5）中的i：打印（third_deriv（float（i）））</p><p>  We can see that the evaluation of several inputs to the third derivative of our function gives the constant expected output of  f &#39;&#39;&#39;(x)=6 .</p><p>我们可以看到，对函数的三阶导数的几个输入求值得到f&#39的恒定预期输出&#39;&#39;（x） =6。</p><p> From a more general perspective, the ability to take multiple derivatives in a fast and easy manner is of practical use to many more general computational fields beyond Deep Learning, such as the study of Dynamical Systems.</p><p>从更一般的角度来看，快速、简单地获取多个导数的能力对于深度学习以外的许多更一般的计算领域都有实际用途，例如动力系统的研究。</p><p>   As you would expect,  grad() takes the  gradient of a  scalar-valued function, meaning a function which maps scalars/vectors to scalars. The gradient of such a function is useful for e.g. backpropagation, where we train a model by backpropagating from a (scalar) loss function to update our model weights.</p><p>正如您所料，grad（）采用标量值函数的梯度，这意味着将标量/向量映射到标量的函数。这种函数的梯度对于反向传播非常有用，例如，我们通过从（标量）损失函数反向传播来更新模型权重来训练模型。</p><p> While  grad() is sufficient for a variety of projects, it is not the only type of differentiation JAX can perform.</p><p>虽然grad（）对于各种项目来说都足够了，但它并不是JAX可以执行的唯一一种差异化类型。</p><p>  For  vector-valued  functions which map vectors to vectors, the analogue to the gradient is the  Jacobian. With the function transformations  jacfwd() and  jacrev(), JAX returns a function which yields the Jacobian when evaluated at a point in the domain.</p><p>对于将向量映射到向量的向量值函数，与梯度类似的是雅可比矩阵。通过函数转换jacfwd（）和jacrev（），JAX返回一个函数，当在域中的某个点求值时，该函数将生成雅可比矩阵。</p><p> def mapping(v): x = v[0] y = v[1] z = v[2] return jnp.array([x*x, y*z])# 3 inputs, 2 outputs# [d/dx x^2 , d/dy x^2, d/dz x^2]# [d/dx y*z , d/dy y*z, d/dz y*z]# [2*x , 0, 0]# [0 , z, y]f = jax.jacfwd(mapping)v = jnp.array([4., 5., 9.])print(f(v))</p><p>def映射（v）：x=v[0]y=v[1]z=v[2]返回jnp。数组（[x*x，y*z]）#3个输入，2个输出#[d/dx x^2，d/dy x^2，d/dz x^2]#[d/dx y*z，d/dy*z，d/dz y*z]#[2*x，0，0]#[0，z，y]f=jax。jacfwd（映射）v=jnp。数组（[4,5,9.]））印刷品（f（v））</p><p>  You can alternatively use a Jacobian, for example, in order to more-efficiently compute the gradient of a function with respect to a weight matrix for each datum in a data matrix.</p><p>例如，您也可以使用雅可比矩阵，以便更有效地计算函数相对于数据矩阵中每个基准的权重矩阵的梯度。</p><p>  Perhaps one of the most exciting aspects of JAX from a Deep Learning perspective is that it makes computing  Hessians exceedingly  easy  and efficient. Because of XLA,  JAX can compute Hessians remarkably faster than PyTorch, which makes it much more practical to implement  higher-order optimization  techniques like  AdaHessian. This fact  alone could be justification enough to use JAX for some practitioners.</p><p>从深度学习的角度来看，JAX最令人兴奋的一个方面可能是，它使计算黑森人变得极其简单和高效。由于XLA，JAX可以比PyTrac计算Hessian的速度快得多，这使得实现高阶优化技术如AdHessian更加实用。这一事实本身就足以为一些从业者提供使用JAX的理由。</p><p>   The slowest run took 8.14 times longer than the fastest. This could mean that an intermediate result is being cached.10 loops, best of 5: 16.3 ms per loop</p><p>最慢的跑步比最快的跑长8.14倍。这可能意味着正在缓存中间结果。10次循环，最佳5次：每次循环16.3毫秒</p><p> As we can see, the calculation takes about  16.3 ms. Let&#39;s try the same calculation in JAX:</p><p>如我们所见，计算大约需要16.3毫秒。Let&#39；让我们在JAX中尝试同样的计算：</p><p>  The slowest run took 47.27 times longer than the fastest. This could mean that an intermediate result is being cached.1000 loops, best of 5: 1.55 ms per loop</p><p>最慢的跑步比最快的跑长47.27倍。这可能意味着正在缓存中间结果。1000圈，最佳5圈：每圈1.55毫秒</p><p>    JAX can even compute  Jacobian-vector products and  vector-Jacobian products. Consider a smooth map between smooth manifolds. JAX can compute the pushforward of this map, mapping tangent vectors at points on the one manifold to tangent vectors on another.</p><p>JAX甚至可以计算雅可比矢量积和雅可比矢量积。考虑光滑流形之间的光滑映射。JAX可以计算这个映射的推进，将一个流形上的点的切向量映射到另一个流形上的切向量。</p><p>  If this part is confusing or unfamiliar, don’t worry! This is an advanced topic and probably not (in and of itself) of relevance or interest to the typical user. We point out the existence of this capability simply to highlight the fact that JAX provides a  very powerful foundation for a  wide variety of computational tasks. For example, pushforwards are important in the field of  Differential Geometry, which we might use JAX to study.</p><p>如果这部分令人困惑或不熟悉，不要担心！这是一个高级主题，可能与典型用户无关。我们指出这种能力的存在只是为了强调JAX为各种各样的计算任务提供了非常强大的基础。例如，向前推在微分几何领域很重要，我们可以使用JAX来研究。</p><p>  Moving past the mathematical to a more practical/computational transformation, we arrive at  vmap(). Consider the case in which we want to repeatedly apply a function to a set of objects. Let’s consider, for example, the task of adding two lists of numbers. The naive way to implement such an operation is to simply utilize a  for loop - i.e. for each number in the first list, add it to the corresponding value in the second list, and write the result to a new list.</p><p>通过数学转换到更实际的/计算转换，我们得到了vmap（）。考虑一下我们想在一组对象上重复应用一个函数的情况。让我们考虑，例如，添加两个数字列表的任务。实现这种操作的简单方法是简单地使用for循环，即对于第一个列表中的每个数字，将其添加到第二个列表中的相应值，并将结果写入一个新列表。</p><p>  With the  vmap() transformation, JAX performs the same computation but  pushes the loop down to primitive operations for better performance [ 6], resulting in an automatically vectorized version of the computation.</p><p>通过vmap（）转换，JAX执行相同的计算，但将循环向下推到基本操作以获得更好的性能[6]，从而生成计算的自动矢量化版本。</p><p>  Of course, we could’ve simply defined our lists as JAX arrays and used JAX&#39;s array addition, but  vmap() is still useful for many reasons.</p><p>当然，我们可以简单地将列表定义为JAX数组，并使用JAX&#39；s数组添加，但由于许多原因，vmap（）仍然很有用。</p><p> One basic reason is that we can write operations in more native Python code and then  vmap() it, leading to highly Pythonic and possibly more readable code. Another reason is of course  generalizing to the cases in which there is  no simple vectorized alternative to implement.</p><p>一个基本原因是，我们可以用更多的本地Python代码编写操作，然后使用vmap（）编写操作，从而生成高度Pythonic的、可能更可读的代码。另一个原因当然是推广到没有简单的矢量化替代方案来实现的情况。</p><p>  Distributed computing has become increasingly important year-over-year, and this holds especially true in Deep Learning, where SOTA models have grown to absolutely astronomical sizes as you can see in the graph below.  GPT-4, for example, will have over  100  trillion parameters.</p><p>分布式计算一年比一年变得越来越重要，这在深度学习中尤其如此，正如下图所示，SOTA模型已经发展到绝对天文数字。例如，GPT-4将有超过100万亿个参数。</p><p>  We&#39;ve discussed above how, thanks to XLA, JAX can compute on  an accelerator easily, but JAX can also compute with  multiple accelerators easily,  performing distributed training of SPMD programs  with a single command -  pmap().</p><p>我们&#39；以上讨论了如何利用XLA，JAX可以轻松地计算加速器，但JAX也可以容易地用多个加速器计算，用单个命令PMAP-（）来执行SPMD程序的分布式训练。</p><p> Consider the example of vector-matrix multiplication. Let’s say we are performing this computation by sequentially computing the dot product of the vector with each row of the matrix. We would need to push these computations through our hardware one at a time.</p><p>考虑向量矩阵乘法的例子。假设我们通过顺序计算向量与矩阵每一行的点积来执行这个计算。我们需要一次一个地通过硬件完成这些计算。</p><p>  With JAX, we can  easily distribute these computations across 4 TPUs by  simply wrapping our operation in  pmap(). This allows us to concurrently perform one dot product on each TPU, significantly increasing our computation speed (for large computations).</p><p>使用JAX，只需将操作包装在pmap（）中，我们就可以轻松地将这些计算分布到4个TPU中。这允许我们在每个TPU上同时执行一个点积，显著提高了计算速度（对于大型计算）。</p><p>  What is very noticeable here is how absolutely minimal the change to our code was. Since JAX is built on XLA, we can change how we map computations to hardware with ease.</p><p>这里非常值得注意的是，对我们的代码所做的更改是如此之小。由于JAX是建立在XLA上的，我们可以轻松地将计算映射到硬件。</p><p>   Just-in-time, or JIT compilation, is a method of executing code that lies between interpretation and ahead-of-time (AoT) compilation. The important fact is that a  JIT-compiler will compile code at runtime into a fast executable, at the cost of a slower first run.</p><p>即时编译（Just-in-time，简称JIT编译）是一种执行介于解释和提前编译（AoT）之间的代码的方法。重要的事实是，JIT编译器会在运行时将代码编译成快速的可执行文件，代价是第一次运行的速度较慢。</p><p> With JIT compilation, code is compiled at runtime, so there is some  initial overhead during the first run of a program given that the code needs to be compiled  and executed. AoT compilation therefore may outperform JIT on a first pass; however, for repeated execution, a JIT-compiled program will use the  previously-compiled, cached code to execute very quickly.  A JIT-compiled program can theoretically run even  faster than the same program if it were AoT compiled given that JIT compilers can use local information for increased optimization by exploiting the fact that the code is compiled on the same machine it will be executed on.</p><p>在JIT编译中，代码是在运行时编译的，因此在程序第一次运行时，由于需要编译和执行代码，因此会有一些初始开销。因此，AoT编译在第一次通过时可能会优于JIT；然而，对于重复执行，JIT编译的程序将使用之前编译的缓存代码来快速执行。JIT编译的程序在理论上可以比AoT编译的同一程序运行得更快，因为JIT编译器可以利用代码在将在其上执行的同一台机器上编译的事实，使用本地信息进行优化。</p><p> Lines can get blurry. For example, when Python is run, it is compiled into bytecode, which is then either interpreted by Python’s virtual machine (e.g. CPython), or compiled to machine code (PyPy). If these details are confusing, don&#39;t worry. The important point is that  JIT-compiling JAX programs allows them to execute extremely quickly.</p><p>线条会变得模糊。例如，当Python运行时，它被编译成字节码，然后由Python的虚拟机（例如CPython）解释字节码，或者编译成机器码（PyPy）。如果这些细节令人困惑，请不要&#39；别担心。重要的一点是，JIT编译JAX程序允许它们以极快的速度执行。</p><p>  XLA primitives are JIT compiled, but JAX  also lets you  JIT compile your own Python functions into XLA-optimized kernels, either as a function decorator  @jit or as a function itself  jit() [ 1].</p><p>XLA原语是JIT编译的，但是JAX也允许JIT将自己的Python函数编译成XLA优化内核，既可以作为函数装饰器JIT，也可以作为函数本身JITE（）1。</p><p> Rather than dispatch kernels to a GPU one operations at a time, JIT will  compile the sequence of operations together into one kernel using XLA, giving an end-to-end compiled, efficient XLA implementation  of your function [ 6][ 7].</p><p>JIT将一次一次的操作调度到GPU中，而不是使用XLA将操作序列编译成一个内核，给出了函数的端到端编译的、高效的XLA实现[6 ] [7 ]。</p><p> To provide an example, let’s define a function which computes the sum of the first three powers of a matrix of values. We compute this function on a 5000 x 5000 matrix three times - once with NumPy, once with JAX, and once with JAX on a JIT-compiled version of the function. First, we perform the experiment on CPU:</p><p>为了提供一个例子，让我们定义一个函数来计算一个值矩阵的前三次幂之和。我们在一个5000 x 5000的矩阵上计算这个函数三次——一次使用NumPy，一次使用JAX，一次使用JIT编译版本的JAX。首先，我们在CPU上进行实验：</p><p> def fn(x): return x + x*x + x*x*xx_np = np.random.randn(5000, 5000).astype(dtype=&#39;float32&#39;)x_jnp = jnp.array(x_np)%timeit -n5 -r5 fn(x_np)%timeit fn(x_jnp).block_until_ready()jitted = jit(fn)jitted(x_jnp)%timeit jitted(x_jnp).block_until_ready()</p><p>def fn（x）：返回x+x*x+x*x*xx_np=np。随机的兰登（5000，5000）。aType（dtype=&#39；float32&#39；）x_jnp=jnp。数组（x_np）%timeit-n5-r5 fn（x_np）%timeit fn（x_jnp）。阻塞_，直到_ready（）jitted=jit（fn）jitted（x_jnp）%timeit jitted（x_jnp）。阻塞_直到_就绪（）</p><p> WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)5 loops, best of 5: 151 ms per loop10 loops, best of 5: 109 ms per loop100 loops, best of 5: 17.7 ms per loop</p><p>警告：absl：未找到GPU/TPU，正在返回CPU。（将TF_CPP_MIN_LOG_LEVEL设置为0，然后重新运行以获取更多信息。）5圈，最佳5:151毫秒/圈10圈，最佳5:109毫秒/圈100圈，最佳5:17.7毫秒/圈</p><p>  We see that  JAX is almost 40% faster than NumPy, and when we JIT the function we find that  JAX is an insane 8.5 times faster than NumPy. These results are already impressive, but let&#39;s up the ante and let JAX compute on a TPU:</p><p>我们看到JAX比NumPy快近40%，当我们对函数进行JIT时，我们发现JAX比NumPy快8.5倍。这些结果已经令人印象深刻，但让我们&#39；提高赌注，让JAX在TPU上计算：</p><p>  In this case, we see that JAX is a staggering  9.3 times faster than NumPy, and if we both JIT the function and compute on TPU we see find that JAX is an  obscene  57 times faster than NumPy.</p><p>在本例中，我们看到JAX比NumPy快9.3倍，如果我们都在TPU上JIT函数和计算，我们会发现JAX比NumPy快57倍。</p><p> This drastic increase in speed is, of course, not without a cost. JAX places restrictions on which functions are permissible to JIT, although functions involving only NumPy operations like the one above are generally permissible. Further, there are limitations regarding JITting through Python control flow, so you&#39;ll have to keep this in mind when writing your functions.</p><p>当然，速度的大幅提高并非没有代价。JAX对JIT允许使用哪些函数进行了限制，尽管通常允许使用只涉及上述NumPy操作的函数。此外，通过Python控制流进行JITting也有一些限制，因此&#39；在编写函数时，我们必须记住这一点。</p><p> Before using  jit, you should make sure that you understand how it works and in what scenarios its use is permissible. If you do not have this understanding but try to use  jit anyway, you will either get error messages that are confusing to you (if you’re lucky), or  untracked and undesirable side-effects that can quietly throw off the accuracy of your results (if you’re unlucky).</p><p>在使用jit之前，您应该确保了解它是如何工作的，以及在什么情况下允许使用它。如果你不了解这一点，但无论如何都要尝试使用jit，你要么会收到让你困惑的错误消息（如果你幸运的话），要么会收到未经跟踪且不受欢迎的副作用，这些副作用会悄悄地影响结果的准确性（如果你不幸运的话）。</p><p>   JAX has 4 main function transformations -  grad() to automatically differentiate a function,  vmap() to automatically vectorize operations,  pmap() for parallel computation of SPMD programs, and  jit() to transform a function into a JIT-compiled version. These transformations are (mostly) composable, very powerful, and have the potential to expedite your programs several times over.</p><p>JAX有4种主要的函数转换——grad（）用于自动区分函数，vmap（）用于自动矢量化操作，pmap（）用于SPMD程序的并行计算，jit（）用于将函数转换为jit编译版本。这些转换（大部分）是可组合的，非常强大，并且有可能使您的程序加速几倍。</p><p>   We saw above how XLA and fundamental JAX transformations have the potential to significantly increase the performance of your programs. While JAX is very powerful and has the potential to dramatically improve productivity in a great many areas, its use requires some care. Especially if you are considering moving from  PyTorch or TensorFlow to JAX, you should understand that JAX’s underlying philosophy is quite different from the two Deep Learning frameworks. We&#39;ll talk about the main difference now.</p><p>我们看到了XLA和基本JAX转换如何有可能显著地提高程序的性能。虽然JAX非常强大，有可能在许多领域显著提高生产率，但它的使用需要谨慎。特别是如果您正在考虑从PyTorch或TensorFlow转移到JAX，您应该了解JAX的基本理念与两个深度学习框架截然不同。我们&#39；我现在来谈谈主要的区别。</p><p>  The main characteristic that differentiates JAX is that its  transformations and compilation are designed to work only for functionally pure programs. While this fact may not be relevant if you just want to use JAX to put NumPy computations on GPU or TPU, it is relevant to a huge number of potential JAX applications, so you should make sure you understand the implications of adopting this paradigm before getting started.</p><p>JAX的主要区别在于，它的转换和编译只适用于功能纯粹的程序。虽然如果你只是想使用JAX在GPU或TPU上进行NumPy计算，这一事实可能并不重要，但它与大量潜在的JAX应用程序有关，因此你应该确保在开始之前理解采用这种范式的含义。</p><p> The central characteristic of a pure function  is that of  referential transparency - a pure function can be  replaced with the result of its evaluation at any time, and the program cannot tell the difference. The function should always have the same effect on the program given the same inputs  regardless of the time or context in which it is executed.</p><p>纯函数的中心特征是引用透明性——纯函数可以随时用其求值结果替换，程序无法分辨两者之间的差异。在给定相同输入的情况下，函数应始终对程序具有相同的效果，而不管它是在什么时间或上下文中执行的。</p><p> This sounds simple in principle, but there certainly exists</p><p>这在原则上听起来很简单，但确实存在</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/jax/">#jax</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>