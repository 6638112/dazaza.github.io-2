<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>通过部署群集内数据平面构建Kubernetes原生SaaS应用</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">通过部署群集内数据平面构建Kubernetes原生SaaS应用</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-22 09:41:28</div><div class="page_narrow text-break page_content"><p>At Pixie, we are working on a Kubernetes native monitoring system which stores and processes the resulting data entirely within a user’s cluster. This is the first in a series of posts discussing techniques and best practices for effectively building Kubernetes native applications. In this post, we explore the trade-offs between using an air-gapped deployment that lives completely within a cluster and a system which splits the control and data planes between the cloud and cluster, respectively.</p><p>在Pixie，我们正在开发Kubernetes本地监控系统，该系统完全在用户集群内存储和处理结果数据。这是讨论有效构建Kubernetes原生应用程序的技术和最佳实践的一系列帖子中的第一篇。在这篇文章中，我们探讨了使用完全位于集群内的空隙部署和分别在云和集群之间拆分控制和数据平面的系统之间的权衡。</p><p>  One benefit of building for the Kubernetes platform is that it simplifies the process of deploying applications to a user’s environment, often requiring only a few simple steps such as applying a set of YAMLs or installing a Helm Chart. Within minutes, users can easily have a running version of the application on their cluster. However, now that these applications are running entirely on prem, it becomes difficult for the developer to manage. In many cases, rolling out major updates or bug fixes relies on having the user manually update their deployment. This is unreliable for the developer and burdensome for the user.</p><p>为Kubernetes平台构建的一个好处是，它简化了将应用程序部署到用户环境的过程，通常只需要几个简单的步骤，如应用一组YAML或安装Helm Chart。在几分钟内，用户就可以轻松地在其群集上拥有该应用程序的运行版本。但是，现在这些应用程序完全在Prem上运行，开发人员很难进行管理。在许多情况下，推出重大更新或错误修复依赖于让用户手动更新其部署。这对开发人员来说是不可靠的，对用户来说是繁重的。</p><p>  To address this problem, we propose a connected on-prem architecture which delegates the responsibility of managing the data and control planes of the application to the deployment running in the cluster and a developer-managed cloud environment, respectively. More concretely, the application deployed in the user’s cluster is solely responsible for collecting data and making that data accessible. Once the foundation of this data layer is established, the logic remains mostly stable and is infrequently updated. Meanwhile, a cloud-hosted system manages the core functionality and orchestration of the application. As the cloud is managed by the developer themselves, they are freely able to perform updates without any dependency on the users. This allows the developer to iterate quickly on the functionality of their system, all while maintaining data locality on prem.</p><p>为了解决这个问题，我们提出了一个互联的本地架构，它将管理应用程序的数据和控制面的责任分别委托给在集群和开发人员管理的云环境中运行的部署。更具体地说，部署在用户集群中的应用程序只负责收集数据并使数据可访问。一旦建立了此数据层的基础，逻辑基本保持稳定，并且不会频繁更新。同时，云托管系统管理应用程序的核心功能和编排。因为云是由开发者自己管理的，所以他们可以自由地执行更新，而不需要依赖用户。这允许开发人员快速迭代其系统的功能，同时保持Prem上的数据局部性。</p><p> This split-responsibility architecture is common in many hardware products, since external factors may make it challenging to deploy updates to software running on physical devices. For instance, despite these physical limitations,  Ubiqiti’s UI is able to offer a rich feature-set by delegating functionality to their cloud and keeping their physical routers within the data plane. Similarly,  WebRTC is a standard built into most modern browsers for handling voice and video data. Although browser updates are infrequent, having the separated data and control layers allows developers to freely build a diverse set of applications on top of WebRTC. This architecture is still relatively uncommon in enterprise software, but has been adopted by popular products such as  Harness,  Streamsets, and  Anthos.</p><p>这种责任分立的架构在许多硬件产品中很常见，因为外部因素可能会使部署在物理设备上运行的软件的更新具有挑战性。例如，尽管有这些物理限制，Ubiqiti的UI通过将功能委托给他们的云并将他们的物理路由器保持在数据平面内，能够提供丰富的功能集。同样，WebRTC是大多数现代浏览器中内置的标准，用于处理语音和视频数据。尽管浏览器更新不频繁，但分离的数据层和控制层允许开发人员在WebRTC之上自由构建各种不同的应用程序集。这种体系结构在企业软件中仍然相对少见，但已经被流行的产品(如Harness、Streamset和Anthos)采用。</p><p> However, designing a connected on-prem architecture is easier said than done. When building such a system, one challenge you may encounter is how to query data from an application running on the user’s cluster via a UI hosted in the cloud. We explore two approaches for doing so:</p><p>然而，设计互联的本地架构说起来容易做起来难。在构建这样的系统时，您可能会遇到的一个挑战是如何通过托管在云中的UI从用户集群上运行的应用程序查询数据。为此，我们探索了两种方法：</p><p>  For brevity, we will refer to the application running on the user’s cluster as a satellite.</p><p>为简洁起见，我们将用户集群上运行的应用程序称为卫星。</p><p>  The simplest approach for executing the query on a satellite is to have the UI make the request directly to the satellite itself. To do this, the UI must be able to get the (1) status and (2) address of the satellite from the cloud, so that it knows whether the satellite is available for querying and where it should make requests to.</p><p>在卫星上执行查询的最简单方法是让UI直接向卫星本身发出请求。要做到这一点，UI必须能够从云中获取卫星的(1)状态和(2)地址，以便它知道卫星是否可用于查询以及它应该向何处发出请求。</p><p>   A common technique to track the status of a program is to establish a heartbeat sequence between the program (the satellite) and the monitoring system (the cloud). This is typically done by having the satellite first send a registration message to the cloud. During registration, the satellite either provides an identifier or is assigned an identifier via the cloud, which is used to identify the satellite in subsequent heartbeat messages.</p><p>跟踪程序状态的常用技术是在程序(卫星)和监控系统(云)之间建立心跳序列。这通常是通过让卫星首先向云发送注册消息来实现的。在注册期间，卫星要么提供标识符，要么通过云分配标识符，该标识符用于在后续的心跳消息中标识卫星。</p><p> Following registration, the satellite begins sending periodic heartbeats to the cloud to indicate it is alive and healthy. Additional information can be sent in these heartbeats. In our case, we also attach the satellite’s IP address. Alternatively, the IP address could have been sent during registration, if it is not subject to change. The cloud records the satellite’s status and address so that it can be queried by the UI.</p><p>注册后，卫星开始向云端发送周期性的心跳信号，以表明它是活的和健康的。可以在这些心跳中发送附加信息。在我们的示例中，我们还附加了卫星的IP地址。或者，如果IP地址不受更改，也可以在注册期间发送。云记录卫星的状态和地址，以便UI可以查询。</p><p> Now, when the UI wants to make a request to a satellite, it first queries the cloud for the address, then directly makes the request to that address.</p><p>现在，当UI想要向卫星发出请求时，它首先向云查询地址，然后直接向该地址发出请求。</p><p> Great! That wasn’t too bad. In many cases, many cloud/distributed satellite architectures already communicate via heartbeats to track satellite state, so sending an additional address is no problem. However... If your UI is running on a browser and your satellite is responding over HTTPS (likely with self-signed certs), you are not done yet...</p><p>太棒了！那还不算太糟。在许多情况下，许多云/分布式卫星架构已经通过心跳通信来跟踪卫星状态，因此发送额外的地址不成问题。然而.。如果您的UI在浏览器上运行，并且您的卫星正在通过HTTPS响应(很可能使用自签名证书)，那么您还没有完成……。</p><p>   The browser is blocking our requests because of the satellite’s SSL certs! A user could go ahead and navigate directly to the satellite’s address, where the browser prompts the user with whether or not they want to bypass the invalid cert.</p><p>因为卫星的SSL证书，浏览器阻止了我们的请求！用户可以继续并直接导航到卫星的地址，在那里浏览器提示用户是否要绕过无效的证书。</p><p>  However, this would need to be done per satellite and is disruptive to the user’s overall experience. It is possible to generate SSL certs for IP addresses, but this is uncommon and isn’t available with most free Certificate Authorities. This approach is also complicated if the satellite’s IP address is subject to change.</p><p>然而，这将需要在每个卫星上完成，并且会破坏用户的整体体验。可以为IP地址生成SSL证书，但这并不常见，大多数免费的证书颁发机构都不提供此功能。如果卫星的IP地址变化，这种方法也很复杂。</p><p>   Pre-generate SSL certs under a subdomain that you control, for instance:  &lt;uuid&gt;.satellites.yourdomain.com. This step is easy to do with any free Certificate Authority  and can be safely done if the subdomain has a well-known DNS address. You should make sure to generate more SSL certs than the number of expected satellites.</p><p>在您控制的子域下预先生成SSL证书，例如：&lt；uuid&gt；.sat ites.youdomain.com。使用任何免费的证书颁发机构都很容易完成此步骤，如果子域具有众所周知的DNS地址，则可以安全地完成此步骤。您应该确保生成比预期卫星数量更多的SSL证书。</p><p> When an satellite registers with the cloud, it should be assigned an unused SSL cert and associated subdomain. The SSL cert should be securely sent to the satellite and the satellite’s proxy should be updated to use the new cert.</p><p>当卫星注册到云时，应该为其分配一个未使用的SSL证书和关联的子域。SSL证书应该安全地发送到卫星，并且卫星的代理应该更新以使用新的证书。</p><p> When the cloud receives the satellite’s IP address from its heartbeats, it updates the DNS record for the satellite’s subdomain to point to the IP address.</p><p>当云从其心跳接收到卫星的IP地址时，它会更新卫星子域的DNS记录以指向该IP地址。</p><p> When executing queries, the UI can now safely make requests to the satellite’s assigned subdomain rather than directly to its IP address, all with valid certs!</p><p>在执行查询时，UI现在可以安全地向卫星分配的子域发出请求，而不是直接向其IP地址发出请求，所有这些都具有有效的证书！</p><p> In the end, making requests directly to the satellites turned out to be more complicated (and hacky) than we’d originally thought. The solution also doesn’t scale well, since the SSL certs need to be pre-generated. Without having a fixed number of satellites, or an upperbound on the number of satellites, it isn’t long before all the certs have been assigned and someone needs to step in and manually generate more. It is possible to generate the certs and their DNS records on the fly, but we’ve found these operations can take too long to propagate to all networks.  It is also important to note that this approach may violate the terms of service for automated SSL generation and is susceptible to usual security risks of wildcard certificates.</p><p>最后，直接向卫星发出请求比我们最初想象的更复杂(也更麻烦)。该解决方案也不能很好地扩展，因为需要预先生成SSL证书。没有固定的卫星数量，或者没有卫星数量的上限，不久之后，所有的证书都已经分配完毕，需要有人介入并手动生成更多的证书。可以动态生成证书及其DNS记录，但我们发现这些操作可能需要太长时间才能传播到所有网络。还需要注意的是，此方法可能会违反自动生成SSL的服务条款，并且容易受到通配符证书的常见安全风险的影响。</p><p> When a satellite is behind a firewall, it will only be queryable by users within the network. This further ensures that no sensitive data leaves the network.</p><p>当卫星位于防火墙之后时，只能由网络内的用户查询。这进一步确保不会有敏感数据离开网络。</p><p>   As seen in the previous approach, it is easiest to have the UI make requests to the cloud to avoid any certificate errors. However, we still want the actual query execution to be handled by the satellites themselves. To solve this, we architected another approach which follows these general steps:</p><p>正如在前面的方法中看到的，让UI向云发出请求以避免任何证书错误是最容易的。但是，我们仍然希望由卫星本身来处理实际的查询执行。为了解决这个问题，我们设计了另一种方法，遵循以下一般步骤：</p><p>  The cloud must be able to handle multiple queries to many different satellites at once. A satellite will stream batches of data in response, which the server needs to send to the correct requestor. With so many messages flying back and forth, all of which need to be contained within their own request/reply channels, we thought this would be the perfect job for a message bus.</p><p>云必须能够同时处理对许多不同卫星的多个查询。作为响应，卫星将流式传输批量数据，服务器需要将这些数据发送给正确的请求者。有这么多消息来回飞来飞去，所有这些消息都需要包含在它们自己的请求/应答通道中，我们认为这将是消息总线的完美工作。</p><p>   We built up a list of criteria that we wanted our message bus to fulfill:</p><p>我们建立了一个希望消息总线满足的标准列表：</p><p> It should receive and send messages quickly, especially since there is a user waiting at the receiving end.</p><p>它应该快速接收和发送消息，特别是在接收端有用户等待的情况下。</p><p> It should be able to handle relatively large messages. An satellite’s query response can be batched into many smaller messages, but the size of a single datapoint can still be non-trivial.</p><p>它应该能够处理相对较大的消息。卫星的查询响应可以批处理成许多较小的消息，但是单个数据点的大小仍然不是很小。</p><p> Similarly, since an satellite’s response may be batched into many messages, the message bus should be able to handle a large influx of messages at any given time.</p><p>类似地，由于卫星的响应可能被批处理成许多消息，因此消息总线应该能够在任何给定时间处理大量涌入的消息。</p><p> It should be easy to start new channels at any time. We may want to create a new channel per request or per satellite, all of which we have no fixed number.</p><p>随时开通新频道应该很容易。我们可能希望为每个请求或每个卫星创建一个新频道，所有这些我们都没有固定的数字。</p><p> We briefly considered Google Pub/Sub, which had strict quota requirements (only 10,000 topics per Google project), and other projects such as Apache Pulsar. However, we primarily considered two messaging systems: Apache Kafka and NATS. General comparisons between Kafka and NATS have been discussed at length in other blogs. In this blog post, we aim to compare these two systems based on our requirements above.</p><p>我们简要地考虑了Google Pub/Sub，它有严格的配额要求(每个Google项目只有10,000个主题)，以及其他项目，如Apache Pulsar。但是，我们主要考虑了两个消息传递系统：Apache Kafka和NATS。卡夫卡和NATS的总体比较已经在其他博客中进行了详细的讨论。在这篇博客文章中，我们的目标是根据上面的要求对这两个系统进行比较。</p><p> We relied heavily on benchmarks that  others have performed to judge latency based on message size and message volume. These results lean in favor of NATS.</p><p>我们在很大程度上依赖于其他人执行的基准测试，以基于消息大小和消息量来判断延迟。这些结果倾向于NATS。</p><p> We also wanted to test each system on our particular use-case, and performed the  following benchmark to do so:</p><p>我们还想在我们的特定用例上测试每个系统，并执行以下基准测试：</p><p> A service running on the server, called RequestProxyer, receives the message and puts it on topic A.</p><p>服务器上运行的名为RequestProxyer的服务接收消息并将其放在主题A上。</p><p>  RequestProxyer reads the message on topic B, and writes a response back out to the WebSocket.</p><p>RequestProxyer读取有关主题B的消息，并将响应写回WebSocket。</p><p>  In this case, the latency recorded for the benchmark is the time from which the websocket message is received in the RequestProxyer, to the time in which the server receives the response message from the subscriber.</p><p>在这种情况下，为基准记录的延迟是从RequestProxyer中接收到WebSocket消息的时间，到服务器从订阅者接收响应消息的时间。</p><p> These benchmarks were run on a 3-node GKE cluster with n1-standard-4 nodes, with a static 6-byte message. These results may not be generalizable to all environments. We also acknowledge that these systems were not built for this particular use-case.</p><p>这些基准测试在具有N1-STANDARD-4节点的3节点GKE集群上运行，具有6字节的静态消息。这些结果可能并不适用于所有环境。我们也承认这些系统不是为这个特定的用例而构建的。</p><p>        We ended up choosing NATS as our messaging system. Benchmarks performed by others and our own benchmark above showed that NATS is capable of efficiently handling our request and response messaging patterns. We also found it was extremely easy to create topics on-the-fly in NATS, whereas creating topics on Kafka can be fairly complicated since partitioning must be determined before start-up. Given that we will support many short-lived queries, we want to avoid any topic creation overhead. These points, paired with the lower operational complexity of NATS made it the clear winner for our case. It is important to note that Kafka&#39;s system is built to provide additional guarantees and has many positives, which may be necessary for other use cases.</p><p>我们最终选择了NATS作为我们的消息传递系统。上面别人执行的基准测试和我们自己的基准测试表明，NATS能够有效地处理我们的请求和响应消息传递模式。我们还发现，在NATS中即时创建主题非常容易，而在Kafka上创建主题可能相当复杂，因为分区必须在启动前确定。鉴于我们将支持许多短期查询，我们希望避免任何主题创建开销。这些要点，再加上NATS较低的操作复杂性，使其成为我们案件的明显赢家。重要的是要注意到，卡夫卡的系统是为了提供额外的保证而建造的，并有许多积极的方面，这可能是其他用例所必需的。</p><p>   The actual implementation of our query request pipeline looks very similar to the benchmark case we ran above.</p><p>我们的查询请求管道的实际实现看起来与我们上面运行的基准测试案例非常相似。</p><p> The cloud service responsible for handling the query requests receives the message and starts up a RequestProxyer instance in a new goroutine.</p><p>负责处理查询请求的云服务收到消息，并在新的goroutine中启动一个RequestProxyer实例。</p><p> The RequestProxyer generates an ID for the query and forwards the query and its ID to the correct satellite by putting a message on the  satellite/&lt;satellite_id&gt; NATS topic. It waits for the response on the  reply-&lt;query-ID NATS topic.</p><p>RequestProxyer为查询生成一个ID，并将查询及其ID转发到正确的卫星，方法是将一条消息放在卫星/&lt；sat_id&gt；NATS主题上。它等待对REPLY-&lt；Query-ID NATS主题的响应。</p><p> The service responsible for handling satellite communication (such as heartbeats) is subscribed to the  satellite/* NATS topic. It reads the query request and sends it to the appropriate satellite via its usual communication channels. The satellite streams the response back to this service. The service then puts these responses on the  reply-&lt;query-id&gt; NATS topic.</p><p>负责处理卫星通信(如心跳)的服务订阅了sat/*NATS主题。它读取查询请求，并通过其通常的通信信道将其发送到适当的卫星。卫星将响应流回此服务。然后，该服务将这些响应放在REPLY-&lt；query-id&gt；NATS主题上。</p><p> The RequestProxyer receives the responses on the  reply-&lt;query-id&gt; topic and sends them back to the UI.</p><p>RequestProxyer接收关于Reply-&lt；query-id&gt；主题的响应，并将它们发送回UI。</p><p> It is worth noting that in this approach, since data is now funneled through the cloud rather than directly from the satellite to the browser, there may be additional network latency.</p><p>值得注意的是，在这种方式下，由于数据现在是通过云传输，而不是直接从卫星传输到浏览器，因此可能会有额外的网络延迟。</p><p> In clusters behind a firewall, proxying the request through the cloud will allow data access to out-of-network users. This can be both a positive and negative, as it makes the application easier to use but relies on potentially sensitive data exiting the network.</p><p>在防火墙后的群集中，通过云代理请求将允许网络外用户访问数据。这既可以是积极的，也可以是消极的，因为它使应用程序更容易使用，但依赖于离开网络的潜在敏感数据。</p><p>  We use both approaches in Pixie, and have found either method allows us to efficiently and reliably query data from our customer’s clusters. By providing both options, customers have the flexibility of choosing the architecture that best meets their security needs. We believe these techniques can be useful for any on-prem connected architecture, and the particular approach should be chosen depending on the overall use-case and constraints specific to the system itself.</p><p>我们在Pixie中使用了这两种方法，并且发现任何一种方法都可以让我们高效可靠地查询客户集群中的数据。通过提供这两个选项，客户可以灵活地选择最符合其安全需求的架构。我们相信这些技术对于任何内部互联架构都很有用，具体的方法应该根据总体用例和特定于系统本身的约束来选择。</p><p> Overall, designing an split data/control plane architecture for Kubernetes native applications will help developers iterate quickly despite the on-prem nature of Kubernetes deployments.</p><p>总体而言，为Kubernetes原生应用程序设计一个分离的数据/控制平面架构将帮助开发人员快速迭代，尽管Kubernetes部署具有本地性质。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.pixielabs.ai/blog/hybrid-architecture/hybrid-architecture/">https://blog.pixielabs.ai/blog/hybrid-architecture/hybrid-architecture/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/群集/">#群集</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/卫星/">#卫星</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>