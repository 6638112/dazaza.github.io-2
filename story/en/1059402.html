<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>关于伦理AI的FTC陈述 FTC Statement on Ethical AI</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">FTC Statement on Ethical AI<br/>关于伦理AI的FTC陈述 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-20 21:27:10</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/9e9245a848f2e1b1766abbe9f7f8d580.jpg"><img src="http://img2.diglog.com/img/2021/4/9e9245a848f2e1b1766abbe9f7f8d580.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Advances in artificial intelligence (AI) technology promise to revolutionize our approach to medicine, finance, business operations, media, and more. But research has highlighted how apparently “neutral” technology can produce troubling outcomes – including discrimination by race or other legally protected classes. For example, COVID-19 prediction models can help health systems combat the virus through efficient allocation of ICU beds, ventilators, and other resources. But as a  recent study in the Journal of the American Medical Informatics Association suggests, if those models use data that reflect existing racial bias in healthcare delivery, AI that was meant to benefit all patients may worsen healthcare disparities for people of color.</p><p>人工智能（AI）技术承诺彻底改变我们的医学，金融，业务运营，媒体等方法的概况。但研究突出了显然“中性”技术如何产生令人不安的结果 - 包括种族或其他法律保护的课程的歧视。例如，Covid-19预测模型可以通过高效分配ICU床，呼吸机和其他资源来帮助健康系统对抗病毒。但作为最近在美国医学信息学协会期刊的研究表明，如果这些模型使用反映医疗保健交付中现有种族偏见的数据，那么旨在使所有患者造成的AI可能会使颜色人民恶化的医疗保健差异。</p><p> The question, then, is how can we harness the benefits of AI without inadvertently introducing bias or other unfair outcomes? Fortunately, while the sophisticated technology may be new, the FTC’s attention to automated decision making is not. The FTC has decades of experience enforcing three laws important to developers and users of AI:</p><p> 那么，问题是，如果我们如何利用AI的福利，而无意中引入偏见或其他不公平的结果？幸运的是，虽然复杂的技术可能是新的，但FTC的注意力决策不是。 FTC有几十年的经验，为AI的开发人员和用户执行了三个法律：</p><p> Section 5 of the FTC Act. The FTC Act prohibits unfair or deceptive practices. That would include the sale or use of – for example – racially biased algorithms.</p><p> FTC法案第5节。 FTC法案禁止不公平或欺骗性做法。这包括销售或使用 - 例如 - 种族偏见的算法。</p><p>  Fair Credit Reporting Act. The FCRA comes into play in certain circumstances where an algorithm is used to deny people employment, housing, credit, insurance, or other benefits.</p><p>  公平信用报告法案。在某些情况下，该算法用于拒绝人才就业，住房，信贷，保险或其他福利的某些情况下发挥作用。</p><p>  Equal Credit Opportunity Act. The ECOA makes it illegal for a company to use a biased algorithm that results in credit discrimination on the basis of race, color, religion, national origin, sex, marital status, age, or because a person receives public assistance.</p><p>  平等信用机会法案。 ECOA使公司违法使用偏置算法，这些算法导致基于种族，颜色，宗教，国家来源，性别，婚姻状况，年龄或因人收到公共援助的信贷歧视。</p><p> Among other things, the FTC has used its expertise with these laws to  report on big data analytics and machine learning; to conduct a  hearing on algorithms, AI and predictive analytics; and to issue  business guidance on AI and algorithms. This work – coupled with FTC enforcement actions – offers important lessons on using AI truthfully, fairly, and equitably.</p><p> 除此之外，FTC利用了这些法律的专业知识，以报告大数据分析和机器学习;对算法进行听证，AI和预测分析;并为AI和算法发出业务指导。这项工作 - 与FTC执法行动相结合 - 在真实，公平和公平地使用AI时提供重要的课程。</p><p> Start with the right foundation. With its mysterious jargon (think: “machine learning,” “neural networks,” and “deep learning”) and enormous data-crunching power, AI can seem almost magical. But there’s nothing mystical about the right starting point for AI: a solid foundation. If a data set is missing information from particular populations, using that data to build an AI model may yield results that are unfair or inequitable to legally protected groups. From the start, think about ways to improve your data set, design your model to account for data gaps, and – in light of any shortcomings – limit where or how you use the model.</p><p> 从正确的基础开始。凭借神秘的行话（想：“机器学习，”神经网络，“和”深度学习“）和巨大的数据嘎吱作用，AI似乎几乎是神奇的。但是AI的正确出发点没有任何神秘的东西：坚实的基础。如果数据集是缺少特定群体的信息，则使用该数据构建AI模型可以产生不公平或不公平的结果，以应对法律保护的组。从一开始，考虑改进数据集的方法，设计模型以考虑数据差距，而 - 鉴于任何缺点 - 限制您使用模型的Where或方式。 </p><p> Watch out for discriminatory outcomes. Every year, the FTC holds PrivacyCon, a showcase for cutting-edge developments in privacy, data security, and artificial intelligence. During  PrivacyCon 2020, researchers presented work showing that algorithms developed for benign purposes like healthcare resource allocation and advertising actually resulted in racial bias. How can you reduce the risk of your company becoming the example of a business whose well-intentioned algorithm perpetuates racial inequity? It’s essential to test your algorithm – both before you use it and periodically after that – to make sure that it doesn’t discriminate on the basis of race, gender, or other protected class.</p><p>留意歧视性结果。每年，FTC都持有PrivacyCon，一个展示隐私，数据安全和人工智能的尖端开发。在PrivacyCon 2020期间，研究人员提出了工作表明，为医疗资源分配和广告等良性目的开发的算法实际上导致种族偏见。如何降低贵公司的风险成为善意算法延续种族不公平的业务的例子？测试您的算法至关重要 - 在您使用它之前以及定期之后，这两者都是 - 以确保它不会根据种族，性别或其他受保护类歧视。</p><p> Embrace transparency and independence. Who discovered the racial bias in the  healthcare algorithm described at PrivacyCon 2020 and later published in Science? Independent researchers spotted it by examining data provided by a large academic hospital. In other words, it was due to the transparency of that hospital and the independence of the researchers that the bias came to light. As your company develops and uses AI, think about ways to embrace transparency and independence – for example, by using transparency frameworks and independent standards, by conducting and publishing the results of independent audits, and by opening your data or source code to outside inspection.</p><p> 拥抱透明度和独立性。谁发现了在Privacycon 2020中描述的医疗保健算法中的种族偏见，后来发表于科学？独立研究人员通过检查大型学术医院提供的数据来发现它。换句话说，它是由于该医院的透明度以及研究人员的独立性，偏见的光临。随着贵公司开发和使用AI，请考虑使用透明度和独立性的方法 - 例如，通过使用透明度框架和独立标准，通过开展和发布独立审计的结果，并通过打开您的数据或源代码来进行外部检查。</p><p> Don’t exaggerate what your algorithm can do or whether it can deliver fair or unbiased results. Under the FTC Act, your statements to business customers and consumers alike must be truthful, non-deceptive, and backed up by evidence. In a rush to embrace new technology, be careful not to overpromise what your algorithm can deliver. For example, let’s say an AI developer tells clients that its product will provide “100% unbiased hiring decisions,” but the algorithm was built with data that lacked racial or gender diversity. The result may be deception, discrimination – and an FTC law enforcement action.</p><p> 不要夸大您的算法可以做的或是否可以提供公平或无偏见的结果。根据FTC法案，您对商业客户和消费者的声明必须是真实的，非欺骗性的，并通过证据支持。急于接受新技术，小心不要过度过度发出你的算法可以提供的东西。例如，让我们说AI开发人员告诉客户，它的产品将提供“100％无偏见的招聘决策”，但算法是用缺乏种族或性别多样性的数据构建的。结果可能是欺骗，歧视 - 和FTC执法行动。</p><p> Tell the truth about how you use data. In our  guidance on AI last year, we advised businesses to be careful about how they get the data that powers their model. We noted the FTC’s  complaint against Facebook, which alleged that the social media giant misled consumers by telling them they could opt in to the company’s facial recognition algorithm, when in fact Facebook was using their photos by default. The FTC’s  recent action against app developer Everalbum reinforces that point. According to the complaint, Everalbum used photos uploaded by app users to train its facial recognition algorithm. The FTC alleged that the company deceived users about their ability to control the app’s facial recognition feature and made misrepresentations about users’ ability delete their photos and videos upon account deactivation. To deter future violations, the  proposed order requires the company to delete not only the ill-gotten data, but also the facial recognition models or algorithms developed with users’ photos or videos.</p><p> 讲述你如何使用数据的真相。在我们去年对AI的指导下，我们建议企业要小心如何获得其模型的数据。我们注意到FTC对Facebook的投诉，据称，社交媒体巨头误导了消费者通过告诉他们可以选择借助默认情况下使用他们的照片的公司的面部识别算法。 FTC最近对App Developer Everalbum的行动加强了该点。根据投诉，Everalbum使用的照片由App用户上传以训练其面部识别算法。 FTC据称，该公司欺骗了用户控制应用程序面部识别特征的能力，并在账户停用后删除他们的照片和视频的歪曲。为了阻止未来的违规行为，所提出的命令要求公司不仅删除了不良数据，还需要使用用户的照片或视频开发的面部识别模型或算法。</p><p> Do more good than harm. To put it in the simplest terms, under the FTC Act, a practice is  unfair if it causes more harm than good. Let’s say your algorithm will allow a company to target consumers most interested in buying their product. Seems like a straightforward benefit, right? But let’s say the model pinpoints those consumers by considering race, color, religion, and sex – and the result is digital redlining (similar to the Department of Housing and Urban Development’s  case against Facebook in 2019). If your model causes more harm than good – that is, in Section 5 parlance, if it causes or is likely to cause substantial injury to consumers that is not reasonably avoidable by consumers and not outweighed by countervailing benefits to consumers or to competition – the FTC can challenge the use of that model as unfair.</p><p> 做得比伤害更好。要把它放在最简单的条件下，根据FTC法案，如果它造成更大的伤害，这种实践是不公平的。假设您的算法将允许公司针对最感兴趣的消费者对购买产品。似乎是一个直截了当的好处，对吧？但是，让模型通过考虑种族，颜色，宗教和性别来定位这些消费者 - 结果是数字红线（类似于2019年对Facebook的住房和城市发展案例的情况）。如果您的模型造成更高的弊大于 - 即在第5节的情况下，如果它导致或可能对消费者合理地避免的消费者造成大量伤害，而不是因消费者或竞争而不超过的消费者，而不是竞争 -  FTC可以挑战这种模型作为不公平的使用。</p><p> Hold yourself accountable – or be ready for the FTC to do it for you. As we’ve noted, it’s important to hold yourself accountable for your algorithm’s performance. Our recommendations for transparency and independence can help you do just that. But keep in mind that if you don’t hold yourself accountable, the FTC may do it for you. For example, if your algorithm results in credit discrimination against a protected class, you could find yourself facing a complaint alleging violations of the FTC Act and ECOA. Whether caused by a biased algorithm or by human misconduct of the more prosaic variety, the FTC takes allegations of credit discrimination very seriously, as its recent action against  Bronx Honda demonstrates.</p><p> 坚持自己责任 - 或者准备好FTC为您做。正如我们所指出的那样，对自己对算法的性能负责是很重要的。我们对透明度和独立性的建议可以帮助您做到这一点。但请记住，如果您不遵守自己负责任，FTC可能会为您做。例如，如果您的算法导致对受保护的课程的信贷歧视，您可以发现自己面临着违反FTC法案和Ecoa的投诉。无论是由偏见的算法还是通过人类不当行为更加平淡的品种，FTC都非常认真地指控信贷歧视，因为其最近对Bronx本田的行动表现出来。</p><p> As your company launches into the new world of artificial intelligence, keep your practices grounded in established FTC consumer protection principles.</p><p> 随着贵公司发射进入人工智能的新世界，使您的做法保持在成立的FTC消费者保护原则。 </p><p>      It is your choice whether to submit a comment. If you do, you must create a user name, or we will not post your comment. The Federal Trade Commission Act authorizes this information collection for purposes of managing online comments. Comments and user names are part of the Federal Trade Commission’s (FTC)  public records system (PDF), and user names also are part of the FTC’s  computer user records system (PDF). We may routinely use these records as described in the FTC’s  Privacy Act system notices. For more information on how the FTC handles information that we collect, please read our  privacy policy.</p><p>您是您选择是否提交评论。 如果您这样做，您必须创建用户名，或者我们不会发布您的评论。 联邦贸易委员会法案授权此信息收集，以便管理在线评论。 评论和用户名是联邦贸易委员会（FTC）公共记录系统（PDF）的一部分，用户姓名也是FTC的计算机用户记录系统（PDF）的一部分。 我们可能经常使用FTC的隐私法制度通知中所述的这些记录。 有关FTC如何处理我们收集的信息的更多信息，请阅读我们的隐私政策。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.ftc.gov/news-events/blogs/business-blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai">https://www.ftc.gov/news-events/blogs/business-blog/2021/04/aiming-truth-fairness-equity-your-companys-use-ai</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/ai/">#ai</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/statement/">#statement</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>