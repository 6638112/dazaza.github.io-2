<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Cloudflare的ClickHouse容量评估框架</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Cloudflare的ClickHouse容量评估框架</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-07 11:04:56</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/8df9388dff13ef0dbd99fe9bef3701eb.png"><img src="http://img2.diglog.com/img/2020/11/8df9388dff13ef0dbd99fe9bef3701eb.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>We use ClickHouse widely at Cloudflare. It helps us with our internal analytics workload, bot management, customer dashboards, and many other systems. For instance, before  Bot Management can analyze and classify our traffic, we need to collect logs. The  Firewall Analytics tool needs to store and query data somewhere too. The same goes for our new  Cloudflare Radar project. We are using ClickHouse for this purpose. It is a big database that can store huge amounts of data and return it on demand. This is not the first time we have talked about ClickHouse, there is a  dedicated blogpost on how we introduced ClickHouse for HTTP analytics.</p><p>我们在Cloudflare上广泛使用ClickHouse。它帮助我们处理内部分析工作负载、BOT管理、客户仪表盘和许多其他系统。例如，在Bot Management能够对我们的流量进行分析和分类之前，我们需要收集日志。防火墙分析工具也需要在某个地方存储和查询数据。同样的道理也适用于我们新的云晕雷达项目。我们正在使用ClickHouse来实现这一目的。这是一个可以存储海量数据并按需返回的大型数据库。这不是我们第一次谈论ClickHouse了，有一篇专门的博文介绍了我们是如何引入ClickHouse进行HTTP分析的。</p><p> Our biggest cluster has more than 100 nodes, another one about half that number. Besides that, we have over 20 clusters that have at least three nodes and the replication factor of three. Our current insertion rate is about 90M rows per second.</p><p>我们最大的集群有100多个节点，另一个大约是这个数字的一半。除此之外，我们有20多个群集，它们至少有三个节点，复制系数为三。我们目前的插入速度约为每秒9000万行。</p><p> We use the standard approach in ClickHouse schema design. At the top level we have clusters, which hold shards, a group of nodes, and a node is a physical machine. You can find technical characteristics of the nodes  here. Stored data is replicated between clusters. Different shards hold different parts of the data, but inside of each shard replicas are equal.</p><p>我们在ClickHouse架构设计中使用标准方法。在顶层，我们有集群，其中包含碎片、一组节点，节点是一台物理机。您可以在此处找到节点的技术特征。存储的数据在群集之间复制。不同的分片保存不同的数据部分，但每个分片副本的内部是相同的。</p><p>    As engineers, we periodically face the question of how many additional nodes we have to order to support the growing demand for the next X months, with disk space as our prime concern.</p><p>作为工程师，我们经常面临这样的问题：我们必须订购多少额外的节点才能支持未来X个月不断增长的需求，而磁盘空间是我们最关心的问题。</p><p> ClickHouse stores extensive information in system tables about the operating processes, which is helpful. From the early days of using ClickHouse we added  clickhouse_exporter as part of our monitoring stack. One of the metrics we are interested in is exposed from the  system.parts table. Roughly speaking, clickhouse_exporter runs  SQL queries asking how many bytes are used by each table. After that, these metrics are sent from Prometheus to  Thanos and stored for at least a year.</p><p>ClickHouse在系统表中存储了有关操作过程的大量信息，这很有帮助。从使用ClickHouse的早期开始，我们就添加了Clickhouse_Exporter作为监控堆栈的一部分。我们感兴趣的指标之一是从System.Parts表中公开的。粗略地说，clickhouse_exporter运行SQL查询，询问每个表使用了多少字节。之后，这些指标将从普罗米修斯送到塔诺斯，并保存至少一年。</p><p> Every time we wanted to make a forecast of disk usage we queried Thanos for historical data using this expression:</p><p>每次我们想要预测磁盘使用情况时，我们都会使用下面的表达式查询Thanos的历史数据：</p><p>  There were a few problems with this approach. Only a few people knew where the notebooks were and how to get them running. It wasn&#39;t trivial to download historical data. And most importantly, it was hard to look at past predictions and assess whether or not they were correct since results were not stored anywhere except internal blog posts. Also, as the number and size of clusters and products grew it became impossible for a single team to work on capacity planning and we needed to get engineers building products involved as they have the most context on how the growth will change in the future.</p><p>这种方法存在一些问题。只有几个人知道笔记本在哪里，以及如何让它们运行。下载历史数据绝非易事。最重要的是，很难查看过去的预测并评估它们是否正确，因为结果除了内部博客帖子外没有存储在任何地方。此外，随着集群和产品的数量和规模的增长，单个团队不可能进行容量规划，我们需要让工程师参与到产品开发中来，因为他们对未来的增长将如何变化有最深入的了解。</p><p> We wanted to automate this process and made calculations more transparent for our colleagues, including those who use ClickHouse for their services. Honestly, at the beginning we weren’t sure if it was even possible and what we would get out of it.</p><p>我们希望将这一过程自动化，并使计算对我们的同事更加透明，包括那些使用ClickHouse提供服务的同事。老实说，一开始我们甚至不确定这是否可能，以及我们能从中得到什么。</p><p>  The crucial moment of adding new nodes for us is a disk space, so this was a place to start. We decided to use  system.parts, as we used it before with the manual approach.</p><p>对于我们来说，添加新节点的关键时刻是磁盘空间，因此这是一个开始。我们决定使用System.Parts，就像我们以前在手动方法中使用的一样。</p><p> Luckily, we started doing it for the cluster that had recently changed its topology. That cluster had  two shards with four and five nodes in every shard. After the topology change, it was replaced  with three shards and three nodes in every shard, but the number of machines and unreplicated data on the disks remained the same. However, it had an impact on our metrics: we previously had four replicated nodes in one shard and five replicated in another, we took one node off from the first shard and two nodes from the second and created a new one based on these three nodes. The new shard was empty, so we just added it, but the total amount of data in the first and the second shards was less as the count of the remaining nodes.</p><p>幸运的是，我们开始为最近更改了拓扑的集群这样做。该集群有两个分片，每个分片中有四个和五个节点。拓扑改变后，它被替换为三个分片和每个分片中的三个节点，但机器数量和磁盘上未复制的数据保持不变。然而，它对我们的指标产生了影响：我们之前在一个碎片中有四个复制节点，在另一个碎片中复制了五个节点，我们从第一个碎片中删除了一个节点，从第二个碎片中删除了两个节点，并基于这三个节点创建了一个新的节点。新的分片是空的，所以我们只是添加了它，但是第一个和第二个分片中的数据总量比剩余节点的数量要少。</p><p> You can see on the graph below in April we had this sharp decrease caused by topology changes. We got ~550T instead of ~850T among all shards and replicas.</p><p>您可以在下图中看到，由于拓扑结构的变化，我们在4月份出现了这种急剧下降。在所有碎片和复制品中，我们得到了~550吨，而不是~850吨。</p><p>  When we tried to train our model based on the real data due to the April drop it thought we had a downward trend. Which was incorrect as we only dropped replicated data. The trend for unreplicated data hadn’t changed. So we decided to take into account only unreplicated data. It saved us from the topology change and node replacement in case of problems with hardware.</p><p>当我们试图根据4月份下跌的真实数据来训练我们的模型时，它认为我们有下降的趋势。这是不正确的，因为我们只删除了复制的数据。未复制数据的趋势没有改变。因此，我们决定只考虑未复制的数据。它省去了在硬件出现问题时更改拓扑和更换节点的麻烦。</p><p>  sum by(cluster) (max by (cluster, shardgroup) (node_clickhouse_shardgroupinfo{} *on (instance) group_right (cluster, shardgroup) sum(table_parts_bytes{cluster=&#34;%s&#34;}) by (instance)))</p><p>SUM BY(CLUSTER)(max BY(CLUSTER，Shardgroup)(node_clickhouse_shardgroupinfo{}*on(实例)GROUP_RIGHT(CLUSTER，Shardgroup)SUM(TABLE_PARTS_BYTES{CLUSTER=&#34；%s&#34；})BY(INSTANCE)。</p><p> We continue using  system.parts from clickhouse_exporter, but instead of counting the whole amount of data we use the maximum of unreplicated data from every shard.</p><p>我们继续使用clickhouse_exporter中的system.part，但是我们使用的不是整个数据量，而是每个分片中未复制数据的最大值。</p><p> In the image below there is the same cluster as in the image above but instead of counting the whole amount of data we look at unreplicated data from all shards. You can clearly see that we continued to grow and didn’t have any drop in data.</p><p>在下图中，有与上图相同的集群，但我们查看的不是整个数据量，而是来自所有碎片的未复制数据。你可以清楚地看到，我们继续增长，数据没有任何下降。</p><p>  Another problem we faced was that we migrated some tables from one cluster to another because we were running out of space and it required immediate action. However, our model didn’t know that part of the tables didn’t live there anymore, and we didn’t want them to be a part of the prediction. To solve this problem we queried Prometheus to get the list of the tables that existed at the prediction time, then filtered historical data to include only these tables and used them as the input for training a model.</p><p>我们面临的另一个问题是，我们将一些表从一个集群迁移到另一个集群，因为我们的空间快用完了，需要立即采取行动。然而，我们的模型不知道部分表格不再存在，我们不希望它们成为预测的一部分。为了解决这个问题，我们查询普罗米修斯以获得预测时存在的表的列表，然后过滤历史数据以仅包括这些表，并使用它们作为训练模型的输入。</p><p>  After determining the correct metrics, we needed to obtain them for our forecasting procedure. Our long-term metrics solution, Thanos, stores billions of data points. Querying it for a cluster with over one hundred nodes even for one day takes a huge amount of time, and we needed these data points for a year.</p><p>在确定正确的指标之后，我们需要为我们的预测过程获取这些指标。我们的长期指标解决方案Thanos存储着数十亿个数据点。查询一个拥有100多个节点的集群，即使是一天也需要大量的时间，我们需要这些数据点一年之久。</p><p> As we planned to use Python we wrote a small client using  aiohttp that concurrently sends HTTP requests to Thanos. The requests are sent in chunks, and every request has start/end dates with a difference of one hour. We needed to get the data for the whole year once and then append new ones day by day. We got csv files: one file for one cluster. The client became a part of the project, and it runs once a day, queries Thanos for new metrics (previous day) and appends data to the files.</p><p>因为我们计划使用Python，所以我们使用aiohttp编写了一个小客户端，该客户端同时向Thanos发送HTTP请求。请求以块形式发送，每个请求的开始/结束日期相差一个小时。我们需要一次获得全年的数据，然后逐日追加新的数据。我们有CSV文件：一个集群一个文件。客户端成为项目的一部分，它每天运行一次，向Thanos查询新的指标(前一天)，并将数据附加到文件中。</p><p>  At this point, we have collected metrics in files, now it’s time to make a forecast. We needed something for time-series metrics, so we chose  Prophet from Facebook. It’s very simple to use, you can follow the documentation and get good results even with the default parameters.</p><p>至此，我们已经收集了文件中的指标，现在是做出预测的时候了。我们需要一些时间序列度量的东西，所以我们选择了Facebook的Prophet。它使用起来非常简单，您可以按照文档说明操作，即使使用默认参数也能获得很好的结果。</p><p> One challenge we faced using Prophet was the need to feed it one data point for a day. In the metric files we have thousands of those for every day. It looks logical to take the point at the end of every day, but it’s not really true. All tables have a retention period, the time for how long we store data in ClickHouse. We don’t know when the data is cleared, it happens gradually throughout the day. So, we decided to take the maximum number for a day.</p><p>我们使用Prophet时面临的一个挑战是每天需要向它提供一个数据点。在公制文件中，我们每天都有数千个这样的数据。在每一天结束时接受这一点看起来很合乎逻辑，但这并不是真的。所有表格都有保留期，即我们在ClickHouse中存储数据的时间。我们不知道数据何时被清除，它在一天中逐渐发生。因此，我们决定采取最大数量的一天。</p><p>  We chose  Grafana to present results, though we needed to store predicted data points somewhere. The first thought was to use Prometheus, but because of high cardinality, we had about 300,000 points in summary for clusters and tables, so we passed. We decided to use ClickHouse itself. We wanted to have both graphs, real and predicted, on the same dashboard. We had real data points in Prometheus and with  mixed data source could do this. However, the problem was the same as the loading of metrics into files, for some clusters it’s impossible to obtain metrics for a long period of time. We added functionality to upload real metrics in ClickHouse as well, now both real and predicted metrics are displayed in Grafana, taken from ClickHouse.</p><p>我们选择Grafana来显示结果，尽管我们需要将预测的数据点存储在某个地方。最初的想法是使用普罗米修斯(Prometheus)，但由于基数很高，我们对集群和表总共有大约30万分，所以我们通过了。我们决定使用ClickHouse本身。我们希望在同一个仪表盘上同时显示真实和预测的图表。我们在普罗米修斯有真实的数据点，混合数据源可以做到这一点。然而，这个问题与将指标加载到文件中是一样的，对于某些集群来说，在很长一段时间内都不可能获得指标。我们还在ClickHouse中添加了上传真实指标的功能，现在真实指标和预测指标都显示在Grafana中，取自ClickHouse。</p><p>     We have a service running in Kubernetes that does all the toil, and we created an environment for other metrics. We have the place where we collect metrics from Thanos and expose them in the required format to Grafana. If we find the right metrics for accounting other resources like IO, CPU or other systems like Kafka we can easily add them to our framework. We can easily replace Prophet with another algorithm, and we can go back months and evaluate how close we were in our prediction according to the real data.</p><p>我们有一个在Kubernetes中运行的服务来完成所有的工作，我们还为其他指标创建了一个环境。我们可以从Thanos收集指标，并以所需的格式将它们公开给Grafana。如果我们找到了计算IO、CPU或其他系统(如Kafka)等其他资源的正确指标，我们就可以很容易地将它们添加到我们的框架中。我们可以很容易地用另一种算法取代Prophet，我们可以回到几个月前，根据真实数据评估我们的预测有多接近。</p><p> With this automation we were able to spot we were going out of disk space for a couple of clusters which we didn’t expect. We have over 20 clusters and have updates for all of them every day. This dashboard is used not only by our colleagues who are direct customers of ClickHouse but by the team who makes a plan for buying servers. It is easy to read and costs none of developers time.</p><p>有了这种自动化，我们能够发现我们的磁盘空间即将耗尽，因为有几个集群是我们意想不到的。我们有20多个集群，每天都有更新。这个仪表盘不仅供ClickHouse的直接客户同事使用，也供制定购买服务器计划的团队使用。它很容易阅读，而且不会花费任何开发人员的时间。</p><p> This project was carried out by the Core SRE team to improve our daily work. If you are interested in this project, check out our  job openings.</p><p>这个项目是由Core SRE团队实施的，目的是改善我们的日常工作。如果你对这个项目感兴趣，请查看我们的职位空缺。</p><p> We didn’t know what we would get at the end, we discussed, looked for solutions and tried different approaches. Huge thanks for this to Nicolae Vartolomei, Alex Semiglazov and John Skopis.</p><p>我们不知道最后会得到什么，我们讨论、寻找解决方案，并尝试了不同的方法。非常感谢尼古拉·瓦托洛米、亚历克斯·塞米格拉佐夫和约翰·斯科皮斯。</p><p>  Analytics  Bot Management  Dashboard  ClickHouse</p><p>Analytics Bot Management Dashboard ClickHouse</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.cloudflare.com/clickhouse-capacity-estimation-framework/">https://blog.cloudflare.com/clickhouse-capacity-estimation-framework/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/capacity/">#capacity</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>