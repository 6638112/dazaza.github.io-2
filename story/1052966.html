<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>医疗计算机视觉中的变压器 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">医疗计算机视觉中的变压器 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-17 19:02:25</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/ab7225a466be0f98f5e730320d47c305.png"><img src="http://img2.diglog.com/img/2021/3/ab7225a466be0f98f5e730320d47c305.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>如果您是最新的变形金刚，很难找到对矩阵力学的更好的解释，这些力学为架构提供了由Jay Alammar撰写的建筑。 EZRA的玛丽瑞拉还在临床NLP中编写了关于基于变压器的语言模型及其应用的一篇优秀的文章。对于本文，我们只能深入了解变形金刚，了解如何在计算机愿景中改变最新技术。</p><p> 变形金刚是一组N个E且网络架构，可将一个序列转换为另一个序列（序列序列）。输入和输出序列可以是文本，时间序列或可以表示为序列的任何其他数据。</p><p> 像序列架构的所有序列一样，变形金刚有两个组件：编码器和解码器。编码器将输入序列转换为嵌入空间。我们可以将嵌入空间视为输入数据的矢量表示。接下来，解码器在嵌入空间中取消数据，并将其转换为输出。</p><p> 序列算法有许多序列示例，SEQ2SEQ是使用反复性神经网络（RNN），特别是LSTM的一个示例，用于编码器和解码器模型。对于自然语言处理近十年来，RNN是最先进的。那么为什么RNN用于自然语言处理不再在流行音中，是什么让变压器特殊？变压器具有特殊的力量：注意。</p><p> 要了解变压器和关注，我们必须了解以前流行的经常性神经网络的局限性。让我们在NLP，机器翻译中查看一个常用用例。 RNN编码器一次处理一个输入令牌（Word）。在每个处理步骤中，产生中间嵌入或“隐藏状态”。在句子的末尾，最终隐藏状态代表整个句子的嵌入。然后将此嵌入陷入解码器网络。同样地，解码器网络采用嵌入并顺序地产生输出序列。</p><p>  这种方法存在一个问题。在长通话中，句子或段落开始时的单词的上下文必须多次通过编码器。在每个处理步骤中，新的隐藏状态必须捕获新单词以及从前一词中的上下文。在我们上面的示例中，在编码器产生嵌入之前，“早期”一词通过RNN两次通过。每个编码步骤必须在所得到的中间嵌入向量中保留上下文。理论上，这是可能的，但是，在实践中，这导致积累的误差和上下文丢失。无论是在培训和使用中，RNN都难以允许远程信息到达解码器。</p><p>  相比之下，变形金刚用两个概念上简单的新想法来解决这个问题。首先，变形金刚中的编码器将整个句子处理整个句子。这消除了需要中间嵌入物以捕获远程信息的约束。由于不再需要输入序列的顺序计算，因此变压器更加计算效率，因为我们能够更好地并行化操作。其次，变压器利用注意机制。我们不会潜入这里的数学，但足以说注意力机制使变形例能够将单词在句子中的重要性相对于彼此进行比分。由于注意机制，变压器更好地捕捉不同词之间的关系。 </p><p>几乎每个NLP任务都达到了最先进的技术。这一成功的大部分都可以归因于他们学习序列数据的任意结构和关系的能力。看到他们在NLP领域的成功后，来自计算机愿景的研究人员开始问：变形金刚的成功是否可以推广到图像？</p><p> 将变压器调整为图像的一部分是为了消除对特定图像的感应偏差的需求。让我们提醒自己：诱导偏差是创建依赖于正在处理的数据的学习算法时的设计选择。在计算机视觉中，具有层聚合的卷积神经网络（例如，最大池）由于其平移不变性的性质而占主导地位，这是一种质量，意味着CNNS能够识别图像特征，无论图像内的位置还是角度。卷积是一个强大的概念，一个感应偏差之一手动“烘焙”到艺术计算机视觉架构的状态。</p><p> 尽管他们的力量，手动选择这些归纳偏差也引入了局限性。翻译不变性，使CNNS有效的属性也导致不良行为。例如，一个网络任务，用于分类图像是否包含面部。 CNNS在识别面部特征方面非常有效：眼睛，耳朵，嘴唇，鼻子和眉毛。然而，CNNS努力学习这些子组件与其他更高级别逻辑之间的空间关系，例如应该存在的每个组件的数量。只要存在子组件，即CNN可能会将图像分类为包含面部的图像。然而，含有多个嘴唇和20只眼睛的脸也将被归类为面部。 CNN没有这些面部特征的数量或相对位置的概念。</p><p>  变压器向桌面带来两种优点。首先，当应用于图像时，已经表明注意机制是卷积的广义情况。这意味着变形金刚能够学会模仿卷曲。其次，变形金刚能够学习高级图像特征之间的复杂空间关系。在我们的脸部示例中，计数面部特征的眼数和相对位置不是变压器架构的问题。这些优势在一起允许变压器解决翻译不变性的局限性，同时仍然灵活地学习可以产生最新性能的业务。</p><p>  计算机视觉，混合架构和唯一的变压器架构中有两种转型机。在语言应用程序中，变压器的输入是一系列令牌嵌入式（类似于单词的语言单位）。对于应用于图像的变压器，仅通过其输入来区分混合动力和变压器的方法：</p><p> 混合：CNN用于产生图像的图像或子区域的嵌入（贴片）。编码或嵌入用作后续变压器的输入。 CNN用于处理图像中的较低级别特征。</p><p> 仅变压器：架构的培训部分负责将修补程序投影到嵌入空间。没有使用手工编码的卷积架构。变压器架构仅负责学习较低级别和更高级别的功能。 </p><p>两种方法导致贴片P映射到尺寸D的嵌入。在混合案件中，我们能够重用预先训练的CNN（例如RESET）来提取较低级别的特征向量。混合方法的一个缺点是我们不会摆脱我们之前提到的归纳偏见问题，因为我们仍然使用预先训练的卷积神经网络来执行初始图像嵌入。归纳偏差的良好方法是混合方法更加计算地高效（相对于变压器仅接近），因为它们通常以掠过的CNN和更少的总模型参数开始。相比之下，唯一的变换器的方法没有归纳偏差，因此必须学习提取低级和高级功能。据我所知，在医疗计算机视觉中，没有当前的例子（截至3月2021年3月）。但是，在更广泛的领域中，诸如图像GPT的变换器的工作已经证明了混合方法不是必需的。鉴于足够的数据和计算，可以训练单独的变压器以产生技术的状态，导致计算机视觉。</p><p>   变压器，计算机视觉和医学的交叉仍处于最早的阶段。实际上，计算机愿景的变压器子场中有很少的论文。当我们专注于医疗计算机愿景的利基时，只有少数公共工程。在这种主题的组合上有了这么少，尚未证明将变压器应用于医学图像的大部分潜力。这是两篇论文，探索变形金刚的未来承诺。我们将专注于论文的方法而不是结果。</p><p>    Ganbert是一个混合变压器方法的第一个例子。为了理解Ganbert，我们需要一些关于问题域的背景，它试图解决：删除对多种医疗扫描的需求。除其他应用中，宠物成像通常用于临床实践以诊断阿尔茨海默病。宠物图像采集涉及注入放射性示踪剂，然后注射扫描。注射示踪剂增加了成像过程的额外的不适和风险。在此之上，宠物扫描不提供很大的解剖细节，因此它们通常伴随着MRI或CT扫描来提供解剖细节。这些多次扫描增加了患者和医院的额外成本和负担。因此，需要一种替代程序，可以提供PET的诊断能力与MRI的解剖细节。</p><p> Ganbert探讨了从MRI T1加权图像综合宠物图像的可能性，从而消除了多个扫描的需要。作者的方法是一种生成的对抗网络，其中发电机是CNN（3D U-Net），并且鉴别器是变压器（伯特）。</p><p> 最初是为自然语言应用而创建的BERT，旨在将标记化和编码的单词作为输入。这些是句子，其中单词被分成令牌，然后通过将令牌转换为ID来编码。编码句子的令牌导致将文本表示为一系列整数。为了与BERT兼容，MRI和PET图像被量化并表示为图像强度值的序列。每个量化的图像强度值类似于NLP应用中的NLP应用中的单词或令牌。配对的MRI和PET序列被连接，变压器模型被培训为屏蔽语言模型（MLM）。对于自然语言应用，屏蔽语言模型是屏蔽文本的一部分，并且使用剩余的文本来预测屏蔽文本的算法是任务的。对于Ganbert，屏蔽了一部分PET序列，并且使用MRI数据任务是任务的，以预测丢失的PET数据。</p><p> 合成另一个图像模型的方法是有前途的。该工作的主要限制是，只有仅提供SSIM，PSNR和RMSE等定量度量，而没有基线对于每个度量的每个度量是可接受的性能水平。用放射科医师进行后续研究的图像将建立AI生成的PET图像的可解释性的基线。</p><p>    组织病理学图像始于大，通常在千兆像素范围内。培训整个图像网络对于这些图像来说是不可行的。应用于组织病理学图像的深度学习的常规方法是使用补丁电平CNN，将图像分成许多较小的贴片并独立处理每个贴片。 </p><p>独立处理时，补丁级算法缺少其他遥远补丁的上下文。 Hatnet的作者使用混合方法的自我关注和CNN来接近问题。 Hatnet通过在三个级别分层处理图像来解决这个问题：单词，袋子和图像（单词→袋→图片）。</p><p> 最低级别，单词是使用从架子预先训练的CNN中编码的图像修补程序，进入256维嵌入向量。这些“Word”-Level编码是架构中最低的级别表示。然后由单词，袋子和图像级别的变压器架构处理字级编码。这三级处理允许架构从临床相关的组织结构中包含图像范围的背景。</p><p> Hatnet的结果为自己说话，击败了下一个最佳方法，y-net，U-net的衍生物（F1分数+ 8％，均衡和召回的平衡措施）。这些结果令人印象深刻，而不是因为Hatnet击败了最先进的国家，而是因为它们代表了将来进步的承诺将变形金刚和注意机制应用于类似的数据。</p><p>  您可能已经注意到上一节中的两个示例都是混合方法，其中变压器与卷积神经网络串联使用。为什么甚至在更广泛的计算机视觉领域中的变形金属近似的少数例子？在此之上，变压器已在NLP中使用3年以上，并且仅在计算机视觉中开始起飞。这么长时间拍了什么？</p><p> 要回答这两个问题，我们需要重新审视变压器和CNN之间的关键差异。变形金刚的最大优势，较少的归纳偏见，也是它们的弱点。如前所述，变压器不会以与卷积神经网络相同的归纳偏差，不随意“预烘焙”。也就是说，卷积神经网络通过手工编码的卷积和聚合步骤系统地将小规模信息减少到全局信息。对于变压器，没有这样的手编码。为了实现超越CNN的结果，变压器必须在培训期间学习此过程。从头开始学习此过程需要大量的数据和计算，这是过去三年的限制步骤。</p><p> 最近电脑愿景中变压器的两个高调应用展示了这篇：谷歌大脑和图像GPT（IGPT）的视觉变压器（VIT）。两种模型通过培训或在大型专有数据集上进行培训来获得结果。 VIT使用了300 000张图片Google JFT数据集，仅在谷歌内部提供。计算是第二限制因素，其延迟了计算机视觉中变形金刚的出现。 IGPT的作者突出显示IGPT-L（第二大IGPT模型培训）训练2500 v100天训练，而同样执行的基于CNN的动量造影模型只花了70 v100天的训练，以达到相同的性能，但差异超过35倍在需要计算中。这些因素将这些因素限制了对具有大型医疗图像数据集的组的组和等效访问加速计算（GPU和TPU）。</p><p>  我希望您已发现这是对临床成像语境中变形金刚的有用介绍。由于访问大型数据集和计算资源，期望在医疗计算机视觉中看到类似的大小进步。我们仍然处于这一领域的早期进展的阶段，许多令人兴奋的里程碑仍然是。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://techblog.ezra.com/transformers-in-medical-computer-vision-643b0af8fc41">https://techblog.ezra.com/transformers-in-medical-computer-vision-643b0af8fc41</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/计算机/">#计算机</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/medical/">#medical</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/图像/">#图像</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1052090.html"><img src="http://img2.diglog.com/img/2021/3/thumb_d74a2c803be2a83db37ef80aa41c5bc7.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1052090.html">科学家解开最古老的“计算机”的奥秘 </a></div><span class="my_story_list_date">2021-3-13 6:27</span></div><div class="col-sm"><div><a target="_blank" href="/story/1051078.html"><img src="http://img2.diglog.com/img/2021/3/thumb_a54d5bf58e4eab79e286c9f53da96011.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1051078.html">革命和NAND盖茨，八分，批发 </a></div><span class="my_story_list_date">2021-3-7 12:11</span></div><div class="col-sm"><div><a target="_blank" href="/story/1049925.html"><img src="http://img2.diglog.com/img/2021/2/thumb_7c3765b0d26a622d9afe52a95f94d759.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1049925.html">在2021年更新“ 101种基本计算机游戏” </a></div><span class="my_story_list_date">2021-2-28 21:54</span></div><div class="col-sm"><div><a target="_blank" href="/story/1049853.html"><img src="http://img2.diglog.com/img/2021/2/thumb_454cc08948008561f331f1ed1777986d.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1049853.html">量子计算机比传统计算机更快地解决了数十年的老问题，快了三百万倍 </a></div><span class="my_story_list_date">2021-2-28 12:4</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>