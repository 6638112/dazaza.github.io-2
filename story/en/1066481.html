<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>证明交易的基于云的测序流消息传递架构 Cloud-Based Sequenced Stream Messaging Architecture at Proof Trading</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Cloud-Based Sequenced Stream Messaging Architecture at Proof Trading<br/>证明交易的基于云的测序流消息传递架构 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-17 23:27:22</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/6d722cf0ea14342e15772f363c9cdee4.png"><img src="http://img2.diglog.com/img/2021/6/6d722cf0ea14342e15772f363c9cdee4.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This is a technical deep-dive post describing the custom messaging middleware that powers  Proof’s Algorithmic Trading Platform in the cloud. For a high-level overview of our system, please refer to  this post.</p><p>这是一个技术深度潜在的帖子，描述了为云中的校正算法交易平台提供幂的自定义消息传递中间件。对于我们系统的高级概述，请参阅此帖子。</p><p>  As described in the  overview post, our system uses the “sequenced stream” architecture.</p><p>  如概述帖子中所述，我们的系统使用“测序流”架构。</p><p> Here’s a quick description of how this works: Every input into the system is assigned a globally unique monotonic sequence number and timestamp by a central component known as a sequencer. This sequenced stream of events is disseminated to all nodes/applications in the system, which only operate on these sequenced inputs, and never on any other external inputs that have not been sequenced. Any outputs from the applications must also first be sequenced before they can be consumed by other applications or the external world. Since all nodes in the distributed system are presented with the exact same sequence of events, it is relatively straightforward for them to arrive at the same logical state after each event, without incurring any overhead or issues related to inter-node communication.</p><p> 以下是对此工作方式的快速描述：系统中的每个输入都被称为定序器称为定序器的中央组件，分配全局唯一的单调序列号和时间戳。该测序的事件流被传播给系统中的所有节点/应用程序，该节点/应用程序仅在这些序列的输入上运行，而不是在尚未排序的任何其他外部输入上运行。在其他应用程序或外部世界可以消耗之前，还必须首先对来自应用程序的任何输出进行排序。由于分布式系统中的所有节点具有完全相同的事件序列，因此它们相对简单地向每个事件到达相同的逻辑状态，而不会产生与节点间通信相关的任何开销或问题。</p><p>  We mention “disseminated to all nodes” casually, as if it was an easy feat. This is in fact a huge challenge, sometimes known as the “fanout problem” — the challenge is that the sequencer, which is performing the critical task of sequencing, cannot also be burdened by the task of knowing all its clients and actively transmitting sequenced messages to each of those clients.</p><p>  我们随便提到“传播到所有节点”，好像这是一个容易的壮举。这实际上是一个巨大的挑战，有时称为“扇出问题” - 挑战是，序列人正在执行排序的关键任务，也不能通过了解所有客户的任务并主动传输排序消息的任务负担每个客户都。</p><p> Often, this problem is solved by using UDP multicast, where the producer produces the messages once and all interested consumers subscribe to a known multicast group to consume the messages. In this solution, the network switches do the “fanout” of data to multiple consumers, sometimes using a hierarchical set of switches. This works really well, though it is not without its pitfalls, given that UDP is unreliable and a good multicast setup is hard. We can’t use multicast in the cloud (no, the  fake multicast overlay is not effective), so we solved this problem using a custom messaging layer, inspired by  Aeron.</p><p> 通常，使用UDP组播解决了此问题，其中生产者生成消息一次以及所有感兴趣的消费者都订阅了一个已知的多播组以消耗邮件。在此解决方案中，网络交换机对多个消费者进行数据的“扇出”，有时使用分层组合。这实际上很好，尽管没有陷阱并非没有陷阱，但鉴于UDP是不可靠的，并且很难进行良好的多播设置。我们不能在云中使用多播（否，虚假的多播覆盖无效），我们使用自定义消息层解决了这个问题，由Aeron启发。</p><p> We didn’t use Aeron directly because even though it calls itself a message transport, it comes with a whole ecosystem of modules and services. We didn’t want to adopt all of the different framework pieces, as that would limit our architectural choices. Plus, we’re just not that familiar with Aeron and don’t know all of the gotchas. Instead, we decided to build our own message bus, taking inspiration from the best and the most relevant parts of Aeron.</p><p> 我们没有直接使用Aeron，因为即使它呼叫自己是留言传输，它还配备了整个模块和服务的生态系统。我们不想采用所有不同的框架件，因为这会限制我们的建筑选择。此外，我们并不熟悉Aeron，并不知道所有的Gotchas。相反，我们决定建立自己的信息巴士，从最佳和最相关的陨石中获取灵感。</p><p>  Yeah, yeah, you’re right — it is a little bit crazy to try to create your own messaging middleware in this day and age. I mean, talk about reinventing the wheel. But it really does make sense, when what you’re trying to do is not necessarily create a better wheel with more features, but rather a minimal specialized wheel that only does very specific things (and does them fast and reliably). We are not aware of any cloud-enabled middleware with a throughput of millions of messages per second at consistent sub-millisecond latency (except perhaps  this, which we haven’t tested).</p><p>  是的，是的，你是对的 - 尝试在这一天和年龄的年龄创建自己的消息中间件有点疯狂。我的意思是，谈谈重新发明轮子。但它确实有意义，当你想要做的时候不一定是一个具有更多功能的更好的轮子，而是一个只有非常具体的东西（并且快速且可靠地实现它们）的最小专业轮。我们不了解所有启用云的中间件，每秒具有数百万条消息的吞吐量，在一致的子毫秒延迟（除了它之外，我们还没有测试）。 </p><p>   Let’s recall the problem we are trying to solve: we have a stream of messages that we need to send from a producer to one or more remote consumers, in the fastest way possible.</p><p>让我们回忆起我们试图解决的问题：我们有一条消息，我们需要以最快的方式从生产者发送到一个或多个远程消费者。</p><p> We do not want to assume anything about an individual consumer: it may be processing data contemporaneously with the producer, or it may be slow and hours behind.</p><p> 我们不想为个人消费者承担任何东西：它可能正在与生产者同时处理数据，或者它可能会缓慢且几个小时后面。</p><p> We do not want consumers to miss messages that were produced while they were down (reliable delivery).</p><p> 我们不希望消费者错过在他们下降时产生的消息（可靠的交付）。</p><p> We do not want the consumers to affect the producer — for example, if we have a slow consumer, we do not want it to somehow “push back” on the producer.</p><p> 我们不希望消费者影响生产者 - 例如，如果我们有一个缓慢的消费者，我们不希望它以某种方式“推回”生产者。</p><p> We do not want the number of consumers to affect the producer or the other consumers — for example, if we were serially transmitting messages to a large number of consumers, the latency for the “last” consumer would increase as the number of consumers goes up, and overall the throughput would decrease.</p><p> 我们不希望消费者的数量来影响生产者或其他消费者 - 例如，如果我们串联向大量消费者传输消息，那么随着消费者的数量上升，“最后”消费者会增加“最后”消费者的延迟而且整体吞吐量会减少。</p><p> In addition, while we wanted to support consumers with varying performance characteristics, we wanted to have an optimized fast path for the common case: an up-to-date consumer that is able to keep up and is processing data in real-time as fast as it can receive it.</p><p> 此外，在我们希望支持具有不同性能特征的消费者的同时，我们希望为常见案例提供优化的快速路径：能够跟上的最新消费者，并且正在实时处理数据因为它可以收到它。</p><p> The solution we came up with is to create a disk-backed queue that is replicated between servers using threads pinned to dedicated CPU cores. Read on to see how this meets all of the above needs.</p><p> 我们提出的解决方案是创建一个磁盘备份队列，它使用固定到专用CPU内核的线程在服务器之间复制。请继续阅读，看看如何满足以上所有需求。 </p><p>  The Replicated Store is a general construct we use to facilitate communication between a producer and one or many consumers. The producer app writes its stream of messages to a store, which is implemented as a memory-mapped file. The use of a memory-mapped file allows the recent contents of the store file to remain in the machine’s virtual memory. One or more threads of a replication server run on the same host, having mapped this same store file. There is one replication server thread per consumer server, and it is busy-spinning on a dedicated core and checking in a tight loop if the last offset (length) of the store file has changed. Once a change is detected, the new records are read immediately and transmitted over TCP to a replication client, which writes them on the remote server to a memory-mapped file. This replicated copy of the store is then read by the consumer apps, again potentially in a tight spin on a dedicated core, though that’s not mandatory and depends on each consumer app’s individual needs.</p><p>复制商店是我们用于促进制作人和一个或许多消费者之间的沟通的一般构造。 Producer App将其消息流写入存储，该商店被实现为内存映射文件。使用内存映射的文件允许存储文件的最近内容保留在机器的虚拟内存中。复制服务器的一个或多个线程在同一主机上运行，​​映射了相同的存储文件。每个消费者服务器有一个Replication Server线程，如果商店文件的最后偏移（长度）已更改，则在专用核心上繁忙 - 旋转紧密循环。检测到更改后，将立即读取新记录并通过TCP传输到复制客户端，该复制客户端将它们写入远程服务器上的内存映射文件。然后，消费者应用程序读取商店的这种复制副本，再次可能在专用核心紧密旋转，但这不是强制性的，取决于每个消费者应用程序的个人需求。</p><p> The end result of this design is that if the consumer is reasonably current with the producer,  the replication process happens entirely in memory, directly from the producer’s memory to the consumer’s memory, with no disk access involved. As soon as the producer writes a record, the size/offset of the producer store file is incremented in memory, and the new offset and records are available as an atomic update to the replication server thread(s) within a microsecond or two. The records can be transmitted over TCP to the consumer servers in ~50μs (kernel/network latency), and then to the consumer apps in a handful more microseconds. (At some point, the operating system will flush the mapped portions of the store files to disk and this can cause latency, but there are ways to tune when and how the OS does this).</p><p> 这种设计的最终结果是，如果消费者与生产者具有相当合理的电流，则复制过程完全在内存中发生，直接从生产者的内存到消费者内存，没有涉及磁盘访问。一旦生产者写入记录，生产者存储文件的大小/偏移量在内存中递增，新的偏移和记录可作为微秒或两个中的Replication Server线程的原子更新。记录可以通过TCP传输到〜50μs（内核/网络延迟）中的消费者服务器，然后在几个微秒内到消费者应用程序。 （在某些时候，操作系统会将商店文件的映射部分刷新到磁盘，这可能导致延迟，但有些方法可以调整操作系统的时间和方式。</p><p> The beauty of this very simple solution is that the producer is completely decoupled from a slow or a far-behind consumer. Even if the consumer apps were started in the middle of the day, the replication server would be able to service them by reading the store file from the first message (this would involve disk access, and that’s ok because performance is not the primary consideration for this use case). Another related effect is that the individual consumers are independent — the relative speed of one consumer does not affect that of another consumer. Also, the consumers are not being sent the messages in a serial fashion, so there is no “first” or “last” consumer that is advantaged or disadvantaged in terms of latency.</p><p> 这一非常简单的解决方案的美丽是生产者从缓慢或远方消费者彻底解耦。即使消费者应用程序在一天中间开始，Replication Server也能够通过从第一条消息读取商店文件（这将涉及磁盘访问，这没问题，因为性能不是主要考虑这个用例）。另一个相关效果是，个人消费者是独立的 - 一个消费者的相对速度不会影响另一个消费者的相对速度。此外，消费者没有以串行方式发送消息，因此没有“第一”或“最后”消费者在延迟方面具有优缺点或不利地位。</p><p> We’ll admit that the design is expensive in that each consumer server requires a dedicated core on the producer’s server (see below section titled “A Note on Hardware Efficiency”). If we needed this to scale to more consumer servers than we have cores, we would use a hierarchical distribution pattern. For now, we haven’t needed to do this.</p><p> 我们将承认，这些设计昂贵，因为每个消费者服务器都需要生产的服务器上的专用核心（请参阅下面标题为“硬件效率注释”的部分）。如果我们需要这缩小到更多的消费者服务器，而不是我们有核心，我们将使用分层分发模式。现在，我们还没有必要这样做。</p><p>  The system is composed together using a number of these replicated stores. The last piece of the puzzle is the IODaemon, which as the name implies, is responsible for performing I/O on a server. The idea is that if there are multiple apps running on a server, they can all get their sequenced stream through a single transmission from the sequencer — IODaemon acts as the replication client. The IODaemon is also responsible for transmitting application output messages (aka “unsequenced” messages) to the sequencer for sequencing, over UDP unicast.</p><p>  系统使用多个复制存储器组成。拼图的最后一块是iodaemon，因为名称暗示，负责在服务器上执行I / O.这个想法是，如果在服务器上运行多个应用程序，则可以通过从semencer  -  iodaemon作为复制客户端来通过单个传输获取它们的顺序流。 iodaemon还负责将应用程序输出消息（AKA“未激活的”消息）传输到序列仪以进行排序，以通过UDP单播进行排序。</p><p>  We won’t go into any deep technical details of TCP vs UDP, but it is a topic worth touching upon. When would you select one over the other?</p><p>  我们不会进入TCP与UDP的任何深度技术细节，但它是一个值得触摸的主题。你什么时候选择一个，另一个？</p><p> TCP is a connection-oriented protocol and the primary benefit is that all packets sent over a TCP connection will reliably make their way from the sender to the receiver and be delivered in the right order. In addition, TCP contains congestion control algorithms that push back on the sender if the consumer is unable to keep up with the data being sent. All of these features can be useful, but they do make the protocol a bit heavy in terms of processing. In addition, you have to deal with establishing and maintaining a connection, and recovering from a broken connection.</p><p> TCP是一种面向连接的协议，主要的好处是，通过TCP连接发送的所有数据包都将可靠地从发件人到接收器的方式，并按正确的顺序传递。此外，TCP包含拥塞控制算法，如果使用者无法跟上发送的数据，则返回发件人。所有这些功能都可以有用，但它们确实使协议在处理方面有点沉重。此外，您必须处理建立和维护连接，并从破损的连接中恢复。 </p><p> If you have a situation where you aren’t too bothered by the lossy nature of UDP, or where you have an easy way to recover from dropped packets, you may decide to forego these features in favor of using a lighter UDP protocol. UDP, like TCP, sits on top of IP, but simply “flings” packets (datagrams) at the network hoping the routers will know where to direct them to get them to the receiver. There is no connection to establish and there is no guarantee that packets will reach their destination. In practice, it works reasonably well, and the data loss is minimal. Another distinct benefit of using UDP for the receiver is that the OS can multiplex the streams coming from multiple senders, as opposed to having to manually combine data coming over multiple TCP connections (where fairness and starvation issues would need to be dealt with).</p><p>如果您有一个情况，您对UDP的有损性质没有太烦扰，或者您可以从丢弃的数据包中恢复轻松的方法，您可以决定放弃这些功能，有利于使用更轻的UDP协议。像TCP一样的UDP坐在IP之上，但只有“删除”数据包（数据报），希望路由器将知道将它们引导到接收器的位置。没有连接建立，无法保证数据包将到达目的地。在实践中，它可以合理地运作，数据丢失很小。使用UDP为接收器使用UDP的另一个不同的益处是OS可以多路复用来自多个发送器的流，而不是必须手动组合超过多个TCP连接的数据（其中需要处理公平和饥饿问题）。</p><p> In our case, we use UDP unicast to transmit messages from the IODaemon to the Sequencer, and we do in fact care about message loss. This doesn’t bother us too much because we have a relatively easy way to detect packet drops. An IODaemon that is sending unsequenced messages to the sequencer is also reading the sequenced stream. The sequenced stream should eventually contain all of these messages being sent by the IODaemon. If a given set of messages are not seen on the sequenced stream within a predefined amount of time (e.g. 500ms), the IODaemon can assume the messages were lost, and can resend those messages. If the messages were in fact lost, the resend may succeed and things go back to normal; if the messages were not lost but simply delayed, the resend will cause duplicate messages to be read by the sequencer,  which is smart enough to discard the duplicates. The end result is that we’ve added reliable delivery to a lossy UDP transmission.</p><p> 在我们的情况下，我们使用UDP单播将来自iodaemon的消息传输到定序器，我们实际上关注消息丢失。这不会让我们烦恼太多，因为我们有一种相对简单的方法来检测数据包滴。向定序器发送未驱换消息的iodaemon也在读取顺序流。序列流最终应包含iodaemon发送的所有这些消息。如果在预定义的时间（例如500ms）内未在排序的流上看到一组给定消息，则iodaemon可以假设邮件丢失，并且可以重新发送这些消息。如果消息实际上丢失，重新发送可能会成功，事情恢复正常;如果消息未丢失但只需延迟，则重新发送将导致序列器读取重复的消息，这智能足以丢弃重复项。最终结果是我们为有损失的UDP传输添加了可靠的交付。</p><p> For disseminating the sequenced stream though, we use TCP, because there is no natural way to recover missed UDP packets from the sequencer. As such, and as described above, consumers come with varying performance characteristics and TCP fanout via the replicated store handles all of those situations well.</p><p> 为了传播测序的流，我们使用TCP，因为没有自然的方法来恢复序列仪中的错过的UDP报文。因此，如上所述，消费者通过复制商店掌握了所有这些情况的不同性能特征和TCP扇出。</p><p>  One way to create a trading system using the sequenced stream architecture is to truly sequence all of the inputs for the system on a single sequenced stream. However, for various reasons, this may get unwieldy especially when the ratio of different types of inputs is very large. For example, we may have 1 thousand order-related messages in the system, but 1 billion market data events. While it is perfectly reasonable to sequence them together onto a single sequenced stream, and capacity-wise, the system would be able to handle them just fine, there is just something icky about having to chew through a billion+ messages in order to process the 1000 order-related messages.</p><p>  使用顺序流架构创建交易系统的一种方法是真正将系统上的所有输入序列序列单个顺序流。然而，出于各种原因，特别是当不同类型的输入的比率非常大时，这可能会难以置信。例如，我们可能在系统中有1千个相关的消息，但是10亿市场数据事件。虽然将它们一起排序到单个测序的流和能力方面是完全合理的，但是系统将能够处理它们，只有icky才能咀嚼十亿+消息以便处理1000订单相关的消息。</p><p> In order to streamline this usage, we sequence market data onto a separate sequenced stream from the rest of the messages. This helps our Order Management System (OMS) significantly since it does not need to deal with market data, and is able to work with a much lighter stream. However, this creates an issue for our Algo Engine, which does need  both the order messages and the market data. For this reason, we actually create a composite stream from the two component streams and feed it into the algo engine. This composite stream is recorded to disk and also available to be streamed away to a remote server if necessary. The resulting image looks something like below.</p><p> 为了简化此用法，我们将市场数据序列到从邮件的其余部分单独的测序流。这有助于我们的订单管理系统（OMS）显着，因为它不需要处理市场数据，并且能够使用更轻微的流。但是，这为我们的ILGO引擎创造了一个问题，这需要订单消息和市场数据。因此，我们实际上从两个组件流创建了一个复合流，并将其馈送到ILGO引擎中。将该复合流记录到磁盘，如果需要，也可用于将其流式传输到远程服务器。结果图像看起来如下。</p><p>  A word about the content of these stream messages. From our prior experience, we knew we wanted to use a binary encoding for our internal communication, and preferably, one that has fields at a fixed position (for ease of random access). After looking at a few options, we landed on  Simple Binary Encoding  (SBE), which is in fact a  FIX standard.</p><p>  关于这些流消息的内容的单词。从我们的先前经验中，我们知道我们希望为我们的内部通信使用二进制编码，优选地，一个具有固定位置的字段的二进制编码（用于便于随机访问）。在查看几个选项后，我们降落在简单的二进制编码（SBE）上，实际上是一个修复标准。</p><p> SBE is  blazing  fast, supports direct field access, supports versioning, and provides a message processor that can generate Java encoder/decoder classes. We didn’t really do a full bake-off, and given that SBE is a FIX standard, we were kinda drawn towards it. After using it for a while, I’ll say that the performance is stellar, but I’m underwhelmed by the feature set, and in particular, support for an evolving schema. We’ve already extended SBE to do what we need it to do, but if I had to do this over, I’d look at  Cap’n Proto or  Flatbuffers, which we may yet do in the future.</p><p> SBE快速燃烧，支持直接现场访问，支持版本控制，并提供可以生成Java Encoder /解码器类的消息处理器。我们并没有真正烘焙，并且鉴于SBE是一个修复标准，我们有点朝着它绘制。使用它一段时间后，我会说性能是恒星，但我被特征集所做的，特别是支持不断发展的架构。我们已经扩展了SBE来做我们需要做的事情，但如果我不得不这样做，我会看看Cap'n Proto或FlinBuffers，我们将来可能会这样做。 </p><p>  Some of you “normal” engineers may be bewildered at the use of dedicated cores all over the place. First off, we never said this needed to be efficient in terms of hardware usage! Second, pinned threads on dedicated cores is a common, and often the only, way of achieving low latency.</p><p>你们中的一些人“正常”工程师可能会在所有这些地方使用专用核心困惑。首先，我们从未说过这是在硬件使用方面有效的！其次，专用核心上的固定线程是一种常见的，通常是实现低延迟的唯一方式。</p><p> These threads run in a tight loop and do not yield to the OS even when there is nothing to process, which is rather impolite in normal software architecture but totally accepted in the world of low latency! The OS has ways around this — for example, if the thread is making a system call every once in a while (e.g.  recvmsg to receive a message from the network), the kernel will take the opportunity to intercept the thread during those calls, if necessary. To get around  that, low-latency developers will use kernel-bypass to cut out the kernel from these processes completely (e.g.  DPDK,  VMA,  OpenOnload). And the rabbit hole goes deeper with FPGAs, real-time kernels, and other such contraptions.</p><p> 这些线程在紧密的循环中运行，即使没有任何可能的流程，也不会屈服于OS，这在正常的软件架构中是相当不礼貌，但在低延迟的世界中完全被接受！操作系统有围绕此方法 - 例如，如果线程在一段时间内每次发出系统调用（例如，从网络接收消息），则内核将掌握在这些呼叫期间拦截线程必要的。要遍及这个，低延迟开发人员将使用内核旁路完全从这些过程中剪掉内核（例如，DPDK，VMA，OpenOnload）。兔子洞与FPGA，实时内核和其他这样的腹部更深。</p><p>  We realize that the design described in this article is not the only way to achieve these outcomes and in fact, we’re not sure if anyone else does it this way at all. We are simply documenting what we landed on after our research, and this remains a work-in-progress.</p><p>  我们意识到本文中描述的设计不是实现这些结果的唯一方法，实际上我们不确定是否有其他人这么做。我们简单地记录了我们在研究后降落的内容，这仍然是一项过程。</p><p> As closing thoughts, I’ll repeat here what I’ve said elsewhere — if you think this is cool or that you can contribute to this work, please reach out at  careers@prooftrading.com. If you are a technologist, you’re good at what you do, and want to help build a modern platform and have an impact, there is likely a role for you at Proof. To show our employees that we care and we appreciate, we make them true partners, with handsome equity grants, possibly larger than anything you’ve seen in your career.</p><p> 作为结束思想，我会在这里重复我在此时在这里所说的 - 如果你认为这很酷或者你可以为这项工作做出贡献，请致电@ProokTrading.com。如果你是技术学家，你擅长你所做的事情，并希望帮助建立一个现代平台并产生影响，有可能为您的证明作用。为了向我们的员工展示我们关心和我们欣赏，我们使他们成为真正的伙伴，拥有英俊的股票赠款，可能比你在职业生涯中看到的任何东西都大。</p><p> If you have questions, reach out to me on Twitter:  @preraksanghvi, or reach us at  info@prooftrading.com.</p><p> 如果您有疑问，请访问我的推特：@preaksanghvi，或在info@prooktrading.com上联系我们。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/prooftrading/proof-engineering-the-message-bus-a7cc84e1104b">https://medium.com/prooftrading/proof-engineering-the-message-bus-a7cc84e1104b</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/交易/">#交易</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/测序/">#测序</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/based/">#based</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/消息/">#消息</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>