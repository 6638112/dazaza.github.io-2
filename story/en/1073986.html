<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>人工智能克服了大脑启发的硬件上的障碍AI Overcomes Stumbling Block on Brain-Inspired Hardware</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">AI Overcomes Stumbling Block on Brain-Inspired Hardware<br/>人工智能克服了大脑启发的硬件上的障碍</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-19 02:00:10</div><div class="page_narrow text-break page_content"><p>Today’s most successful artificial intelligence algorithms, artificial neural networks, are loosely based on the intricate webs of real neural networks in our brains. But unlike our highly efficient brains, running these algorithms on computers guzzles shocking amounts of energy: The biggest models  consume nearly as much power as five cars over their lifetimes.</p><p>今天最成功的人工智能算法，人工神经网络，松散地基于我们大脑中复杂的神经网络网络。但与我们高效的大脑不同，在计算机上运行这些算法消耗了惊人的能量：最大的型号在其寿命中消耗的能量几乎相当于五辆汽车。</p><p> Enter neuromorphic computing, a closer match to the design principles and physics of our brains that could become the energy-saving future of AI. Instead of shuttling data over long distances between a central processing unit and memory chips, neuromorphic designs imitate the architecture of the jelly-like mass in our heads, with computing units (neurons) placed next to memory (stored in the synapses that connect neurons). To make them even more brain-like, researchers combine neuromorphic chips with analog computing, which can process continuous signals, just like real neurons. The resulting chips are vastly different from the current architecture and computing mode of digital-only computers that rely on binary signal processing of 0s and 1s.</p><p>进入神经形态计算，它与我们大脑的设计原理和物理更接近，可能成为人工智能节能的未来。与在中央处理器和内存芯片之间长距离传输数据不同，神经形态设计模仿我们大脑中果冻状物质的结构，计算单元（神经元）放在内存旁边（存储在连接神经元的突触中）。为了让它们更像大脑，研究人员将神经形态芯片与模拟计算相结合，模拟计算可以像真实神经元一样处理连续信号。由此产生的芯片与当前纯数字计算机的架构和计算模式大不相同，后者依赖0和1的二进制信号处理。</p><p> With the brain as their guide, neuromorphic chips promise to one day demolish the energy consumption of data-heavy computing tasks like AI. Unfortunately, AI algorithms haven’t played well with the analog versions of these chips because of a problem known as device mismatch: On the chip, tiny components within the analog neurons are mismatched in size due to the manufacturing process. Because individual chips aren’t sophisticated enough to run the latest training procedures, the algorithms must first be trained digitally on computers. But then, when the algorithms are transferred to the chip, their performance breaks down once they encounter the mismatch on the analog hardware.</p><p>以大脑为向导，神经形态芯片有望有朝一日消除人工智能等数据密集型计算任务的能耗。不幸的是，人工智能算法没有很好地与这些芯片的模拟版本配合，因为一个被称为设备不匹配的问题：在芯片上，由于制造过程，模拟神经元中的微小组件在尺寸上不匹配。由于单个芯片不够复杂，无法运行最新的训练程序，因此必须首先在计算机上对算法进行数字训练。但是，当算法被转移到芯片上时，一旦遇到模拟硬件上的不匹配，它们的性能就会崩溃。</p><p>  Now, a  paper published last month in the  Proceedings of the National Academy of Sciences has finally revealed a way to bypass this problem. A team of researchers led by  Friedemann Zenke at the Friedrich Miescher Institute for Biomedical Research and  Johannes Schemmel at Heidelberg University showed that an AI algorithm known as a spiking neural network — which uses the distinctive communication signal of the brain, known as a spike — could work with the chip to learn how to compensate for device mismatch. The paper is a significant step toward analog neuromorphic computing with AI.</p><p>现在，上个月发表在《美国国家科学院院刊》上的一篇论文终于揭示了绕过这个问题的方法。弗里德里希·米谢尔生物医学研究所（Friedrich Miescher Institute for Biomedical Research）的弗里德曼·泽克（Friedmann Zenke）和海德堡大学（Heidelberg University）的约翰·谢梅尔（Johannes Schemmel）领导的一个研究团队表明，一种被称为尖峰神经网络（spiking neural network）的人工智能算法——它使用大脑独特，被称为尖峰-可以与芯片一起学习如何补偿设备失配。这篇论文是人工智能向模拟神经形态计算迈出的重要一步。</p><p> “The amazing thing is that it worked so well,” said  Sander Bohte, a spiking neural network expert at CWI, the national research institute for mathematics and computer science in the Netherlands. “It is quite an achievement and likely a blueprint for more with analog neuromorphic systems.”</p><p>“令人惊讶的是，它工作得如此之好，”荷兰国家数学和计算机科学研究所CWI的顶尖神经网络专家桑德·博特说。“这是一个相当大的成就，很可能是模拟神经形态系统的蓝图。”</p><p> The importance of analog computing to brain-based computing is subtle. Digital computing can effectively represent one binary aspect of the brain’s spike signal, an electrical impulse that shoots through a neuron like a lightning bolt. As with a binary digital signal, either the spike is sent out, or it’s not. But spikes are sent over time continuously — it’s an analog signal — and how our neurons decide to send out a spike in the first place is also continuous, based on a voltage inside the cell that changes over time. (When the voltage reaches a specific threshold compared to the voltage outside the cell, the neuron sends a spike.)</p><p>模拟计算对基于大脑的计算的重要性是微妙的。数字计算可以有效地代表大脑尖峰信号的一个二进制方面，即像闪电一样射入神经元的电脉冲。与二进制数字信号一样，尖峰信号要么发出，要么不发出。但是尖峰信号是随着时间的推移不断发送的——这是一种模拟信号——我们的神经元决定首先发送尖峰信号的方式也是连续的，基于细胞内随时间变化的电压。（当电压达到与细胞外电压相比的特定阈值时，神经元会发出一个尖峰。）</p><p> “In analog lies the beauty of the brain’s core computations. Emulating this key aspect of the brain is one of the main drivers of neuromorphic computing,” said  Charlotte Frenkel, a neuromorphic engineering researcher at the University of Zurich and ETH Zurich.</p><p>“在模拟中，大脑的核心计算是美丽的。模拟大脑的这一关键方面是神经形态计算的主要驱动力之一，”苏黎世大学和苏黎世联邦理工大学的神经形态工程研究人员Charlotte Frenkel说。</p><p>  In 2011, a group of researchers at Heidelberg University began developing a neuromorphic chip with both analog and digital aspects to closely model the brain for neuroscience experiments. Now led by Schemmel, the team has unveiled the latest version of the chip, dubbed BrainScaleS-2. Every analog neuron on the chip mimics a brain cell’s incoming and outgoing currents and voltage changes.</p><p>2011年，海德堡大学（Heidelberg University）的一组研究人员开始开发一种兼具模拟和数字功能的神经形态芯片，以便为神经科学实验建立大脑模型。现在由Schemmel领导，该团队发布了最新版本的芯片，名为BrainScaleS-2。芯片上的每个模拟神经元都模拟脑细胞的输入和输出电流及电压变化。</p><p> “You really have a dynamical system that is continuously exchanging information,” said Schemmel. And because the materials have different electrical properties, the chip transfers information 1,000 times faster than our brains.</p><p>“你真的有一个不断交换信息的动力系统，”Schemmel说。由于这些材料具有不同的电学性质，芯片传输信息的速度比我们的大脑快1000倍。</p><p> But since the analog neurons’ properties vary ever so slightly — the device mismatch problem — the voltages and levels of current also varies across neurons. The algorithms can’t handle this, since they were trained on computers with perfectly identical digital neurons, and suddenly their on-chip performance plummets.</p><p>但是，由于模拟神经元的特性变化非常微小——设备失配问题——神经元之间的电压和电流水平也会发生变化。这些算法无法处理这个问题，因为它们是在拥有完全相同的数字神经元的计算机上训练的，而芯片上的性能突然暴跌。</p><p> The new work shows a way forward. By including the chip in the training process, the authors showed that spiking neural networks could learn how to correct for the varying voltages on the BrainScaleS-2 chip. “This training setup is one of the first convincing proofs that variability can not only be compensated [for], but also likely exploited,” Frenkel said.</p><p>这项新工作显示了前进的方向。通过在训练过程中加入芯片，作者展示了尖峰神经网络可以学习如何校正BrainScaleS-2芯片上的变化电压。弗伦克尔说：“这种训练方式是最早令人信服的证据之一，证明变异性不仅可以得到补偿，还可能被利用。”。</p><p> To deal with device mismatch, the team combined an approach that allows the chip to talk to the computer with a new learning method called surrogate gradients, co-developed by Zenke specifically for spiking neural networks. It works by changing the connections between neurons to minimize how many errors a neural network makes in a task. (This is similar to the method used by non-spiking neural networks, called backpropagation.)</p><p>为了解决设备失配问题，该团队将一种允许芯片与计算机对话的方法与一种称为替代梯度的新学习方法相结合，该方法由Zenke联合开发，专门用于刺激神经网络。它通过改变神经元之间的连接来最小化神经网络在任务中的错误。（这与称为反向传播的非尖峰神经网络使用的方法类似。）</p><p>  Effectively, the surrogate gradient method was able to correct for the chip’s imperfections during training on the computer. First, the spiking neural network performs a simple task using the varying voltages of the analog neurons on the chip, sending recordings of the voltages back to the computer. There, the algorithm automatically learns how to best alter connections between its neurons to still play nicely with the analog neurons, continuously updating them on the chip while it learns. Then, when the training is complete, the spiking neural network performs the task on the chip. The researchers report that their network reached the same level of accuracy on a speech and vision task as the top spiking neural networks performing the task on computers. In other words, the algorithm learned exactly what changes it would need to make to overcome the device mismatch problem.</p><p>有效地，替代梯度法能够在计算机训练期间纠正芯片的缺陷。首先，脉冲神经网络利用芯片上模拟神经元的不同电压执行简单任务，将电压记录发送回计算机。在那里，算法会自动学习如何最好地改变神经元之间的连接，以便仍能与模拟神经元很好地配合，并在学习的同时在芯片上不断更新它们。然后，当训练完成时，脉冲神经网络在芯片上执行任务。研究人员报告说，他们的网络在语音和视觉任务上达到了与在计算机上执行该任务的顶尖神经网络相同的精度水平。换句话说，该算法准确地了解了克服设备不匹配问题需要进行哪些更改。</p><p> “The performance they achieved to do a real problem with this system is a big accomplishment,” said  Thomas Nowotny, a computational neuroscientist at the University of Sussex. And, as expected, they do so with impressive energy efficiency; the authors say that running their algorithm on the chip consumed about 1,000 times less energy than a standard processor would require.</p><p>瑟赛克斯大学的神经科学家Thomas Nowotny说：“他们用这个系统实现了一个真正的问题，这是一个巨大的成就。”而且，正如预期的那样，它们的能效令人印象深刻；作者们说，在芯片上运行他们的算法所消耗的能量是标准处理器所需能量的1000倍。</p><p>  However, Frenkel points out that while the energy consumption is good news so far, neuromorphic chips will still need to prove themselves against hardware that has been optimized for similar speech and vision recognition tasks, rather than standard processors. And Nowotny cautions that the approach may have trouble scaling up to large practical tasks, since it still requires shuttling the data back and forth between computer and chip.</p><p>然而，弗伦克尔指出，虽然到目前为止，能耗是个好消息，但神经形态芯片仍需要在针对类似语音和视觉识别任务而不是标准处理器进行优化的硬件上证明自己。Nowotny警告说，这种方法可能难以扩展到大型实际任务，因为它仍然需要在计算机和芯片之间来回传输数据。</p><p> The long-term goal is for spiking neural networks to train and run on neuromorphic chips from start to finish, without any need for a computer at all. But that would require building a new chip generation, which takes years, Nowotny said.</p><p>长期目标是使神经网络从头到尾在神经形态芯片上训练和运行，而完全不需要计算机。但诺沃特尼说，这需要建设新一代芯片，这需要数年时间。</p><p> For now, Zenke and Schemmel’s team is happy to show that spiking neural network algorithms can handle the minuscule variations between analog neurons on neuromorphic hardware. “You can rely on 60 or 70 years of experience and software history for digital computing,” Schemmel said. “For this analog computing, we have to do everything on our own.”</p><p>目前，Zenke和Schemmel的团队很高兴地证明，尖峰神经网络算法可以处理神经形态硬件上模拟神经元之间的微小变化。“你可以依靠60或70年的数字计算经验和软件历史，”Schemmel说。“对于这种模拟计算，我们必须自己做一切。”</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/硬件/">#硬件</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/人工智能/">#人工智能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/克服/">#克服</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/overcomes/">#overcomes</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/芯片/">#芯片</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>