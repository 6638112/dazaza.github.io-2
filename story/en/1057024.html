<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>一个人萨斯的建筑 The Architecture of a One-Man SaaS</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">The Architecture of a One-Man SaaS<br/>一个人萨斯的建筑 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-08 21:07:20</div><div class="page_narrow text-break page_content"><p>This is a long-form post breaking down the setup I use to run a SaaS. From load balancing to cron job monitoring to subscription and payments. There&#39;s a lot of ground to cover, so buckle up!</p><p>这是一个长形的帖子，分解了我用来运行SaaS的设置。从负载平衡到Cron作业监控到订阅和付款。在那里覆盖了很多地面，所以扣了！</p><p> As grandiose as the title of this article might sound, I should clarify we’re talking about a low-stress, one-person company that I run from my flat here in Germany. It&#39;s fully self-funded, and I really like to take things slow. It&#39;s probably not what most people imagine when I say &#34;tech startup&#34;.</p><p> 正如这篇文章的标题一样，宏伟可能会听起来，我应该澄清我们谈论一个低压力，我在德国的公寓里奔跑。它＆＃39;完全自资，我真的很想把事情慢。它可能不是大多数人想象的，当我说和＃34;科技创业公司＆＃34;</p><p> I wouldn&#39;t be able to do this without the vast amount of open-source software and managed services at my disposal. I feel like I’m standing on the shoulders of giants, who did all the hard work before me, and I’m very grateful for that.</p><p> 我不会在没有大量的开源软件和管理服务时执行此操作。我觉得我站在巨人的肩膀上，谁在我面前做过所有艰苦的工作，我非常感激。</p><p> For context, I run a one-man SaaS, and this is a more detailed version of my post on  the tech stack I use. Please consider your own circumstances before following my advice, your own context matters when it comes to  technical choices, there&#39;s no holy grail.</p><p> 对于上下文，我运行一个人的SaaS，这是我使用的技术堆栈上的帖子的更详细版本。在遵循我的建议之前，请考虑自己的情况，在技术选择方面，您自己的上下文很重要，而且没有圣杯。</p><p> I use Kubernetes on AWS, but don’t fall into the trap of thinking you need this. I learned these tools over several years mentored by a very patient team. I&#39;m productive because this is what I know best, and I can focus on shipping stuff instead. Your mileage may vary.</p><p> 我在AWS上使用Kubernetes，但不要陷入思维的陷阱，你需要这个。我在一个非常耐心的团队中学到了几年内的这些工具。我＆＃39; m生产性，因为这是我最擅长的，我可以专注于运输东西。你的旅费可能会改变。</p><p> By the way, I drew inspiration for the format of this post from  Wenbin Fang’s blog post. I really enjoyed reading his article, and you might want to check it out too!</p><p> 顺便说一下，我画了灵感来自Wenbin Fang的博客文章的这篇文章的格式。我真的很喜欢阅读他的文章，你也想看看它！</p><p>   My infrastructure handles multiple projects at once, but to illustrate things I’ll use  Panelbear, my most recent SaaS, as a real-world example of this setup in action.</p><p>   我的基础架构立即处理多个项目，但是为了说明我将使用PanelBear，我最近的SaaS的东西，作为此设置的真实界限。 </p><p>  From a technical point of view, this SaaS processes a large amount of requests per second from anywhere in the world, and stores the data in an efficient format for real time querying.</p><p>从技术角度来看，此SaaS从世界任何地方处理每秒大量请求，并以有效的格式存储数据以进行实时查询。</p><p> Business-wise it&#39;s still in its infancy (I launched six months ago), but it has grown rather quickly for my own expectations, especially as I originally built it for myself as a Django app using SQLite on a single tiny VPS. For my goals at the time, it worked just fine and I could have probably pushed that model quite far.</p><p> 仍然在其婴儿期间（我六个月前推出的商业＆＃39），但它已经增长了很快，尤其是我最初为自己建造它作为一个小型vps的sqlite的Django应用程序。为了我当时的目标，它就好了，我可能已经推动了这一模型。</p><p> However, I grew increasingly frustrated having to reimplement a lot of the tooling I was so accustomed to: zero downtime deploys, autoscaling, health checks, automatic DNS / TLS / ingress rules, and so on. Kubernetes spoiled me, I was used to dealing with higher level abstractions, while retaining control and flexibility.</p><p> 但是，我越来越沮丧必须重新实现很多工具我如此习惯：零停机部署，自动播放，健康检查，自动DNS / TLS / Ingress规则，等等。 Kubernetes宠坏了我，我被习惯于处理更高的抽象，同时保持控制和灵活性。</p><p> Fast forward six months, a couple of iterations, and even though my current setup is still a Django monolith, I&#39;m now using Postgres as the app DB, ClickHouse for analytics data, and Redis for caching. I also use Celery for scheduled tasks, and a custom event queue for buffering writes. I run most of these things on a managed Kubernetes cluster (EKS).</p><p> 快进六个月，几个迭代，即使我现在的设置仍然是一个Django Monolith，I＆＃39; M现在使用Postgres作为App DB，点击分析数据，以及Redis进行缓存。我还使用Celery进行计划任务，以及用于缓冲写入的自定义事件队列。我在托管的Kubernetes群集（eks）上运行大部分这些东西。</p><p>  It may sound complicated, but it&#39;s practically an old-school monolithic architecture running on Kubernetes. Replace Django with Rails or Laravel and you know what I&#39;m talking about. The interesting part is how everything is glued together and automated: autoscaling, ingress, TLS certificates, failover, logging, monitoring, and so on.</p><p>  它可能听起来很复杂，但它实际上是在Kubernetes上运行的老学校单片架构。用铁轨或小旅行替换django，你知道我在谈论什么。有趣的部分是一切如何粘在一起和自动化：自动播放，入口，TLS证书，故障转移，记录，监控等。</p><p> It&#39;s worth noting I use this setup across multiple projects, which helps keep my costs down and launch experiments really easily (write a Dockerfile and git push). And since I get asked this a lot: contrary to what you might be thinking, I actually spend very little time managing the infrastructure, usually 0-2 hours per month total. Most of my time is spent developing features, doing customer support, and growing the business.</p><p> 值得注意的是我在多个项目中使用这个设置，这有助于保持我的成本并可轻松启动实验（写入DockerFile和Git Push）。既然我被问到了这一点：与你可能正在思考的相反，我实际上花了很少的时间管理基础设施，通常每月0-2小时。我的大部分时间都花在开发功能，做客户支持，并越来越多的业务。</p><p> That said, these are the tools I’ve been using for several years now and I’m pretty familiar with them. I consider my setup simple for what it’s capable of, but it took many years of production fires at my day job to get here. So I won’t say it’s all sunshine and roses.</p><p> 也就是说，这些是我已经使用了几年的工具，我很熟悉他们。我认为我的设置很简单，因为它的能力是什么，但在我的日常工作中需要多年的生产火灾。所以我不会说这是阳光和玫瑰。 </p><p> I don&#39;t know who said it first, but what I tell my friends is: &#34;Kubernetes makes the simple stuff complex, but it also makes the complex stuff simpler&#34;.</p><p>我不知道谁首先说了谁，但我告诉我的朋友是：＆＃34; kubernetes制作简单的东西复杂，但它也使得复杂的东西更简单＆＃34;</p><p>  Now that you know I have a managed Kubernetes cluster on AWS and I run various projects in it, let&#39;s make the first stop of the tour: how to get traffic into the cluster.</p><p>  既然你知道我在AWS上拍摄了一个托管的kubernetes集群，我在它中运行各种项目，让＆＃39; s发出第一个停止：如何将流量进入群集。</p><p> My cluster is in a private network, so you won’t be able to reach it directly from the public internet. There’s a couple of pieces in between that control access and load balance traffic to the cluster.</p><p> 我的群集在私人网络中，因此您将无法直接从公共互联网到达。控制访问权限和负载均衡流量之间有几个部分。</p><p> Essentially, I have Cloudflare proxying all traffic to an NLB (AWS L4 Network Load Balancer). This Load Balancer is the bridge between the public internet and my private network. Once it receives a request, it forwards it to one of the Kubernetes cluster nodes. These nodes are in private subnets spread across multiple availability zones in AWS. It&#39;s all managed by the way, but more on that later.</p><p> 基本上，我将CloudFlare代理到NLB的所有流量（AWS L4网络负载平衡器）。此负载平衡器是公共互联网和我的私人网络之间的桥梁。一旦收到请求，它就会将其转发到一个Kubernetes群集节点。这些节点位于AWS中的多个可用区域的私有子网中。它逐渐管理，但更稍后更多。</p><p> Traffic gets cached at the edge, or forwarded to the AWS region where I operate.</p><p> 流量在边缘缓存，或转发到我操作的AWS区域。</p><p> &#34;But how does Kubernetes know which service to forward the request to?&#34; - That’s where  ingress-nginx comes in. In short: it&#39;s an NGINX cluster managed by Kubernetes, and it&#39;s the entrypoint for all traffic inside the cluster.</p><p> ＆＃34;但Kubernetes如何知道将请求转发到哪个服务？＆＃34; - 那就是Ingress-Nginx进入的地方。简而言之：由Kubernetes管理的NginX群集，它＆＃39;＆＃39;群体中的所有流量的入口点。</p><p> NGINX applies rate-limiting and other traffic shaping rules I define before sending the request to the corresponding app container. In Panelbear’s case, the app container is Django being served by  Uvicorn.</p><p> nginx应用速率限制和其他流量整形规则我在向相应的应用程序容器发送请求之前定义。在PanelBear的情况下，应用程序容器是uvicorn服务的Django。 </p><p> It&#39;s not much different from a traditional nginx/gunicorn/Django in a VPS approach, with added horizontal scaling benefits and an automated CDN setup. It’s also a “setup once and forget” kind of thing, mostly a few files between Terraform/Kubernetes, and it’s shared by all deployed projects.</p><p>它与VPS方法中的传统nginx / gunicorn / django不同，＆＃39;在VPS方法中，增加了水平缩放优势和自动化CDN设置。它也是一个“Setup一次忘记”的东西，主要是Terraform / Kubernetes之间的一些文件，它是由所有部署的项目共享的。</p><p> When I deploy a new project, it’s essentially 20 lines of ingress configuration and that’s it:</p><p> 当我部署一个新项目时，它基本上是20行的入口配置，这是：</p><p> apiVersion :  networking.k8s.io/v1beta1  kind :  Ingress  metadata :    namespace :  example   name :  example - api  annotations :    kubernetes.io/ingress.class :   &#34;nginx&#34;    nginx.ingress.kubernetes.io/limit-rpm :   &#34;5000&#34;    cert-manager.io/cluster-issuer :   &#34;letsencrypt-prod&#34;    external-dns.alpha.kubernetes.io/cloudflare-proxied :   &#34;true&#34;   spec :   tls :   -   hosts :    -  api.example.com   secretName :  example - api - tls  rules :   -   host :  api.example.com   http :    paths :    -   path :  /   backend :    serviceName :  example - api   servicePort :  http</p><p> apiersion：networking.k8s.io/v1beta1种类：Ingress元数据：命名空间：示例名称：示例 -  API注释：Kubernetes.io/ingress.class：＆＃34; nginx＆＃34; nginx.ingress.kubernetes.io/limit-rpm：＆＃34; 5000＆＃34; cert-manager.io/cluster-issuer：＆＃34; letsencrypt-prod＆＃34; External-DNS.Alpha.kubernetes.io/cloudflare-proxied：＆＃34;真实＆＃34; SPEM：TLS： - 主机： -  API.EXAMPLEM。秘密名称：示例 -  API  -  TLS规则： - 主机：API.EXAMPLECOM HTTP：PATHS： - 路径：/ bathend：serviceName：示例 -  API ServicePort：HTTP</p><p> Those annotations describe that I want a DNS record, with traffic proxied by Cloudflare, a TLS certificate via letsencrypt, and that it should rate-limit the requests per minute by IP before forwarding the request to my app.</p><p> 这些注释描述了我想要一个DNS记录，通过LetSencrypt通过CloudFlare代理的流量，并且它应该在将请求转发到我的应用之前通过IP限制每分钟的请求。</p><p> Kubernetes takes care of making those infra changes to reflect the desired state. It’s a little verbose, but it works well in practice.</p><p> Kubernetes负责使那些infra改变以反映所需的状态。这有点冗长，但它在实践中很好。</p><p>   Whenever I push to master one of my projects, it kicks off a CI pipeline on GitHub Actions. This pipeline runs some codebase checks, end-to-end tests (using Docker compose to setup a complete environment), and once these checks pass it builds a new Docker image that gets pushed to ECR (the Docker registry in AWS).</p><p>   每当我推动掌握我的一个项目之一时，它会在GitHub操作上启动CI管道。该管道运行一些代码库检查，端到端测试（使用Docker撰写撰写以设置完整的环境），一旦这些检查传递，它就会构建一个被推送到ECR的新Docker映像（AWS中的Docker注册表）。</p><p> As far as the application repo is concerned, a new version of the app has been tested and is ready to be deployed as a Docker image:</p><p> 就应用程序repo而言，已测试新版本的应用程序，并准备将部署为Docker Image： </p><p>  &#34;So what happens next? There’s a new Docker image, but no deploy?&#34; - My Kubernetes cluster has a component called  flux. It automatically keeps in sync what is currently running in the cluster and the latest image for my apps.</p><p>＆＃34;那么接下来会发生什么？有一个新的Docker图像，但没有部署？＆＃34; - 我的Kubernetes群集有一个名为Flux的组件。它会自动保持同步当前在群集中运行的内容以及我的应用程序的最新图像。</p><p>  Flux automatically triggers an incremental rollout when there’s a new Docker image available, and keeps record of these actions in an &#34;Infrastructure Monorepo&#34;.</p><p>  当有一个新的Docker Image提供了新的Docker Image时，助焊剂会自动触发增量卷展栏，并在AN＆＃34中保留这些动作的记录;基础设施Monorepo＆＃34;</p><p> I want version controlled infrastructure, so that whenever I make a new commit on this repo, between Terraform and Kubernetes, they will make the necessary changes on AWS, Cloudflare and the other services to synchronize the state of my repo with what is deployed.</p><p> 我想要版本控制的基础架构，这样只要我在Terraform和Kubernetes之间做出新的提交，他们将对AWS，CloudFlare和其他服务进行必要的更改，以将我的回购状态与部署的内容同步。</p><p> It’s all version-controlled with a linear history of every deployment made. This means less stuff for me to remember over the years, since I have no magic settings configured via clicky-clicky on some obscure UI.</p><p> 它全部版本控制，每个部署的线性历史记录。这意味着多年来我要记住的东西更少，因为我没有通过Clicky Clicky在一些晦涩的ui上配置了魔法设置。</p><p>   A few years ago I used the Actor model of concurrency for various company projects, and fell in love with many of the ideas around its ecosystem. One thing lead to another and soon I was reading books about Erlang, and its philosophy around  letting things crash.</p><p>   几年前，我使用了各种公司项目的actor并发的演员模型，并与其生态系统周围的许多想法坠入爱河。有一件事导致另一件事，很快我就读了关于erlang的书籍，以及它在让事情崩溃的情况下的哲学。</p><p> I might be stretching the idea too much, but in Kubernetes I like to think of liveliness probes and automatic restarts as a means to achieve a similar effect.</p><p> 我可能会伸展太多的想法，但在血腥的人中，我喜欢想到活力探测和自动重启作为实现类似效果的手段。</p><p> From the  Kubernetes documentation:“The kubelet uses liveness probes to know when to restart a container. For example, liveness probes could catch a deadlock, where an application is running, but unable to make progress. Restarting a container in such a state can help to make the application more available despite bugs.”</p><p> 从Kubernetes文档中：“Kubelet使用Liventive探针知道何时重新启动容器。例如，Live Probes可以捕获僵局，其中应用程序正在运行，但无法进行进度。在这样的状态下重新启动容器可以帮助您更好地提供应用程序。“ </p><p> In practice this has worked pretty well for me. Containers and nodes are meant to come and go, and Kubernetes will gracefully shift the traffic to healthy pods while healing the unhealthy ones (more like killing). Brutal, but effective.</p><p>在实践中，这对我来说很好。容器和节点旨在来，血管对齐会优雅地将流量转移到健康的豆荚，同时治愈不健康的豆荚（更像杀戮）。野蛮，但有效。</p><p>  My app containers auto-scale based on CPU/Memory usage. Kubernetes will try to pack as many workloads per node as possible to fully utilize it.</p><p>  我的应用程序容器基于CPU /内存使用情况自动规模。 Kubernetes将尝试每个节点打包尽可能多的工作负载以充分利用它。</p><p> In case there’s too many Pods per node in the cluster, it will automatically spawn more servers to increase the cluster capacity and ease the load. Similarly, it will scale down when there’s not much going on.</p><p> 如果群集中的每个节点的POD太多，它将自动产生更多服务器以增加集群容量并缓解负载。同样，当没有太多的情况下，它将缩减。</p><p>  apiVersion :  autoscaling/v1  kind :  HorizontalPodAutoscaler  metadata :    name :  panelbear - api   namespace :  panelbear  spec :    scaleTargetRef :    apiVersion :  apps/v1   kind :  Deployment   name :  panelbear - api   minReplicas :   2    maxReplicas :   8    targetCPUUtilizationPercentage :   50</p><p>  apiersion：autocaling / v1种类：stallypodautoscaler元数据：名称：panelbear  -  api命名空间：panelbear spec：scaletargetref：appiersion：appionion：appsion：appsion：appsion：deployment name：panelbear  -  api minreplicas：2 maxreplicas：8 targetcpuutilizationpercenty：5</p><p> In this example, it will automatically adjust the number of  panelbear-api pods based on the CPU usage, starting at 2 replicas but capping at 8.</p><p> 在此示例中，它将根据CPU使用情况自动调整PanelBear-API Pods的数量，从2副本开始，但盖住8。</p><p>  When defining the ingress rules for my app, the annotation  cloudflare-proxied: &#34;true&#34; is what tells the Kubernetes that I want to use Cloudflare for DNS, and to proxy all requests via it’s CDN and DDoS protection too.</p><p>  在为我的应用程序定义入口规则时，注释CloudFlare-Proxied：＆＃34;真实＆＃34;是告诉Kubernetes我想使用CloudFlare for DNS，并通过它的CDN和DDOS保护来代理所有请求。</p><p> From then on, it’s pretty easy to make use of it. I just set standard HTTP cache headers in my applications to specify which requests can be cached, and for how long.</p><p> 从那时起，它很容易利用它。我只是在我的应用程序中设置标准HTTP缓存标头，以指定可以缓存哪个请求，以及多长时间。 </p><p>  Cloudflare will use those response headers to control the caching behavior at the edge servers. It works amazingly well for such a simple setup.</p><p>CloudFlare将使用那些响应标头来控制边缘服务器的缓存行为。这对于这么简单的设置非常好。</p><p> I use  Whitenoise to serve static files directly from my app container. That way I avoid needing to upload static files to Nginx/Cloudfront/S3 on each deployment. It has worked really well so far, and most requests will get cached by the CDN as it gets filled. It&#39;s performant, and keeps things simple.</p><p> 我使用Whitenoise直接从我的应用程序容器提供静态文件。这样，我避免在每个部署上将静态文件上传到nginx / cloudfront / s3。到目前为止，它已经很好地工作了，大多数请求将被CDN填补的CDN缓存。它的表演，并保持简单的事情。</p><p> I also use NextJS for a few static websites, such as the landing page of  Panelbear. I could serve it via Cloudfront/S3 or even Netlify or Vercel, but it was easy to just run it as a container in my cluster and let Cloudflare cache the static assets as they are being requested. There’s zero added cost for me to do this, and I can re-use all tooling for deployment, logging and monitoring.</p><p> 我还使用NextJS了解一些静态网站，例如Panelbear的着陆页。我可以通过CloudFront / S3甚至NetWify或Vercel服务，但很容易将其作为群集中的容器运行，让CloudFlare在所要求的要求时缓存静态资产。我来执行此操作的零增加成本，我可以重新使用所有工具进行部署，记录和监控。</p><p>  Besides static file caching, there&#39;s also application data caching (eg. results of heavy calculations, Django models, rate-limiting counters, etc...).</p><p>  除了静态文件缓存，还有＃39;还有应用数据缓存（例如，重计算结果，Django模型，限速计数器等）。</p><p> On one hand I leverage an in-memory  Least Recently Used (LRU) cache to keep frequently accessed objects in memory, and I’d benefit from zero network calls (pure Python, no Redis involved).</p><p> 一方面，我利用最近使用的内存最少使用（LRU）缓存以保持频繁访问的对象在内存中，并且我会受益于零网络调用（纯Python，没有涉及的Redis）。</p><p> However, most endpoints just use the in-cluster Redis for caching. It&#39;s still fast and the cached data can be shared by all Django instances, even after re-deploys, while an in-memory cache would get wiped.</p><p> 但是，大多数端点只需使用群集redis进行缓存。它＆＃39仍然快速，缓存的数据可以由所有Django实例共享，即使在重新部署之后也可以共享，而内存中缓存会被擦除。</p><p>  My Pricing Plans are based on analytics events per month. For this some sort of metering is necessary to know how many events have been consumed within the current billing period and enforce limits. However, I don&#39;t interrupt the service immediately when a customer crosses the limit. Instead a &#34;Capacity depleted&#34; email is automatically sent, and a grace period is given to the customer before the API starts rejecting new data.</p><p>  我的定价计划基于每月的分析事件。对于这种一些计量，必须知道在当前结算周期内消耗了多少事件并强制实施限制。但是，当客户交叉限制时，我不会立即中断服务。而不是a＆＃34;容量耗尽＆＃34;电子邮件已自动发送，并在API开始拒绝新数据之前给客户提供宽限期。 </p><p> This is meant to give customers enough time to decide if an upgrade makes sense for them, while ensuring no data is lost. For example during a traffic spike in case their content goes viral or if they&#39;re just enjoying the weekend and not checking their emails. If the customer decides to stay in the current plan and not upgrade, there is no penalty and things will go back to normal once usage is back within their plan limits.</p><p>这意味着为客户提供足够的时间来决定升级是否对它们有意义，同时确保没有数据丢失。例如，在交通飙升期间，如果他们的内容变得病毒，或者如果它们＆＃39;重新享受周末并没有检查他们的电子邮件。如果客户决定留在目前的计划并没有升级，那么一旦使用内部的计划限制，就没有罚款，事情将恢复正常。</p><p> So for this feature I have a function that applies the rules above, which require several calls to the DB and ClickHouse, but get cached 15 minutes to avoid recomputing this on every request. It&#39;s good enough and simple. Worth noting: the cache gets invalidated on plan changes, otherwise it might take 15 minutes for an upgrade to take effect.</p><p> 因此，对于此功能，我有一个应用上述规则的函数，这需要多个调用DB和Clickhouse，但是要缓存15分钟，以避免在每个请求上重新计算此项。它＆＃39; s的足够好，简单。值得注意的是：缓存在计划变更时无效，否则可能需要15分钟才能生效。</p><p> @cache ( ttl = 60   *   15 )   def   has_enough_capacity ( site :  Site )   - &gt;   bool :    &#34;&#34;&#34;  Returns True if a Site has enough capacity to accept incoming events,  or False if it already went over the plan limits, and the grace period is over.  &#34;&#34;&#34;</p><p> @cache（ttl = 60 * 15）def has_enough_capacity（站点：网站） - ＆gt; BOOL：＆＃34;＆＃34;＆＃34;如果站点有足够的容量接受传入事件，则返回true，或者如果它已经过计划限制，则宽限期结束。 ＆＃34;＆＃34;＆＃34;</p><p>  While I enforce global rate limits at the nginx-ingress on Kubernetes, I sometimes want more specific limits on a per endpoint/method basis.</p><p>  虽然我在Kubernetes的Nginx-Ingress执行全球速率限制时，我有时需要对每个端点/方法的更具体限制。</p><p> For that I use the excellent  Django Ratelimit library to easily declare the limits per Django view. It&#39;s configured to use Redis as a backend for keeping track of the clients making the requests to each endpoint (it stores a hash based on the client key, and not the IP).</p><p> 为此，我使用优秀的Django RAIRIMIT库来轻松声明每个Django视图的限制。它配置为使用REDIS作为后端使用REDIS，以便跟踪将请求的客户端追踪每个端点（它基于客户端密钥存储散列，而不是IP）。</p><p>  class   MySensitiveActionView ( RatelimitMixin ,  LoginRequiredMixin ) :   ratelimit_key  =   &#34;user_or_ip&#34;   ratelimit_rate  =   &#34;5/m&#34;   ratelimit_method  =   &#34;POST&#34;   ratelimit_block  =   True    def   get ( ) :    . . .    def   post ( ) :    . . .</p><p>  class mysensitiveActionView（Ratelimitmixin，Loginrequiredmixin）：Ratelimit_Key =＆＃34; user_or_ip＆＃34; Ratelimit_rate =＆＃34; 5 / m＆＃34; Ratelimit_Method =＆＃34;邮政＆＃34; RATELIMIT_BLOCK = TRUE DEF GET（）：。 。 。 def post（）：。 。 。</p><p> In the example above, if the client attempts to POST to this particular endpoint more than 5 times per minute, the subsequent call will get rejected with a  HTTP 429 Too Many Requests status code.</p><p> 在上面的示例中，如果客户端尝试将此特定的端点发布到每分钟超过5次，则随后的呼叫将被HTTP 429拒绝太多请求状态代码。 </p><p>   Django gives me an admin panel for all my models for free. It’s built-in, and It’s pretty handy for inspecting data for customer support work on the go.</p><p>Django免费为我的模型提供管理面板。它是内置的，它非常方便检查客户支持工作的数据。</p><p>  I added actions to help me manage things from the UI. Things like blocking access to suspicious accounts, sending out announcement emails, and approving full account deletion requests (first a soft delete, and within 72 hours a full destroy).</p><p>  我添加了帮助我从UI管理事物的操作。像阻止访问可疑帐户的东西一样，发送公告电子邮件，并批准完整的帐户删除请求（首先是一个软删除，并且在72小时内完全销毁）。</p><p> Security-wise: only staff users are able to access the panel (me), and I’m planning to add 2FA for extra security on all accounts.</p><p> 安全性：只能访问面板（ME）的工作人员，我计划在所有帐户中添加2FA以额外的安全性。</p><p> Additionally every time a user logs in, I send an automatic security email with details about the new session to the account’s email. Right now I send it on every new login, but I might change it in the future to skip known devices. It’s not a very “MVP feature”, but I care about security and it was not complicated to add. At least I’d be warned if someone logged in to my account.</p><p> 此外，每次用户登录时，我都会向帐户的电子邮件发送有关新会话的详细信息，请发送自动安全电子邮件。现在我将其发送到每个新的登录，但我可能会在将来更改它以跳过已知的设备。这不是一个非常“MVP功能”，但我关心安全性，添加并没有复杂。如果有人登录我的帐户，我至少会警告。</p><p> Of course, there&#39;s a lot more to hardening an application than this, but that&#39;s out of the scope of this post.</p><p> 当然，还有很多东西来加强应用程序而不是这个，但是这个帖子的范围＆＃39;</p><p>   Another interesting use case is that I run a lot of different scheduled jobs as part of my SaaS. These are things like generating daily reports for my customers, calculating usage stats every 15 minutes, sending staff emails (I get a daily email with the most important metrics) and whatnot.</p><p>   另一个有趣的用例是我运行了很多不同的预定作业作为我的SaaS的一部分。这些是为客户生成日常报告，每15分钟计算使用统计数据，发送员工电子邮件（我收到每日电子邮件，最重要的指标）和Whatnot。</p><p> My setup is actually pretty simple, I just have a few Celery workers and a Celery beat scheduler running in the cluster. They are configured to use Redis as the task queue. It took me an afternoon to set it up once, and luckily I haven’t had any issues so far.</p><p> 我的设置实际上很简单，我只有几个芹菜工人和一个在群集中运行的芹菜击败计划程序。它们配置为使用REDIS作为任务队列。我花了一个下午来设置一次，幸运的是到目前为止我没有任何问题。 </p><p> I want to get notified via SMS/Slack/Email when a scheduled task is not running as expected. For example when the weekly reports task is stuck or significantly delayed. For that I use  Healthchecks.io, but checkout  Cronitor and  CronHub too, I&#39;ve been hearing great things about them as well.</p><p>当计划的任务未按预期运行时，我想通过SMS / Slack /电子邮件通知。例如，当每周报告任务卡住或显着延迟时。为此，我使用healthchecks.io，但是结账结束和cronhub，我也一直听到他们的伟大事物。</p><p>  To abstract their API, I wrote a small Python snippet to automate the monitor creation and status pinging:</p><p>  要抽象他们的API，我写了一款小型Python代码段来自动化监视器创建和状态ping：</p><p> def   some_hourly_job ( ) :    # Task logic    . . .    # Ping monitoring service once task completes   TaskMonitor (   name = &#34;send_quota_depleted_email&#34; ,   expected_schedule = timedelta ( hours = 1 ) ,   grace_period = timedelta ( hours = 2 ) ,    ) . ping ( )</p><p> def some_hourly_job（）：＃任务逻辑。 。 。 #ping监视服务一旦完成任务完成任务介绍（名称=＆＃34; send_quota_depleeted_email＆＃34;，permaned_schedule = timedelta（hours = 1），grace_period = timedelta（小时= 2），）。 ping（）</p><p>  All my applications are configured via environment variables, old school but portable and well supported. For example, in my Django  settings.py I’d setup a variable with a default value:</p><p>  所有我的应用程序都是通过环境变量配置的，旧学校但便携式和支持良好。例如，在我的django settings.py中，我会使用默认值设置变量：</p><p>     apiVersion :  v1  kind :  ConfigMap  metadata :    namespace :  panelbear   name :  panelbear - webserver - config  data :    INVITE_ONLY :   &#34;True&#34;    DEFAULT_FROM_EMAIL :   &#34;The Panelbear Team &lt;support@panelbear.com&gt;&#34;    SESSION_COOKIE_SECURE :   &#34;True&#34;    SECURE_HSTS_PRELOAD :   &#34;True&#34;    SECURE_SSL_REDIRECT :   &#34;True&#34;</p><p>     apiersion：v1种类：configmap元数据：命名空间：panelbear名称：panelbear  -  webserver  - 配置数据：invite_only：＆＃34;真＆＃34; default_from_email：＆＃34; panelbear团队＆lt; support@panelbear.com& gt;＆＃34; session_cookie_secure：＆＃34;真＆＃34; secure_hsts_preload：＆＃34;真＆＃34; secure_sl_redirect：＆＃34;真＆＃34;</p><p>  The way secrets are handled is pretty interesting: I want to also commit them to my infrastructure repo, alongside other config files, but secrets should be encrypted.</p><p>  处理方式的秘密是非常有趣的：我想向我的基础架构回购，以及其他配置文件，但应该加密秘密。</p><p> For that I use  kubeseal in Kubernetes. This component uses asymmetric crypto to encrypt my secrets, and only a cluster authorized to access the decryption keys can decrypt them.</p><p> 为此，我在kubernetes中使用kubeseal。此组件使用不对称的Crypto加密我的秘密，只有授权访问解密密钥的群集可以解密它们。 </p><p>   The cluster will automatically decrypt the secrets and pass them to the corresponding container as an environment variable:</p><p>群集将自动解密秘密并将其传递给相应的容器作为环境变量：</p><p>  To protect the secrets within the cluster, I use AWS-managed encryption keys via  KMS, which are rotated regularly. This is a single setting when creating the Kubernetes cluster, and it&#39;s fully managed.</p><p>  要保护群集中的秘密，我可以通过KMS使用AWS管理的加密密钥，定期旋转。创建Kubernetes集群时，这是一个设置，它完全管理。</p><p> Operationally what this means is that I write the secrets as environment variables in a Kubernetes manifest, I then run a command to encrypt them before committing, and push my changes.</p><p> 在操作上，这意味着我将秘密写为kubernetes中的环境变量，然后我在提交之前运行一个命令来加密它们，然后推动我的更改。</p><p> The secrets are deployed within a few seconds, and the cluster will take care of automatically decrypting them before running my containers.</p><p> 秘密在几秒钟内部署，并且在运行我的容器之前，群集将自动解密它们。</p><p>  For experiments I run a vanilla Postgres container within the cluster, and a Kubernetes cronjob that does daily backups to S3. This helps keep my costs down, and it’s pretty simple for just starting out.</p><p>  对于实验，我在群集中运行一个vanilla postgres容器，以及每天备份的Kubernetes Cronjob到S3。这有助于保持我的成本，并且刚刚开始，这很简单。</p><p> However, as a project grows, like Panelbear, I move the database out of the cluster into RDS, and let AWS take care of encrypted backups, security updates and all the other stuff that’s no fun to mess up.</p><p> 但是，随着项目的增长，如PanelBear，我将数据库从群集中移出到RDS中，让AWS处理加密的备份，安全更新以及所有其他任何其他东西，这些内容无需搞得搞砸。</p><p> For added security, the databases managed by AWS are still deployed within my private network, so they’re unreachable via the public internet.</p><p> 为了增加安全性，由AWS管理的数据库仍然部署在我的专用网络中，因此它们通过公共互联网无法访问。 </p><p>  I rely on  ClickHouse for efficient storage and (soft) real-time queries over the analytics data in Panelbear. It’s a fantastic columnar database, incredibly fast and when you structure your data well you can achieve high compression ratios (less storage costs = higher margins).</p><p>我依靠Clickhouse进行高效存储和（软）实时查询PanelBear中的分析数据。这是一个奇妙的柱状数据库，令人难以置信的快速，当您构建数据时，您可以实现高压缩比（较少的存储成本=更高的边距）。</p><p> I currently self-host a ClickHouse instance within my Kubernetes cluster. I use a StatefulSet with encrypted volume keys managed by AWS. I have a Kubernetes CronJob that periodically backups up all data in an efficient columnar format to S3. In case of disaster recovery, I have a couple of scripts to manually backup and restore the data from S3.</p><p> 我目前在我的Kubernetes集群中自我主持一个单击小时实例。我使用由AWS管理的加密卷密钥使用状态。我有一个Kubernetes Cronjob，它定期以高效的柱状格式备份所有数据到S3。在灾难恢复的情况下，我有几个脚本来手动备份和恢复来自S3的数据。</p><p> ClickHouse has been rock-solid so far, and it’s an impressive piece of software. It’s the only tool I wasn’t already familiar with when I started my SaaS, but thanks to their docs I was able to get up and running pretty quickly.</p><p> Clickhouse到目前为止已经摇滚固体，这是一款令人印象深刻的软件。这是我唯一熟悉的工具，当我开始我的萨斯时，但感谢他们的文档，我能够快速起床和运行。</p><p> I think there’s a lot of low hanging fruit in case I wanted to squeeze out even more performance (eg. optimizing the field types for better compression, pre-computing materialized tables and tuning the instance type), but it’s good enough for now.</p><p> 我认为我想挤出更多的性能（例如，优化更好压缩的字段类型，预先计算物化表并调整实例类型），但现在已经足够了，但现在足够好。</p><p>  Besides Django, I also run containers for Redis, ClickHouse, NextJS, among other things. These containers have to talk to each other somehow, and that somehow is via the built-in service discovery in Kubernetes.</p><p>  除了Django，我还运行redis，Clickhouse，NextJ的容器等。这些容器必须以某种方式互相交流，以某种方式通过Kubernetes中内置的服务发现。</p><p> It’s pretty simple: I define a Service resource for the container and Kubernetes automatically manages DNS records within the cluster to route traffic to the corres</p><p> 它非常简单：我定义了容器的服务资源，Kubernetes会自动管理群集中的DNS记录，以将流量路由到校正</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://anthonynsimon.com/blog/one-man-saas-architecture/">https://anthonynsimon.com/blog/one-man-saas-architecture/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/建筑/">#建筑</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/萨斯/">#萨斯</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/man/">#man</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>