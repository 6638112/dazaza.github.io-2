<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Kubernetes集群中OOMKill警报指南Guide to OOMKill Alerting in Kubernetes Clusters</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Guide to OOMKill Alerting in Kubernetes Clusters<br/>Kubernetes集群中OOMKill警报指南</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-24 07:00:20</div><div class="page_narrow text-break page_content"><p>RAM is most likely the scarcest resource that is first exhausted on your servers. If you’re serious about running software under Linux/Unix, you’re certainly aware of what an OOMKill is.</p><p>RAM很可能是服务器上最先耗尽的最稀缺的资源。如果您真的想在Linux / Unix下运行软件，那么您肯定会知道OOMKill是什么。</p><p>  Short refresher: when a program requests a new memory page from the kernel two things can happen.</p><p>  简短的更新：当程序从内核请求一个新的内存页时，可能会发生两种情况。</p><p>  There is a free memory page: The kernel page assigns the page to the process and everything is great.</p><p>  有一个空闲的内存页面：内核页面将页面分配给进程，一切都很好。</p><p>  The system is Out Of Memory (OOM): The kernel chooses a process based on its ‘badness’ (mainly by how much ram it uses). It sends a SIGKILL to the process. This forces the receiving process to exit with exit code  137. All the memory pages belonging to that process are free and now the kernel can fulfill the memory request.</p><p>  系统内存不足（OOM）：内核根据其“不良”（主要取决于其使用的内存量）选择进程。它将SIGKILL发送到该进程。这迫使接收进程退出，并带有退出代码137。属于该进程的所有内存页面都是空闲的，现在内核可以满足内存请求。</p><p>  Lately, I had a task to add alerting to a sizeable Kubernetes cluster. The cluster has ~100 active Deployments with autoscaling of nodes up to ~50 nodes at peak times. The cluster is well maintained and has a robust autoscaling strategy. All deployments have resource limits defined. Sometimes, some of the deployed pods would breach the memory limits. In those cases, it would be nice to find out when that happens and investigate the cause of it.</p><p>  最近，我有一项任务是向大型Kubernetes集群添加警报。群集具有约100个活动部署，在高峰时间可自动缩放节点，最多可扩展约50个节点。集群维护良好，并具有强大的自动扩展策略。所有部署都定义了资源限制。有时，某些已部署的Pod会违反内存限制。在这些情况下，最好找出何时发生并调查原因。</p><p>  Prometheus and Alertmanager were already deployed. So I’ve thought that alerting on OOMKills will be as easy. I just had to find the right metric(s) indicating that OOMKill has happened and write an alerting rule for it. Given the length of this post, you could imagine how wrong I was!</p><p>  Prometheus和Alertmanager已经部署。因此，我认为针对OOMKills发出警报将非常容易。我只需要找到表明OOMKill已经发生的正确度量标准，并为此编写警报规则。鉴于这篇文章的篇幅，您可以想象我有多错误！</p><p>    A brief Google search has led me to the  kube pod state metric. It turns out it has a metric called  kube_pod_container_status_last_terminated_reason. The value of the metric is  1 when a container in a pod has terminated with an error. Based on the exit code, the  reason label will be set to  OOMKilled if the exit code was  137. That sounded promising! So I’ve created an alert for that.</p><p>    简短的Google搜索使我了解了kube pod状态指标。事实证明，它具有一个称为kube_pod_container_status_last_terminated_reason的度量。当容器中的容器因错误终止时，度量标准的值为1。根据退出代码，如果退出代码为137，则原因标签将设置为OOMKilled。这听起来很有希望！因此，我为此创建了一个警报。</p><p>  As usual, things are rarely straightforward. As soon as the container restarts, the value of this metric will be  1. For alerting purposes, one has to combine it with another metric that will change when a pod restarts.  kube_pod_container_status_restarts_total does that. Combine the two - and Bingo! It Worked!</p><p>像往常一样，事情很少是简单的。容器重新启动后，此度量标准的值将为1。出于警报目的，必须将其与另一个在容器重启时会更改的度量标准结合起来。 kube_pod_container_status_restarts_total做到了。结合两个-和宾果游戏！有效！</p><p>    For a brief moment, I’ve thought that I was done. I was about to declare victory over OOMKills in production! But then a puzzle came my way: One of our software developers has come forward. He claimed that one of his pods was running out of memory and he couldn’t see any alerts for it.</p><p>    有一小会儿，我以为我做完了。我正要宣布在生产中击败OOMKills！但后来我困惑了：我们的一位软件开发人员挺身而出。他声称自己的一个吊舱内存不足，因此看不到任何警报。</p><p>  At first, I wasn’t inclined to believe that his diagnosis of running out of memory was correct. Mainly because his Pod didn’t even restart! But then I looked at the graph of the memory use of the Pod. It did show the usual pattern: Memory usage would grow, reach its peak at the memory limit, and then suddenly drop.</p><p>  起初，我不倾向于相信他关于内存不足的诊断是正确的。主要是因为他的Pod甚至没有重启！但是随后我查看了Pod的内存使用情况图。它确实显示出通常的模式：内存使用率将增长，在内存限制时达到峰值，然后突然下降。</p><p>  I’ve asked the developer for the gory details of the implementation. It turned out that the init process in the container would start a child process and wait for the result of it. If the child process would exit with an error, it would return an error to the requester and not terminate (because - why should it?).</p><p>  我已要求开发人员提供实施的详细信息。事实证明，容器中的init进程将启动一个子进程并等待其结果。如果子进程退出并出现错误，它将向请求者返回错误而不终止（因为-为什么要这样做？）。</p><p>  That is when it dawned to me - my alerting is effective only if container exits. This is usually the case when the init process of the container is OOMKilled. But there is no guarantee this will happen if a child of the init is OOMKilled. In the case where the container’s init tries to handle OOMKill by itself, my alerting is not triggering!</p><p>  那是对我的曙光-我的警报仅在容器退出时才有效。当容器的初始化过程为OOMKilled时，通常就是这种情况。但是不能保证如果init的子代被OOMKilled会发生。如果容器的init尝试自行处理OOMKill，则不会触发我的警报！</p><p>    Given that OOMKills are as old as Unix, I thought: surely someone will have a solution for this already.</p><p>    鉴于OOMKills与Unix一样古老，我想：肯定会有人为此提供解决方案。</p><p>  I’ve ensued onto a frantic search for some kind of metric exporter for this. I just needed the number of OOMKill events in a pod, or at least in a Docker container. Here is what I’ve found:</p><p>  为此，我疯狂地寻找某种度量标准导出器。我只需要一个Pod或至少一个Docker容器中的OOMKill事件数。这是我发现的：</p><p>    My first stop was cAdvisor itself. It turns out that cAdvisor is  getting the OOMKill events, but not exporting them as a Prometheus metric and no one really seems to care. So that was a dead-end.</p><p>我的第一站是cAdvisor本身。事实证明，cAdvisor正在获取OOMKill事件，但未将其导出为Prometheus指标，似乎没有人真正在乎。所以那是死胡同。</p><p>        I’ve tried the latest version of  the Docker image, but once started it crashes and burns with:</p><p>        我尝试了最新版本的Docker映像，但是一旦启动，它就会崩溃并烧毁：</p><p>      F1120 22:04:21.571246 1 main.go:73] Could not create log watcherI1120 22:04:21.572066 1 main.go:64] Starting prometheus metrics</p><p>      F1120 22：04：21.571246 1 main.go：73]无法创建日志监视程序I1120 22：04：21.572066 1 main.go：64]启动Prometheus指标</p><p>  As it seems no one has committed any code to in over a year. It has a low number of stars (14). All that meant that I was back to square one.</p><p>  似乎一年来没有人提交任何代码。它的星星数量少（14）。所有这些意味着我回到了第一位。</p><p>    Having a hard time finding an existing solution meant only one thing: I will have to write my own.</p><p>    很难找到一个现有的解决方案只意味着一件事：我将不得不编写自己的解决方案。</p><p>  A cursory look at  Docker’s events delivered everything I needed. There is an event called  oom. Docker emits this event every time the OOMKiller process gets active in the container. Now I was only missing a piece of code that will listen to those events and export them as Prometheus metrics.</p><p>  粗略地看一下Docker的事件，就提供了我所需的一切。有一个称为oom的事件。每当OOMKiller进程在容器中处于活动状态时，Docker都会发出此事件。现在，我只缺少一段代码来监听那些事件并将它们导出为Prometheus指标。</p><p>  This is how  missing-container-metrics was born. What it does is to connect to a local Docker instance (via  /var/run/docker.sock). It lists all existing containers as a starting point. And then it listens to Docker events. Using those events, it keeps track of the currently running containers. It also gathers the basic stats of each container it knows about:</p><p>  这就是缺少容器度量标准的诞生方式。它的作用是连接到本地Docker实例（通过/var/run/docker.sock）。它列出了所有现有容器作为起点。然后，它侦听Docker事件。使用这些事件，它可以跟踪当前正在运行的容器。它还收集它知道的每个容器的基本统计信息：</p><p>    By design, it is not Kubernetes specific. This means it can be used with a plain Docker. But it also has a couple of very convenient Kubernetes specific features.</p><p>根据设计，它不是特定于Kubernetes的。这意味着它可以与普通Docker一起使用。但是它还具有几个非常方便的Kubernetes特定功能。</p><p>  Whenever it finds a container label for the pod name or namespace, it adds them as a label to the exported metrics. Also, label naming is compatible with  kube-state-metrics.</p><p>  只要找到容器名称或名称空间的容器标签，就会将它们作为标签添加到导出的指标。同样，标签命名与kube-state-metrics兼容。</p><p>      In a Kubernetes cluster,  missing-container-metrics needs to run on every node. The simplest way to achieve this is to use a daemon-set. The source code comes with an example  daemon set deployment.</p><p>      在Kubernetes集群中，容器丢失指标需要在每个节点上运行。实现此目的的最简单方法是使用守护程序集。源代码附带一个示例守护程序集部署。</p><p>    The most interesting issue I’ve found was where I’ve least expected it: Fluentd!</p><p>    我发现的最有趣的问题是我最不期望的问题：流利！</p><p>  Fluentd log forwarder for node/pod/kubelet logs to the log aggregator. When the volume of logs was very high, Fluentd is OOMKilled.</p><p>  将节点/ pod / kubelet日志的流利日志转发器发送到日志聚合器。当日志量很高时，Fluentd被OOM杀死。</p><p>  Looking at the details of how Fluentd works, it becomes clear what is going on.</p><p>  查看Fluentd如何工作的细节，可以清楚了解发生了什么。</p><p>  Fluentd has one main process (that ends up being init process in the container). This main process forks a worker process that forwards the logs. When the worker process dies for some reason (for example OOMKill), the main process starts a new one. This leads to an endless loop of spawn/OOMKill.</p><p>  Fluentd有一个主要进程（最终是容器中的init进程）。这个主进程派生一个转发日志的工作进程。当工作进程由于某种原因（例如OOMKill）而死时，主进程将启动一个新进程。这导致无休止的生成/ OOMKill循环。</p><p>  The fact that Fluentd is the log forwarder is very unfortunate. OOMKill loop would stop the log forwarding, so you could not ‘see’ what is going on by inspecting the logs.</p><p>Fluentd是日志转发器这一事实非常不幸。 OOMKill循环将停止日志转发，因此您无法通过检查日志来“查看”正在发生的情况。</p><p>    If you want to make sure that your Kubernetes cluster is healthy, it is essential to alert on OOMKills. This enables you to know when processes hit their memory limits. Be it because of memory leaks or wrongly configured memory limits.</p><p>    如果要确保Kubernetes集群运行状况良好，请务必对OOMKills发出警报。这使您可以知道进程何时达到其内存限制。是因为内存泄漏或错误配置的内存限制。</p><p>  It turns out that monitoring for OOMKills in Kubernetes is not as an easy task as one might think. Using  missing-container-metrics makes it much easier though.</p><p>  事实证明，对Kubernetes中的OOMKills进行监视并不像人们想象的那样容易。但是，使用漏失容器度量标准会使其变得更加容易。</p><p>  So go ahead, deploy  missing-container-metrics to your cluster. You might be surprised how many of OOMKills you have not been noticing.</p><p>  因此，继续将缺少容器的指标部署到您的集群。您可能会惊讶于您没有注意到多少OOMKills。</p><p>  I hope that it will be useful to you, and will save you the time that I’ve spent searching for the solution.</p><p>  希望对您有用，并可以节省您寻找解决方案所花费的时间。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.netice9.com/blog/guide-to-oomkill-alerting-in-kubernetes-clusters">https://www.netice9.com/blog/guide-to-oomkill-alerting-in-kubernetes-clusters</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/集群/">#集群</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/oomkill/">#oomkill</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/容器/">#容器</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>