<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Hopfield Networks是您所需要的一切</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Hopfield Networks是您所需要的一切</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-08-29 16:18:11</div><div class="page_narrow text-break page_content"><p>这篇博客文章解释了论文Hopfield Networks Is All You Need和相应的新PyTorch Hopfield层。</p><p>我们引入了一个新的能量函数和相应的新的更新规则，它保证收敛到能量函数的一个局部极小值。</p><p>新的能量函数是Krotov和Hopfield和Demircigil等人提出的现代Hopfield网络(即密集联想存储器)的推广(离散状态\(右)连续状态)。新的具有连续状态的现代Hopfield网络保持了其离散对应网络的特征：</p><p>令人惊讶的是，新的更新规则就是“关注就是一切”中引入的变压器网络的注意力机制，我们利用这些新的洞察力来分析变压器模型。</p><p>这篇博文分为三个部分。首先，我们从传统的Hopfield网络过渡到现代的Hopfield网络，并通过我们的新能量函数将其推广到连续态。其次，给出了新能量函数的性质及其与变压器网络自关注机制的联系。最后，我们介绍并解释了一个新的PyTorch层(Hopfield层)，它建立在我们工作的基础上。我们展示了几个实际使用案例，即现代Hopfield网络和注意免疫曲目分类、Hopfield池和两个集合的关联。</p><p>联想记忆是最早的人工神经模型之一，可以追溯到20世纪60年代和70年代。最著名的是由John Hopfield于1982年提出的Hopfield网络。顾名思义，联想记忆网络的主要目的是将输入与其最相似的模式相关联。换句话说，目的是存储和检索模式。我们从经典Hopfield网络的回顾开始。</p><p>最简单的联想记忆就是我们要存储的N个模式(黑体符号{x}i=1}^N)的外积之和(Hebbian学习规则)。在经典的Hopfield网络中，这些模式是极(二进制)的，即\(boldSymbol{x}_i\in\-1，1^d\)，其中\(d\)是模式的长度。对应的权重矩阵\(\boldbol{W}\)为：</p><p>\[\BEGIN{等式}\boldSymbol{W}=\SUM_I^N\boldSymbol{x}_I\boldSymbol{x}_I^T\。\Tag{1}\end{等式}\]权重矩阵\(\boldSymbol{W}\)存储模式，可以从状态模式\(\boldSymbol{\Xi}\)开始检索模式。</p><p>从现在开始，我们将\(N\)个存储的模式表示为\({\boldbol{x}_i\}_{i=1}^N\)，并将任何状态模式或状态表示为\(\boldbol{\xi}\)。</p><p>基本同步更新规则是将状态模式\(\boldSymbol{\xi}\)与权重矩阵\(\boldSymbol{W}\)重复相乘，减去偏差并取符号：</p><p>\[\BEGIN{EQUAGE}\boldSymbol{\xi^{t+1}}=\text{SGN}(\boldSymbol{W}\boldSymbol{\Xi}^t-\boldSymbol{b})\，\Tag{2}\Label{eq：restorage}\end{等式}\]其中\(\boldSymbol{b}\in\mathbb{R}^d\)是偏置向量，异步更新规则仅对\(\boldSymbol{\xi}\)的一个组件执行此更新，然后选择下一个组件进行更新。如果\(\boldSymbol{\xi^{t+1}}=\boldSymbol{\xi^{t}}\)，则达到收敛。</p><p>公式的更新规则的异步版本。\eqref{eq：restorage}最小化能量函数\(\text{E}\)：</p><p>\[\BEGIN{等式}\text{E}=-\frac{1}{2}\boldbol{\xi}^T\boldbol{W}\boldbol{\xi}+\boldbol{\xi}^T\boldbol{b}=-\frac{1}{2}\sum_{i=1}^d\sum_{j=1}^d w_{ij}\xi_i\xi_j+\sum_{i=1}。\tag{3}\label{eq:energy_hopfield}\end{equation}\]在Bruck，Goles-Chacc等人的论文中导出。与原始Hopfield论文相比，收敛性质取决于权重矩阵\(\boldSymbol{W}\)的结构和节点更新的方法：</p><p>对于\(w_{ii}\geq 0\)和\(w_{ij}=w_{ji}\)的异步更新，更新收敛到稳定状态。</p><p>对于具有\(w_{ij}=w_{ji}\)的同步更新，更新收敛到稳定状态或长度为2的极限环。</p><p>对于异步更新规则和对称权重，\(\text{E}(\boldbol{\xi}^{t+1})\leq\text{E}(\boldbol{\xi}^{t})\)成立。当\(\text{E}(\boldSymbol{\xi}^{t+1})=\text{E}(\boldSymbol{\xi}^{t})\)更新\(\boldbol{\xi}^t\)的每个组件时，将达到\(\text{E}\)中的局部最小值。所有存储的模式\(\{\boldSymbol{x}_i\}_{i=1}^N\)应为</p><p>\[\BEGIN{公式}\boldSymbol{x}_i=\text{sgn}(\boldSymbol{W}\boldSymbol{x}_i-\boldSymbol{b})\。\Tag{4}\end{公式}\]在以下示例中，未使用偏置矢量。这意味着取反像，即一次翻转所有像素，会产生相同的能量。</p><p>我们从一个说明性的Hopfield网络示例开始。应先存储一个输入图像，然后再检索。输入图像为：</p><p>由于关联存储器具有极性状态和模式(或二进制状态和模式)，因此我们将输入图像转换为黑白图像：</p><p>权重矩阵\(\boldbol{W}\)是此黑白图像\(\boldbol{x}_{\text{Hmer}}\)的外积：</p><p>\[\BEGIN{等式}\boldSymbol{W}=\boldSymbol{x}_{\Text{荷马}}\boldSymbol{x}_{\Text{荷马}}^T\，\qquad\boldSymbol{x}_{\Text{荷马}}\in\-1，1\}^d\，\tag{5}\label{eq:weight_matrix}\end{equation}\]如果遮蔽一半像素，是否可以恢复原始图像？遮罩图像为：</p><p>这是我们的初始状态\(\boldSymbol{\xi}\)。该初始状态通过与权重矩阵\(\boldSymbol{W}\)相乘来更新，它需要一次更新，直到恢复原始图像。</p><p>如果我们存储多个图案，会发生什么情况？然后，根据三个存储图案(三个输入图像)的外积之和构建权重矩阵：</p><p>\[\BEGIN{等式}\boldSymbol{W}=\sum_{i=1}^3\boldSymbol{x}_i\boldSymbol{x}_i^T\，\qquad\boldSymbol{x}_i\in\-1，1\^d\.。\tag{6}\end{等式}\]在此图中，左侧显示三个存储的模式，右侧显示屏蔽的状态模式\(\boldbol{\xi}\)以及检索到的模式\(\boldbol{\xi}^{\text{new}}\)。</p><p>查看上面一行的图像可能会显示检索过程不再完美。但有两个有趣的事实需要考虑：</p><p>遮罩原始图像会引入许多像素值\(-1\)。因此，我们有一个奇怪的行为，即内积\(\langle\boldsymbol{x}_{\text{Homer}}^{\text{masked}}，\boldSymbol{x}_{\Text{bart}}\Rangel\)大于内积\(\langle\boldsymbol{x}_{\text{Homer}}^{\text{masked}}，\boldSymbol{x}_{\Text{Homer}}\Rangel\)。</p><p>如上所述，如果没有使用偏置矢量，则图案的反转，即一次翻转所有像素，产生相同的能量。</p><p>虽然上面图像的检索看起来不正确，但实际上是正确的。但是，对于下行示例，检索不再正确。\(2\cdot\boldbol{x}_{\text{marge}}\)的权重只是覆盖了\(\boldbol{x}_{\text{Hmer}}\)的权重。对于这两个示例，仅显示了第一个更新步骤之后的检索，但在执行进一步更新步骤时结果不会更改。下图显示了6个模式的Hopfield网络检索。</p><p>显然，检索图案是不完美的。有人可能会怀疑Hopfield Networks的有限存储容量，参见Amit等人。然而，我们现在证明存储容量不是不完全检索的直接原因。无错误检索模式的存储容量是：</p><p>\[\Begin{公式}C\cong\frac{d}{2\\tag{7}\label{eq:storage_hopfield}\end{equation}\](D)}\，log\[\Begin{公式}C\cong 0.14d\]。\tag{8}\label{eq:storage_hopfield2}\end{equation}\]在示例中，存储容量为\(C\cong 0.14d=0.14\CDOT64\CDOT64\SIM570\)，因此存储容量不足并不是导致检索错误的直接原因，而是与示例模式相关，因此检索存在错误。</p><p>因此，我们需要一种模型，该模型允许拆分紧密的模式，以便(强)相关的模式可以被区分出来。</p><p>公式中所述的存储容量。\eqref{eq：storage_Hopfield}和等式中。Eqref{eq：storage_Hopfield2}被导出为\(w_{ii}=0\).最近，Folli et al.。分析了具有w{ii}\geq0的Hopfield网络的存储容量。对于w_(Ii)\geq_0，对于错误百分比较小的模式检索，存储容量为\(C\cong 0.14d\)。比率\(C/d\)通常称为载荷参数，用\(\α\)表示。Folli等人。结果表明，存在第二个α非常大的区域，其存储容量更大，即存在更多的固定点。然而，Rocci等人。和Gosti等人。报道称，这些固定点对于非常大的\(\α\)是不稳定的，并且没有吸引盆地。</p><p>存储容量是Hopfield网络的一个重要特性。现代Hopfield网络(又称密集联想存储器)引入了一种新的能量函数来代替方程中的能量。\eqref{eq：energy_Hopfield}以创建更高的存储容量。离散的现代Hopfield网络首先由Krotov和Hopfield引入，然后由Demircigil等人推广：</p><p>\[\BEGIN{等式}\TEXT{E}=-\SUM_{i=1}^N F(\boldSymbol{x_i}^T\boldSymbol{\xi})\，\tag{9}\label{eq:energy_krotov2}\end{equation}\]其中\(F\)是交互函数，\(N\)也是存储的模式数。</p><p>\[\BEGIN{公式}C\cong\frac{1}{2(2a-3)！！}\frac{d^{a-1}}{\log(D)}\.。\tag{10}\label{eq:storage_krotov}\end{equation}\]\[\Begin{公式}C\cong\Alpha_{a}d^{a-1}\，\tag{11}\label{eq:storage_krotov2}\end{equation}\]其中\(\Alpha_a\)是一个常数，它取决于差错概率的(任意)阈值。</p><p>\[\Begin{公式}\Text{E}=-\SUM_{i=1}^N\text{exp}(\boldsymbol{x}_i^T\boldsymbol{\xi})\，\tag{12}\label{eq:energy_demircigil}\end{equation}\]\[\Begin{公式}\Text{E}=-\Text{EXP}\BIG(\Text{lse}(1，\boldSymbol{X}^T\boldSymbol{\Xi})\BIG)\，\tag{13}\label{eq:energy_demircigil2}\end{equation}\]\[\Begin{等式}\Text{lse}(\Beta，\boldSymbol{z})=\Beta^{-1}\log\Bigg(\SUM_{l=1}^N\Text{EXP}(\Beta z_l)\Bigg)\。\标记{14}\结束{公式}\]\[\开始{公式}C\cong 2^{\frac{d}{2}}\]\。\tag{15}\label{eq:storage_demircigil}\end{equation}\]现在我们来看一下更新规则，它对两个公式都有效。\eqref{eq：energy_krotov2}和eq.。对于极性图案，即-1，1^d中的(boldSymbol{\xi})，我们用(boldSymbol{\xi}[l])表示第(L)个分量。利用方程的能量函数。\eqref{eq：Energy_krotov2}或Eq.。Eqref{eq：energy_demircigil}中，第(l\)个分量\(boldSymbol{\xi}[l]\)的更新规则由当前状态(boldSymbol{\xi}\)与翻转了分量\(\boldSymbol{\xi}[l]\)的状态的能量之差来描述。分量\(\粗体符号{\xi}[l]\)被更新以降低能量。更新规则为：</p><p>\[\begin{equation}\boldsymbol{\xi}^{\text{new}}[l]=\text{sgn}\bigg[-\text{E}\big(\boldsymbol{\xi}^{(l+)}\BIG)+\Text{E}\BIG(\boldSymbol{\xi}^{(l-)}\BIG)\BIG]\，\Tag{16}\END{公式}\]\[\begin{equation}\boldsymbol{\xi}^{\text{new}}[l]=\Text{sgn}\BIGG[\BUD_{i=1}^N\Text{EXP}\BIG(\boldSymbol{x}_i^T~\boldSymbol{\xi}^{(l+)}\BIG)-\SUM_{i=1}^N\Text{EXP}(\boldSymbol{x}_i^T~\boldSymbol{\xi}^{(l。-)}\BIG)\BIG]\，\tag{17}\label{eq:restorage_demircigil}\end{equation}\]其中\(\boldSymbol{\xi}^{(l+)}[l]=1\)和\(boldSymbol{\xi}^{(l-)}[l]=-1\)和\(boldSymbol{\xi}^{(l+)}[k]=\boldSymbol{\xi}^{(l-)}[k]=\boldSymbol{\xi}[k]\)表示\(k\neq l\)。</p><p>在Demircigil等人的论文中，证明了使方程能量函数最小化的更新规则。Eqref{eq：energy_demircigil}，在一次(异步)更新当前状态\(\boldSymbol{\xi}\)后以高概率收敛。注意，当前状态\(\boldSymbol{\xi}\)的一次更新对应于\(d\)个异步更新步骤，即对\(d\)个单个组件\(\boldSymbol{\xi}[l]\)(\(l=1，\ldots，d\))进行一次更新。</p><p>与经典的Hopfield网络不同，现代Hopfield网络没有公式中定义的权重矩阵。\eqref{eq：Weight_Matrix}。相反，能量函数是每个存储的模式\(\boldSymbol{x}_i\)与状态模式\(\boldSymbol{\xi}\)的点积的函数之和。</p><p>使用公式。\eqref{eq：restorage_demircigil}，我们再次尝试从6个存储的模式中检索荷马。与经典的Hopfield网络相比，它现在可以流畅地工作，不仅适用于6种模式，还适用于更多模式：</p><p>与传统的Hopfield网络相比，增加的存储容量现在可以拆分关闭的模式。我们现在能够区分(强)相关的模式，并且可以从许多模式中检索一个特定的模式。</p><p>我们推广了方程的能量函数。\eqref{eq：energy_demircigil2}转换为连续值模式。我们使用负能量方程的对数。\eqref{eq：energy_demircigil2}，并添加二次项。二次项保证状态\(\boldSymbol{\xi}\)的范数保持有限。新能量函数定义为：</p><p>\[\BEGIN{公式}\text{E}=-\text{lse}\BIG(\beta，\boldSymbol{X}^T\boldSymbol{\xi}\BIG)+\frac{1}{2}\boldSymbol{\xi}^T\boldSymbol{\xi}+\beta^{-1}\log N+\frac{1}{2}M^2\，由矩阵\(\tag{18}\end{equation}\label{eq:energy_sepp}\]{X}=(BoldSymbol{x}_1，\ldots，\boldSymbol{x}_N)\)由\(N\)连续存储模式构成的范数，其中\(M\)是所有存储模式中的最大范数。</p><p>根据Krotov和Hopfield的新论文，现代Hopfield网络的存储模式(boldSymbol{X}^T)可以看作是从(boldSymbol{\xi})到隐藏单元的权重，而(boldSymbol{X})可以看作是从隐藏单元到(boldSymbol{\xi})的权重。有了这种解释，我们不存储模式，而是像经典的Hopfield网络一样，在我们的模型中只使用权重。</p><p>方程的能量函数。Eqref{eq：Energy_Sepp}允许通过Yuille和Rangarajan描述的凹凸过程(CCCP)导出状态模式\(boldSymbol{\xi}\)的更新规则。</p><p>总能量\(\text{E}(\boldbol{\xi})\)分为凸项和凹项：\(\text{E}(\boldbol{\xi})=\text{E}_1(\boldbol{\xi})+\text{E}_2(\boldbol{\xi})\)。</p><p>术语\(\frac{1}{2}\boldSymbol{\xi}^T\boldSymbol{\xi}+C=\text{E}_1(\boldSymbol{\xi})\)是凸的(\(C\)是独立于\(\boldSymbol{\xi}\)的常数)。</p><p>项\(-\text{lse}\BIG(\beta，\boldSymbol{X}^T\boldSymbol{\xi}\BIG)=\text{E}_2(\boldSymbol{\xi})\)是凹的(LSE是凸的，因为它的Hessian是正半定的，如本文附录所示)</p><p>\[\begin{equation}\nabla_{\boldsymbol{\xi}}\text{E}_1(\boldsymbol{\xi}^{t+1})=-\nabla_{\boldsymbol{\xi}}\text{E}_2(\boldsymbol{\xi}^{t})\tag{19}\label{eq:update_sepp1}\end{equation}\]\[\begin{equation}\nabla_{\boldsymbol{\xi}}\left(\FRAC{1}{。2}\boldSymbol{\xi}^T\boldSymbol{\xi}+C\right)(\boldSymbol{\xi}^{t+1})=\nabla_{\boldSymbol{\xi}}\text{lse}\BIG(\beta，\boldSymbol{X}^T\boldSymbol{\xi}^t\BIG)\tag{20}\label{eq:update_sepp2}\end{equation}\]\[\begin{equation}\boldsymbol{\xi}^{t+1}=\boldSymbol{X}\Text{Softmax}\BIG(\beta\boldSymbol{X}^T\boldSymbol{\Xi}^t\BIG)\，\tag{21}\label{eq:update_sepp3}\end{equation}\]\[\begin{equation}\boldsymbol{\xi}^{\text{new}}=\boldSymbol{X}\Text{Softmax}\BIG(\beta\boldSymbol{X}^T\boldSymbol{\xi}\BIG)\.。\tag{22}\label{eq:update_sepp4}\end{equation}\]采用凹凸过程获取更新规则，保证了能量函数的单调递减。</p><p>指数存储能力和一次更新后的收敛性继承了Demircigil等人的思想，全局收敛到局部最小值意味着所有的极限点都是由方程迭代产生的。Eqref{eq：update_sepp4}是Eq的能量函数的固定点(局部极小点或鞍点)。\eqref{eq：Energy_Sepp}(几乎可以肯定没有找到最大值，在任何实验中都没有遇到鞍点)。</p><p>新的连续能量函数允许将我们的示例扩展到连续模式。在下面，我们将使用公式从许多连续存储的模式中检索连续荷马。\eqref{eq：update_sepp4}。首先，我们必须将输入图像转换为灰度图像：</p><p>我们再一次看到荷马被完美地找回了。我们已经考虑了这样的情况：模式彼此之间有足够的差异，因此迭代收敛到一个靠近存储模式之一的固定点，然而，如果一些存储模式彼此相似，则在相似模式附近出现亚稳态。开始于该亚稳态附近或在类似图案之一处的迭代收敛到该亚稳态。学习动力可以由逆温度\(\β\)控制，见公式。\eqref{eq：update_sepp4}。较高的β值对应于较低的温度，意味着各个花样的吸引盆地保持分离，不太可能出现亚稳态。另一方面，低的β值对应于较高的温度，更有可能形成亚稳态。我们现在查看相同的示例，但是我们使用的不是\(\beta=8\)，而是\(\beta=0.5\)。检索到的状态现在是多个存储模式的叠加。</p><p>为了更清楚地说明这一点，如果我们使用不同的\(\beta\)值进行检索，我们将更仔细地了解结果是如何变化的：</p><p>我们将介绍变压器网络的(自我)注意。接下来，我们将引导您完成这三个步骤。</p><p>对于\(S\)状态模式\(\boldSymbol{\xi}=(\boldSymbol{\xi}_1，\ldots，\boldSymbol{\xi}_S)\)，等式。\eqref{eq：update_sepp3}可以概括为：</p><p>\[\begin{equation}\boldsymbol{\Xi}^{\text{new}}=\boldSymbol{X}\Text{Softmax}\BIG(\BETA\boldSymbol{X}^T\boldSymbol{\xi}\BIG)\.。标签{23}\标签{eq：UPDATE_GENERIAL}\end{公式}\]我们首先将\(\boldSymbol{X}^T\)视为\(N\)个原始存储模式\(\boldSymbol{Y}=(\boldSymbol{y}_1，\ldots，\boldSymbol{y}_N)^T\)，它们通过\(\boldSymbol{W}_K\)映射到关联空间，和(boldSymbol{\xi}^T\)作为\(S\)原始状态模式\(boldSymbol{R}=(boldSymbol{\xi}_1，\ldots，\boldSymbol{\xi}_S)^T\)，它们通过\(boldSymbol{W}_Q\)映射到一个关联空间。</p><p>\[\BEGIN{等式}\boldSymbol{Q}=\boldSymbol{\Xi}^T=\boldSymbol{R}\boldSymbol{W}_Q\，\tag{24}\label{eq:mapping_Q}\end{equation}\]\[\begin{equation}\boldsymbol{K}=\boldSymbol{X}^T=\boldSymbol{Y}\boldSymbol{W}_K\，\tag{25}\label{eq:mapping_K}\end{equation}\]\[\begin{equation}\beta=\FRAC{1}{\sqrt{d_k}}\，\Tag{26}\end{公式}\]\[\BEGIN{公式}\BIG(\boldSymbol{q}^{\text{new}}\BIG)^T=\boldSymbol{K}^T\text{Softmax}\Left(\frac{1}{\sqrt{d_k}}\boldSymbol{K}\boldSymbol{q}^T\Right)\。\Tag{27}\label{eq:update_generalized2}\end{equation}\]in Eq.。\eqref{eq：Mapping_q}和eq.。Eqref{eq：Mapping_K}、\(\boldSymbol{W}_Q\)和\(\boldSymbol{W}_K\)是将各个图案映射到关联空间的矩阵。\eqref{eq：update_Generalized2}，Softmax按列应用于矩阵\(\boldSymbol{K}\boldSymbol{Q}^T\)。</p><p>接下来，我们简单地转置方程。Eqref{eq：update_Generalized2}，这也意味着Softmax现在以行方式应用于其转置输入\(\boldSymbol{Q}\boldSymbol{K}^T\)，并获得：</p><p>\[\BEGIN{公式}\粗体符号{q}^{\text{new}}。</p><p>.</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://ml-jku.github.io/hopfield-layers/">https://ml-jku.github.io/hopfield-layers/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/networks/">#networks</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/hopfield/">#hopfield</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>