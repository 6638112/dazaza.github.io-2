<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>找到要说的话：语言模型的隐藏状态可视化 Finding the Words to Say: Hidden State Visualizations for Language Models</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Finding the Words to Say: Hidden State Visualizations for Language Models<br/>找到要说的话：语言模型的隐藏状态可视化 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-21 11:21:06</div><div class="page_narrow text-break page_content"><p>By visualizing the hidden state between a model&#39;s layers, we can get some clues as to the model&#39;s &#34;thought process&#34;.</p><p>通过可视化模型各层之间的隐藏状态，我们可以获得有关模型思想过程的一些线索。</p><p>  Part 2: Continuing the pursuit of making Transformer language models more transparent, this article showcases a collection of visualizations to uncover mechanics of language generation inside a pre-trained language model. These visualizations are all created using  Ecco, the open-source package we&#39;re releasing In the first part of this series,  Interfaces for Explaining Transformer Language Models, we showcased interactive interfaces for input saliency and neuron activations. In this article, we will focus on the hidden state as it evolves from model layer to the next. By looking at the hidden states produced by every transformer decoder block, we aim to gleam information about how a language model arrived at a specific output token. This method is explored by Voita et al. . Nostalgebraist   presents compelling visual treatments showcasing the evolution of token rankings, logit scores, and softmax probabilities for the evolving hidden state through the various layers of the model.</p><p>  第2部分：继续追求使Transformer语言模型更加透明，本文展示了一系列可视化工具，以揭示预先训练的语言模型内部的语言生成机制。这些可视化都是使用Ecco创建的，Ecco是我们发布的开源软件包。在本系列的第一部分“解释变压器语言模型的接口”中，我们展示了用于输入显着性和神经元激活的交互式接口。在本文中，我们将集中讨论隐藏状态从模型层发展到下一层的过程。通过查看每个转换器解码器块产生的隐藏状态，我们旨在了解有关语言模型如何到达特定输出令牌的信息。 Voita等人探索了这种方法。 。 Nostalgebraist提出了引人注目的视觉处理方法，展示了通过模型各个层级的隐藏状态不断演变的令牌等级，logit得分和softmax概率的演变。</p><p>  The following figure recaps how a transformer language model works. How the layers result in a final hidden state. And how that final state is then projected to the output vocabulary which results in a score assigned to each token in the model&#39;s vocabulary. We can see here the top scoring tokens when DistilGPT2 is fed the input sequence &#34; 1, 1, &#34;:</p><p>  下图概述了转换器语言模型的工作方式。图层如何导致最终的隐藏状态。然后，如何将最终状态投影到输出词汇表上，从而为模型词汇表中的每个标记分配一个分数。当向DistilGPT2输入输入序列＆＃34;时，我们可以在此处看到得分最高的标记。 1，1，＆＃34 ;：</p><p> Ecco provides a view of the model&#39;s top scoring tokens and their probability scores. # Generate one token to complete this input string output  =  lm . generate ( &#34; 1, 1, 1,&#34; ,  generate = 1 ) # Visualize output . layer_predictions ( position = 6 ,  layer = 5 )</p><p> Ecco提供了该模型的最高得分标记及其概率得分的视图。 ＃生成一个令牌以完成此输入字符串output = lm。生成（＆＃34; 1，1，1，＆＃34;生成= 1）＃可视化输出。 layer_predictions（position = 6，layer = 5）</p><p>Which would show the following breakdown of candidate output tokens and their probability scores:   Applying the same projection to internal hidden states of the model gives us a view of how the model&#39;s conviction for the output scoring developed over the processing of the inputs. This projection of internal hidden states gives us a sense of which layer contributed the most to elevating the scores (and hence ranking) of a certain potential output token.</p><p>这将显示以下候选输出标记及其概率得分的细分：将相同的投影应用于模型的内部隐藏状态，可以使我们了解模型如何基于对输入处理的信念来进行输出评分。内部隐藏状态的这种投影使我们感觉到哪个层对提高某个潜在输出令牌的得分（以及排名）贡献最大。</p><p>  Viewing the evolution of the hidden states means that instead of looking only at the candidates output tokens from projecting the final model state, we can look at the top scoring tokens after projecting the hidden state resulting from each of the model&#39;s six layers.</p><p>  查看隐藏状态的演变意味着，我们不仅可以从预测最终模型状态中查看候选输出令牌，还可以在投影出由模型的六层中的每一层所产生的隐藏状态之后，查看得分最高的令牌。</p><p>This visualization is created using the same method above with omitting the &#39;layer&#39; argument (which we set to the final layer in the previous example, layer #5): Resulting in:  You can experiment with these visualizations and experiment with them on your own input sentences at the following colab link:    Another visual perspective on the evolving hidden states is to re-examine the hidden states after selecting an output token to see how the hidden state after each layer ranked that token. This is one of the many perspectives explored by Nostalgebraist   and the one we think is a great first approach. In the figure on the side, we can see the ranking (out of +50,0000 tokens in the model&#39;s vocabulary) of the token &#39; 1&#39; where each row indicates a layer&#39;s output.</p><p>使用与上面相同的方法创建了此可视化效果，而省略了“ ## layer＆＃39;参数（我们在上一个示例的第5层中将其设置为最后一层）：结果：您可以在以下colab链接上尝试这些可视化效果并在自己的输入语句上进行实验：另一个关于不断演变的隐藏视觉效果状态是在选择输出令牌后重新检查隐藏状态，以查看每一层之后的隐藏状态如何对该令牌进行排名。这是Nostalgebraist探索的许多观点之一，我们认为这是一个很好的第一种方法。在侧面的图中，我们可以看到令牌的排名（在模型词汇表中的+50000个令牌中）。 1＆＃39;其中每一行表示一个图层的输出。 </p><p> The same visualization can then be plotted for an entire generated sequence, where each column indicates a generation step (and its output token), and each row the ranking of the output token at each layer:</p><p>然后可以为整个生成的序列绘制相同的可视化效果，其中每一列表示生成步骤（及其输出令牌），每一行表示输出令牌在每一层的排名：</p><p>     Visualizaing the evolution of the hidden states sheds light on how various layers contribute to generating this sequence as we can see in the following figure:</p><p>     可视化隐藏状态的演变揭示了各层如何生成此序列，如下图所示：</p><p>      We are not limited to watching the evolution of only one (the selected) token for a specific position. There are cases where we want to compare the rankings of multiple tokens  in the same position regardless if the model selected them or not.</p><p>      我们不仅限于观看特定位置的一个（选定）代币的演变。在某些情况下，无论模型是否选择，我们都希望比较同一位置中多个令牌的排名。</p><p>  One such case is the number prediction task described by Linzen et al.  which arises from the English language phenomenon of subject-verb agreement. In that task, we want to analyze the model&#39;s capacity to encode  syntactic number (whether the subject we&#39;re addressing is singular or plural) and  syntactic subjecthood (which subject in the sentence we&#39;re addressing).</p><p>  一种这样的情况是Linzen等人描述的数字预测任务。这是由主语-动词一致的英语现象引起的。在该任务中，我们要分析模型对句法数字（无论我们处理的主题是单数还是复数）和句法主体性（我们在处理的主题中的哪一个）编码的能力。</p><p>   To answer correctly, one has to first determine whether we&#39;re describing the keys (possible subject #1) or the cabinet (possible subject #2). Having decided it is the keys, the second determination would be whether it is singular or plural.</p><p>   为了正确回答，必须首先确定我们是在描述钥匙（可能的主题1）还是机柜（可能的主题2）。确定了密钥后，第二个确定将是单数还是复数。</p><p>    The figures in this section visualize the hidden-state evolution of the tokens &#34; is&#34; and &#34; are&#34;. The numbers in the cells are their ranking in the position of the blank (Both columns address the same position in the sequence, they&#39;re not subsequent positions as was the case in the previous visualization).</p><p>    本节中的图将令牌的隐藏状态演变可视化。是＆＃34;和＆＃34;是＆＃34;。单元格中的数字是它们在空白位置中的排名（两列都指向序列中的相同位置，它们不是先前可视化中的后续位置）。</p><p> The first figure (showing the rankings for the sequence &#34;The keys to the cabinet&#34;) raises the question of why do five layers fail the task and only the final layer sets the record straight. This is likely a similar effect to that observed in BERT of the final layer being the most task-specific . It is also worth investigating whether that capability of succeeding at the task is predominantly localized in Layer 5, or if the Layer is only the final expression in a circuit  spanning multiple layers which is especially sensitive to subject-verb agreement.</p><p> 第一个数字（显示序列＆＃34;橱柜钥匙的排名）引发了一个问题，即为什么五层未能完成任务，而只有最后一层才能使记录保持直线。这很可能类似于在最后一层的BERT中观察到的最特定于任务的效果。同样值得研究的是，完成任务的能力是否主要集中在第5层中，或者该层是否只是跨越多层的电路中的最终表达，这对主语-动词一致特别敏感。 </p><p>  This method can shed light on questions of bias and where they might emerge in a model. The following figures, for example, probe for the model&#39;s gender expectation associated with different professions:</p><p>这种方法可以揭示偏见问题以及它们在模型中可能出现的位置。例如，以下数字探讨了与不同专业相关的模型的性别期望：</p><p>  More systemaic and nuanced examination of bias in contextualized word embeddings (another term for the vectors we&#39;ve been referring to as &#34;hidden states&#34;) can be found in  .</p><p>  可以在中找到对系统化词嵌入中的偏见（我们一直称其为“隐藏状态”的向量的另一个术语）进行更系统和细致入微的检查。</p><p>  You can proceed to do your own experiments using Ecco and the three notebooks in this article: You can report issues you run into at the Ecco&#39;s Github page. Feel free to share any interesting findings at the Ecco  Discussion board. I invite you again to read  Interpreting GPT the Logit Lens and see the various ways the author examines such a visualization. I leave you with a small gallery of examples showcasing the responses of different models to different input prompts.     This article was vastly improved thanks to feedback on earlier drafts provided by Abdullah Almaatouq, Anfal Alatawi, Fahd Alhazmi, Hadeel Al-Negheimish, Isabelle Augenstein, Jasmijn Bastings, Najwa Alghamdi, Pepa Atanasova, and Sebastian Gehrmann.</p><p>  您可以使用Ecco和本文中的三个笔记本进行自己的实验：您可以在Ecco的Github页面上报告遇到的问题。随时在Ecco讨论板上分享任何有趣的发现。我再次邀请您阅读解释Logit镜头的GPT，并查看作者检查这种可视化效果的各种方法。我为您提供了一小部分示例，这些示例展示了不同模型对不同输入提示的响应。感谢Abdullah Almaatouq，Anfal Alatawi，Fahd Alhazmi，Hadeel Al-Negheimish，Isabelle Augenstein，Jasmijn Bastings，Najwa Alghamdi，Pepa Atanasova和Sebastian Gehrmann提供的早期草稿的反馈，大大改善了本文。</p><p>    If you found this work helpful for your research, please cite it as following: Alammar, J. (2021). Finding the Words to Say: Hidden State Visualizations for Language Models [Blog post]. Retrieved from https://jalammar.github.io/hidden-states/</p><p>    如果您发现这项工作对您的研究有所帮助，请引用以下内容：Alammar，J.（2021）。找到要说的话：语言模型的隐藏状态可视化[博客文章]。取自https://jalammar.github.io/hidden-states/</p><p> BibTex: @misc{alammar2021hiddenstates, title={Finding the Words to Say: Hidden State Visualizations for Language Models}, author={Alammar, J}, year={2021}, url={https://jalammar.github.io/hidden-states/}}</p><p> BibTex：@misc {alammar2021hiddenstates，title = {找到要说的话：语言模型的隐藏状态可视化}}，author = {Alammar，J}，year = {2021}，url = {https://jalammar.github.io / hidden-states /}} </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://jalammar.github.io/hidden-states/">https://jalammar.github.io/hidden-states/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/可视化/">#可视化</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/words/">#words</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>