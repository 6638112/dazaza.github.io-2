<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>骑行硬件彩票</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">骑行硬件彩票</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-13 19:36:33</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/48c8187db4e8d7358e9b3ba2566febcb.png"><img src="http://img2.diglog.com/img/2020/11/48c8187db4e8d7358e9b3ba2566febcb.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This post heavily relies on illustrations, which are embedded images. If you’re reading this over email, it might be best to read it directly on  substack as some email clients clip long emails and block images.</p><p>这篇文章在很大程度上依赖插图，插图是嵌入的图像。如果你是通过电子邮件阅读这篇文章，最好是直接在子栈上阅读，因为一些电子邮件客户端会剪辑长邮件并屏蔽图像。</p><p> As you can tell from the previous two posts on Page Street Labs, I have been obsessed with Very Large Parameter (VLP) models lately. I wasn’t always this way. On my personal blog and Twitter feed, I have written enough about the culture of building models by stacking layers and praying it works. Ever since we figured out that adding more parameters (more layers specifically) helps, folks have been pushing that limit. Here’s an example from ImageNet:</p><p>正如你可以从佩奇大街实验室的前两篇帖子中看出的那样，最近我一直痴迷于超大参数(VLP)模型。我并不总是这样。在我的个人博客和推特上，我已经写了足够多的关于通过层层叠加和祈祷它奏效来建立模型的文化。自从我们发现添加更多参数(具体地说是添加更多层)会有帮助之后，人们就一直在挑战这一极限。下面是ImageNet的一个例子：</p><p>  And most of those efforts are B-O-R-I-N-G (but with a side of good lessons that may not be widely applicable). However, something is fundamentally different about Very Large Parameter models (think GPT-3 scale and beyond) in terms of their capabilities.</p><p>这些努力中的大多数都是B-O-R-I-N-G(但也有一些好的经验教训可能并不广泛适用)。然而，非常大的参数模型(想一想GPT-3及以上)在它们的功能方面有一些根本的不同之处。</p><p>  Sara Hooker from Google published an essay on arXiv (btw, should this be on arXiv too? There’s a BibTeX entry at the end ... sempai cite me please?) explaining how certain areas of research get a lot of attention and win a lot of support — including software and hardware — over others that the article calls “ The Hardware Lottery”, and how this discourages innovation investments in other areas. The hardware lottery is not new; not even in computer science, but the central plea of that essay is “Hardware Lotteries are holding us back and should be avoided.”</p><p>谷歌(Google)的萨拉·胡克(Sara Hooker)在arxiv上发表了一篇文章，顺便说一句，这篇文章也应该出现在arxiv上吗？结尾处有一个BibTeX条目。森派引用我的话好吗？)。文章解释了某些领域的研究如何获得大量关注和支持--包括软件和硬件--胜过其他被文章称为“硬件彩票”的领域，以及这如何阻碍了对其他领域的创新投资。硬件彩票并不新鲜；即使在计算机科学中也不是，但这篇文章的核心论点是“硬件彩票正在拖累我们，应该避免。”</p><p> Hardware lotteries, and more generally, resource lotteries, have existed for as long as technology has, simply because of the nature of innovation and the economics of capital intensive enterprises. For example, sophisticated hydraulic engines were replaced by electrical counterparts over the decades, but until then a lot of interesting ideas and applications (e.g. Bessemer process and the mass production of steel) came out of hydraulic technology. Even with electrical actuators, the strength of the electrical field you can generate limits the pressure you can produce while hydraulics are only limited by the strength of materials. So, even if Electricity was a uniformly superior/efficient technology, there are no absolute winners; It all depends on the context. I can keep going, but this is not a post on the evolution of innovation (a favorite topic of mine so we will no doubt revisit that in a later post). This is a post about  exploiting the inevitable hardware lotteries. To do that fully, we need to understand scaling of models before we write them off as wasteful exercises.</p><p>硬件彩票，更广泛地说，资源彩票，随着技术的存在而存在了很长时间，这仅仅是因为创新的本质和资本密集型企业的经济。例如，几十年来，精密的液压发动机被电动发动机取代，但在此之前，许多有趣的想法和应用(如贝塞默工艺和钢铁的大规模生产)都来自液压技术。即使使用电动执行机构，你所能产生的电场强度也会限制你所能产生的压力，而液压只会受到材料强度的限制。因此，即使电力是一种一致优越/高效的技术，也没有绝对的赢家；这完全取决于背景。我可以继续下去，但这不是一个关于创新进化的帖子(这是我最喜欢的话题，所以我们无疑会在后面的帖子里再讨论这个话题)。这是一篇关于利用不可避免的硬件彩票的帖子。要完全做到这一点，我们需要了解模型的缩放，然后才能将其视为浪费的练习。</p><p>  Our intuitions about scaling are often flawed.  Strange things happen at the extremes of scale. Practically all theories about every aspect of life — natural or human-made — break down when scaled in either direction. Let’s take a few examples:</p><p>我们对规模的直觉往往是错误的。奇怪的事情发生在规模的极端。实际上，所有关于生活方方面面的理论--无论是自然的还是人为的--都会在任何一个方向上被打破。让我们举几个例子：</p><p> Time: We can plan for the next few hours mentally straightforwardly, with some effort, and with the aid of a calendar for the next few days, but many of us struggle thinking about consequences for the next few years. Very few people can think about the implications for, say, the next ten years (many of them unsurprisingly are famed investors and that’s not an accident).</p><p>时间：我们可以在头脑中直截了当地计划接下来的几个小时，付出一些努力，并在日历的帮助下为接下来的几天做计划，但我们中的许多人都在为未来几年的后果而苦苦思索。很少有人能想到，比方说，未来十年的影响(他们中的许多人都是著名的投资者，这并不是偶然的)。</p><p> Money: As money scales, folks have trouble understanding what it is and what it can do as the nature of money itself changes with accumulation. As Marx notes:</p><p>货币：随着货币规模的扩大，人们很难理解它是什么，它能做什么，因为货币本身的性质随着积累而变化。正如马克思所指出的：</p><p> The accumulation of capital, which originally appeared only as its quantiative extension, comes to fruition, as we have seen, through a progressive qualitative change in its composition. — Das Kapital (1867)</p><p>资本的积累最初只是作为其数量的延伸，但正如我们所看到的，它是通过其构成的渐进式质变而取得成果的。《资本论》(1867年)。</p><p> Lottery winners routinely get discombobulated when confronting their winnings, and most Americans have trouble grasping the extent of our national debt.</p><p>彩票中奖者在面对他们的中奖时通常会感到困惑，大多数美国人在把握我们国家债务的程度上都有困难。</p><p>  The nuance between the simple accumulation of capital and its qualitative effects is best illustrated in this  supposed conversation between Fitzgerald and Hemingway in a 1920 Paris café:</p><p>在菲茨杰拉德和海明威1920年在巴黎一家咖啡馆里的对话中，简单的资本积累和其质量效应之间的细微差别得到了最好的诠释：</p><p> Fitzgerald:  “The rich are different from us.” Hemingway:  “Yes, they have more money.”</p><p>菲茨杰拉德：“富人和我们不一样。”海明威：“是的，他们有更多的钱。”</p><p> Crowds: People scale differently, too. It is not uncommon for a large collection of mediocre hires to come together and form a brilliant organizational unit. While individual and small group opinions are less interesting, Twitter, one of the largest opinion scaling experiments, has produced a sea change in thinking (#metoo, #BLM, ...) and massive information operations at once.</p><p>人群：人们的规模也不同。一大批平庸的员工走到一起，组成一个出色的组织单位，这并不少见。虽然个人和小团体的意见不那么有趣，但推特(Twitter)，最大的意见衡量实验之一，已经在思维上产生了翻天覆地的变化(#MeToo，#BLM，...)。和海量信息操作同时进行。</p><p> Physical Sciences: Nature is self-organized hierarchically, and it’s not a flaw that entirely new laws are needed to understand different scales (for e.g., Quantum Mechanics at a sub-atomic scale and General Relativity at a cosmological scale). In other words, every quantitative shift is accompanied by its own qualitative shifts.</p><p>物理科学：自然界是有层次的自组织的，需要全新的定律来理解不同的尺度(例如，亚原子尺度的量子力学和宇宙尺度的广义相对论)，这并不是一个缺陷。换句话说，每一次量变都伴随着它自身的质变。</p><p> “The whole becomes not only more than but very different from the sum of its parts.” — Anderson (1972), More is Different.</p><p>“整体不仅比各部分之和更大，而且非常不同。”-安德森(1972)，“更多是不同的。”--安德森(1972)，“更多是不同的”。</p><p> Even in an exact field like mathematics,  singular limits exist, and known recurrences break down past a limit.</p><p>即使在像数学这样的精确领域，奇异极限也是存在的，已知的递归也会超出极限。</p><p>  So, a natural question:  What happens when we scale the number of parameters in a neural network to absurd levels? Are there “emergent” realities that cannot be explained by the component parts?</p><p>因此，一个自然的问题是：当我们将神经网络中的参数数量调整到荒谬的水平时，会发生什么？是否存在组成部分无法解释的“紧急”现实？</p><p> We already see some of this in VLP models like GPT-3 where the model is able to “solve” several unseen problems in natural language or other domains after seeing only a few examples (so-called “zero-shot” / “few-shot” generalization). But we don’t really understand how or why that happens. Studying this emergent reality should be the foremost preoccupation for anyone working on VLP models.</p><p>我们已经在像GPT-3这样的VLP模型中看到了其中的一些，在GPT-3中，该模型能够在只看到几个例子(所谓的“零概率”/“少概率”泛化)之后，在自然语言或其他领域中“解决”几个未见过的问题。但我们并不真正了解这种情况是如何发生的，也不知道为什么会发生这种情况。研究这个新出现的现实应该是任何研究VLP模型的人的首要任务。</p><p>  Often discussions in AI wander into (sometimes unwarranted) comparisons with the human brain. One argument against parameter scaling is that  human brains run on the power of an electric shaver, we could be wasting our time, effort, and energy running these VLP models on GPUs/TPUs. This argument is based on some flawed assumptions:</p><p>人工智能领域的讨论经常与人脑相提并论(有时毫无根据)。反对参数缩放的一个论点是，人脑依靠电动剃须刀的力量运行，我们可能会浪费时间、精力和精力在GPU/TPU上运行这些VLP模型。这种说法是基于一些有缺陷的假设：</p><p> 1. The human brain is a perfect piece of engineering and should be mimicked. This assumption is a trap for human thinking; indeed, few things are as marvelous to the human mind as the human brain. Francois Jacob, in his influential 1977 article, “ Evolution and Tinkering” explains it best:</p><p>1.人脑是一件完美的工程，应该被模仿。这一假设是人类思维的陷阱；事实上，很少有东西能像人脑那样对人类思维产生奇妙的影响。弗朗索瓦·雅各布(Francois Jacob)在1977年发表的一篇颇具影响力的文章《进化与修补》中对此进行了最好的解释：</p><p> It is hard to realize that the living world as we know it is just one of the many possibilities; that its actual structure results from the history of the earth. … They represent, not a perfect product of engineering, but a patchwork of odd sets of pieced together when and where opportunities arose.</p><p>我们很难认识到，我们所知道的生命世界只是众多可能性中的一种；它的实际结构是由地球的历史决定的。…。它们代表的不是完美的工程产品，而是在机会出现的时间和地点拼凑而成的奇特组合。</p><p> Opportunism reflects the “very nature of a historical process full of contingency”. In other words, we are a product of a variety of lotteries — physical, ecological, and historical. Changing the nature of these lotteries would lead to different outcomes and not necessarily provably better outcomes.</p><p>机会主义反映了“充满偶然性的历史进程的本质”。换句话说，我们是各种彩票的产物--实物彩票、生态彩票和历史彩票。改变这些彩票的性质将导致不同的结果，而不一定是可以证明的更好的结果。</p><p> 2. Low power != Fewer parameters. The ultra-low-power nature of human brains has more to do with its substrate than the number of connections. In fact, studies in evolutionary neurobiology and comparative neuroanatomy reveal strong correlations between body weight, brain weight, and the number of neurons.</p><p>2.低功耗！=较少的参数。人类大脑的超低能量特性更多地与其底物有关，而不是连接的数量。事实上，进化神经生物学和比较神经解剖学的研究表明，体重、大脑重量和神经元数量之间存在很强的相关性。</p><p>  And it’s not that the brains of hominid species have remained static either. In fact, some researchers like Suzana Herculano-Houzel argue that  Homo Sapiens have actually got a hardware upgrade over their ancestors primarily due to the invention of cooking, which provides a means to improve energy density in food, much like upgrading your power supply unit because you added extra GPUs.</p><p>这也不是说原始人的大脑一直处于静止状态。事实上，像Suzana Herculano-Houzel这样的一些研究人员认为，智人实际上比他们的祖先有了硬件升级，这主要是因为烹饪的发明，它提供了一种提高食物能量密度的方法，就像你因为增加了额外的GPU而升级了你的电源装置一样。</p><p>  Perhaps, the future of AI will be very large parameter models running on  ultra-low-power bioelectronics.</p><p>也许，人工智能的未来将是在超低功耗生物电子设备上运行的非常大的参数模型。</p><p> Aside:  Pruning and  Distillation of AI models are ways to reduce power consumption while approximating capabilities. Pruning removes inconsequential weights from a model, while distillation trains a seperate smaller model to mimic the outputs of a larger model in a teacher-student fashion. While these approaches have practical applications, they don’t create new capabilites over existing models.</p><p>旁白：修剪和蒸馏人工智能模型是在接近能力的情况下降低功耗的方法。修剪去除了模型中无关紧要的权重，而蒸馏则训练单独的较小模型，以师生的方式模拟较大模型的输出。虽然这些方法有实际应用，但它们不会在现有模型上创建新的功能。</p><p>  The hardware lottery is a kind of resource lottery. Resource lotteries for innovation are not new in science and engineering, and if we look back, even nature has several examples of such lotteries. In fact, a general observation would be that   resource lotteries are inevitable and we are also better served by focusing on answering interesting questions posed by current realities than an imagined future. In trying to create a uniform exploration of idea spaces divorced from economic/practical realities (to “avoid the hardware lottery”), we would be missing out on interesting research opportunities by shunting works simply because they don’t fit our current understanding of how the human brain works or is capable of.</p><p>硬件彩票是资源彩票的一种。创新的资源彩票在科学和工程领域并不新鲜，如果我们回顾一下，即使是大自然也有几个这样的彩票例子。事实上，一个普遍的观察是，资源彩票是不可避免的，专注于回答当前现实提出的有趣问题，而不是想象中的未来，也会更好地服务于我们。如果我们试图创造一种脱离经济/现实的想法空间的统一探索(以“避免硬件抽奖”)，那么我们将错失有趣的研究机会，因为它们不符合我们目前对人脑是如何工作或有能力工作的理解，只因为它们不符合我们目前对人脑是如何工作或有能力工作的理解，这仅仅是因为它们不符合我们目前对人脑如何工作或有能力工作的理解。</p><p> In particular, one has to keep in mind that not all big models are alike, and Very Large Parameter models are uniquely interesting in that they add more capabilities to the model in ways we don’t understand today.</p><p>尤其需要注意的是，并不是所有的大型模型都是一样的，而非常大的参数模型之所以特别有趣，是因为它们以我们今天不理解的方式为模型添加了更多功能。</p><p>  One way to look at AI modeling of today is to imagine ourselves in some Cambrian era with all sorts of brains proliferating from the most efficient to the least efficient. With competition for resources, the least efficient options will eventually get culled out, but in their wake, they may leave behind an understanding we would not achieve otherwise. Efficiency and capability of intelligent systems are two separate goals and any arguments in limiting the exploration of one goal for another comes from over-investing and extrapolating the limitations of current technologies.</p><p>看待当今人工智能建模的一种方式是想象我们身处寒武纪时代，各种各样的大脑从最高效到最低效率不断涌现。随着对资源的争夺，效率最低的选择最终将被淘汰，但随之而来的是，它们可能会留下一种谅解，否则我们就无法实现这一点。智能系统的效率和能力是两个不同的目标，限制对一个目标和另一个目标的探索的任何争论都来自于过度投资和推断当前技术的局限性。</p><p> Despite an electric shaver-like power efficiency, the human brain has limits that some of the Very Large Parameter models transcend (even if that’s unreliable today). A future I would like to live in is where human brains are augmented with capabilities that seem alien to me at this time of writing, via a second brain that does things so much differently than our wet brains.</p><p>尽管有电动剃须刀般的能效，但人脑有一些非常大的参数模型所超越的极限(即使这在今天是不可靠的)。我想要生活的未来是，人类的大脑被增强了能力，在我撰写这篇文章的时候，这些能力对我来说似乎是陌生的，通过第二个大脑，它做事情的方式与我们湿润的大脑有很大的不同。</p><p>  Acknowledgments: Many thanks to  Melanie Mitchell, Jen-Hao Yeh, and  Cristian Strat for comments on early drafts of this.</p><p>致谢：非常感谢梅勒妮·米切尔、叶仁浩和克里斯蒂安·斯特拉特对这本书早期草稿的评论。</p><p>  @misc{clarity:ride-hardware-lottery, author = {Delip Rao}, title = {Ride the Hardware Lottery!}, howpublished = {\url{https://pagestlabs.com/clarity/ride-hardware-lottery}}, month = {November}, year = {2020}}</p><p>@misc{Clarity：骑行-硬件-彩票，作者={Delip Rao}，标题={骑硬件彩票！}，如何发布={\url{https://pagestlabs.com/clarity/ride-hardware-lottery}}，月={11}，年份={2020}}</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://pagestlabs.substack.com/p/ride-the-hardware-lottery">https://pagestlabs.substack.com/p/ride-the-hardware-lottery</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/硬件/">#硬件</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/hardware/">#hardware</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1034844.html"><img src="http://img2.diglog.com/img/2020/11/thumb_7e158e30fd4e364ef3ccf9b99a500622.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1034844.html">苹果公司的Greg Joswiak、Craig Federighi和John Ternus就M1的开发、Mac的未来、购买第一版硬件等问题进行了采访</a></div><span class="my_story_list_date">2020-11-13 19:0</span></div><div class="col-sm"><div><a target="_blank" href="/story/1032932.html"><img src="http://img2.diglog.com/img/2020/11/thumb_658894646018d3332aa745e8ce20f3d5.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1032932.html">FPGA PS2键盘打字机(硬件历险记，第8部分)</a></div><span class="my_story_list_date">2020-11-3 14:45</span></div><div class="col-sm"><div><a target="_blank" href="/story/1032654.html"><img src="http://img2.diglog.com/img/2020/11/thumb_0223d07386be748502b18ba2d0b454f9.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1032654.html">“把一些东西放进硬件”(ASIC)通常不会让程序变得更快</a></div><span class="my_story_list_date">2020-11-2 2:48</span></div><div class="col-sm"><div><a target="_blank" href="/story/1032325.html"><img src="http://img2.diglog.com/img/2020/10/thumb_c066198a7d3914710b3fa57a66a0e513.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1032325.html">RISC-V正试图发起一场开放硬件革命</a></div><span class="my_story_list_date">2020-10-31 2:1</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>