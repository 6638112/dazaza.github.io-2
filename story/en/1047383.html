<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>感：深度学习的新开源视频理解框架 Sense: A New Open-Source Video Understanding Framework for Deep Learning</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Sense: A New Open-Source Video Understanding Framework for Deep Learning<br/>感：深度学习的新开源视频理解框架 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-09 20:19:56</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/aadda445121c7d02ce8e7b10a1bb4c90.png"><img src="http://img2.diglog.com/img/2021/2/aadda445121c7d02ce8e7b10a1bb4c90.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>senseis an inference engine to serve powerful neural networks for action recognition, with a lowcomputational footprint. In this repository, we provide:</p><p>感觉是一个推理引擎，可为功能强大的神经网络提供动作识别功能，且计算量较小。在此存储库中，我们提供：</p><p> Two models out-of-the-box pre-trained on millions of videos of humans performingactions in front of, and interacting with, a camera. Both neural networks are small, efficient, and run smoothly in real time on a CPU.</p><p> 开箱即用的两个模型已经在数百万个视频的视频中进行了预训练，这些视频是人类在摄像机前进行动作并与之交互的。这两个神经网络都很小，高效，并且可以在CPU上实时平稳运行。</p><p> Demo applications showcasing the potential of our models: gesture recognition, fitness activity tracking, livecalorie estimation.</p><p> 演示应用程序展示了我们模型的潜力：手势识别，健身活动跟踪，实时卡路里估计。</p><p> A pipeline to record and annotate your own video dataset and train a custom classifier on top of our models with an easy-to-use script to fine-tune our weights.</p><p> 记录和注释您自己的视频数据集的管道，并通过易于使用的脚本在我们的模型上训练自定义分类器，以微调权重。</p><p>    The following steps are confirmed to work on Linux (Ubuntu 18.04 LTS and 20.04 LTS) and macOS (Catalina 10.15.7).</p><p>    确认以下步骤可在Linux（Ubuntu 18.04 LTS和20.04 LTS）和macOS（Catalina 10.15.7）上运行。</p><p>     We recommended creating a new virtual environment to install our dependencies using conda or   virtualenv. The following instructions will help create a conda environment.</p><p>     我们建议创建一个新的虚拟环境，以使用conda或virtualenv安装依赖项。以下说明将有助于创建conda环境。</p><p>    Note:  pip install -r requirements.txt only installs the CPU-only version of PyTorch.To run inference on your GPU, another version of PyTorch should be installed. For instance:</p><p>    注意：pip install -r requirements.txt仅安装PyTorch的仅CPU版本。要在GPU上运行推理，应安装另一版本的PyTorch。例如： </p><p>    Pre-trained weights can be downloaded from  here. Follow theinstructions there to create an account and download the weights. Once downloaded, unzip the folder and move thefolder named  backbone into  sense/resources. In the end, your resources folder structure should look likethis:</p><p>预先训练的砝码可以从这里下载。按照那里的说明创建一个帐户并下载砝码。下载完成后，解压缩该文件夹，然后将名为骨干的文件夹移动到有意义/资源中。最后，您的资源文件夹结构应如下所示：</p><p> resources├── backbone│ ├── strided_inflated_efficientnet.ckpt│ └── strided_inflated_mobilenet.ckpt├── fitness_activity_recognition│ └── ...├── gesture_detection│ └── ...└── ...</p><p> 资源├──骨干│├──strided_inflated_efficientnet.ckpt│└──strided_inflated_mobilenet.ckpt├──Fitness_activity_recognition│└──...├──手势检测│└──...└──...</p><p> Note: The remaining folders in  resources/ will already have the necessary files -- only  resources/backboneneeds to be downloaded separately.</p><p> 注意：resources /中的其余文件夹将已经具有必要的文件-仅资源/ backboneneeds需要单独下载。</p><p>   To get started, try out the demos we&#39;ve provided. Inside the  sense/examples directory, you will find 3 Python scripts, run_gesture_recognition.py,  run_fitness_tracker.py, and  run_calorie_estimation.py. Launching each demo is assimple as running the script in terminal as described below.</p><p>   首先，请尝试我们提供的演示。在sense / examples目录中，您将找到3个Python脚本，分别是run_gesture_recognition.py，run_fitness_tracker.py和run_calorie_estimation.py。如下所述，启动每个演示程序就像在终端中运行脚本一样简单。</p><p>  examples/run_gesture_recognition.py applies our pre-trained models to hand gesture recognition.30 gestures are supported (see full list here).</p><p>  examples / run_gesture_recognition.py将我们训练有素的模型应用于手势识别。支持30种手势（请参见此处的完整列表）。</p><p>    examples/run_fitness_tracker.py applies our pre-trained models to real-time fitness activity recognition and calorie estimation.In total, 80 different fitness exercises are recognized (see full list here).</p><p>    examples / run_fitness_tracker.py将我们训练有素的模型应用于实时健身活动识别和卡路里估算。总共可以识别80种不同的健身运动（请参见此处的完整列表）。</p><p>   Weight, age, height should be respectively given in kilograms, years and centimeters. If not provided, default values will be used.</p><p>   体重，年龄，身高应分别以千克，年和厘米为单位。如果未提供，将使用默认值。 </p><p>  --camera_id=CAMERA_ID ID of the camera to stream from --path_in=FILENAME Video file to stream from. This assumes that the video was encoded at 16 fps.</p><p>--camera_id = CAMERA_ID要从--path_in = FILENAME流式传输的摄像机的ID。假定视频以16 fps编码。</p><p>    Place your camera on the floor, angled upwards with a small portion of the floor visible</p><p>    将相机放在地板上，向上倾斜，一小部分地板可见</p><p>  In order to estimate burned calories, we trained a neural net to convert activity features to the corresponding  MET value.We then post-process these MET values (see correction and aggregation steps performed  here)and convert them to calories using the user&#39;s weight.</p><p>  为了估算燃烧的卡路里，我们训练了一个神经网络将活动特征转换为相应的MET值。然后，我们对这些MET值进行后处理（请参阅此处执行的校正和聚合步骤），并使用用户将其转换为卡路里。的重量。</p><p> If you&#39;re only interested in the calorie estimation part, you might want to use  examples/run_calorie_estimation.py which has a slightly moredetailed display (see video  here which compares two videos produced by that script).</p><p> 如果您仅对卡路里估算部分感兴趣，则可能要使用examples / run_calorie_estimation.py，它的显示会稍微更详细（请参见此处的视频，该脚本比较了该脚本制作的两个视频）。</p><p>   The estimated calorie estimates are roughly in the range produced by wearable devices, though they have not been verified in terms of accuracy.From our experiments, our estimates correlate well with the workout intensity (intense workouts burn more calories) so, regardless of the absolute accuracy, it should be fair to use this metric to compare one workout to another.</p><p>   估计的卡路里估计值大致在可穿戴设备产生的范围内，尽管尚未经过准确性方面的验证。根据我们的实验，我们的估计值与锻炼强度紧密相关（剧烈锻炼消耗更多的卡路里），因此无论绝对准确性，使用此指标将一项锻炼与另一项锻炼进行比较应该是公平的。</p><p>   This section will describe how you can build your own custom classifier on top of our models. Our models will serveas a powerful feature extractor that will reduce the amount of data you need to build your project.</p><p>   本节将描述如何在我们的模型之上构建自己的自定义分类器。我们的模型将充当功能强大的特征提取器，这将减少构建项目所需的数据量。</p><p>  First, run the  tools/sense_studio/sense_studio.py script and open  http://127.0.0.1:5000/ in your browser.There you can set up a new project in a location of your choice and specify the classes that you want to collect.</p><p>  首先，运行tools / sense_studio / sense_studio.py脚本并在浏览器中打开http://127.0.0.1:5000/。您可以在您选择的位置设置一个新项目，并指定想要的类。收藏。 </p><p> The tool will prepare the following file structure for your project, so you can insert the recorded videos into thecorresponding folders:</p><p>该工具将为您的项目准备以下文件结构，因此您可以将录制的视频插入相应的文件夹中：</p><p> /path/to/your/dataset/├── videos_train│ ├── class1│ │ ├── video1.mp4│ │ ├── video2.mp4│ │ └── ...│ ├── class2│ │ ├── video3.mp4│ │ ├── video4.mp4│ │ └── ...│ └── ...├── videos_valid│ ├── class1│ │ ├── video5.mp4│ │ ├── video6.mp4│ │ └── ...│ ├── class2│ │ ├── video7.mp4│ │ ├── video8.mp4│ │ └── ...│ └── ...└── project_config.json</p><p> / path / to /您的/数据集/├──video_train│├──class1││├──video1.mp4│││├──video2.mp4││└──...│├──class2││├ ──video3.mp4││├──video4.mp4││└──...│└──...├──video_valid│├──class1││├──video5.mp4││├── video6.mp4││└──...│├──class2││├──video7.mp4││├──video8.mp4││└──...│└──...└── project_config.json</p><p> One sub-folder for each class with as many videos as you want (but at least one!)</p><p> 每个班级一个子文件夹，其中包含您想要的视频（但至少一个！）</p><p> In some cases, as few as 2-5 videos per class have been enough to achieve excellent performance!</p><p> 在某些情况下，每堂课只有2-5个视频就足以实现出色的表现！</p><p>  Once your data is prepared, run this command to train a customized classifier on top of one of our features extractor:</p><p>  准备好数据后，请运行以下命令以在我们的功能提取器之一之上训练自定义分类器：</p><p>   The training script should produce a checkpoint file called  classifier.checkpoint at the root of the dataset folder.You can now run it live using the following script:</p><p>   训练脚本应在数据集文件夹的根目录下生成一个名为classifier.checkpoint的检查点文件。您现在可以使用以下脚本实时运行它：</p><p>    You can further improve your model&#39;s performance by training on top of temporally annotated data;individually tagged frames that identify the event locally in the video versus treating every frame with the samelabel. For instructions on how to prepare your data with temporal annotations, refer to this page.</p><p>    通过在时间批注数据的基础上进行训练，可以进一步提高模型的性能；可以使用单个标记的帧在视频中本地标识事件，而不是使用相同的标签处理每个帧。有关如何准备带有时间注释的数据的说明，请参阅此页面。 </p><p> After preparing your dataset with our temporal annotations tool, pass  --temporal_training as an additionalflag to the  train_classifier.py script.</p><p>使用我们的时间注释工具准备数据集后，将--temporal_training作为附加标志传递给train_classifier.py脚本。</p><p>   If you&#39;re interested in mobile app development and want to run our models on iOS devices, pleasecheck out  sense-iOS for step by step instructions on howto get our gesture demo to run on an iOS device. One of the steps involves converting our Pytorchmodels to the TensorFlow Lite format.</p><p>   如果您对移动应用程序开发感兴趣并且想在iOS设备上运行我们的模型，请查看Sense-iOS，以获取有关如何使手势演示在iOS设备上运行的逐步说明。步骤之一涉及将我们的Pytorchmodels转换为TensorFlow Lite格式。</p><p>    If you want to convert a custom classifier, set the classifier name to &#34;custom_classifier&#34;,and provide the path to the dataset directory used to train the classifier using the &#34;--path_in&#34; argument.</p><p>    如果要转换自定义分类器，请将分类器名称设置为＆＃34; custom_classifier＆＃34 ;，并使用＆＃34;-path_in＆＃34;提供用于训练分类器的数据集目录的路径。论据。</p><p>      @misc{ sense2020blogpost,  author =   {Guillaume Berger and Antoine Mercier and Florian Letsch and Cornelius Boehm and   Sunny Panchal and Nahua Kang and Mark Todorovich and Ingo Bax and Roland Memisevic },  title =   {Towards situated visual AI via end-to-end learning on video clips },  howpublished =   {\url{https://medium.com/twentybn/towards-situated-visual-ai-via-end-to-end-learning-on-video-clips-2832bd9d519f} },  note =   {online; accessed 23 October 2020 },  year= 2020,}</p><p>      @misc {sense2020blogpost，作者= {Guillaume Berger和Antoine Mercier以及Florian Letsch和Cornelius Boehm和Sunny Panchal和Nahua Kang和Mark Todorovich和Ingo Bax和Roland Memisevic}，标题= {通过端到端学习来实现视觉AI在视频剪辑上}，如何发布= {\ url {https://medium.com/twentybn/towards-situated-visual-ai-via-end-to-end-learning-on-video-clips-2832bd9d519f}}，请注意= {在线； 2020年10月23日访问，}，年份= 2020，}</p><p>   The code is copyright (c) 2020 Twenty Billion Neurons GmbH under an MIT Licence. See the file LICENSE for details. Note that this licenseonly covers the source code of this repo. Pretrained weights come with a separate license available  here.</p><p>   该代码在MIT许可下的版权（c）2020年200亿个Neurons GmbH。有关详细信息，请参见文件LICENSE。请注意，此许可证仅涵盖此存储库的源代码。预训练砝码带有单独的许可证，可在此处获得。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/TwentyBN/sense">https://github.com/TwentyBN/sense</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/开源/">#开源</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/深度学习/">#深度学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/深度/">#深度</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/open/">#open</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>