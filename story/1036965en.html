<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>现实世界中的拜占庭式失败A Byzantine failure in the real world</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">A Byzantine failure in the real world<br/>现实世界中的拜占庭式失败</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-28 12:20:09</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/f795bedda3e1583b8a82b9a0f5a5e556.png"><img src="http://img2.diglog.com/img/2020/11/f795bedda3e1583b8a82b9a0f5a5e556.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>When we review design documents at Cloudflare, we are always on the lookout for Single Points of Failure (SPOFs). Eliminating these is a necessary step in architecting a system you can be confident in. Ironically, when you’re designing a system with built-in redundancy, you spend most of your time thinking about how well it functions when that redundancy is lost.</p><p>当我们在Cloudflare审查设计文档时，我们总是在寻找单点故障（SPOF）。消除这些是构建您有信心的系统的必要步骤。具有讽刺意味的是，当您设计具有内置冗余的系统时，您会花费大量时间来考虑丢失冗余时系统的功能。</p><p> On November 2, 2020, Cloudflare had an  incident that impacted the availability of the API and dashboard for six hours and 33 minutes. During this incident, the success rate for queries to our API periodically dipped as low as 75%, and the dashboard experience was as much as 80 times slower than normal. While Cloudflare’s edge is massively distributed across the world (and kept working without a hitch), Cloudflare’s control plane (API &amp; dashboard) is made up of a large number of microservices that are redundant across two regions. For most services, the databases backing those microservices are only writable in one region at a time.</p><p> 2020年11月2日，Cloudflare发生了一个事件，该事件在6小时33分钟内影响了API和仪表板的可用性。在此事件期间，对我们API的查询成功率周期性下降至75％，并且仪表板体验比正常情况下慢80倍之多。尽管Cloudflare的优势已在全球范围内广泛分布（并且一直保持正常运行），但Cloudflare的控制平面（API和仪表板）由大量的在两个区域冗余的微服务组成。对于大多数服务，支持这些微服务的数据库一次只能写在一个区域中。</p><p> Each of Cloudflare’s control plane data centers has multiple racks of servers. Each of those racks has two switches that operate as a pair—both are normally active, but either can handle the load if the other fails. Cloudflare survives rack-level failures by spreading the most critical services across racks. Every piece of hardware has two or more power supplies with different power feeds. Every server that stores critical data uses RAID 10 redundant disks or storage systems that replicate data across at least three machines in different racks, or both. Redundancy at each layer is something we review and require. So—how could things go wrong?</p><p> 每个Cloudflare的控制平面数据中心都有多个服务器机架。这些机架中的每个机架都具有两个成对工作的开关-通常都处于活动状态，但是如果另一个发生故障，则其中一个可以处理负载。 Cloudflare通过在机架之间分布最关键的服务来抵抗机架级故障。每个硬件都有两个或多个电源不同的电源。每台存储关键数据的服务器都使用RAID 10冗余磁盘或存储系统，它们可以在不同机架中的至少三台计算机之间（或同时在这两者之间）复制数据。我们需要审查和要求每一层都有冗余。那么-怎么会出错？</p><p> In this post we present a timeline of what happened, and how a difficult failure mode known as a Byzantine fault played a role in a cascading series of events.</p><p> 在这篇文章中，我们介绍了发生的事情的时间表，以及称为拜占庭式故障的困难故障模式如何在一系列事件中发挥了作用。</p><p>  At 14:43, a network switch started misbehaving. Alerts began firing about the switch being unreachable to pings. The device was in a partially operating state: network control plane protocols such as  LACP and  BGP remained operational, while others, such as vPC, were not. The vPC link is used to synchronize ports across multiple switches, so that they appear as one large, aggregated switch to servers connected to them. At the same time, the data plane (or forwarding plane) was not processing and forwarding all the packets received from connected devices.</p><p>  14:43，网络交换机开始行为异常。警报开始触发有关ping无法到达的开关。设备处于部分运行状态：LACP和BGP等网络控制平面协议仍可运行，而vPC等其他协议则未运行。 vPC链接用于在多个交换机之间同步端口，以使它们在连接到与它们相连的服务器时显示为一台大型聚合交换机。同时，数据平面（或转发平面）未处理和转发从连接的设备收到的所有数据包。</p><p> This failure scenario is completely invisible to the connected nodes, as each server only sees an issue for some of its traffic due to the load-balancing nature of LACP. Had the switch failed fully, all traffic would have failed over to the peer switch, as the connected links would&#39;ve simply gone down, and the ports would&#39;ve dropped out of the forwarding LACP bundles.</p><p> 由于LACP的负载平衡特性，每个服务器只能看到部分流量问题，因此这种故障情况对于连接的节点是完全不可见的。如果交换机完全故障，所有流量都将故障转移到对等交换机，因为所连接的链路将简单地断开，并且端口将从转发LACP捆绑中退出。</p><p> Six minutes later, the switch recovered without human intervention. But this odd failure mode led to further problems that lasted long after the switch had returned to normal operation.</p><p> 六分钟后，切换恢复，无需人工干预。但是这种奇特的故障模式导致了进一步的问题，这些问题在交换机恢复到正常操作后持续了很长时间。</p><p>  The rack with the misbehaving switch included one server in our etcd cluster. We use  etcd heavily in our core data centers whenever we need strongly consistent data storage that’s reliable across multiple nodes.</p><p>交换机行为异常的机架在我们的etcd集群中包含一台服务器。每当我们需要在多个节点上可靠的高度一致的数据存储时，我们都会在核心数据中心大量使用etcd。</p><p> In the event that the cluster leader fails, etcd uses the  RAFT protocol to maintain consistency and establish consensus to promote a new leader. In the RAFT protocol, cluster members are assumed to be either available or unavailable, and to provide accurate information or none at all. This works fine when a machine crashes, but is not always able to handle situations where different members of the cluster have conflicting information.</p><p> 如果集群领导者发生故障，etcd将使用RAFT协议保持一致性并建立共识以提升新领导者。在RAFT协议中，假定群集成员可用或不可用，并且仅提供准确的信息或根本不提供任何信息。当计算机崩溃时，这可以很好地工作，但并不总是能够处理群集中不同成员具有冲突信息的情况。</p><p>  Network traffic between node 1 (in the affected rack) and node 3 (the leader) was being sent through the switch in the degraded state,</p><p>  节点1（在受影响机架中）和节点3（领导者）之间的网络流量正在通过处于降级状态的交换机发送，</p><p> Network traffic between node 1 and node 2 were going through its working peer, and</p><p> 节点1和节点2之间的网络流量正在通过其工作对等方，并且</p><p> This caused cluster members to have conflicting views of reality, known in distributed systems theory as a  Byzantine fault. As a consequence of this conflicting information, node 1 repeatedly initiated leader elections, voting for itself, while node 2 repeatedly voted for node 3, which it could still connect to. This resulted in ties that did not promote a leader node 1 could reach. RAFT leader elections are disruptive, blocking all writes until they&#39;re resolved, so this made the cluster read-only until the faulty switch recovered and node 1 could once again reach node 3.</p><p> 这导致集群成员对现实有冲突的观点，在分布式系统理论中被称为拜占庭式错误。由于此冲突的信息，节点1反复发起了领导者选举，为自己投票，而节点2反复投票了仍可以连接到的节点3。这导致没有促进领导者节点1到达的联系。 RAFT领导者的选举具有破坏性，阻止所有写入，直到它们被解决为止，因此这使群集变为只读，直到故障交换机恢复并且节点1可以再次到达节点3。</p><p>   Cloudflare’s control plane services use relational databases hosted across multiple clusters within a data center. Each cluster is configured for high availability. The cluster setup includes a primary database, a synchronous replica, and one or more asynchronous replicas. This setup allows redundancy within a data center. For cross-datacenter redundancy, a similar high availability secondary cluster is set up and replicated in a geographically dispersed data center for disaster recovery. The cluster management system leverages etcd for cluster member discovery and coordination.</p><p>   Cloudflare的控制平面服务使用跨数据中心内多个群集托管的关系数据库。每个群集均配置为具有高可用性。群集设置包括一个主数据库，一个同步副本和一个或多个异步副本。此设置允许数据中心内的冗余。对于跨数据中心冗余，建立了类似的高可用性辅助群集，并在地理上分散的数据中心中进行复制以进行灾难恢复。集群管理系统利用etcd进行集群成员的发现和协调。</p><p> When etcd became read-only, two clusters were unable to communicate that they had a healthy primary database. This triggered the automatic promotion of a synchronous database replica to become the new primary. This process happened automatically and without error or data loss.</p><p> 当etcd变为只读时，两个群集无法传达它们具有正常的主数据库的信息。这触发了自动升级同步数据库副本成为新的主副本。此过程自动发生，没有错误或数据丢失。</p><p> There was a defect in our cluster management system that requires a rebuild of all database replicas when a new primary database is promoted. So, although the new primary database was available instantly, the replicas would take considerable time to become available, depending on the size of the database. For one of the clusters, service was restored quickly. Synchronous and asynchronous database replicas were rebuilt and started replicating successfully from primary, and the impact was minimal.</p><p>我们的集群管理系统中存在一个缺陷，当升级一个新的主数据库时，它需要重建所有数据库副本。因此，尽管新的主数据库是立即可用的，但是根据数据库的大小，副本将需要花费大量时间才能变得可用。对于其中一个群集，服务已快速恢复。重建了同步和异步数据库副本，并成功地从主数据库开始复制，并且影响很小。</p><p> For the other cluster, however, performant operation of that database  required a replica to be online. Because this database handles authentication for API calls and dashboard activities, it takes a lot of reads, and one replica was heavily utilized to spare the primary the load. When this failover happened and no replicas were available, the primary was overloaded, as it had to take all of the load. This is when the main impact started.</p><p> 但是，对于另一个群集，该数据库的高性能操作需要副本在线。由于该数据库处理API调用和仪表板活动的身份验证，因此需要进行大量读取，并且大量使用了一个副本来减轻主数据库的负担。当此故障转移发生且没有副本可用时，主数据库过载，因为它必须承担所有负载。这是主要影响开始的时间。</p><p>  At this point we saw that our primary authentication database was overwhelmed and began shedding load from it. We dialed back the rate at which we push SSL certificates to the edge, send emails, and other features, to give it space to handle the additional load. Unfortunately, because of its size, we knew it would take several hours for a replica to be fully rebuilt.</p><p>  在这一点上，我们看到主要的身份验证数据库已不堪重负，并开始减少它的负载。我们回拨了将SSL证书推送到边缘，发送电子邮件和其他功能的速率，以为其提供空间来处理额外的负载。不幸的是，由于它的大小，我们知道完全重建一个副本要花费几个小时。</p><p> A silver lining here is that every database cluster in our primary data center also has online replicas in our secondary data center. Those replicas are not part of the local failover process, and were online and available throughout the incident. The process of steering read-queries to those replicas was not yet automated, so we manually diverted API traffic that could leverage those read replicas to the secondary data center. This substantially improved our API availability.</p><p> 这里有一线希望是，我们主要数据中心中的每个数据库群集在我们次要数据中心中也都具有在线副本。这些副本不是本地故障转移过程的一部分，并且已联机并且在整个事件中都可用。将读取查询引导到这些副本的过程尚未实现自动化，因此我们手动将可能利用这些读取副本利用的API流量转移到了辅助数据中心。这大大改善了我们的API可用性。</p><p>  The Cloudflare dashboard, like most web applications, has the notion of a user session. When user sessions are created (each time a user logs in) we perform some database operations and keep data in a Redis cluster for the duration of that user’s session. Unlike our API calls, our user sessions cannot currently be moved across the ocean without disruption. As we took actions to improve the availability of our API calls, we were unfortunately making the user experience on the dashboard worse.</p><p>  与大多数Web应用程序一样，Cloudflare仪表板也具有用户会话的概念。创建用户会话后（每次用户登录），我们将执行一些数据库操作，并在该用户会话期间将数据保留在Redis集群中。与我们的API调用不同，我们的用户会话目前无法在不中断的情况下跨海移动。当我们采取措施改善API调用的可用性时，很不幸，我们使仪表板上的用户体验变得更糟。</p><p> This is an area of the system that is currently designed to be able to fail over across data centers in the event of a disaster, but has not yet been designed to work in both data centers at the same time. After a first period in which users on the dashboard became increasingly frustrated, we failed the authentication calls fully back to our primary data center, and kept working on our primary database to ensure we could provide the best service levels possible in that degraded state.</p><p> 该区域目前被设计为在发生灾难时能够跨数据中心进行故障转移，但尚未设计为可同时在两个数据中心中工作。在仪表盘上的用户变得越来越沮丧的第一阶段之后，我们完全无法通过身份验证调用返回到我们的主数据中心，并继续在我们的主数据库上工作，以确保在这种降级状态下我们可以提供最佳的服务水平。</p><p>  The instant the first database replica rebuilt, it put itself back into service, and performance resumed to normal levels. We re-ramped all of the services that had been turned down, so all asynchronous processing could catch up, and after a period of monitoring marked the end of the incident.</p><p>  重建第一个数据库副本后，它立即重新投入使用，性能恢复到正常水平。我们重新调整了所有已关闭的服务，以便所有异步处理都可以赶上，并且经过一段时间的监视后，事件已结束。</p><p>  The cascade of failures in this incident was interesting because each system, on its face, had redundancy. Moreover, no system fully failed—each entered a degraded state. That combination meant the chain of events that transpired was considerably harder to model and anticipate. It was frustrating yet reassuring that some of the possible failure modes were already being addressed.</p><p>此事件中的级联故障很有趣，因为每个系统在表面上都具有冗余性。此外，没有系统完全失败-每个系统都进入降级状态。这种结合意味着所发生的事件链很难建模和预测。令人沮丧但又令人放心的是，某些可能的故障模式已经得到解决。</p><p> A team was already working on fixing the limitation that requires a database replica rebuild upon promotion. Our user sessions system was inflexible in scenarios where we’d like to steer traffic around, and redesigning that was already in progress.</p><p> 一个团队已经在努力解决需要升级时重建数据库副本的限制。我们的用户会话系统在需要引导流量并进行重新设计的情况下不灵活。</p><p> This incident also led us to revisit the configuration parameters we put in place for things that auto-remediate. In previous years, promoting a database replica to primary took far longer than we liked, so getting that process automated and able to trigger on a minute’s notice was a point of pride. At the same time, for at least one of our databases, the cure may be worse than the disease, and in fact we may not want to invoke the promotion process so quickly. Immediately after this incident we adjusted that configuration accordingly.</p><p> 此事件还导致我们重新考虑为自动修复的事情而设置的配置参数。在过去的几年中，将数据库副本升级到主数据库花费的时间远远超过了我们希望的时间，因此，使该过程自动化并能够在一分钟内触发就可以了。同时，对于我们的至少一个数据库，治愈可能比疾病还差，实际上，我们可能不想这么快地调用推广过程。此事件发生后，我们立即调整了配置。</p><p> Byzantine Fault Tolerance (BFT) is a hot research topic. Solutions have been known since 1982, but have had to choose between a variety of engineering tradeoffs, including security, performance, and algorithmic simplicity. Most general-purpose cluster management systems choose to forgo BFT entirely and use protocols based on PAXOS, or simplifications of PAXOS such as RAFT, that perform better and are easier to understand than BFT consensus protocols. In many cases, a simple protocol that is known to be vulnerable to a rare failure mode is safer than a complex protocol that is difficult to implement correctly or debug.</p><p> 拜占庭容错（BFT）是一个热门的研究主题。解决方案自1982年以来就广为人知，但必须在各种工程设计之间做出选择，包括安全性，性能和算法简单性。大多数通用集群管理系统选择完全放弃BFT，而使用基于PAXOS的协议，或者使用PAXOS的简化版本（例如RAFT），其性能比BFT共识协议更好，更易于理解。在许多情况下，已知容易遭受罕见故障模式攻击的简单协议比难以正确实施或调试的复杂协议更安全。</p><p> The first uses of BFT consensus were in safety-critical systems such as aircraft and spacecraft controls. These systems typically have hard real time latency constraints that require tightly coupling consensus with application logic in ways that make these implementations unsuitable for general-purpose services like etcd. Contemporary research on BFT consensus is mostly focused on applications that cross trust boundaries, which need to protect against malicious cluster members as well as malfunctioning cluster members. These designs are more suitable for implementing general-purpose services such as etcd, and we look forward to collaborating with researchers and the open source community to make them suitable for production cluster management.</p><p> BFT共识的最初用途是在安全关键型系统中，例如飞机和航天器控制系统。这些系统通常具有严格的实时延迟约束，这些约束要求将共​​识与应用程序逻辑紧密耦合，从而使这些实现不适用于etcd等通用服务。关于BFT共识的当代研究主要集中在跨越信任边界的应用程序上，这些应用程序需要防止恶意集群成员和故障集群成员的攻击。这些设计更适合于实现诸如etcd之类的通用服务，我们期待与研究人员和开源社区合作，使其适合生产集群管理。</p><p> We are very sorry for the difficulty the outage caused, and are continuing to improve as our systems grow. We’ve since fixed the bug in our cluster management system, and are continuing to tune each of the systems involved in this incident to be more resilient to failures of their dependencies.  If you’re interested in helping solve these problems at scale, please visit  cloudflare.com/careers.</p><p> 对于中断造成的困难，我们深表歉意，并且随着我们的系统的发展，这种情况正在持续改善。此后，我们已修复了集群管理系统中的错误，并正在继续调整此事件中涉及的每个系统，以更灵活地应对依赖关系的失败。如果您有兴趣大规模解决这些问题，请访问cloudflare.com/careers。</p><p>  Post Mortem  API  Postgres  Outage  Engineering</p><p>  Post Mortem API Postgres中断工程</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.cloudflare.com/a-byzantine-failure-in-the-real-world/">https://blog.cloudflare.com/a-byzantine-failure-in-the-real-world/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/现实/">#现实</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/failure/">#failure</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>