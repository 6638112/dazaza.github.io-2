<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习的可解释性：概述Interpretability in Machine Learning: An Overview</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Interpretability in Machine Learning: An Overview<br/>机器学习的可解释性：概述</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-30 11:52:01</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/89fe201ca980245bf971d5f27cd6706e.png"><img src="http://img2.diglog.com/img/2020/11/89fe201ca980245bf971d5f27cd6706e.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This essay provides a broad overview of the sub-field of machine learning interpretability. While not exhaustive, my goal is to review conceptual frameworks, existing research, and future directions.</p><p>本文提供了机器学习可解释性子领域的广泛概述。尽管不详尽，但我的目标是审查概念框架，现有研究和未来方向。</p><p> I follow the categorizations used in Lipton et al.&#39;s  Mythos of Model Interpretability, which I think is the best paper for understanding the different definitions of interpretability. We&#39;ll go over many ways to formalize what &#34;interpretability&#34; means. Broadly, interpretability focuses on the  how. It&#39;s focused on getting some notion of an explanation for the decisions made by our models. Below, each section is operationalized by a concrete question we can ask of our machine learning model using a specific definition of interpretability. If you&#39;re new to all this, we&#39;ll first briefly explain why we might care about interpretability at all.</p><p> 我遵循Lipton等人的模型可解释性神话中使用的分类，我认为这是了解不同可解释性定义的最佳论文。我们将探讨许多方法来形式化“可解释性”的含义。广义上讲，可解释性集中在方式上。它着重于对我们的模型所做出的决策进行解释。下面，每个部分都由一个具体问题进行操作，我们可以使用可解释性的特定定义来询问我们的机器学习模型。如果您不熟悉所有这些，我们将首先简要说明为什么我们可能根本不关心可解释性。</p><p>  First, interpretability in machine learning is useful because it can aid in trust. As humans, we may be reluctant to rely on machine learning models for certain critical tasks, e.g., medical diagnosis, unless we know &#34;how they work.&#34; There&#39;s often a fear of the unknown when trusting in something opaque, which we see when people confront new technology, and this can slow down adoption. Approaches to interpretability that focus on transparency could help mitigate some of these fears.</p><p>  首先，机器学习中的可解释性很有用，因为它可以帮助建立信任。作为人类，除非我们知道“它们是如何工作的”，否则我们可能不愿意依靠机器学习模型来完成某些关键任务，例如医学诊断。当人们信任一些不透明的东西时，通常会担心未知。当人们面对新技术时，我们会看到这种未知，这会减慢采用速度。着眼于透明度的可解释性方法可以帮助减轻其中的一些担忧。</p><p> Second,  safety. There is almost always some  shift in distributions between model training and deployment. Failures to generalize or phenomena like  Goodhart&#39;s Law, such as  specification gaming, are open problems that could lead to issues in the near future. Approaches to interpretability, which explain the model&#39;s representations or which features are most relevant, could help diagnose these issues earlier and provide more opportunities to remedy the situation.</p><p> 第二，安全性。在模型训练和部署之间的分布几乎总是会有所变化。归纳失败或诸如规范博弈之类的古德哈特定律之类的现象是未解决的问题，有可能在不久的将来引发问题。可解释性的方法可以解释模型的表示形式或哪些特征最相关，可以帮助及早诊断这些问题，并提供更多的机会来纠正这种情况。</p><p> Third, and perhaps most interestingly, contestability. As we delegate more decision-making to ML models, it becomes important for the people to appeal these decisions. Black-box models provide no such recourse because they don&#39;t decompose the decision into anything contestable. This lack of contestability has already led to significant criticism of proprietary recidivism predictors like  COMPAS. Approaches to interpretability, which focus on decomposing the model into sub-models or illustrate a chain of reasoning, could help with such appeals.</p><p> 第三，也许是最有趣的是可竞争性。随着我们将更多的决策委托给ML模型，人们对这些决策提出上诉变得很重要。黑匣子模型无法提供此类资源，因为它们不会将决策分解为任何可竞争的事物。缺乏竞争性已经导致对COMPAS等专有累犯预测变量的严重批评。着重于将模型分解为子模型或说明推理链的可解释性方法可能有助于解决此类问题。</p><p>    According to Lipton et al.,  transparency as interpretability refers to the model&#39;s properties that are useful to understand and can be known before the training begins; they propose three distinct (but related) questions.</p><p>    根据Lipton等人的观点，透明度作为可解释性是指模型的属性，这些属性易于理解，可以在培训开始之前就知道；他们提出了三个不同的（但相关的）问题。</p><p>  This property addresses whether or not a human could go through each step of the algorithm and check if each step is reasonable to them. Linear models and decision trees are often cited as interpretable models using such justifications; the computation they require is simple, and it is relatively easy to interpret  each of the steps executed when a prediction is made. Linear models also have the nice property that the parameters themselves have a very direct mapping–they represent how important different input features are. For example, I trained a linear classifier on MNIST. Here are some of the weights, each of which corresponds to a pixel value:</p><p>  该属性用于解决人类是否可以遍历算法的每个步骤并检查每个步骤是否对他们合理。线性模型和决策树经常被引用为使用这些理由的可解释模型。他们所需的计算很简单，并且在做出预测时解释每个执行的步骤相对容易。线性模型还具有很好的属性，即参数本身具有非常直接的映射关系-它们表示不同输入特征的重要性。例如，我在MNIST上训练了线性分类器。以下是一些权重，每个权重对应于一个像素值：</p><p> 0.00000000e+00, 0.00000000e+00, 0.00000000e+00, -1.47542413e-03,-1.67811041e-04, -3.83280468e-02, -8.10846867e-02, -5.01943218e-02,-2.90314621e-02, -2.65494116e-02, -8.29385683e-03, 0.00000000e+00,0.00000000e+00, 1.67390785e-04, 3.92789141e-04, 0.00000000e+00,0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00]</p><p>0.00000000e + 00，0.00000000e + 00，0.00000000e + 00，-1.47542413e-03，-1.67811041e-04，-3.83280468e-02，-8.10846867e-02，-5.01943218e-02，-2.90314621e-02 ，-2.65494116e-02，-8.29385683e-03，0.00000000e + 00,0.00000000e + 00，1.67390785e-04，3.92789141e-04，0.00000000e + 00,0.00000000e + 00，0.00000000e + 00，0.00000000e +00，0.00000000e + 00]</p><p> By themselves, these weights are hard to interpret. Even if we knew which pixels they corresponded to, it&#39;s difficult to pin down what a particular pixel even represents, especially for large images. However, there is an easy trick to turn these weights into something interpretable. We reshape them into the same dimension as our model inputs and view it as an image, with the pixel color represented by the weight value.</p><p> 这些权重本身很难解释。即使我们知道它们对应的像素，也很难确定特定像素甚至代表的像素，特别是对于大图像。但是，有一个简单的技巧可以将这些权重转变为可以解释的东西。我们将它们重塑为与模型输入相同的尺寸，并将其视为图像，像素颜色由权重值表示。</p><p>     In both cases, we can see that the blue regions, representing positive weight, correspond to a pixel configuration that looks roughly like the digit being detected. In the case of 0, we can see a distinct blank spot in the center of the image and a curve-like shape around it; the curves of the 3 are also apparent.</p><p>     在这两种情况下，我们都可以看到代表正权重的蓝色区域与看起来像被检测到的数字大致一样的像素配置相对应。在0的情况下，我们可以在图像中心看到一个明显的空白点，并在其周围看到曲线状的形状； 3的曲线也很明显。</p><p> However, Lipton et al. point out that this desideratum can be less about the specific  type of model and more about the its  size. A decision tree with a billion nodes, for example, may still be challenging to understand. Understanding is also about  holding most of the model in your mind, which is often about how the model is parameterized [1].</p><p> 但是，Lipton等。指出，这种需求可能与模型的特定类型无关，而与模型的大小有关。例如，拥有十亿个节点的决策树可能仍然很难理解。理解还涉及将大多数模型牢记在心，这通常与如何参数化模型有关[1]。</p><p> Of course, parameterization is not the whole story. There are methods like K-Nearest Neighbors, which are parameterized by your entire dataset; this could be billions of points. Yet, there is a sense in which KNN is still interpretable despite its massive size. We can clearly describe what the algorithm does, and we can even see &#34;why&#34; it made a particular choice because the algorithm is simple to understand.</p><p> 当然，参数化还不是全部。有诸如K最近邻居之类的方法，这些方法由整个数据集进行参数化。这可能是数十亿点。然而，从某种意义上说，尽管KNN规模庞大，但仍然可以解释。我们可以清楚地描述该算法的作用，甚至可以看到“为什么”之所以做出特殊选择，因为该算法易于理解。</p><p>  Another desirable feature is to understand what the model is doing at each step. For example, imagine a decision tree whose nodes correspond to easily identifiable factors like age or height. The model&#39;s prediction can be interpreted in terms of what decisions are taken at different nodes of the tree. In general, such a detailed analysis (of decisions taken by the model per-timestep) is difficult because the model&#39;s performance is very tightly coupled with the representations used. Raw features, e.g., RGB pixel values, are often not very interpretable by themselves but could be very informative for the learning model. On the other hand, interpretable features may not be the most informative for the model.</p><p>  另一个理想的功能是了解模型在每个步骤中的功能。例如，想象一个决策树，其节点对应于容易识别的因素，例如年龄或身高。可以根据在树的不同节点上做出的决策来解释模型的预测。通常，由于模型的性能与所使用的表示非常紧密地耦合在一起，因此（对模型按时间步长做出的决策）进行这样的详细分析非常困难。原始特征（例如RGB像素值）通常自身无法很好地解释，但对于学习模型可能非常有用。另一方面，可解释的特征可能不是该模型最有用的信息。</p><p>   It seems like there would be at least some useful information in these features; &#34;ones&#34; tend to have less area (so average_luminosity would be lower), &#34;eights&#34; might have more corners, etc. Yet, the resulting decision tree of depth 3 (shown below) achieves only 33% training accuracy. Going all the way to the depth of ten only bumps it to around 50%.</p><p>   这些功能似乎至少会有一些有用的信息； “一个”趋向于具有较小的面积（因此average_luminosity会更低），“八个”可能具有更多的角，等等。然而，结果得到的深度为3的决策树（如下所示）仅达到33％的训练精度。一路深入到十的深度只会将其提高到50％左右。</p><p>  If we look at the nodes, we can perhaps understand what&#39;s going on. At the top, we can see that our model will predict a &#34;one&#34; if the width is less than 7.5 pixels, which seems intuitive as &#34;one&#34; is likely going to be the thinnest digit. Near the bottom, we see that the number of corners is being used to differentiate between &#34;deven&#34; and &#34;four.&#34; This also seems reasonable as &#34;four&#34; s do have more visual corners than &#34;seven&#34; s. But these are very crude features, and hence the overall performance is still not very good.</p><p>如果我们查看这些节点，则也许可以了解发生了什么。在顶部，我们可以看到，如果宽度小于7.5像素，我们的模型将预测为“ 1”，这看起来很直观，因为“ 1”可能是最薄的数字。在底部附近，我们看到拐角的数量被用来区分“ deven”和“四个”。这似乎也很合理，因为“四个”确实比“七个”具有更多的可视角。但是这些都是非常粗糙的功能，因此总体性能仍然不是很好。</p><p> I compare these hand-designed features with the raw visual features by training a decision tree (of depth 3) using the pixel values, i.e., a vector of 784 grayscale values. The resulting model, shown below, gets 50% train and test accuracy.</p><p> 我通过使用像素值（即784个灰度值的向量）训练决策树（深度为3）来将这些手工设计的功能与原始视觉功能进行比较。如下所示，生成的模型可获得50％的训练和测试精度。</p><p>  Here, it&#39;s not clear why these pixel values were chosen to be the splitting points. And yet the resulting decision tree, for the same number of nodes, performs much better. In this simple case, the performance vs. interpretability trade-off in representation is quite significant.   Algorithmic Transparency: Does the algorithm confer any guarantees?</p><p>  在这里，不清楚为什么选择这些像素值作为分割点。但是，对于相同数量的节点，最终的决策树的性能要好得多。在这种简单的情况下，表示形式的性能与可解释性之间的权衡非常重要。算法透明度：算法是否提供任何保证？</p><p> This question asks if our learning algorithm has any desirable properties which are easy to understand. For example, we might know that the algorithm only outputs sparse models, or perhaps it always converges to a specific type of solution. In these cases, the resulting learned model can be more amenable to analysis. For example, the  Hard Margin SVM is guaranteed to find a unique solution that maximizes the margin. Similarly, the perceptron is guaranteed to find parameters (not necessarily unique ones, though) that achieve a training loss of 0 if the data are linearly separable.</p><p> 这个问题问我们的学习算法是否具有任何易于理解的理想属性。例如，我们可能知道该算法仅输出稀疏模型，或者可能始终会收敛到特定类型的解决方案。在这些情况下，生成的学习模型可能更易于分析。例如，保证金支持向量机（Shard Margin SVM）可以确保找到一种独特的解决方案，以最大化保证金。同样，如果数据是线性可分离的，则可以保证感知器找到参数（虽然不一定是唯一的），但可以达到0的训练损失。</p><p> When it comes to deep learning, a full overview of results would be beyond the scope of this essay. In general, though, there will be many other configurations of values that will also perform comparably. This means we don&#39;t have a direct analogy to the notion of some unique set of weights that perform well on the task at hand.</p><p> 当涉及深度学习时，对结果的完整概述将超出本文的范围。通常，尽管如此，会有许多其他的值配置也可以比较执行。这意味着我们无法直接将某些独特的权重概念与在当前任务中表现良好的概念进行类比。</p><p> The KNN algorithm discussed earlier illustrates another level of understanding regarding &#34;what the algorithm does in simple terms&#34; beyond mechanical transparency. KNN is easy to describe as &#34;it reports the labels of the points closest to the input&#34;. The part of this property that&#39;s doing the most work here is how we describe. Obviously, most ML models can be described as &#34;it finds parameters which satisfy certain constraints&#34;, but this is a very broad and unactionable statement. It seems harder to find a description at the same level of granularity for neural nets beyond something like &#34;it learns a high-dimensional manifold that maps onto the input data&#34;. Even though we can often get succinct summaries of what an ML model is doing, only some of them are informative from a simulatable perspective.</p><p> 前面讨论的KNN算法说明了机械透明性以外对“简单算法中的功能”的另一种理解。 KNN很容易描述为“它报告最接近输入的点的标签”。该属性在此处完成最多工作的部分是我们的描述方式。显然，大多数ML模型可以描述为“它找到满足某些约束的参数”，但这是一个非常广泛且不可行的陈述。除了像“它学习映射到输入数据的高维流形”之类的东西外，在神经网络的相同粒度级别上查找描述似乎更加困难。即使我们经常可以得到关于ML模型所做的简要总结，但从可模拟的角度来看，其中只有一部分是有益的。</p><p>  Lipton et al. pose four questions on  post-hoc interpretability, which refers to things we can learn from the model after training has finished.</p><p>  Lipton等。对事后解释性提出四个问题，这是指我们在训练结束后可以从模型中学到的东西。</p><p>  Like how humans can provide post-hoc justifications for their actions, it could be informative to have models that can also explain, probably as natural language statements. Naive methods of pairing text with decisions, however, will likely optimize for something like &#34;how credible the explanation sounds to a human&#34; rather than &#34;how accurate the explanation is at summarizing the steps taken by the model.&#34;.</p><p>就像人类如何为自己的行为提供事后证明一样，拥有可以解释的模型（可能是自然语言陈述）也可能很有帮助。然而，将文本与决策配对的幼稚方法可能会针对诸如“解释对人类听起来的可信度”之类的东西进行优化，而不是针对“对模型所采取的步骤进行总结时解释的准确性”。</p><p> While this seems clearly desirable, it&#39;s still an emerging field, and Lipton et al. only offer one paper, which focuses on RL. On  ConnectedPapers, I found that the referenced paper is part of a larger related field of  reinforcement learning with human advice. This seems to focus on the converse problem: given human explanations, how can models incorporate them into their decision-making? Maybe insights here can eventually be used in the other direction.</p><p> 尽管这似乎显然是可取的，但它仍然是一个新兴领域，Lipton等人。只提供一篇针对RL的论文。在ConnectedPapers上，我发现引用的论文是通过人的建议进行的强化学习的一个较大相关领域的一部分。这似乎集中在相反的问题上：给定人类的解释，模型如何将其纳入决策中？也许这里的见解最终可以用于另一个方向。</p><p>   Saliency maps are a broad class of approaches that look at how a change in the input (or parts of the input) changes the output. A straightforward way to do this is to take the derivative of the loss function with respect to the input. Beyond this simplistic approach, many modifications involve averaging the gradient, perturbing the input, or local approximations.   Understanding Deep Networks via Extremal Perturbations and Smooth Masks has a good overview of the work in this area.</p><p>   显着性图是一类广泛的方法，这些方法着眼于输入（或输入的部分）变化如何改变输出。一种简单的方法是采用损失函数相对于输入的导数。除了这种简单的方法外，许多修改还涉及平均梯度，扰动输入或局部近似。通过极端扰动和平滑蒙版了解深层网络可以很好地概述这一领域的工作。</p><p> As an example, I trained a CNN on MNIST and did a simple gradient visualization on an image of this 3:</p><p> 举例来说，我在MNIST上训练了CNN，并对这3张图片进行了简单的渐变可视化：</p><p>  Using PyTorch, I computes the derivative of the logit (that corresponds to the digit “three”) with respect to the input image. This gave me the image below. Here, the white pixels correspond to parts of the image that would increase the logit value for “three”, and the black pixels correspond to the reverse. We can see the rough curves of the digit “three” come through.</p><p>  使用PyTorch，我针对输入图像计算了logit的导数（对应于数字“三”）。这给了我下面的图片。在此，白色像素对应于将增加“三”的对数值的图像部分，黑色像素对应于相反的方向。我们可以看到数字“ 3”的粗略曲线通过。</p><p>  Note how this is different from the visualization we previously had with the linear classifier in red and blue in the first section. Those visualizations represented the importance in  aggregate for the entire input space. The visualization here is meaningful only for this specific input. For a different input, e.g., a different instance of the digit “three,” the local gradient would look different, as shown below:</p><p>  请注意，这与我们先前在第一部分中用红色和蓝色的线性分类器进行可视化处理有何不同。这些可视化代表了整个输入空间的总体重要性。此处的可视化仅对于此特定输入有意义。对于不同的输入（例如，数字“ 3”的不同实例），局部梯度看起来会有所不同，如下所示：</p><p>   Another family of approaches focuses on visualizing with respect to the model parameters themselves, rather than the input. A lot of the work has been done by Chris Olah, Shan Carter, Ludwig Schubert, and others on  distill.pub. Their work in this area has gone from  visualizing the activations of specific neurons and layers to entire  maps of activations for many networks, to  decomposing models into interpretable building blocks. Another excellent visualization resource for this type of work is the  OpenAI Microscope. Progress here has been very exciting, but it remains to be seen if similar approaches can be found for neural nets that focus on tasks other than image recognition.</p><p>   另一类方法侧重于相对于模型参数本身而不是输入的可视化。克里斯·奥拉（Chris Olah），珊·卡特（Shan Carter），路德维希·舒伯特（Ludwig Schubert）等人在distill.pub上完成了许多工作。他们在这一领域的工作已经从可视化特定神经元和层的激活变为许多网络的整个激活图，再到将模型分解为可解释的构造块。此类工作的另一个出色的可视化资源是OpenAI Microscope。此处的进展非常令人兴奋，但是对于能够专注于图像识别以外任务的神经网络，是否可以找到类似的方法还有待观察。</p><p> Explanation by Example: Can the model show what else in the training data it thinks are related to this input/output?</p><p>举例说明：模型能否显示其认为与该输入/输出有关的训练数据中的其他内容？</p><p> This question asks for what other training examples are similar to the current input. When the similarity metric is just distance in the original feature space, this is akin to a KNN model with K = 1. More sophisticated methods may look for examples similar in the representation or latent space the model is using. The human justification for this type of approach is that it is similar to reasoning by analogy, where we present a related scenario to support our actions.</p><p> 该问题询问其他哪些培训示例与当前输入内容相似。当相似性度量只是原始特征空间中的距离时，这类似于K = 1的KNN模型。更复杂的方法可能会寻找模型正在使用的表示空间或潜在空间中的相似示例。这种方法的人为理由是，它类似于类推推理，在此我们提出一个相关场景来支持我们的行动。</p><p> While I think this is useful, it definitely doesn&#39;t seem like all we need for understanding or even most of what we would need.</p><p> 虽然我认为这很有用，但似乎并不需要我们理解所需要的全部，甚至不是我们需要的大部分。</p><p>  For a deeper dive into specific techniques, I recommend  A Survey Of Methods For Explaining Black-Box Models, which covers a wide variety of approaches for many different specific machine learning models as well as model-agnostic algorithms. For neural nets,  Explainable Deep Learning: A Field Guide for the Uninitiated provides an in-depth read. For other conceptual surveys of the field,  Definitions, methods, and applications in interpretable machine learning and  Explainable Machine Learning for Scientific Insights and Discoveries. The Explainable Machine Learning paper, in particular, is quite nice because it gives a hierarchy of increasingly more interpretable models across several domains and use cases.</p><p>  为了更深入地研究特定技术，我建议对黑匣子模型的方法进行调查，该方法涵盖了针对许多不同的特定机器学习模型以及与模型无关的算法的多种方法。对于神经网络，《可解释的深度学习：入门指南》提供了深入的阅读。对于该领域的其他概念性调查，可解释性机器学习和可解释性机器学习中的定义，方法和应用有助于科学洞察和发现。特别是，Explainable Machine Learning论文非常出色，因为它给出了跨多个领域和用例的，越来越可解释的模型的层次结构。</p><p>  Broadly, I think there are two main directions that interpretability research should go, outside of the obvious direction of &#34;find better ways to formalize what we mean by interpretability.&#34; These two areas are evaluation and utility.</p><p>  概括地说，我认为可解释性研究应该走两个主要方向，而“显而易见的方向”是“找到更好的方式来规范我们所指的可解释性”。这两个领域是评估和实用性。</p><p>  The first area is to find better ways of evaluating these numerous interpretability methods. For many of these visualization-based approaches, a default method seems to be sanity-checking with a human-in-the-loop, ensuring that interpretable features are being highlighted. Indeed, that&#39;s what we did for the MNIST examples above. However, we can&#39;t do this at scale; it would be infeasible to add human evaluation for every single model output.</p><p>  第一个领域是找到评估这些众多可解释性方法的更好方法。对于这些基于可视化的方法中的许多方法，默认方法似乎是通过人工检测来进行健全性检查，以确保突出显示可解释的功能。确实，这就是我们为上面的MNIST示例所做的工作。但是，我们不能大规模地这样做。为每个模型输出添加人工评估是不可行的。</p><p> Moreover,  Sanity Checks for Saliency Maps, a recent paper, makes a strong case for why this is not enough. As mentioned earlier, saliency maps represent a broad class of approaches that try to understand what parts of the input are essential for the model&#39;s output, often through the gradient. The outputs of several of these methods are shown below. Upon visual inspection, they might seem reasonable as they all seem to focus on the relevant parts of the image.</p><p> 此外，最近的论文《健全性地图的合理性检查》（Sanity Checks for Saliency Maps）很好地说明了为什么这还不够。如前所述，显着性图表示一类广泛的方法，这些方法通常通过渐变来了解输入的哪些部分对于模型的输出必不可少。这些方法中的几种的输出如下所示。通过目视检查，它们似乎合理，因为它们似乎都集中在图像的相关部分。</p><p>  However, the very last column is the output, not for a saliency map, but for an edge detector applied to the input. This makes it not a function of the model but merely the input. Yet, it can output &#34;saliency maps,&#34; which are visually comparable to these other results. This might cause us to wonder if the other approaches are really telling us something about the model. The authors propose several tests to investigate.</p><p>但是，最后一列是输出，不是用于显着图，而是用于应用于输入的边缘检测器。这使其不是模型的功能，而仅仅是输入。但是，它可以输出“显着性图”，这些显性图在视觉上可与其他结果进行比较。这可能使我们想知道其他方法是否真的在告诉我们有关模型的信息。作者提出了几种测试方法进行调查。</p><p> The first test compares the saliency map of a trained model with a model that has randomly initialized weights. Here, clearly, if the saliency maps look similar, it is more dependent on the input and not the model&#39;s parameters.</p><p> 第一个测试将经过训练的模型的显着性图与具有随机初始化的权重的模型进行比较。在这里，很明显，如果显着图看起来相似，则它更多地取决于输入，而不是模型的参数。</p><p> The second test compares the saliency map of a trained model with a trained model that was given randomly permuted labels. Once again, if the saliency maps look similar, this is also a sign of input dependence because the same &#34;salient&#34; features have been used to justify two different labels.</p><p> 第二项测试将经过训练的模型的显着性图与经过随机排列的标签的经过训练的模型进行比较。再次，如果显着图看起来相似，这也是输入依赖的标志，因为已经使用了相同的“显着”特征来证明两个不同的标签是正确的。</p><p> Overall, the authors find that the basic gradient map shows desired sensitivity to the above tests, whereas specific other approaches like Guided BackProp do not. Before this, attribution research was more subjective. For example, the paper on  Grad-CAM goes in-depth over how their saliency map can help aid in providing explanations or identifying bias for the dataset. But they do not consider the sensitivity of their approach to model parameters.</p><p> 总的来说，作者发现基本梯度图对上述测试显示出所需的灵敏度，而其他特定方法（如Guided BackProp）则没有。在此之前，归因研究更为主观。例如，关于Grad-CAM的论文深入探讨了其显着性图如何帮助提供解释或识别数据集的偏见。但是他们没有考虑其方法对模型参数的敏感性。</p><p> In the above paper on sanity-checks, they find that Grad-CAM is sensitive to changes in the input, which is good, but I definitely would like to see these sanity-checks being applied more frequently. Outside of new approaches, I think additional benchmarks for interpretability that mimic real-world use cases could be of great value to the field.</p><p> 在上述有关健全性检查的论文中，他们发现Grad-CAM对输入的变化敏感，这很好，但是我绝对希望看到这些健全性检查被更频繁地应用。在新方法之外，我认为模仿现实世界用例的其他可解释性基准对于该领域可能具有重要价值。</p><p> Another approach in this direction is to back-chain from the explanations that people use in everyday life to derive better benchmarks.  Explanation in Artificial Intelligence: Insights from the Social Sciences provides an overview of where philosophy and social science can meet ML in the middle. Of course, the final arbiter for all this is how well people can actually use and interpret these interpretability results, which brings me to my second point.</p><p> 朝这个方向发展的另一种方法是，将人们在日常生活中使用的解释推后链，以得出更好的基准。人工智能中的解释：来自社会科学的见解概述了哲学和社会科学可以在中间遇到ML的地方。当然，所有这一切的最终仲裁者是人们如何有效地使用和解释这些可解释性结果，这使我想到了第二点。</p><p>  The second area is to ensure that these interpretability approaches are actually providing value. Even if we find ways of explaining models that are actually sensitive to the learned parameters (and everything else), I think it remains to be seen if these explanations are useful in practice. At least for current techniques, I think the answer is uncertain and possibly even negative.</p><p>  第二个领域是确保这些可解释性方法确实在提供价值。即使我们找到解释实际上对学习的参数（以及其他所有参数）敏感的模型的方法，但我认为这些解释在实践中是否有用还有待观察。至少对于当前的技术，我认为答案是不确定的，甚至可能是负面的。</p><p> Manipulating and Measuring Model Interpretability, a large pre-registered study [2] from Microsoft Research, found that models with additional information like model weights were often not useful in helping users decide how to make more accurate judgments on their own or even notice when the model was wrong. Users were given either a black-box model or a more interpretable one.</p><p>微软研究院进行的一项大型预注册研究[2]操纵和衡量模型的可解释性发现，带有模型权重之类的附加信息的模型通常对帮助用户自行决定如何做出更准确的判断甚至在何时发现决策时无济于事。模型是错误的。为用户提供了黑盒模型或更具解释性的模型。</p><p>  &#34;[o]n typical examples, we saw no significant difference between a transparent model with few features and a black-box model with many features in terms of how closely participants followed the model&#39;s predictions. We also saw that people would have been better off simply following the models rather than adjusting their predictions. Even more surprisingly, we found that transparent models had the unwanted effect of impairing people&#39;s ability to correct inaccurate predictions, seemingly due to people being overwhelmed by the additional information that the transparent model presented&#34;</p><p>  “ [在典型示例中，就参与者对模型的预测的密切程度而言，几乎没有特征的透明模型与具有很多特征的黑匣子模型之间没有显着差异。我们还看到人们的状况会更好仅仅令人惊讶的是，我们发现透明模型具有有害的影响，削弱了人们纠正不正确的预测的能力，这似乎是由于人们对透明模型提供的附加信息不知所措。”</p><p> Another work,  Interpreting Interpretability: Understanding Data Scientists&#39; Use of Interpretability Tools for Machine Learning, found that even data scientists may not understand what interpretable visualizations tell them. This can inspire unwarranted confidence in the underlying model, even leading to an ad-hoc rationalization of suspicious results.</p><p> 另一篇著作《解释可解释性：理解数据科学家对机器学习的解释性工具的使用》发现，即使数据科学家也可能不理解可解释的可视化告诉他们的内容。这会激发人们对基本模型的不必要的信心，甚至导致可疑结果的临时合理化。</p><p> Lastly,  Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior? is a recent study of five interpretability techniques and how they empirically help humans simulate model. The authors found very few benefits from any of the methods. One noteworthy finding is that the explanations which were rated to be of higher quality by participants were not very useful in actually improving human performance.</p><p> 最后，评估可解释的AI：哪些算法解释可帮助用户预测模型行为？是对五种可解释性技术以及它们如何凭经验帮助人类模拟模型的最新研究。作者发现，任何一种方法的好处都很少。一项值得注意的发现是，被参与者评定为高质量的解释对实际改善人类绩效不是很有用。</p><p> All of this points to the difficulties ahead for interpretability research. These approaches and visuals are liable to be misused and misinterpreted. Even once we get improved notions of interpretability with intuitive properties, it remains to be seen if we can use them to achieve the benefits I listed out in the very beginning. While it certainly seems more challenging to formalize interpretability than to use it well, I&#39;m glad that empirical tests are already being done; they can hopefully also guide where the research goes next.</p><p> 所有这些都指出了可解释性研究面临的困难。这些方法和视觉效果容易被误用和误解。即使一旦我们有了直观属性的可解释性的改进概念，是否可以使用它们来实现我一开始就列出的好处，还有待观察。虽然形式化解释性当然要好于使用它，这无疑具有挑战性，但我很高兴已经进行了实证检验。他们希望也可以指导下一步的研究。</p><p> Finally, lurking behind all, this is the question of decreased performance and adoption. It&#39;s obvious these days that </p><p> 最后，潜伏在所有方面的是性能下降和采用率下降的问题。这些天很明显</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/machine/">#machine</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>