<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>NVIDIA推出恩惠：高性能ARM CPU，用于大AI系统 Nvidia Unveils Grace: A High-Performance Arm CPU for Use in Big AI Systems</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Nvidia Unveils Grace: A High-Performance Arm CPU for Use in Big AI Systems<br/>NVIDIA推出恩惠：高性能ARM CPU，用于大AI系统 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-13 01:12:08</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/bd8f6ee86dec2adb40f7db47f6032190.jpg"><img src="http://img2.diglog.com/img/2021/4/bd8f6ee86dec2adb40f7db47f6032190.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Kicking off another busy Spring GPU Technology Conference for NVIDIA, this morning the graphics and accelerator designer is announcing that they are going to once again design their own Arm-based CPU. Dubbed Grace – after Grace Hopper, the computer programming pioneer and US Navy rear admiral – the CPU is NVIDIA’s latest stab at more fully vertically integrating their hardware stack by being able to offer a high-performance CPU alongside their regular GPU wares. According to NVIDIA, the chip is being designed specifically for large-scale neural network workloads, and is expected to become available in NVIDIA products in 2023.</p><p>揭开了NVIDIA的另一个繁忙的春季GPU技术会议，今天早上的图形和加速器设计师正在宣布他们将再次设计自己的基于ARM的CPU。被称为Grace  - 在Grace Hopper之后，计算机编程先锋和美国海军后海军上将 -  CPU是NVIDIA通过能够提供高性能CPU与其常规GPU商品一起提供高性能CPU的最新刺激。根据NVIDIA的说法，该芯片专为大规模神经网络工作负载而设计，预计将在2023年的NVIDIA产品中获得。</p><p> With two years to go until the chip is ready, NVIDIA is playing things relatively coy at this time. The company is offering only limited details for the chip – it will be based on a future iteration of Arm’s Neoverse cores, for example – as today’s announcement is a bit more focused on NVIDIA’s future workflow model than it is speeds and feeds. If nothing else, the company is making it clear early on that, at least for now, Grace is an internal product for NVIDIA, to be offered as part of their larger server offerings. The company isn’t directly gunning for the Intel Xeon or AMD EPYC server market, but instead they are building their own chip to complement their GPU offerings, creating a specialized chip that can directly connect to their GPUs and help handle enormous, trillion parameter AI models.</p><p> 随着两年的时间去筹码准备就绪，NVIDIA目前正在比较伴随着相对的宾馆。该公司仅提供了芯片的有限细节 - 它将基于未来的ARM的新古董核心迭代 - 例如，随着今天的公告有点专注于NVIDIA的未来工作流模型，而不是速度和饲料。如果没有别的，该公司将尽早清楚，至少目前，Grace是NVIDIA的内部产品，作为其较大服务器产品的一部分提供。该公司并不直接为英特尔Xeon或AMD EPYC服务器市场推出，而是建立自己的芯片以补充他们的GPU产品，可以创建一个专业芯片，可以直接连接到他们的GPU并帮助处理巨大的亿万的参数ai楷模。</p><p>  More broadly speaking, Grace is designed to fill the CPU-sized hole in NVIDIA’s AI server offerings. The company’s GPUs are incredibly well-suited for certain classes of deep learning workloads, but not all workloads are purely GPU-bound, if only because a CPU is needed to keep the GPUs fed. NVIDIA’s current server offerings, in turn, typically rely on AMD’s EPYC processors, which are very fast for general compute purposes, but lack the kind of high-speed I/O and deep learning optimizations that NVIDIA is looking for. In particular, NVIDIA is currently bottlenecked by the use of PCI Express for CPU-GPU connectivity; their GPUs can talk quickly amongst themselves via NVLink, but not back to the host CPU or system RAM.</p><p>  更广泛地说，Grace旨在填充NVIDIA的AI服务器产品中的CPU大小孔。该公司的GPU非常适合某些类别的深度学习工作负载，但并非所有工作负载都是纯粹的GPU限制，如果只需要CPU来保持GPU喂养。 NVIDIA的当前服务器产品又依赖于AMD的EPYC处理器，这非常快速地用于一般计算目的，但缺乏NVIDIA正在寻找的高速I / O和深度学习优化。特别是，NVIDIA目前是通过使用PCI Express进行CPU-GPU连接的瓶颈瓶颈;他们的GPU可以通过NVLINK在自己的状态中快速交谈，但不会返回主CPU或系统RAM。</p><p> The solution to the problem, as was the case even before Grace, is to use NVLink for CPU-GPU communications. Previously NVIDIA has worked with the OpenPOWER foundation to get NVLink into POWER9 for exactly this reason, however that relationship is seemingly on its way out, both as POWER’s popularity wanes and POWER10 is skipping NVLink. Instead, NVIDIA is going their own way by building an Arm server CPU with the necessary NVLink functionality.</p><p> 解决问题的解决方案，即使在恩典之前也是如此，是使用NVLink进行CPU-GPU通信。以前的NVIDIA已经与OpenPower Foundation合作，以获得NVLink进入Power9的原因，但是这种关系似乎正在出路，既是电力的普及游程和电力10正在跳过NVLink。相反，NVIDIA通过使用必要的NVLink功能构建ARM服务器CPU来完成自己的方式。</p><p> The end result, according to NVIDIA, will be a high-performance and high-bandwidth CPU that is designed to work in tandem with a future generation of NVIDIA server GPUs. With NVIDIA talking about pairing each NVIDIA GPU with a Grace CPU on a single board – similar to today’s mezzanine cards – not only does CPU performance and system memory scale up with the number of GPUs, but in a roundabout way, Grace will serve as a co-processor of sorts to NVIDIA’s GPUs. This, if nothing else, is a very NVIDIA solution to the problem, not only improving their performance, but giving them a counter should the more traditionally integrated AMD or Intel try some sort of similar CPU+GPU fusion play.</p><p> 根据NVIDIA的说法，最终结果将是一个高性能和高带宽的CPU，旨在与未来生成NVIDIA服务器GPU的串联工作。与nvidia谈论每个nvidia gpu与一个板上的恩典cpu配对 - 类似于今天的夹层卡 - 不仅CPU性能和系统内存与GPU的数量相比，但是以环形交叉路口方式，恩典将作为一个对NVIDIA的GPU的各种处理器。这是，如果没有别的，是一个非常nvidia解决问题的解决方案，不仅提高了他们的性能，还要给他们一个柜台，应该更传统上集成的AMD或英特尔尝试某种类似的CPU + GPU融合播放。</p><p> By 2023 NVIDIA will be up to NVLink 4, which will offer at least 900GB/sec of bandwidth between the CPU and GPU, and over 600GB/sec between Grace CPUs. Critically, this is greater than the memory bandwidth of the CPU, which means that NVIDIA’s GPUs will have a cache coherent link to the CPU that can access the system memory at full bandwidth, and also allowing the entire system to have a single shared memory address space. NVIDIA describes this as balancing the amount of bandwidth available in a system, and they’re not wrong, but there’s more to it. Having an on-package CPU is a major means towards increasing the amount of memory NVIDIA’s GPUs can effective access and use, as memory capacity continues to be the primary constraining factors for large neural networks – you can only efficiently run a network as big as your local memory pool.</p><p> 到2023年，NVIDIA将达到NVLINK 4，其将在CPU和GPU之间提供至少900GB / SEC，而GRACE CPU之间的300GB /秒超过600GB /秒。批判性地，这大于CPU的内存带宽，这意味着NVIDIA的GPU将与CPU的高速缓存相干链路，可以在满带宽下访问系统存储器，并且还允许整个系统具有单个共享存储器地址空间。 NVIDIA描述了这一点是平衡系统中可用的带宽量，而且它们没有错误，但它有更多。拥有一个包装CPU是增加内存数量的MVIDIA的GPU可以有效访问和使用的主要手段，因为内存容量仍然是大型神经网络的主要约束因素 - 您只能有效地将网络与您一样大的网络本地内存池。</p><p>  And this memory-focused strategy is reflected in the memory pool design of Grace, as well. Since NVIDIA is putting the CPU on a shared package with the GPU, they’re going to put the RAM down right next to it. Grace-equipped GPU modules will include a to-be-determined amount of LPDDR5x memory, with NVIDIA targeting at least 500GB/sec of memory bandwidth. Besides being what’s likely to be the highest-bandwidth non-graphics memory option in 2023, NVIDIA is touting the use of LPDDR5x as a gain for energy efficiency, owing to the technology’s mobile-focused roots and very short trace lengths. And, since this is a server part, Grace’s memory will be ECC-enabled, as well.</p><p>  并且这种记忆的策略也反映在恩典的内存池设计中。由于NVIDIA将CPU与GPU的共享包装，因此它们将使RAM放在旁边。恩惠配备的GPU模块将包括待定量的LPDDR5x内存量，NVIDIA定位至少500GB /秒的内存带宽。除了在2023年可能成为最高带宽非图形存储器选项外，NVIDIA还将LPDDR5x推出使用LPDDR5X作为能源效率的增益，由于该技术的重点束缚根和非常短的痕量长度。而且，由于这是服务器部分，因此Grace的内存将成为ECC启用。 </p><p> As for CPU performance, this is actually the part where NVIDIA has said the least. The company will be using a future generation of Arm’s Neoverse CPU cores, where the initial N1 design has already been  turning heads. But other than that, all the company is saying is that the cores should break 300 points on the SPECrate2017_int_base throughput benchmark, which would be comparable to some of AMD’s second-generation 64 core EPYC CPUs. The company also isn’t saying much about how the CPUs are configured or what optimizations are being added specifically for neural network processing. But since Grace is meant to support NVIDIA’s GPUs, I would expect it to be stronger where GPUs in general are weaker.</p><p>至于CPU性能，这实际上是NVIDIA最少的部分。该公司将使用未来一代ARM的新夜CPU核心，其中初始N1设计已经转动头部。但除此之外，所有公司都说的是，核心应该在SpectRate2017_int_Base吞吐量基准上打破300点，这将与AMD的第二代64核心ePYC CPU相媲美。该公司还没有说如何配置CPU或正在专门为神经网络处理添加哪些优化。但由于恩典旨在支持NVIDIA的GPU，我希望它能够更强大，因为GPU一般较弱。</p><p> Otherwise, as mentioned earlier, NVIDIA big vision goal for Grace is significantly cutting down the time required for the largest neural networking models. NVIDIA is gunning for 10x higher performance on 1 trillion parameter models, and their performance projections for a 64 module Grace+A100 system (with theoretical NVLink 4 support) would be to bring down training such a model from a month to three days. Or alternatively, being able to do real-time inference on a 500 billion parameter model on an 8 module system.</p><p> 否则，如前所述，NVIDIA的恩典氛围的目标大大减少了最大神经网络模型所需的时间。 NVIDIA在1万亿参数型号上进行10倍的性能，以及64个模块Grace + A100系统的性能预测（具有理论NVLink 4支持）将是从一个月到三天的培训。或者，能够在8个模块系统上对5000亿参数模型进行实时推断。</p><p> Overall, this is NVIDIA’s second real stab at the data center CPU market – and the first that is likely to succeed. NVIDIA’s  Project Denver, which was originally announced just over a decade ago, never really panned out as NVIDIA expected. The family of custom Arm cores was never good enough, and never made it out of NVIDIA’s mobile SoCs. Grace, in contrast, is a much safer project for NVIDIA; they’re merely licensing Arm cores rather than building their own, and those cores will be in use by numerous other parties, as well. So NVIDIA’s risk is reduced to largely getting the I/O and memory plumbing right, as well as keeping the final design energy efficient.</p><p> 总的来说，这是NVIDIA在数据中心CPU市场的第二个真实刺激 - 第一个可能成功的第一个。 NVIDIA的项目丹佛最初在十年前宣布，从未真正被淘汰，因为NVIDIA预期。自定义武器核心的家庭从来没有足够的好处，从来没有用NVIDIA的移动SOC。相比之下，Grace是NVIDIA的一个更安全的项目;他们只是许可手臂核心而不是自己建造自己的核心，而那些核心也将被许多其他各方使用。因此，NVIDIA的风险降低到很大程度上让I / O和内存管道右侧，并保持最终设计能源效率。</p><p> If all goes according to plan, expect to see Grace in 2023. NVIDIA is already confirming that Grace modules will be available for use in HGX carrier boards, and by extension DGX and all the other systems that use those boards. So while we haven’t seen the full extent of NVIDIA’s Grace plans, it’s clear that they are planning to make it a core part of future server offerings.</p><p> 如果所有人都根据计划进行，期待在2023年看到恩典.Nvidia已经证实，恩典模块可用于HGX载波，并通过扩展DGX和所有使用这些板的其他系统。因此，虽然我们没有看到NVIDIA的全部恩典计划，但很明显他们计划使其成为未来服务器产品的核心部分。</p><p>  And even though Grace isn’t shipping until 2023, NVIDIA has already lined up their first customers for the hardware – and they’re supercomputer customers, no less. Both the Swiss National Supercomputing Centre (CSCS) and Los Alamos National Laboratory are announcing today that they’ll be ordering supercomputers based on Grace. Both systems will be built by HPE’s Cray group, and are set to come online in 2023.</p><p>  即使恩典不是发货，直到2023年，NVIDIA已经排队了他们的第一个客户的硬件 - 它们是超级计算机客户，不少。瑞士国家超级计算中心（CSCS）和洛斯阿拉莫斯国家实验室都宣布他们今天将根据恩典订购超级计算机。两个系统都将由HPE的CRAY组构建，并在2023年设置上网。</p><p> CSCS’s system, dubbed Alps, will be replacing their current Piz Daint system, a Xeon plus NVIDIA P100 cluster. According to the two companies, Alps will offer 20 ExaFLOPS of AI performance, which is presumably a combination of CPU, CUDA core, and tensor core throughput. When it’s launched, Alps should be the fastest AI-focused supercomputer in the world.</p><p> CSCS的系统称为阿尔卑斯山，将更换当前的Piz Daint系统，Xeon Plus Nvidia P100集群。根据两家公司，阿尔卑斯al将提供20个AI性能的EXAFLOPS，这可能是CPU，CUDA核心和张量核心吞吐量的组合。当它推出时，阿尔卑斯al应该是世界上最快的互联的超级计算机。</p><p>  Interestingly, however, CSCS’s ambitions for the system go beyond just machine learning workloads. The institute says that they’ll be using Alps as a general purpose system, working on more traditional HPC-type tasks as well as AI-focused tasks. This includes CSCS’s traditional research into weather and the climate, which the pre-AI Piz Daint is already used for as well.</p><p>  然而，有趣的是，CSCS对系统的抱负超出了机器学习工作负载。该研究所表示，他们将使用ALPS作为通用系统，致力于更传统的HPC型任务以及以AI为中心的任务。这包括CSCS对天气和气候的传统研究，也可以使用预先使用的天气和气候。 </p><p> As previously mentioned, Alps will be built by HPE, who will be basing on their previously-announced Cray EX architecture. This would make NVIDIA’s Grace the second CPU option for Cray EX, along with AMD’s EPYC processors.</p><p>如前所述，阿尔卑斯al将由HPE构建，他将在其先前宣布的CRAY EX架构上基于其基础。这将使NVIDIA的恩典成为CRAY EX的第二个CPU选项以及AMD的EPYC处理器。</p><p> Meanwhile Los Alamos’ system is being developed as part of an ongoing collaboration between the lab and NVIDIA, with LANL set to be the first US-based customer to receive a Grace system. LANL is not discussing the expected performance of their system beyond the fact that it’s expected to be “leadership-class,” though the lab is planning on using it for 3D simulations, taking advantage of the largest data set sizes afforded by Grace. The LANL system is set to be delivered in early 2023.</p><p> 与此同时，LOS Alamos的系统正在开发作为实验室和NVIDIA之间正在进行的协作的一部分，其中LANL设置为将获得宽限系统的第一个基于美国的客户。 LANL并没有讨论其系统的预期绩效，超出了它预计它是“领导类别”的事实，尽管实验室正在计划用于3D仿真，但利用宽限性提供的最大数据集规模。 LANL系统设置为在2023年初交付。</p><p>  POST A COMMENT      I think everyone saw this coming - Nvidia&#39;s been hiring a bunch of server-focused design engineers for a while, and the apparent breakup with IBM (Power10 has no NVlink) pointed pretty strongly to an Nvidia server processor.  So what&#39;s the core going to be? N2, or a future Neoverse V-series? If it&#39;s going to be in systems in early 2023, it&#39;s gotta be pretty far in development already.  Reply</p><p>  发表评论我认为每个人都看到这个即将到来的 -  nvidia＆＃39;曾一直雇用一堆以服务器为中心的设计工程师，并且与IBM（Power10没有NVLINK）的明显分手相当强烈地指向NVIDIA服务器处理器。那么核心将是什么＆＃39; n2，还是未来的新v系列？如果它＆＃39;在2023年初将在系统中，它已经很远在发展中。回复 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.anandtech.com/show/16610/nvidia-unveils-grace-a-highperformance-arm-server-cpu-for-use-in-ai-systems">https://www.anandtech.com/show/16610/nvidia-unveils-grace-a-highperformance-arm-server-cpu-for-use-in-ai-systems</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/nvidia/">#nvidia</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/性能/">#性能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/推出/">#推出</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/unveils/">#unveils</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>