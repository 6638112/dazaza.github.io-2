<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习介绍 Introduction to Machine Learning for Finance</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Introduction to Machine Learning for Finance<br/>机器学习介绍 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-20 08:43:50</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/5f62c840c83ca304689bb58cce50b18b.jpeg"><img src="http://img2.diglog.com/img/2021/4/5f62c840c83ca304689bb58cce50b18b.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>The purpose of this article and further ones in the “ML 101” series is to discuss the fundamental concepts of Machine Learning. I want to ensure that all the concepts I might use in the future are clearly defined and explained. One of the most significant issues with the adoption of Machine Learning into the field of finance is the concept of a “black box.” There’s a lot of ambiguity about what happens behind the scenes in many of these models, so I am hoping that, in taking a deep-dive into the theory, we can help dispel some worries we might have about trusting these models.</p><p>本文的目的是“ML 101”系列中的进一步的目的是讨论机器学习的基本概念。我希望确保我在未来可能使用的所有概念都明确和解释。通过机器学习到金融领域的最重要问题之一是“黑匣子”的概念。这些模型中许多景象背后发生了很多歧义，所以我希望这一点，在深入进入理论中，我们可以帮助消除一些我们可能拥有关于信任这些模型的担忧。</p><p>  “[Machine Learning is a] field of study that gives computers the ability to learn without being explicitly programmed.” (Arthur Samuel, 1959)</p><p>  “[机器学习是a]研究领域，使计算机能够在没有明确编程的情况下学习。” （Arthur Samuel，1959）</p><p> Arthur Samuel, considered to be the originator of the term “Machine Learning” defined it with the above quote. This definition still stands today but has been developed with time.</p><p> Arthur Samuel认为是“机器学习”一词的发起人，以上述报价定义了它。这个定义仍然存在今天，但已经随着时间的推移而发展。</p><p> “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” (Tom Mitchell, 1998)</p><p> “据说一个计算机程序在某些任务T和一些性能测量P中学习了经验E，如果它在T的性能下，通过P测量而改善了经验E.” （汤姆米切尔，1998）</p><p> Tom Mitchell, a computer science professor at Carnegie Mellon University defined a well-structured machine learning problem in 1998 with the above description.</p><p> Carnegie Mellon大学的计算机科学教授Tom Mitchell在1998年定义了一个结构良好的机器学习问题，以上描述。</p><p> Let’s break both of these quotes down a little further. The key element in both of these definitions is the word “learn” and that’s exactly what the foundation of machine learning is — we want machines to learn without us specifically telling them what should be learned.</p><p> 让我们进一步打破这两个报价。这两个定义中的关键元素是“学习”一词，这正是机器学习的基础 - 我们希望没有我们的机器，没有我们特别讲述应该学习什么。</p><p> Typically, we might want our algorithm to learn how to get from one point to another. That initial point could be a dataset of factors that describe a situation, such as body measurements of a person for example — height, weight, BMI, blood pressure levels etc, and our final point that we want our algorithm to reach could be the life expectancy of that particular person.</p><p> 通常，我们可能希望我们的算法学习如何从一个点到另一个点。该初始点可以是描述一种情况的因素数据集，例如人体测量的人的身体测量 - 高度，重量，BMI，血压水平等，以及我们希望我们算法达到的最终点可能是生命特定人的期望。 </p><p> Ultimately, what we want to do is to provide the start point and the endpoint of our journey and tell the algorithm to figure out which path to take — that’s what we classify as learning.</p><p>最终，我们想要做的是提供我们旅程的起点和终点，并讲述算法弄清楚要采取的路径 - 这就是我们作为学习的分类。</p><p> This is typical to what we call a “supervised learning” problem — unsupervised learning is the alternative, which does not in fact declare a specific endpoint, but instead tells the algorithm to locate similarities between all of the data points.</p><p> 这是典型的，我们所谓的“监督学习”问题 - 无监督的学习是替代方案，其实际上并不声明特定的端点，而是告诉算法在所有数据点之间定位相似性。</p><p> The commonality however, is that we do still start without initial dataset and then provide an indication to our algorithm of what we want the output to look like, with varying levels of specification.</p><p> 然而，普通性是，我们仍然在没有初始数据集的情况下开始，然后为我们希望输出看起来像的算法，规格的不同程度的算法提供指示。</p><p> In the second definition, we can consider our experience E to be our initial input dataset, the task T to be the instruction that this dataset must be trained to match our output dataset and our performance P as being some form of accuracy metric that essentially informs us of how well a model is performing.</p><p> 在第二个清晰度中，我们可以考虑我们的经验E才能成为我们的初始输入数据集，任务T是该数据集必须培训的指令，以与我们的输出数据集和我们的性能P匹配为基本上通知的某种形式的准确度指标。我们的模型表现如何。</p><p>  So how does all this learning happen? Well, from a high-level view, it happens through the concept of functions. For those that may not be completely familiar, a function is a relation from a set of inputs to a set of possible outputs[1]. If we look at what our definition of machine learning is, the role of the function is exactly what we want our machine to learn, or estimate. We want it to know how to get from our inputs to our outputs, essentially find out what the relation is.</p><p>  那么所有这些学习如何发生这种情况？嗯，从高级视图中，它发生在功能的概念。对于那些可能不完全熟悉的人，函数是从一组可能的输出到一组可能输出的输入关系[1]。如果我们看看我们对机器学习的定义是什么，功能的作用正是我们希望我们的机器学习或估算。我们希望它知道如何从我们的投入到我们的产出中获取，从而基本上找出了该关系的内容。</p><p> The goal of any machine learning algorithm therefore, is to find the best possible approximation for our function that takes our input variables to as close to our output variables as possible.</p><p> 因此，任何机器学习算法的目标是为我们的函数找到最佳可能近似，它将输入变量尽可能接近我们的输出变量。</p><p> The word generalisation is how we apply this to future scenarios. Essentially, what we have built is a relationship between our inputs and outputs. But what if we now give our relationship a new set of inputs? Will it still be able to transform these unseen inputs into the correct set of outputs? Generalisation is the ability to make correct predictions on unseen data.</p><p> Word泛化是我们如何将其应用于未来的情景。从本质上讲，我们构建的是我们的投入和输出之间的关系。但如果我们现在给我们的关系是什么新的投入？它仍然能够将这些看不见的输入转换为正确的输出集吗？概括是对看不见的数据进行正确预测的能力。 </p><p> Ultimately, that is what we want our machine learning algorithms to do. We want them to understand relationships in the data that stand the test of data they haven’t seen before.</p><p>最终，这就是我们希望我们的机器学习算法做的事情。我们希望他们了解数据中的数据中的关系，这些数据都在以前没有看到的数据测试。</p><p>     We’ve plotted 4 known data points on a graph and we expect them to have some form of underlying relationship that we are looking to estimate. We’ll call the underlying function  f(x) and we have produced 3 basic estimation models, that we call  h(x).</p><p>     我们在图表上绘制了4个已知的数据点，我们希望他们有一些我们正在寻求估计的潜在关系。我们将呼叫底层函数f（x），我们制作了3个基本估计模型，我们调用h（x）。</p><p> The first estimation method is just to draw a line connecting each datapoint. This provides us with a perfect fit for our seen data because we have estimated the exact  f(x) value at every  x point. This model however has no generalisable properties and if we add an unseen datapoint (the point in red), our model will have no way of estimating this since there is no procedure in place for this  x value.</p><p> 第一个估计方法只是为了绘制连接每个数据点的线。这为我们提供了完美的适合我们所看到的数据，因为我们估计了每个X点的精确f（x）值。但是，此模型没有任何恒定的属性，如果我们添加了一个不间断的数据点（红色的点），我们的模型将无法估计这一点，因为此X值没有程序。</p><p> Alternatively, we could use a 3rd order polynomial, which appears to be another good fit for the seen data points. It gathers the general shape of the points and depending on where the unseen datapoint is plotted, this model has the ability to generalise.</p><p> 或者，我们可以使用第三阶多项式，这似乎是另一种良好的适合于所看到的数据点。它收集了点的一般形状，具体取决于绘制了看不见的数据点的位置，该模型具有泛化的能力。</p><p> A 1st order polynomial in the above case is a straight line. We can see that it doesn’t provide a perfect fit due to the shape of the input data, but again, depending on our  x and  f(x) relationship, this estimation has the ability to generalise to our unseen data.</p><p> 上述情况下的第一阶多项式是直线。我们可以看到它没有由于输入数据的形状而提供完美的拟合，但是，根据我们的x和f（x）的关系，该估计能够概括到我们的未见数据。</p><p> This concept is one of the most important to keep in mind about machine learning. We are not looking for the perfect model, because we don’t expect this perfect model to generalise. We’re looking for the best estimation to the data we know in the hope it will also perform well on the data we haven’t exposed the model to yet. In addition to this, the final 2 graphs informs us that the model we choose is always unique to the problem that we are aiming to solve, there is no “one size fits all.”</p><p> 这一概念是牢记机器学习最重要的概念之一。我们不是在寻找完美的模型，因为我们不指望这种完美的模型概括。我们正在寻找对我们所知道的数据的最佳估计，希望它还在我们没有暴露模型的数据上表现良好。除此之外，最终的2个图表告诉我们，我们选择的模型总是是我们旨在解决的问题的独特，没有“一定尺寸适合所有人”。</p><p> This brings us to the next conversation — machine learning vs pattern recognition. These are often terms that get grouped together and used interchangeably, but they do actually have unique definitions.</p><p> 这为我们带来了下一个对话 - 机器学习与模式识别。这些通常是将其分组并可互换使用的条款，但它们实际上具有独特的定义。 </p><p>  Machine Learning is learning from experience, the experience being a set of input data that we provide for our algorithm. In machine learning, our experience, task and performance are clearly defined. Machine Learning is also called supervised learning, because we provide both an input and a target output in our data.</p><p>机器学习正在从经验中学习，经验是我们提供我们算法的一组输入数据。在机器学习中，我们的经验，任务和性能明确定义。机器学习也被称为监督学习，因为我们提供了数据中的输入和目标输出。</p><p> Pattern Recognition is the process of finding patterns in our data. It is also called unsupervised learning because, instead of telling the model specifically what the output should be, we ask it to find patterns and relationships within the data. Our experience E will be only feature variables with no target output and our task and performance are more broadly defined because we don’t have a specific path we want our algorithm to follow.</p><p> 模式识别是在我们的数据中查找模式的过程。它也被称为无监督的学习，因为，而不是讲述该模型应该是什么，我们要求它找到数据内的模式和关系。我们的经验E将只是没有目标输出的功能变量，并且我们的任务和性能更广泛地定义，因为我们没有特定的路径我们希望遵循算法。</p><p>  Data consisting of 1 feature — age and 1 label — height → univariate supervised learning</p><p>  由1个功能和1个标签组成的数据 - 高度→单变量监督学习</p><p> Data consisting of 3 features — age, height, gender and 1 label — life expectancy → multivariable supervised learning</p><p> 由3种特征组成的数据 - 年龄，高度，性别和1个标签 - 预期寿命→多变量监督学习</p><p> Data consisting of 1 feature — a series of prices of the Dow Jones Index, with the aim of classifying the prices into similar groupings → univariate unsupervised learning</p><p> 由1个功能组成的数据 - 道琼斯指数的一系列价格，目的是将价格分类为类似的分组→单变量无人监督的学习</p><p> Data consisting of 3 features — attributes of a song, tempo, length and key, with the aim of classifying them into groups of similar songs that can hopefully be separable by genre → multivariate unsupervised learning</p><p> 由3个特征组成的数据 - 歌曲，节奏，长度和钥匙的属性，目的是将它们分类为类似的歌曲组，这些歌曲可以有希望被流派可分变→多变量无人监督的学习</p><p>   Classification is a machine learning task where we have a discrete set of outcomes. Classification is often binary, meaning that there are two possible options for the outcome, normally yes/no or 1/0. Examples of classification include face detection, smile detection, spam classification.</p><p>   分类是一种机器学习任务，我们有一个离散的一组结果。分类通常是二进制的，这意味着结果有两种可能的选项，通常是/否或1/0。分类的例子包括面部检测，微笑检测，垃圾邮件分类。 </p><p> Regression is a machine learning task where we have a real-valued outcome in a continuous sub-space. Basically, this means that our outcome is a continuous variable, but that it is usually confined to some sort of range. Examples of regression include age estimation, stock value prediction, temperature prediction.</p><p>回归是一项机器学习任务，我们在连续的子空间中具有真实值的结果。基本上，这意味着我们的结果是一个连续变量，但它通常被限制在某种范围内。回归的例子包括年龄估计，库存值预测，温度预测。</p><p>   In the diagram above, the  N is the set of all natural numbers. These are all integers greater than 0. The set of natural numbers would look something like  {1,2,3,4,…}. The  R is the set of all real numbers. This includes all natural numbers, integers, rational and irrational numbers. The only values that the set of real numbers doesn’t include are imaginary numbers, such as  i, used to represent the square root of -1, but we don’t encounter these in machine learning so it isn’t something we have to worry about. The important point to note is that classification takes any natural number value, whilst regression takes any real numbered value.</p><p>   在上图中，n是所有自然数的集合。这些都是大于0的整数。自然数字集看起来像{1,2,3,4，...}。 R是所有实数的集合。这包括所有自然数，整数，理性和非理性数。该组的实数不包括的唯一值是假想的数字，例如i，用于表示-1的平方根，但我们在机器学习中不遇到这些，所以它不是我们必须的担心。值得注意的是，分类采用任何自然数值，而回归采用任何真实编号值。</p><p>   Multiple data points make up a dataset. Data points are also called instances. Our machine learning algorithm will use a dataset to create its hypothesis function (estimation function) or find a pattern.</p><p>   多个数据点构成数据集。数据点也称为实例。我们的机器学习算法将使用数据集创建其假设函数（估计函数）或找到模式。</p><p> In supervised learning, a data point is made up of a series of inputs, denoted by  X, which we call features, and an output, denoted by  y, which we call labels. We can group them as tuples in the notation  {X,y}.</p><p> 在监督学习中，数据点由一系列输入组成，由x表示，我们调用特征和输出，由y表示，我们调用标签。我们可以将它们分组为符号{x，y}中的元组。</p><p> Labels are what our hypothesis function  h(x) tries to predict. Features are what our hypothesis function  h(x) will use to predict our label.</p><p> 标签是我们的假设函数h（x）试图预测的内容。功能是我们的假设函数h（x）将用于预测我们的标签。</p><p> It is assumed that there is some implicit relationship between x and y, otherwise we will be attempting to predict an output based on a variable that doesn’t actually provide any relevant information. This will likely create an inaccurate and fairly useless algorithm.</p><p> 假设x和y之间存在一些隐式关系，否则我们将尝试基于实际提供任何相关信息的变量来预测输出。这可能会产生不准确和相当无用的算法。</p><p> It’s important to note that for a given problem, all data points must have the same fixed length set of features. We can’t pass in different lengths because our algorithm won’t be able to generalise.</p><p> 值得注意的是，对于给定的问题，所有数据点必须具有相同的固定长度集。我们不能通过不同的长度，因为我们的算法无法概括。 </p><p>    Let’s say that  x, our features, are a series of weights of people, measured in kg with a range of 50kg — 120kg. We’ll say that y is the corresponding heights of those people, measured in cm and ranging from 150cm — 200cm. We want to find the best way to use the  x value to predict the  y. We can start by plotting these points, they might look a little bit like the following diagram. If we decide to use a linear regression to estimate our data, we can plot the straight line estimation on our graph. From graph theory, we know that this line will take the form  y = mx + c, where  m is the gradient of the line and  c is the y-intercept. In ML theory, we convert this into an estimation function:</p><p>让我们说X，我们的特征是一系列的人，以kg测量，范围为50kg  -  120kg。我们将说Y是那些人的相应高度，以厘米为单位测量，从150厘米 -  200cm的范围内。我们希望找到使用x值来预测y的最佳方法。我们可以通过绘制这些点来开始，它们可能看起来像下图一样。如果我们决定使用线性回归来估计我们的数据，我们可以在图表上绘制直线估计。从图形理论来看，我们知道这条线将采用y = mx + c，其中m是行的梯度，c是y-erlcept。在ML理论中，我们将其转换为估计函数：</p><p>  In univariate linear regression, that is regression with a single  x variable, our solution space is all possible values of theta_0 and theta_1.</p><p>  在单变量线性回归中，与单个x变量的回归，我们的解决方案空间是THETA_0和THETA_1的所有可能值。</p><p> So the  h(x) that is produced by our model is our training algorithm. We are currently in the stage of training our algorithm and we haven’t provided any unseen data yet to see how well it generalises.</p><p> 所以我们模型产生的H（x）是我们的训练算法。我们目前处于培训算法的阶段，我们还没有提供任何看不见的数据，以了解它的概括。</p><p> We have stated a training algorithm, in the example above, to be the algorithm  h(x). The theta parameters are essentially what our algorithm tries to learn. By looking through all our feature and label combinations, it tries to find the best values for theta. The “best” value will be calculated based on whatever our performance metric is, so our algorithm will pick a set of theta values that provide the optimal performance value.</p><p> 在上面的示例中，我们已经说明了一种训练算法，是算法H（x）。 Theta参数本质上是我们的算法尝试学习的内容。通过查看所有功能和标签组合，它试图找到最佳值。 “最佳”值将根据我们的性能度量是什么，因此我们的算法将选择一组提供最佳性能值的θ值。</p><p> Note that the above example is an extremely simplified version. It is highly likely that our dataset will contain many more than one feature and we may have hundreds or thousands of theta parameters.</p><p> 请注意，上面的示例是一个非常简化的版本。我们的数据集很可能包含多个特征，我们可能有数百或数千个参数。</p><p>   We have intrinsic parameters, which are parameters unique to each model we build. These are the parameters learned by the model such as coefficients in a linear regression like in the example above or weights in an artificial neural network. These are essentially the components of our hypothesis function.</p><p>   我们有内在的参数，这是我们构建的每个模型都是唯一的参数。这些是由模型学习的参数，例如在上面的示例中的线性回归中的系数或人工神经网络中的重量。这些基本上是我们假设函数的组成部分。</p><p> We also have hyper parameters, which can be adjusted by the user and are chosen based on the best performance that they provide. These are far smaller in number than intrinsic parameters. Examples of these are the number of nodes in an artificial neural network, degree of a polynomial for multiple linear regression or the number of features we provide per instance.</p><p> 我们还具有超参数，可以由用户调整，并根据它们提供的最佳性能选择。这些数量远小于内在参数。这些示例是人工神经网络中的节点的数量，多项式的多项式的多项式的多项式或我们实例提供的功能数量。 </p><p>  Future articles in the ML 101 series will cover other important topics, including Regressions, Neural Networks, Decision Trees and Clustering to name a few. Keep an eye out for those and the articles where we merge these concepts with financial data to build our prediction engines.</p><p>ML 101系列中的未来文章将涵盖其他重要主题，包括回归，神经网络，决策树和聚类来命名几个。 留意那些和文章，我们将这些概念与财务数据合并以建立我们的预测发动机。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://algofin.substack.com/p/ml-101-an-introduction">https://algofin.substack.com/p/ml-101-an-introduction</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/machine/">#machine</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>