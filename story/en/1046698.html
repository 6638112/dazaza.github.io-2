<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>在单个ThreadRipper工作站上实现11M IOPS和66 GB / S IO Achieving 11M IOPS and 66 GB/S IO on a Single ThreadRipper Workstation</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Achieving 11M IOPS and 66 GB/S IO on a Single ThreadRipper Workstation<br/>在单个ThreadRipper工作站上实现11M IOPS和66 GB / S IO </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-29 22:34:37</div><div class="page_narrow text-break page_content"><p>TL;DR Modern disks are so fast that system performance bottleneck shifts to RAM access and CPU. With up to 64 cores, PCIe 4.0 and 8 memory channels, even a single-socket AMD ThreadRipper Pro workstation makes a hell of a powerful machine - if you do it right!</p><p>TL; DR现代磁盘是如此之快，以至于系统性能瓶颈转移到了RAM访问和CPU上。拥有多达64个内核，PCIe 4.0和8个内存通道，即使是单插槽的AMD ThreadRipper Pro工作站也能使一台功能强大的机器陷入困境-如果您操作正确的话！</p><p>  In this post I’ll explain how I configured my AMD ThreadRipper Pro workstation with 10 PCIe 4.0 SSDs to achieve  11M IOPS with 4kB random reads and  66 GiB/s throughput with larger IOs - and what bottlenecks &amp; issues I fixed to get there. We’ll look into Linux block I/O internals and their interaction with modern hardware. We’ll use tools &amp; techniques, old and new, for measuring bottlenecks - and other adventures in the kernel I/O stack.</p><p>  在这篇文章中，我将说明如何配置具有10个PCIe 4.0 SSD的AMD ThreadRipper Pro工作站，以实现1100万IOPS，4kB随机读取和66 Gb / s吞吐率以及更大的IO，以及哪些瓶颈和问题。我解决的问题。我们将研究Linux块I / O内部及其与现代硬件的交互。我们将使用工具＆amp;衡量瓶颈的新旧技术，以及内核I / O堆栈中的其他功能。</p><p>  $  dstat -pcmrd---procs--- ----total-usage---- ------memory-usage-----  --io/total- -dsk/total-run blk new|usr sys idl wai stl| used free buf cach| read writ| read writ 32 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 33 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 33 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 32 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 32 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.0M 0 |  42G 0 32 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.0M 0 |  42G 0 32 0 0| 28 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 32 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 32 0 0| 27 72 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0 32 0 0| 27 73 0 0 0|2232M 249G 61M 568M| 11.1M 0 |  42G 0</p><p>  $ dstat -pcmrd --- procs --- ---- total-usage ---- ------ memory-usage ----- --io / total- -dsk / total-run blk new | usr sys idl wai stl |用过的免费buf cach |阅读命令|读命令32 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 33 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 33 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 32 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 32 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.0M 0 | 42G 0 32 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.0M 0 | 42G 0 32 0 0 | 28 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 32 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 32 0 0 | 27 72 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0 32 0 0 | 27 73 0 0 0 | 2232M 249G 61M 568M | 11.1M 0 | 42G 0</p><p> With 4kB blocksize random reads, I get over 11 million IOPS, at 42 GiB/ (~45 GB/s) throughput. I will go through technical details and larger blocksizes below, but I’ll start by answering the  “why” question first.</p><p> 使用4kB块大小的随机读取，我获得了超过1100万次IOPS，吞吐量为42 GiB /（〜45 GB / s）。我将在下面详细介绍技术细节和较大的块，但首先要回答“为什么”问题。</p><p>    Why would you even need such IO throughput in a single machine? Shouldn’t I be building a 50-node cluster in the cloud “for scalability”? This is exactly the point of my experiment - do you really want to have all the complexity of clusters or performance implications of remote storage  if you can run your I/O heavy workload on just one server with local NVMe storage? How many databases out there need to sustain even “only” 1M disk IOPS? Or if you really  do need that sweet  1 TB/s data scanning speed, you could do this with 10-20 well-configured cluster nodes instead of 200. Modern hardware is powerful, if used right!</p><p>    为什么甚至在一台计算机上都需要这样的IO吞吐量？我是否应该“为了可扩展性”在云中构建50个节点的集群？这正是我实验的重点-如果您可以仅在具有本地NVMe存储的一台服务器上运行I / O繁重的工作，您是否真的想拥有集群的所有复杂性或远程存储的性能影响？多少个数据库甚至需要维持“仅” 1M磁盘IOPS？或者，如果您确实确实需要1 TB / s的数据扫描速度，则可以使用10-20个配置良好的群集节点（而不是200个）来完成此任务。如果使用得当，现代硬件功能强大！</p><p> I’m well aware of various enterprise data management functional requirements, such as availability, remote replication, data sharing or just huge data volumes that may direct you away from using local NVMe SSDs as your primary storage and I am not arguing against that. Some of these requirements can be addressed in software, some not. The scope of this article is to show the  raw performance even cheap commodity hardware gives you nowadays, so if your company is paying 10x more for 100x lower throughput, it’s good to be aware of other options.</p><p> 我非常了解各种企业数据管理功能需求，例如可用性，远程复制，数据共享或庞大的数据量，它们可能会导致您不使用本地NVMe SSD作为主要存储，因此我对此并不反对。其中一些要求可以通过软件解决，而某些则不能。本文的目的是显示当今廉价的日用品硬件所能提供的原始性能，因此，如果您的公司为提高100倍的吞吐量而支付10倍的费用，那么最好知道其他选择。</p><p> My future plans for this article series include running I/O heavy performance tests with various database engines, other ideas are welcome too!</p><p> 我对本系列文章的未来计划包括使用各种数据库引擎运行I / O重性能测试，也欢迎其他想法！ </p><p>     Max number of CPU cores is 64 (128 threads), but I bought a 16-core version because I’m cheap</p><p>CPU核心的最大数量为64（128个线程），但我购买了16核版本，因为我的价格便宜</p><p> All 16 cores can run at 3.9 GHz sustained base frequency, with 4.3 GHz max “boost” frequency</p><p> 所有16个内核都可以3.9 GHz持续基频运行，最大“升压”频率为4.3 GHz</p><p> So, the excellent throughput of this workstation does not come only from CPU processing speeds, but from the additional bandwidth 8 memory channels and 128 PCIe 4.0 lanes offer! A  single PCIe 4.0 lane gives you about 1.969 GB/s bandwidth in each direction (PCIe is switched, point-to-point full duplex). In theory, 128 lanes should mean that the CPU can handle ~250 GB/s (2 terabit/s!) PCIe traffic in each direction, if all lanes were fully used.</p><p> 因此，该工作站的出色吞吐量不仅取决于CPU的处理速度，还取决于8个存储通道和128个PCIe 4.0通道所提供的额外带宽！单个PCIe 4.0通道在每个方向上为您提供大约1.969 GB / s的带宽（PCIe是交换式的，点对点全双工）。从理论上讲，如果所有通道都被充分使用，则128条通道应该意味着CPU可以在每个方向上处理〜250 GB / s（2 TB / s！）PCIe流量。</p><p> Note that on modern CPUs, most PCIe lanes are connected directly to the CPU (as are memory channels) and do not go through some external “southbridge” controller or “front-side bus”.</p><p> 请注意，在现代CPU上，大多数PCIe通道都直接连接到CPU（与内存通道一样），并且不通过某些外部“南桥”控制器或“前端总线”。</p><p> This also means that a single PCIe 4.0 x4 card can achieve close to 8 GB/s data transfer speed, assuming that the devices can handle it and you do not hit other bottlenecks first. This leads us to the next section, suitable SSDs.</p><p> 这也意味着单个PCIe 4.0 x4卡可以达到接近8 GB / s的数据传输速度，前提是设备可以处理它并且您不会首先遇到其他瓶颈。这将引导我们进入下一部分，合适的SSD。</p><p>   The Samsung 980 Pro is a true PCIe 4.0 SSD, with specs claiming 7000 MB/s read and 5000 MB/s write throughput (with caveats). Its internal controller is capable of true PCIe 4.0 transfer speeds and is not an old PCIe 3.0 chip that just presents it as a PCIe 4.0 compatible one with low GT/s. I understand that some other “PCIe4” SSDs out there will max out at the PCIe3 speeds (~3.5 GB/s) as they don’t have new generation controllers in them.</p><p>   三星980 Pro是一款真正的PCIe 4.0 SSD，其规格声称读取速度为7000 MB / s，写入吞吐量为5000 MB / s（带有警告）。它的内部控制器能够实现真正的PCIe 4.0传输速度，而不是旧的PCIe 3.0芯片，而只是将其表示为具有低GT / s的PCIe 4.0兼容芯片。我知道其他一些“ PCIe4” SSD将以PCIe3速度（〜3.5 GB / s）达到最大速度，因为它们中没有新一代的控制器。</p><p> I ended up buying 8 x 1 TB SSDs, eventually used for data volumes and 2 x 500 GB ones for boot disks and software. I also have a 380 GB Intel Optane 905P SSD for low latency writes (like transaction logs), but more about that in a future post.</p><p> 我最终购买了8 x 1 TB SSD，最终用于数据量，而2 x 500 GB用来引导磁盘和软件。我也有一个380 GB的Intel Optane 905P SSD，用于低延迟写入（例如事务日志），但在以后的文章中会详细介绍。 </p><p>   $ sudo nvme listNode SN Model ---------------- -------------------- ---------------------------/dev/nvme0n1 S5P2NG0N902798J Samsung SSD 980 PRO 1TB /dev/nvme1n1 S5P2NG0NA02399T Samsung SSD 980 PRO 1TB /dev/nvme2n1 S5P2NG0NA04362H Samsung SSD 980 PRO 1TB /dev/nvme3n1 S5P2NG0N902802P Samsung SSD 980 PRO 1TB /dev/nvme4n1 S5P2NG0NA00551P Samsung SSD 980 PRO 1TB /dev/nvme5n1 S5P2NG0NA03266N Samsung SSD 980 PRO 1TB /dev/nvme6n1 S5P2NG0NA01498X Samsung SSD 980 PRO 1TB /dev/nvme7n1 S5P2NG0NA04358V Samsung SSD 980 PRO 1TB /dev/nvme8n1 S5NYNG0N906374T Samsung SSD 980 PRO 500GB /dev/nvme9n1 S5NYNG0N906379K Samsung SSD 980 PRO 500GB</p><p>$ sudo nvme listNode SN模型---------------- -------------------- -------- ------------------- / dev / nvme0n1 S5P2NG0N902798J Samsung SSD 980 PRO 1TB / dev / nvme1n1 S5P2NG0NA02399T Samsung SSD 980 PRO 1TB / dev / nvme2n1 S5P2NG0NA04362H Samsung SSD 980 PRO 1TB / dev / nvme3n1 S5P2NG0N902802P三星SSD 980 PRO 1TB / dev / nvme4n1 S5P2NG0NA00551P三星SSD 980 PRO 1TB / dev / nvme5n1 S5P2NG0NA03266N三星SSD 980 PRO 1TB / dev / nvme6n1 S5P2NG0NA01498X dev / nvme8n1 S5NYNG0N906374T Samsung SSD 980 PRO 500GB / dev / nvme9n1 S5NYNG0N906379K Samsung SSD 980 PRO 500GB</p><p> Samsung’s specs pages ( 1, 2) state that the max sequential read &amp; write speeds as 7000 MB/s &amp; 5000 MB/s for the 1 TB drives using PCIe 4.0. The IO pattern doesn’t really have to be sequential, just with big enough I/O sizes. When these cards are plugged in to PCIe 3.0 slots, you’d get only ~3500 MB/s read and write speeds due to the PCIe 3.0 x4 throughput limitations.</p><p> 三星的规格页面（1，2）指出最大连续读取＆amp;写入速度为7000 MB / s使用PCIe 4.0的1 TB驱动器为5000 MB / s。 IO模式实际上不必是顺序的，只需具有足够大的I / O大小即可。将这些卡插入PCIe 3.0插槽后，由于PCIe 3.0 x4吞吐量的限制，您的读写速度仅为3500 MB / s。</p><p> Take the write speeds with a grain of salt, as TLC &amp; QLC cards have slower multi-bit writes into the main NAND area, but may have some DIMM memory for buffering writes and/or a “TurboWrite buffer” (as Samsung calls it) that uses part of the SSDs NAND as faster SLC storage. It’s done by issuing single-bit “SLC-like” writes into TLC area. So, once you’ve filled up the “SLC” TurboWrite buffer at 5000 MB/s, you’ll be bottlenecked by the TLC “main area” at 2000 MB/s (on the 1 TB disks).</p><p> 像TLC和amp; amp;一样，采用一粒盐的写入速度。 QLC卡对主NAND区域的多位写入速度较慢，但​​可能有一些DIMM内存用于缓冲写入和/或“ TurboWrite缓冲区”（如Samsung所说），它使用SSD NAND的一部分作为较快的SLC存储。通过向TLC区域发出一位“类似于SLC”的写入来完成。因此，一旦以5000 MB / s的速度填充了“ SLC” TurboWrite缓冲区，TLC“主区域”将以2000 MB / s（在1 TB磁盘上）出现瓶颈。</p><p> Apparently on the 980’s the TurboWrite buffer defaults to ~6 GB on the 1 TB SSD, but it’s dynamic and can grow up to 108 GB if there’s high write demand  and enough unused NAND space. Samsung also has their own  designed-in-house disk controller (Elpis) that is built for achieving PCIe 4.0 speeds. It can handle 128 I/O queues (with 64k command sets per queue!). This  article is a good reference about this disk - and an overview of complexity (and potential bottlenecks) of modern SSDs!</p><p> 显然在980年代，TurboWrite缓冲区在1 TB SSD上默认为〜6 GB，但它是动态的，如果有大量的写入需求和足够的未使用NAND空间，它可以增长到108 GB。三星还拥有自己的内部磁盘控制器（Elpis），该磁盘控制器旨在实现PCIe 4.0速度。它可以处理128个I / O队列（每个队列有64k命令集！）。本文是有关此磁盘的很好参考-并概述了现代SSD的复杂性（和潜在的瓶颈）！</p><p> As I want to focus on just the max  raw block I/O performance for this article, I will run my tests directly against the NVMe block devices (without a filesystem or LVM on the I/O path). NVMe block devices have per-CPU multi-queues (MQ) enabled by default and device interrupts are “striped” across all CPUs. I’ll present some LVM, multiqueue and file system tests in future articles.</p><p> 因为我只想关注本文的最大原始块I / O性能，所以我将直接针对NVMe块设备（在I / O路径上没有文件系统或LVM）运行测试。 NVMe块设备默认情况下启用了按CPU的多队列（MQ），并且设备中断在所有CPU上“分段”。我将在以后的文章中介绍一些LVM，多队列和文件系统测试。</p><p>   I used Ubuntu 20.10 with Ubuntu-provided Linux kernel 5.8.0-29-generic for these tests. I briefly tested the (currently) latest Linux kernel 5.11-rc4 too, it has some  io_uring enhancements, but got lower throughput out of it. There are some big AMD ThreadRipper (power-aware scheduler) updates in it, apparently with some unsolved performance regressions.</p><p>   在这些测试中，我使用了Ubuntu 20.10和Ubuntu提供的Linux内核5.8.0-29-generic。我也简短地测试了（当前）最新的Linux内核5.11-rc4，它具有一些io_uring增强功能，但是吞吐量较低。其中有一些大型的AMD ThreadRipper（可感知功耗的调度程序）更新，显然有一些未解决的性能下降。</p><p> I first ran a single-disk test to avoid hitting any system-wide throughput bottlenecks when accessing all 10 SSDs simultaneously. This test aimed to give me a theoretical max throughput of a single disk. I used  fio with  --io_uring  asynchronous I/O option. As expected, it was more efficient than  libaio.</p><p> 我首先进行了单磁盘测试，以避免在同时访问所有10个SSD时遇到任何系统范围的吞吐量瓶颈。该测试旨在为我提供单个磁盘的理论最大吞吐量。我在--io_uring异步I / O选项中使用了fio。不出所料，它比libaio更有效。 </p><p> I ended up using the following  fio command for my narrowly focused synthetic benchmark. I’ll explain the reasoning for some of the command line options later in this post.</p><p>我最终在狭窄的综合基准测试中使用了以下fio命令。在本文的后面，我将解释某些命令行选项的原因。</p><p>  #!/bin/bash[ $# -ne 3 ] &amp;&amp; echo Usage $0 numjobs /dev/DEVICENAME BLOCKSIZE &amp;&amp; exit 1fio --readonly --name=onessd \ --filename=$2 \ --filesize=100g --rw=randread --bs=$3  --direct=1 --overwrite=0 \ --numjobs=$1  --iodepth=32 --time_based=1 --runtime=3600 \ --ioengine= io_uring \ --registerfiles --fixedbufs \ --gtod_reduce=1 --group_reporting</p><p>  ＃！/ bin / bash [$＃-ne 3]＆amp;＆amp; echo用法$ 0 numjobs / dev / DEVICENAME BLOCKSIZE＆amp;＆amp;退出1fio --readonly --name = onessd \ --filename = $ 2 \ --filesize = 100g --rw = randread --bs = $ 3 --direct = 1 --overwrite = 0 \ --numjobs = $ 1- iodepth = 32 --time_based = 1 --runtime = 3600 \ --ioengine = io_uring \ --registerfiles --fixedbufs \ --gtod_reduce = 1 --group_reporting</p><p> Ok, let’s first run  fio with 3 concurrent workers doing 4kB reads against a single disk only (with queue_depth=32 per worker), to see the maximum theoretical throughput of such a disk in my machine:</p><p> 好的，让我们首先在3个并发工作线程进行4kB读取的情况下，仅对单个磁盘进行读取（每个工作线程的queue_depth = 32），以查看机器中此类磁盘的最大理论吞吐量：</p><p>  Looks like we got even more IOPS out of the disk than Samsung promised - their specs said “only” 1,000,000 IOPS per disk. If you wonder why I’m running the single-disk test with 3 concurrent worker processes - it turns out that a single process maxes out a single CPU at about 450k IOPS on my setup. So, a single process is physically incapable of submitting (and reaping) more than 450k I/Os per second of CPU time (being 100% on CPU). Assuming that its CPU core was running at around 3.9 GHz (and didn’t have much else to do), it translates to about 3,900,000,000 / 450,000 = 8,666  CPU cycles used for submitting + issuing + completing + reaping each I/O.</p><p>  看起来我们从磁盘中获得的IOPS比三星承诺的还要多-他们的规格说“每个磁盘”仅1,000,000 IOPS。如果您想知道为什么我要使用3个并发的辅助进程来运行单磁盘测试，那么事实证明，在我的设置中，单个进程使单个CPU的最大运行速度为450k IOPS。因此，单个进程实际上无法在每秒CPU时间（每秒100％在CPU上）上提交（和获取）超过450k I / O。假设它的CPU核心运行在3.9 GHz左右（并且没有其他事情要做），那么它将转化为大约3,900,000,000 / 450,000 = 8,666个CPU周期，用于提交+发行+完成+获得每个I / O。</p><p> If you look into the metrics below, you see that my 3 worker processes used about 9-10% of CPU capacity of my machine with 32 (logical) processors:</p><p> 如果查看下面的指标，您会发现我的3个工作进程使用了​​32个（逻辑）处理器的计算机的CPU容量约占9-10％：</p><p> $  dstat -pcrmd---procs--- ----total-usage---- ------memory-usage----- --io/total- -dsk/total- run blk new| usr  sys idl wai stl| used free buf cach|  read writ| read writ 3.0 0 0|  2  8 91 0 0|1792M 249G 41M 406M| 1145k 0 |4472M 0  3.0 0 0|  2  7 91 0 0|1793M 249G 41M 406M| 1150k 0 |4493M 0  3.0 0 0|  2  8 91 0 0|1793M 249G 41M 406M| 1152k 0 |4499M 0  3.0 0 0|  1  8 91 0 0|1793M 249G 41M 406M| 1151k 0 |4498M 0</p><p> $ dstat -pcrmd --- procs --- ---- total-usage ---- ------ memory-usage ----- --io / total- -dsk / total-run blk new | usr sys idl wai stl |用过的免费buf cach |阅读命令|读命令3.0 0 0 | 2 8 91 0 0 | 1792M 249G 41M 406M | 1145k 0 | 4472M 0 3.0 0 0 | 2 7 91 0 0 | 1793M 249G 41M 406M | 1150k 0 | 4493M 0 3.0 0 0 | 2 8 91 0 0 | 1793M 249G 41M 406M | 1152k 0 | 4499M 0 3.0 0 0 | 1 8 91 0 0 | 1793M 249G 41M 406M | 1151k 0 | 4498M 0</p><p> Sidenote: It looks like  dstat on Ubuntu 20.10 has some CPU utilization rounding &amp; reporting errors. I used other tools to verify that my CPU numbers in this article are similar.</p><p> 旁注：Ubuntu 20.10上的dstat看起来有一些CPU利用率四舍五入报告错误。我使用其他工具来验证本文中的CPU编号是否相似。 </p><p> How about large I/Os? Let’s try 1 MB sized reads, something that a database engine would be using for scanning large tables:</p><p>大型I / O呢？让我们尝试1 MB大小的读取，这是数据库引擎将用于扫描大型表的读取：</p><p>   Seems like we couldn’t actually do 1 MB sized reads, as we are issuing 13.8k read operations for ~6800 MB below. Either  fio wasn’t able to issue 1 MB-sized IOs or the larger requests got split into ~512kB chunks somewhere in the kernel:</p><p>   似乎我们实际上无法进行1 MB大小的读取，因为我们在下面发布了约6.800 MB的1.38万个读取操作。 Fio不能发出1 MB大小的IO，或者更大的请求被分成了内核中某处的〜512kB块：</p><p> ----total-usage---- ---procs--- ------memory-usage----- --io/total- -dsk/total-usr sys idl wai stl|run blk new| used free buf cach| read writ| read writ 0 1 99 0 0| 0 0 0|1990M 249G 54M 441M| 13.8k 0 | 6807M 0 0 1 99 0 0| 0 0 0|1990M 249G 54M 441M| 13.7k 0 | 6799M 0 0 1 99 0 0| 0 0 0|1990M 249G 54M 441M| 13.8k 0 | 6805M 0 0 1 99 0 0| 0 0 0|1990M 249G 54M 441M| 13.7k 0 | 6803M 0</p><p> ---- total-usage ---- --- procs --- ------ memory-usage ----- --io / total- -dsk / total-usr sys idl wai stl | run blk新|用过的免费buf cach |阅读命令|读命令0 1 99 0 0 | 0 0 0 | 1990M 249G 54M 441M | 13.8k 0 | 6807M 0 0 1 99 0 0 | 0 0 0 | 1990M 249G 54M 441M | 13.7k 0 | 6799M 0 0 1 99 0 0 | 0 0 0 | 1990M 249G 54M 441M | 13.8k 0 | 6805M 0 0 1 99 0 0 | 0 0 0 | 1990M 249G 54M 441M | 13.7k 0 | 6803M 0</p><p> Thanks to the big I/O sizes, every request takes a lot of time as the disk’s DMA controller copies the request’s data to relevant memory location in RAM. PCIe 4.0 x4 max theoretical transfer rate is about 7.877 GB/s, even the PCIe transfer from the disk controller memory to CPU will take over 120  us per MB, in addition to the flash reading and SSD controller’s latency.</p><p> 由于I / O大小较大，每个请求都需要花费大量时间，因为磁盘的DMA控制器会将请求的数据复制到RAM中的相关内存位置。 PCIe 4.0 x4的最大理论传输速率约为7.877 GB / s，即使是从磁盘控制器内存到CPU的PCIe传输，也需要超过120 us / MB，除了闪存读取和SSD控制器的延迟。</p><p> So, as you see from the CPU usage numbers above, my CPUs were 99% idle, despite running 3 concurrent  fio workers and  top confirmed that. What were the worker processes doing then? They were mostly sleeping, waiting for events in io_uring completion queue, with WCHAN  io_cqring_wait:</p><p> 因此，从上面的CPU使用率数据可以看出，尽管我运行3个并发的fio工作程序，但我的CPU还是空闲了99％，并且top确认了这一点。工人流程当时在做什么？他们大部分时间都在睡觉，等待W_io_cqring_wait在io_uring完成队列中的事件：</p><p> $ sudo  psn -G syscall,wchan -a -p ^fioLinux Process Snapper v0.18 by Tanel Poder [https://0x.tools]Sampling /proc/syscall, wchan, stat for 5 seconds... finished.=== Active Threads ============================================================================= samples | avg_threads | comm | state | syscall | wchan ------------------------------------------------------------------------------------------------ 203 |  2.03 | (fio) | Sleep (Interruptible) | io_uring_enter | io_cqring_wait 100 | 1.00 | (fio) | Sleep (Interruptible) | select | do_select 85 | 0.85 | (fio) | Sleep (Interruptible) | clock_nanosleep | hrtimer_nanosleep 14 | 0.14 | (fio) | Disk (Uninterruptible) | openat | __blkdev_get 12 | 0.12 | (fio) | Running (ON CPU) | io_uring_enter | 0 12 | 0.12 | (fio) | Sleep (Interruptible) | [running] | io_cqring_wait 9 | 0.09 | (fio) | Running (ON CPU) | io_uring_enter | io_cqring_wait 2 | 0.02 | (fio) | Running (ON CPU) | [running] | 0 2 | 0.02 | (fio) | Sleep (Interruptible) | [running] | 0 1 | 0.01 | (fio) | Running (ON CPU) | futex | futex_wait_queue_me samples: 100 (expected: 100)total processes: 4, threads: 5runtime: 5.00, measure time: 0.18</p><p> $ sudo psn -G syscall，wchan -a -p ^ fioLinux进程快照程序v0.18，由Tanel Poder [https://0x.tools]采样/ proc / syscall，wchan，统计5秒钟...完成。== =活动线程=============================================== ============================样本| avg_threads |通讯|州| syscall | chan ------------------------------------------------- ----------------------------------------------- 203 | 2.03 | （fio）|睡眠（可中断）| io_uring_enter | io_cqring_wait 100 | 1.00 | （fio）|睡眠（可中断）|选择| do_select 85 | 0.85 | （fio）|睡眠（可中断）| clock_nanosleep | hrtimer_nanosleep 14 | 0.14 | （fio）|磁盘（不间断）| openat | __blkdev_get 12 | 0.12 | （fio）|正在运行（在CPU上）| io_uring_enter | 0 12 | 0.12 | （fio）|睡眠（可中断）| [运行中] io_cqring_wait 9 | 0.09 | （fio）|正在运行（在CPU上）| io_uring_enter | io_cqring_wait 2 | 0.02 | （fio）|正在运行（在CPU上）| [运行中] 0 2 | 0.02 | （fio）|睡眠（可中断）| [运行中] 0 1 | 0.01 | （fio）|正在运行（在CPU上）| futex | futex_wait_queue_me样本：100（预期：100）总进程：4，线程：5运行时：5.00，测量时间：0.18</p><p>   In the following two sections I show how I tried different OS level I/O configuration options (direct vs cached I/O and using an I/O scheduler). These sections go pretty deep into Linux kernel troubleshooting topics, if you want to skip this and read about the hardware configuration challenges, jump to the  Multi-disk test section.</p><p>   在以下两节中，我将展示如何尝试不同的OS级别I / O配置选项（直接或缓存I / O以及使用I / O调度程序）。这些部分深入探讨了Linux内核故障排除主题，如果您想跳过本主题并阅读有关硬件配置的挑战，请跳至“多磁盘测试”部分。 </p><p> I used  --direct=1 option that forces files to be opened with  O_DIRECT flag - bypassing the OS pagecache. When running millions of IOPS, you want to minimize the CPU overhead of every operation and copying, searching &amp; replacing pages in the OS pagecache will radically increase your CPU usage and memory traffic. Most mature database engines have a built-in cache anyway, so why duplicate work (and memory usage).</p><p>我使用--direct = 1选项强制使用O_DIRECT标志打开文件-绕开了OS页面缓存。当运行数百万个IOPS时，您希望将每个操作以及复制，搜索和复制的CPU开销降到最低。替换操作系统页面缓存中的页面将从根本上增加您的CPU使用率和内存流量。无论如何，大多数成熟的数据库引擎都有内置的缓存，因此为什么要重复工作（和使用内存）。</p><p> But for fun, I ran the same test with  --direct=0 anyway and the results are below:</p><p> 但是为了好玩，我还是使用--direct = 0进行了相同的测试，结果如下：</p><p> $ dstat -pcmrd---procs--- ----total-usage---- ------memory-usage----- --io/total- -dsk/total- run blk new|usr  sys idl wai stl| used free buf cach| read writ| read writ  75 0 0| 0  100 0 0 0|1997M 1864M 244G 243M|5947 0 |2953M 0  60 3.0 0| 0  100 0 0 0|2005M 1193M 245G 243M|7188 11.0 |3593M 244k  56 2.0 0| 0  99 0 1 0|1993M 1363M 245G 243M|6290 0 |3143M 0  56 1.0 0| 0  99 0 1 0|1992M 1316M 245G 240M|6545 0 |3266M 0  38 19 0| 0  99 0 1 0|1984M 1271M 245G 237M|6493 0 |3239M 0 ^C</p><p> $ dstat -pcmrd --- procs --- ---- total-usage ---- ------ memory-usage ----- --io / total- -dsk / total-run blk new | usr sys idl wai stl |用过的免费buf cach |阅读命令|读取命令75 0 0 | 0 100 0 0 0 | 1997M 1864M 244G 243M | 5947 0 | 2953M 0 60 3.0 0 | 0 100 0 0 0 | 2005M 1193M 245G 243M | 7188 11.0 | 3593M 244k 56 2.0 0 | 0 99 0 1 0 | 1993M 1363M 245G 243M | 6290 0 | 3143M 0 56 1.0 0 | 0 99 0 1 0 | 1992M 1316M 245G 240M | 6545 0 | 3266M 0 38 19 0 | 0 99 0 1 0 | 1984M 1271M 245G 237M | 6493 0 | 3239M 0 ^ C</p><p> Wait, what? My  fio test with 3 workers somehow keeps all the CPUs 100% busy in  kernel mode? The runnable threads ( run column) alternates between 38 and 75? The read throughput has dropped from over 6 GiB/s to 3 GiB/s. But why do we have so much CPU activity? Is that just how it is?</p><p> 等一下我的由3名工作人员组成的fio测试以某种方式使所有CPU在内核模式下保持100％繁忙？可运行线程（运行列）在38和75之间交替显示？读取吞吐量已从6 GiB / s以上降至3 GiB / s。但是为什么我们有这么多的CPU活动？就是这样吗？</p><p> Let’s not guess or give up, but measure! As the kernel CPU usage is very high, we have a couple of options for drilling down.</p><p> 我们不要猜测或放弃，而要衡量！由于内核CPU使用率很高，我们提供了两个可供选择的选项。</p><p>  $ sudo psn -G syscall,wchanLinux Process Snapper v0.18 by Tanel Poder [https://0x.tools]Sampling /proc/syscall, wchan, stat for 5 seconds... finished.=== Active Threads ================================================================================================ samples | avg_threads | comm | state | syscall | wchan ------------------------------------------------------------------------------------------------------------------- 4218 |  59.41 | (io_wqe_worker-*) | Disk (Uninterruptible) | [kernel_thread] | wait_on_page_bit_common 1698 |  23.92 | (io_wqe_worker-*) | Running (ON CPU) | [running] | 0 54 | 0.76 | (fio) | Running (ON CPU) | [running] | 0 18 | 0.25 | (kswapd*) | Running (ON CPU) | [running] | 0 8 | 0.11 | (io_wqe_worker-*) | Running (ON CPU) | [kernel_thread] | 0 5 | 0.07 | (io_wqe_worker-*) | Disk (Uninterruptible) | [running] | 0 5 | 0.07 | (io_wqe_worker-*) | Disk (Uninterruptible) | [running] | wait_on_page_bit_common 5 | 0.07 | (kworker/*:*-events) | Running (ON CPU) | [running] | 0 4 | 0.06 | (fio) | Running (ON CPU) | io_uring_enter | io_cqring_wait 4 | 0.06 | (io_wqe_worker-*) | Running (ON CPU) | [kernel_thread] | wait_on_page_bit_common 1 | 0.01 | (fio) | Running (ON CPU) | io_uring_enter | 0 1 | 0.01 | (io_wqe_worker-*) | Running (ON CPU) | [kernel_thread] | io_wqe_worker 1 | 0.01 | (rcu_sched) | Running (ON CPU) | [running] | 0</p><p>  $ sudo psn -G syscall，wchanLinux进程快照程序v0.18，由Tanel Poder [https://0x.tools]采样/ proc / syscall，wchan，stat持续5秒钟...已完成。===活动线程=== ================================================== ======================================样品avg_threads |通讯|州| syscall | chan ------------------------------------------------- -------------------------------------------------- ---------------- 4218 | 59.41 | （io_wqe_worker- *）|磁盘（不间断）| [kernel_thread] | wait_on_page_bit_common 1698 | 23.92 | （io_wqe_worker- *）|正在运行（在CPU上）| [运行中] 0 54 | 0.76 | （fio）|正在运行（在CPU上）| [运行中] 0 18 | 0.25 | （kswapd *）|正在运行（在CPU上）| [运行中] 0 8 | 0.11 | （io_wqe_worker- *）|正在运行（在CPU上）| [kernel_thread] | 0 5 | 0.07 | （io_wqe_worker- *）|磁盘（不间断）| [运行中] 0 5 | 0.07 | （io_wqe_worker- *）|磁盘（不间断）| [运行中] wait_on_page_bit_common 5 | 0.07 | （kworker / *：*-事件）|正在运行（在CPU上）| [运行中] 0 4 | 0.06 | （fio）|正在运行（在CPU上）| io_uring_enter | io_cqring_wait 4 | 0.06 | （io_wqe_worker- *）|正在运行（在CPU上）| [kernel_thread] | wait_on_page_bit_common 1 | 0.01 | （fio）|正在运行（在CPU上）| io_uring_enter | 0 1 | 0.01 | （io_wqe_worker- *）|正在运行（在CPU上）| [kernel_thread] | io_wqe_worker 1 | 0.01 | （rcu_sched）|正在运行（在CPU上）| [运行中] 0</p><p> So, it’s not my 3  fio processes that magically eat all the CPU time, but there’s a lot of  io_wge_worker-N kernel threads that are doing something! It starting to look like  “it’s not me, it’s you - Linux kernel”. When you scroll the above output right, you see that the top entry reports these threads waiting in thread state  D - uninterruptible (usually) disk I/O sleep and the kernel function ( wchan) that requested the sleep is  wait_on_page_bit_common. That’s the common WHAN that shows up whenever you’re waiting in cached I/O (pagecache) codepath. We still don’t know whether this is just a “waiting for slow I/O via pagecache to complete” scenario or some kernel issue. Remember that there’s a significant amount of these kernel worker threads also burning CPU in kernel mode, not waiting for anything (the 2nd highlighted line with “Running (ON CPU)” above).</p><p> 因此，不是我的3个fio进程神奇地消耗了所有CPU时间，而是有很多io_wge_worker-N内核线程正在执行某些操作！它开始看起来像“不是我，而是您-Linux内核”。当向右滚动上述输出时，您会看到最上面的条目报告这些线程在线程状态D中等待-不间断（通常）磁盘I / O睡眠，而请求睡眠的内核函数（wchan）是wait_on_page_bit_common。这是每次您在缓存的I / O（页面缓存）代码路径中等待时都会显示的常见WHAN。我们仍然不知道这仅仅是“等待通过页面缓存完成缓慢的I / O”场景还是某些内核问题。请记住，有很多这样的内核工作线程也在内核模式下燃烧CPU，而不是等待任何东西（上面第二行突出显示的行带有“ Running（ON CPU）”）。 </p><p> The good news is that we can easily drill down further! pSnapper allows you to sample  /proc/PID/stack too to get a rough idea of in which kernel locations any of your threads are. You will have to scroll all the way to the right, until you see the highlighted functions:</p><p>好消息是我们可以轻松地进一步深入分析！ pSnapper还允许您对/ proc / PID / stack进行采样，以大致了解任何线程位于哪个内核位置。您必须一直向右滚动，直到看到突出显示的功能：</p><p> $ sudo psn -G syscall,wchan, kstackLinux Process Snapper v0.18 by Tanel Poder [https://0x.tools]Sampling /proc/syscall, stack, wchan, stat for 5 seconds... finished.=== Active Threads ======================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================= samples | avg_threads | comm | state | syscall | wchan | kstack ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 686 | 19.06 | (io_wqe_worker-*) | Running (ON CPU) | [running] | 0 | - 418 | 11.61 | (io_wqe_worker-*) | Disk (Uninterruptible) | [kernel_thread] | wait_on_page_bit_common | kthread()-&gt;io_wqe_worker()-&gt;io_worker_handle_work()-&gt;io_wq_submit_work()-&gt;io_issue_sqe()-&gt;io_read()-&gt;blkdev_read_iter()-&gt;generic_file_read_iter()-&gt;generic_file_buffered_read()-&gt;wait_on_page_bit_common() 384 | 10.67 | (io_wqe_worker-*) | Running (ON CPU) | [running] | 0 | kthread()-&gt;io_wqe_worker()-&gt;io_worker_handle_work()-&gt;io_wq_submit_work()-&gt;io_issue_sqe()-&gt;io_read()-&gt;blkdev_read_iter()-&gt;generic_file_read_iter()-&gt;generic_file_buffered_read()-&gt;wait_on_page_bit_common() 284 | 7.89 | (io_wqe_worker-*) | Running (ON CPU) | [running] | 0 | kthread()-&gt;io_wqe_worker()-&gt;io_worker_handle_work()-&gt;io_wq_submit_work()-&gt;io_issue_sqe()-&gt;io_read()-&gt;blkdev_read_iter()-&gt;generic_file_read_iter()-&gt;generic_file_buffered_read()-&gt; page_cache_sync_readahead()-&gt;force_page_cache_readahead()-&gt;page_cache_readahead_unbounded()-&gt;__page_cache_alloc()-&gt;alloc_pages_current()-&gt;__alloc_pages_nodemask()-&gt;__alloc_pages_slowpath.constprop.0()-&gt;try_to_free_pages()-&gt;do_try_to_free_pages()-&gt;shrink_zones()-&gt;shrink_node()-&gt;shrink_node_memcgs()-&gt;shrink_lruvec()-&gt;shrink 233 | 6.47 | (io_wqe_worker-*) | Running (ON CPU) | [running] | 0 | kthread()-&gt;io_wqe_worker()-&gt;io_worker_handle_work()-&gt;io_wq_submit_work()-&gt;io_issue_sqe()-&gt;io_read()-&gt;blkdev_read_iter()-&gt;generic_file_read_iter()-&gt;generic_file_buffered_read()-&gt; page_cache_sync_readahead()-&gt;force_page_cache_readahead()-&gt;page_cache_readahead_unbounded()-&gt;__page_cache_alloc()-&gt</p><p> $ sudo psn -G syscall，wchan，kstack Linux进程快照程序v0.18，由Tanel Poder [https://0x.tools]采样/ proc / syscall，堆栈，wchan，stat持续5秒钟...已完成。===活动线程================================================== ================================================== ================================================== ================================================== ================================================== ================================================== ================================================== ================================================== ================================================== ================================================== ================================================== ==================================================样品| avg_threads |通讯|州| syscall | wchan | kstack ------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- -------------------------------------------------- ------------------- 686 | 19.06 | （io_wqe_worker- *）|正在运行（在CPU上）| [运行中] 0 | -418 | 11.61 | （io_wqe_worker- *）|磁盘（不间断）| [kernel_thread] | wait_on_page_bit_common | kthread（）-＆gt; io_wqe_worker（）-＆gt; io_worker_handle_work（）-＆gt; io_wq_submit_work（）-＆gt; io_issue_sqe（）-＆gt; io_read（）-＆gt; blkdev_read_iter（）-＆gt; generic_file_read_it_buffer ）-＆gt; wait_on_page_bit_common（）384 | 10.67 | （io_wqe_worker- *）|正在运行（在CPU上）| [运行中] 0 | kthread（）-＆gt; io_wqe_worker（）-＆gt; io_worker_handle_work（）-＆gt; io_wq_submit_work（）-＆gt; io_issue_sqe（）-＆gt; io_read（）-＆gt; blkdev_read_iter（）-＆gt; generic_file_read_it_buffer ）-＆gt; wait_on_page_bit_common（）284 | 7.89 | （io_wqe_worker- *）|正在运行（在CPU上）| [运行中] 0 | kthread（）-＆gt; io_wqe_worker（）-＆gt; io_worker_handle_work（）-＆gt; io_wq_submit_work（）-＆gt; io_issue_sqe（）-＆gt; io_read（）-＆gt; blkdev_read_iter（）-＆gt; generic_file_read_it_buffer ）-＆gt; page_cache_sync_readahead（）-＆gt; force_page_cache_readahead（）-＆gt; page_cache_readahead_unbounded（）-＆gt; __ page_cache_alloc（）-＆gt; alloc_pages_current（）-＆gt; __ alloc_pages_nodemask（）-＆gt; __ alloc_pages_slowto（免费）。 ＆gt; do_try_to_free_pages（）-＆gt; shrink_zones（）-＆gt; shrink_node（）-＆gt; shrink_node_memcgs（）-＆gt; shrink_lruvec（）-＆gt; shrink 233 | 6.47 | （io_wqe_worker- *）|正在运行（在CPU上）| [运行中] 0 | kthread（）-＆gt; io_wqe_worker（）-> io_worker_handle_work（）-＆gt; io_wq_submit_work（）-＆gt; io_issue_sqe（）-＆gt; io_read（）-＆gt; blkdev_read_iter（）-＆gt; generic_file_read_it_buffer ）-＆gt; page_cache_sync_readahead（）-＆gt; force_page_cache_readahead（）-＆gt; page_cache_readahead_unbounded（）-＆gt; __ page_cache_alloc_alloc（）-＆gt;</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/">https://tanelpoder.com/posts/11m-iops-with-10-ssds-on-amd-threadripper-pro-workstation/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/io/">#io</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cpu/">#cpu</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>