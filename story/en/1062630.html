<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么加载平衡GRPC很棘手？ Why load balancing gRPC is tricky?</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Why load balancing gRPC is tricky?<br/>为什么加载平衡GRPC很棘手？ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-14 14:05:21</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/5/6b34bf4f02fbf5fecf0daafacf7bf21b.png"><img src="http://img2.diglog.com/img/2021/5/6b34bf4f02fbf5fecf0daafacf7bf21b.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>With growth of microservices in the past few years,  gRPC has gained a lot of popularity for inter-communication among these smaller services. Behind the scenes gRPC uses http/2 to multiplex many requests within the same connection and duplex streaming.</p><p>随着过去几年微服务的增长，GRPC在这些较小的服务之间进行了很多人气。在幕后，GRPC使用HTTP / 2来多用同一连接内的许多请求和双工流。</p><p> Using a fast, very light, binary protocol with structured data as the communication medium among services, is indeed very attractive, but there are some considerations when using gRPC, most important of all is how to handle load balancing.</p><p> 使用快速非常轻的二进制协议，具有结构化数据作为服务之间的通信媒介，确实非常有吸引力，但在使用GRPC时存在一些考虑因素，最重要的是如何处理负载平衡。</p><p>  gRPC connections are sticky. Meaning that when a connection is made from client to the server, the same connection will be reused for many requests ( multiplexed) for as long as possible. This is done to avoid all the initial time and resources spent for tcp handshakes. So when a client grabs a connection to an instance of the server, it will hold on to it.</p><p>  GRPC连接是粘性的。意思是，当从客户端到服务器进行连接时，可以为许多请求（多路复用）重复使用相同的连接，只要尽可能长。这是为了避免为TCP握手花费的所有初始时间和资源。因此，当客户端抓取到服务器实例时，它将保持它。</p><p> Now, when the same client starts sending large volumes of requests, they will all go to the same server instance. And that is exactly the problem, there will be no chance of distributing that load to the other instances.  They all go to the same instance.</p><p> 现在，当同一客户端开始发送大量的请求时，它们都将转到同一服务器实例。这正是问题，没有机会将该负载分发给其他实例。他们都转到同样的例子。</p><p>  Below are some approaches to load balancing gRPC inter-communication and some details with each approach.</p><p>  以下是负载平衡GRPC间通信和各种方法的一些方法的方法。</p><p>  When load balancing is done on the server-side, it leaves the client very thin and completely unaware of how it is handled on the servers:</p><p>  当在服务器端完成负载平衡时，它会使客户端非常瘦，完全不知道它在服务器上处理方式：</p><p>   A Network Load Balancer operates at the layer-4 of  OSI (Open Systems Interconnection) model. Therefore it is very fast and can handle much more connections. When a new TCP traffic connection comes in, the load balancer selects an instance and the connection is routed to that single instance for the life of the connection.</p><p>   网络负载平衡器在OSI（开放系统互连）模型的第4层处运行。因此它非常快，可以处理更多的连接。当新的TCP流量连接进入时，负载均衡器选择实例，并将连接路由到连接的寿命的该单一实例。 </p><p> Remember now that gRPC connections are  sticky and  persistent, so it will hold on to the  same connection between the client and same server instance behind the load balancer, as long as it can.</p><p>现在请记住，GRPC连接是粘性且持久的，因此它将保持在客户端与负载均衡器后面的同一服务器实例之间的相同连接，只要它可以。</p><p>   If load (memory or cpu) on a single server instance goes higher than the auto scaling policy, it will cause a new instance(s) to be spun up in that target group.</p><p>   如果单个服务器实例上的加载（内存或CPU）高于自动缩放策略，则会导致新实例在该目标组中旋转。</p><p> But a new instance in the target group is NOT going to be helpful. Why? Again, because gRPC connections are persistent and sticky.The client that was sending large volumes of requests, will keep sending them to them  same server instance it has the connection to.</p><p> 但目标组中的一个新实例不会有所帮助。为什么？同样，因为GRPC连接是持久性和粘性的。发送大量请求的客户端将继续向他们发送相同的服务器实例它具有连接。</p><p> Therefore new server instances are spun up, but none of the requests overload is going to the new instance(s). Same single server instance with heavy usage is still receiving loads of requests from the client (as the client keeps reusing the same connection).</p><p> 因此，新的服务器实例已旋转，但没有一个请求过载将进入新实例。具有沉重用法的单个服务器实例仍在接收来自客户端的请求（因为客户端保留同一连接）。</p><p> There is a chance that the auto scaling policy keeps triggering and adding new instances to the target group (as there is a single instance with overloaded cpu/memory). But these new instances receive close to zero traffic. The auto scaling policy could keep triggering and potentially maximizing the allowed instances in the target group, without actually benefiting from the requests being sent to the new instances.</p><p> 有可能使自动缩放策略保持触发并向目标组添加新实例（因为有一个具有过载的CPU /内存的单个实例）。但这些新实例接近零流量。自动缩放策略可以继续触发并可能最大化目标组中的允许实例，而无需实际受益于发送到新实例的请求。</p><p>  To have a chance of distributing the load basically we have to give up the stickiness and persistent connections with one of the following approaches:</p><p>  为了有机会分发负载，我们必须用以下方法之一放弃粘性和持久的连接：</p><p>  If you have control over the connecting gRPC client, you can force the client to disconnect periodically and reconnect.This act will force the client to send a new request to the load balancer and as a response to this request this time a healthier instance will be returned. This technique forces the load to be balanced.</p><p>  如果您对连接GRPC客户端进行了控制，则可以将客户端定期断开连接并重新连接。这一动作将强制客户将新请求向负载均衡器发送并作为对此请求的响应，这次将此时间更健康实例回。这种技术强制负载是平衡的。 </p><p>  If you do NOT have control over the connecting gRPC clients, you can implement similar logic on the server side. Making the server to forcefully shut down the connections after sometime, and when they reconnect, it automatically causes the new connections to go to the healthier instances.</p><p>如果您没有控制连接GRPC客户端，则可以在服务器端实现类似的逻辑。使服务器在某个时候强制关闭连接，当他们重新连接时，它会自动导致新的连接转到更健康的实例。</p><p>   AWS Application Load Balancers are a layer-7 load balancer and support http/2, up until Oct.2020 the support for http/2 was from the client to the load balancer only. Then the connection was downgraded to http/1.1 from the load balancer to the backing target instances behind the ALB.</p><p>   AWS应用程序负载均衡器是第7层负载均衡器和支持HTTP / 2，直到10月20日至10月20日，HTTP / 2的支持仅从客户端到负载均衡器。然后，将连接从负载均衡器从负载均衡器中降级到HTTP / 1.1到BALB背后的支持目标实例。</p><p> Therefore previously you could not use ALB with gRPC as it requires http/2 all the way through.</p><p> 因此，此前您无法使用GRPC使用ALB，因为它一直需要HTTP / 2。</p><p> But on Oct.29.2020 AWS  announced that ALB now supports the http/2 all the way, making it a great fit for gRPC communications.</p><p> 但是，在2010年10月.2020。AWS宣布，ALB现在一直支持HTTP / 2，使其非常适合GRPC通信。</p><p> Although this feature from ALBs is relatively new but the arguments for sticky gRPC connections and NLB are also applicable with ALBs. If yu end up with a single sticky and persistent connection from a client that starts sending a large volume of requests, you are overloading a single server instance that holds that connection and the requests are not being load balanced as they should.</p><p> 虽然ALB的此功能相对较新，但粘性GRPC连接和NLB的参数也适用于ALB。如果yu从开始发送大量请求的客户端的单个粘性和持久的连接，则重新装载一个服务器实例，该服务器实例可容纳连接，并且请求不是应加载均衡。</p><p>  Similarly we can put our server instances behind DNS Service Discovery, instead of an Elastic Load Balancer. Service Discovery is basically a DNS service that when a request comes in, it will return the list of IP addresses for all the instances behind it (or a subset of the healthy ones), in random order. So when a client is choosing which server to connect to, and does a DNS lookup, service discovery returns back the IP addresses of backing instances sorted.</p><p>  同样，我们可以将我们的服务器实例放在DNS服务发现后，而不是弹性负载平衡器。服务发现基本上是DNS服务，当请求进入时，它将以随机顺序返回其后面的所有实例的IP地址列表（或健康的子集）。因此，当客户端选择要连接的服务器并执行DNS查找时，服务发现返回排序的备份实例的IP地址。</p><p> Pretty much all the concerns with NLB are applicable to DNS Service Discovery load balancing as well. When a client grabs a connection to a single instance it will stick to it and keep reusing it. No matter how it has found the server instance, be it ALB, NLB or Service Discovery.</p><p> 几乎所有与NLB的关注都适用于DNS服务发现负载平衡。当客户端抓住与单个实例的连接时，它将坚持并继续重新使用。无论它如何找到服务器实例，都是IT ALB，NLB或服务发现。 </p><p>   If you have full control over the clients, you can implement load balancing logic in the client-side. Making the clients aware of all the available servers and their health and choosing which one to connect to.This will cause the clients to be heavier in the logic. So they not only should contain the logic to do what they are supposed to do, but also they also need to implement the logic for load balancing, health checks, etc.</p><p>如果您完全控制客户端，可以在客户端实现负载平衡逻辑。使客户端了解所有可用的服务器及其健康以及选择连接到的所有服务器和选择哪一个。这将导致客户在逻辑中更重。因此，它们不仅应该包含逻辑要做他们应该做的事情，而且还需要实现负载平衡，健康检查等的逻辑。</p><p> This is a viable option, under one condition:  If you have full control over all the clients. You can’t let faulty clients connect to your service and cause all sorts of load balancing issues. It only takes one faulty client to cause enough troubles.</p><p> 这是一个可行的选项，在一个条件下：如果您完全控制所有客户端。您不能让错误的客户端连接到您的服务并导致各种负载平衡问题。它只需要一个有缺陷的客户来引起足够的麻烦。</p><p>   As recommended by the official gRPC load balancing, this method is using an external load balancer or  one-arm load balancer for distributing the traffic between server instances.</p><p>   根据官方GRPC负载均衡的推荐，此方法使用外部负载均衡器或单臂负载均衡器，用于在服务器实例之间分发流量。</p><p> Client reaches out to an external service and it will return a list of available servers, service discovery and all other required information.</p><p> 客户端达到外部服务，它将返回可用服务器，服务发现和所有其他所需信息的列表。</p><p> Ideally there will be some logic in the client-side as well to help making the decision. This approach is prone to the problem with sticky connections that is mentioned above, so it needs to be implemented carefully.</p><p> 理想情况下，客户端将有一些逻辑，也可以帮助决定决定。这种方法容易出于上面提到的粘性连接的问题，因此需要仔细实施。</p><p> The  official implementation uses a  Per Call Load Balancing and not a  per connection one. So each call is being load balanced separately. This is the ideal and the desired case and it will avoid having heavy sticky connections.</p><p> 官方实现使用每个呼叫负载平衡，而不是每个连接。所以每个呼叫都是单独平衡的负载。这是理想的和理想的情况，它将避免具有沉重的粘性连接。</p><p> Look-aside checks all the boxes, but with one big drawback:  It requires a fully dedicated service.</p><p> 看看检查所有框，还有一个大缺点：它需要一个完全专用的服务。 </p><p> So to benefit from look-aside load balancing, you need to implement and deploy a brand new dedicated service to just load balance the gRPC connections between your other service. Every new service comes with its own maintenance, operation, monitoring, alerting, etc.</p><p>因此，要从外观负载均衡中受益，您需要实现并部署一个全新的专用服务，只需加载您的其他服务之间的GRPC连接。每项新服务都有自己的维护，操作，监控，警报等。</p><p>  Server-Side load balancing, has very important concerns, we can not benefit from one of the main advantages of gRPC which is sticky reusable connections.</p><p>  服务器端负载平衡，具有非常重要的问题，我们无法受​​益于GRPC的主要优点，这是粘性可重复使用的连接。</p><p> Client-Side load balancing, requires full control to the clients, if there is one faulty client, it could break all the plans.</p><p> 客户端负载平衡，需要完全控制客户端，如果有一个错误的客户端，它可能会破坏所有的计划。</p><p> Look-Aside load balancing, is the most logical and performant solution to load balance gRPC connections, but then it requires a full and dedicated service of its own, meaning a new service in your architecture to implement and operate. Also a new point of failure.</p><p> 看起来除了负载平衡，是负载均衡Grpc连接的最逻辑和性能的解决方案，但它需要自己的完整和专用服务，这意味着架构中的新服务来实现和操作。也是一个新的失败点。</p><p> The concern and challenge of handling the increase in load in a gRPC architecture is an important one. It needs proper attention to be handled properly and the solution depends on the situation and how much control we have.</p><p> 处理GRPC架构中负载增加的关注和挑战是一个重要的。它需要适当关注妥善处理，解决方案取决于我们所拥有的情况。</p><p>     Finally, as everything else in software engineering, gRPC comes with trade offs. It’s critical to be aware of the trade offs and choose accordingly.</p><p>     最后，作为软件工程中的所有其他东西，GRPC都带来了贸易问题。意识到贸易问题是至关重要的，并相应地选择。</p><p> If you have found this article interesting, you can    follow me on twitter as I share my learnings there.</p><p> 如果您发现本文有趣，您可以在Twitter上关注我，因为我在那里分享了我的学习。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://majidfn.com/blog/20201222-grpc-load-balancing/">https://majidfn.com/blog/20201222-grpc-load-balancing/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/加载/">#加载</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/balancing/">#balancing</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/客户端/">#客户端</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>