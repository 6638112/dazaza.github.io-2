<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>回归和线性组合 Regression and Linear Combinations</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Regression and Linear Combinations<br/>回归和线性组合 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-05 20:47:06</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/5/73ee401cabb285ee9c8ec4d23ba8a81f.png"><img src="http://img2.diglog.com/img/2021/5/73ee401cabb285ee9c8ec4d23ba8a81f.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Recently I’ve been helping out with a linear algebra course organized by  Tai-Danae Bradley and  Jack Hidary, and one of the questions that came up a few times was, “why should programmers care about the concept of a linear combination?”</p><p>最近，我一直在帮助大达纳尔布拉德利和杰克海关组织的线性代数课程，以及几次提出的问题是“为什么程序员应该关心线性组合的概念？”</p><p> For those who don’t know, given vectors  , a  linear combination of the vectors is a choice of some coefficients   with which to weight the vectors in a sum  .</p><p> 对于那些不了解的人，给定载体的线性组合是载体的选择，其中一些系数，其将载体以总和重量。</p><p> I must admit, math books do a poor job of painting the concept as more than theoretical—perhaps linear combinations are only needed for proofs, while the real meat is in matrix multiplication and cross products. But no, linear combinations truly lie at the heart of many practical applications.</p><p> 我必须承认，数学书籍做了一个糟糕的绘画概念，因为唯一只需要线性组合只需要证明，而真正的肉是矩阵乘法和交叉产品。但不，线性组合真正躺在许多实际应用的核心。</p><p> In some cases, the  entire goal of an algorithm is to find a “useful” linear combination of a set of vectors. The vectors are the building blocks (often a vector space or subspace basis), and the set of linear combinations are the legal ways to combine the blocks. Simpler blocks admit easier and more efficient algorithms, but their linear combinations are less expressive. Hence, a tradeoff.</p><p> 在某些情况下，算法的整个目标是找到一组矢量的“有用”线性组合。向量是构建块（通常是向量空间或子空间），并且该组线性组合是组合块的法律方式。更简单的块承认更容易和更高效的算法，但它们的线性组合不太表达。因此，权衡。</p><p> A concrete example is regression. Most people think of regression in terms of  linear regression. You’re looking for a linear function like   that approximates some data well. For multiple variables, you have, e.g.,   as a vector of input variables, and   as a vector of weights, and the function is  .</p><p> 一个具体的例子是回归。大多数人在线性回归方面的回归。您正在寻找像这样的线性函数近似于一些数据。对于多个变量，您具有例如输入变量的向量，并且作为权重的矢量，并且该功能是。</p><p> To avoid the shift by   (which makes the function affine instead of purely linear; formulas of purely linear functions are easier to work with because the shift is like a pesky special case you have to constantly account for), authors often add a fake input variable   which is always fixed to 1, and relabel   as   to get   as the final form. The optimization problem to solve becomes the following, where your data set to approximate is  .</p><p> 为了避免偏移（这使得函数仿现而不是纯线性;纯线性函数的公式更容易使用，因为Shift就像一个讨厌的特殊情况，你必须不断考虑），作者通常添加假输入变量这始终固定为1，并重新标记为最终表格。解决的优化问题成为以下内容，您的数据设置为近似的情况。</p><p>  In this case, the function being learned—the output of the regression—doesn’t look like a linear combination. Technically it is, just not in an interesting way.</p><p>  在这种情况下，正在学习的功能 - 回归的输出 - 不像线性组合。从技术上讲，它是不是以有趣的方式。 </p><p> It becomes more obviously related to linear combinations when you try to model non-linearity. The idea is to define a class of functions called  basis functions  , and allow your approximation to be any linear combination of functions in  , i.e., any function in the span of B.</p><p>当您尝试模拟非线性时，它变得更加明显与线性组合相关。该想法是定义称为基函数的一类功能，并允许您的近似是I.，即B的跨度中的任何功能的任何线性组合。</p><p>  Again, instead of weighting each coordinate of the input vector with a  , we’re weighting each basis function’s contribution (when given the whole input vector) to the output. If the basis functions were to output a single coordinate ( ), we would be back to linear regression.</p><p>  同样，代替加权输入向量的每个坐标，我们加权每个基本函数的贡献（当给定整个输入向量时）到输出。如果基础函数是输出单个坐标（），则会恢复到线性回归。</p><p> Then the optimization problem is to choose the weights to minimize the error of the approximation.</p><p> 然后优化问题是选择权重，以最小化近似的错误。</p><p>  As an example, let’s say that we wanted to do regression with a basis of quadratic polynomials. Our basis for three input variables might look like</p><p>  作为一个例子，让我们说我们希望以二次多项式的基础进行回归。我们的三个输入变量的基础可能看起来像</p><p>  Any quadratic polynomial in three variables can be written as a linear combination of these basis functions. Also note that if we treat this as the basis of a vector space, then a vector is a tuple of 10 numbers—the ten coefficients in the polynomial. It’s the same as  , just with a different interpretation of what the vector’s entries mean. With that, we can see how we would compute dot products, projections, and other nice things, though they may not have quite the same geometric sensibility.</p><p>  三个变量中的任何二次多项式都可以作为这些基函数的线性组合写入。另请注意，如果我们将其视为矢量空间的基础，则向量是10个数字的元组 - 多项式中的十个系数。它与载体的条目意味着不同的解释相同。有了这个，我们可以看到我们如何计算Dot产品，预测和其他美好事物，尽管它们可能没有相同的几何敏感性。</p><p> These are not the usual basis functions used for polynomial regression in practice (see the note at the end of this article), but we can already do some damage in writing regression algorithms.</p><p> 这些不是在实践中用于多项式回归的通常基础函数（参见本文末尾的音符），但我们已经在写回归算法中已经造成一些损坏。</p><p>  Although there is a  closed form solution to many regression problems (including the quadratic regression problem, though with a slight twist), gradient descent is a simple enough solution to showcase how an optimization solver can find a useful linear combination. This code will be written in Python 3.9.  It’s on Github.</p><p>  虽然有一个封闭的形式解决方案到许多回归问题（包括二次回归问题，但虽然有轻微的扭曲），但梯度下降是一种简单的解决方案来展示如何优化求解器如何找到有用的线性组合。此代码将写入Python 3.9。它在github上。 </p><p>     The  linear_combination function returns a function that computes the weighted sum of the basis functions. Now we can define the error on a dataset, as well as for a single point</p><p>Linear_combination函数返回一个计算基础函数的加权之和的函数。现在我们可以在数据集中定义错误，以及单点</p><p>  We can then define the gradient of the error function with respect to the weights and a single data point. Recall, the error function is defined as</p><p>  然后，我们可以定义相对于权重和单个数据点的误差函数的梯度。回想一下，错误函数定义为</p><p>    Since we’ll do stochastic gradient descent, the error formula is a bit simpler. We compute it not for the whole data set but only a single random point at a time. So the error is</p><p>    由于我们会做随机梯度下降，因此错误公式有点简单。我们计算它不适用于整个数据集，而只有一次只有一个随机点。所以错误是</p><p>  Then we compute the gradient with respect to the individual entries of  , using the chain rule and noting that the only term of the linear combination that has a nonzero contribution to the gradient for   is the term containing  . This is one of the major benefits of using linear combinations: the gradient computation is easy.</p><p>  然后，我们使用链规则计算梯度，使用链规则并注意到为梯度的非零贡献的线性组合的唯一术语是包含的术语。这是使用线性组合的主要好处之一：梯度计算很容易。</p><p>  Another advantage to being linear is that this formula is agnostic to the content of the underlying basis functions. This will hold so long as the weights don’t show up in the formula for the basis functions. As an exercise: try changing the implementation to use  radial basis functions around each data point. (see the note at the end for why this would be problematic in real life)</p><p>  线性的另一个优点是该公式对基础函数的含量不可知。只要权重没有出现在基础函数的公式中，这将保持。作为练习：尝试更改实现以在每个数据点周围使用径向基函数。 （在最终看看为什么这在现实生活中会有问题）</p><p>      Depending on the randomness, it may take a few thousand steps, but it typically converges to an error of &lt; 1. Here’s the plot of error against gradient descent steps.</p><p>      根据随机性，它可能需要几千步，但它通常会收敛到误差的误差; 1.这是对梯度下降步骤的错误的曲线。</p><p>    The real polynomial kernel. We chose a simple set of polynomial functions. This is closely related to the concept of a “kernel”, but the  “real” polynomial kernel uses slightly different basis functions. It scales some of the basis functions by  . This is OK because a linear combination can compensate by using coefficients that are appropriately divided by  . But why would one want to do this? The answer boils down to a computational efficiency technique called the “ Kernel trick.” In short, it allows you to compute the dot product between two linear combinations of vectors in this vector space without explicitly representing the vectors in the space to begin with. If your regression algorithm uses only dot products in its code (as is true of  the closed form solution for regression), you get the benefits of nonlinear feature modeling without the cost of computing the features directly. There’s a lot more mathematical theory to discuss here (cf.  Reproducing Kernel Hilbert Space) but I’ll have to leave it there for now.</p><p>    真正的多项式内核。我们选择了一套简单的多项式函数。这与“内核”的概念密切相关，但“真实”多项式内核使用略微不同的基函数。它缩放了一些基础函数。这是可以的，因为线性组合可以通过使用适当划分的系数来补偿。但为什么有人想这样做？答案归结为一种称为“核心伎俩”的计算效率技术。简而言之，它允许您在该矢量空间中的两个线性组合之间计算点产品，而无需明确表示空间中的向量开始。如果您的回归算法仅在其代码中使用点产品（根据回归的封闭式解决方案而定），则可以获得非线性特征建模的好处，而无需直接计算功能的成本。这里有很多数学理论（参见再现内核希尔伯特空间），但我现在必须在那里留下它。 </p><p> What’s wrong with the radial basis function exercise? This exercise asked you to create a family of basis functions, one for each data point. The problem here is that having so many basis functions makes the linear combination space too expressive. The optimization will  overfit the data. It’s like a lookup table: there’s one entry dedicated to each data point. New data points not in the training would be rarely handled well, since they aren’t in the “lookup table” the optimization algorithm found. To get around this, in practice one would add an extra term to the error corresponding to the L1 or L2 norm of the weight vector. This allows one to ensure that the total size of the weights is small, and in the L1 case that usually corresponds to most weights being zero, and only a few weights (the most important) being nonzero. The process of penalizing the “magnitude” of the linear combination is called  regularization.</p><p>径向基函数锻炼有什么问题？ 此练习要求您创建一个基础函数，一个是每个数据点的基本函数。 这里的问题在于，具有如此多的基础函数使线性组合空间太表达。 优化将过度使用数据。 它就像一个查找表：有一个专用于每个数据点的条目。 不在培训中的新数据点很少处理，因为它们不在“查找表”中找到的优化算法。 为了解决这个问题，在实践中，人们将增加一个额外的术语，以对应于权重向量的L1或L2标准的误差。 这允许人们确保重量的总大小很小，并且在L1情况下通常对应于大多数权重为零，并且只有几个权重（最重要的）是非零的。 惩罚线性组合的“幅度”的过程被称为正则化。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://jeremykun.com/2021/03/29/regression-and-linear-combinations/">https://jeremykun.com/2021/03/29/regression-and-linear-combinations/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/回归/">#回归</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/linear/">#linear</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/函数/">#函数</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>