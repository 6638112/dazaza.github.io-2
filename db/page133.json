[{"category": "", "categoryclass": "", "imagename": "17fc0b65066fe60754f721ca374b8e44.jpg", "infoid": 1067217, "ip": "", "isanchordig": 1, "ischecked": 1, "isdelete": 0, "isneo": 1, "mark": "", "name": "", "note": "\u5efa\u7acb\u4e00\u4e2a\u9762\u5411\u6d88\u8d39\u7684Fintech\u516c\u53f8\u662f\u6602\u8d35\u7684\u3002\u5982\u679c\u60a8\u60f3\u5728\u62e5\u6324\u7684\u516c\u53f8\u62e5\u6324\u7684\u90e8\u95e8\u4e2d\u5efa\u7acb\u4e00\u4e2a\uff0c\u5e76\u4e14\u8d44\u91d1\u4e30\u5bcc\u7684\u521d\u521b\u516c\u53f8\uff0c\u5b83\u53ef\u4ee5\u8d85\u7ea7\u6602\u8d35\u3002\n \u8fd9\u662f\u6211\u4eec\u57282020\u5e74\u4ee3\u540e\u671f\u5b66\u5230\u7684\u8bfe\u7a0b\uff0c\u901a\u8fc7\u68c0\u67e5\u8bb8\u591a\u65b0\u624b\u7684\u7ecf\u8425\u7ed3\u679c\u3002\n Neobanks\u57fa\u672c\u4e0a\u662f\u94f6\u884c\u57fa\u7840\u8bbe\u65bd\u7684\u8f6f\u4ef6\u5c42\uff0c\u63d0\u4f9b\u6d88\u8d39\u8005\u7684\u6570\u5b57\u7b2c\u4e00\uff0c\u79fb\u52a8\u53cb\u597d\u548c\u5f80\u5f80\u7684\u4f4e\u6536\u5de5\u94f6\u884c\u670d\u52a1\u3002\u63a8\u52a8\u91cd\u65b0\u601d\u8003\u6d88\u8d39\u8005\u94f6\u884c\u4e1a\u662f\u4e00\u79cd\u5168\u7403\u52aa\u529b\uff0c\u5728\u57fa\u672c\u4e0a\uff0cNeobanks\u5728\u4f60\u53ef\u4ee5\u60f3\u5230\u7684\u6bcf\u4e2a\u5e02\u573a\u4e0a\u3002\u79c1\u4eba\u6295\u8d44\u8005\u5c55\u793a\u4e86\u6597\u58eb\u7ade\u4e89Neobanks\u7684\u63a8\u51fa\uff0c\u56e0\u4e3a\u4ed6\u4eec\u6709\u53ef\u80fd\u4e3a\u7528\u6237\u63d0\u4f9b\u5b89\u5168 - \u5ba2\u6237 - \u957f\u65f6\u95f4\u4ea7\u751f\u6536\u5165\u3002\n  \u4ea4\u6613\u6240\u63a2\u7d22\u521d\u521b\u516c\u53f8\uff0c\u5e02\u573a\u548c\u91d1\u94b1\u3002\u6bcf\u5929\u65e9\u4e0a\u90fd\u5728\u989d\u5916\u7684\u7d27\u7f29\u6216\u6bcf\u5468\u516d\u83b7\u5f97\u4ea4\u6362\u65f6\u4e8b\u901a\u8baf\u3002\n  \u6295\u8d44\u8005\u8bc1\u660e\u613f\u610f\u5728\u8bb8\u591a\u65b0\u7530\u7eb3\u514b\u65af\u7684\u589e\u957f\u548c\u4ea7\u54c1\u4e2d\u8d44\u52a9\u5de8\u989d\u6295\u8d44\uff0c\u5bfc\u81f4\u72ec\u89d2\u517d\u6025\u5267\u8fd0\u884c\u7ed3\u679c\u3002\u7b80\u800c\u8a00\u4e4b\uff0c\u867d\u7136\u7f8e\u56fd\u6d88\u8d39\u8005Fintech Chime\u62ab\u9732\u4e86\u79ef\u6781\u7684EBITDA  - \u4e00\u4e2a\u8c03\u6574\u540e\u7684\u76c8\u5229\u80fd\u529b\u516c\u5236 - \u8bb8\u591a\u65b0\u7684\u65b0\u53f0\uff0c\u6211\u4eec\u5df2\u7ecf\u770b\u5230\u7684\u6570\u5b57\u5df2\u7ecf\u8bc1\u660e\u4e86\u7f3a\u4e4f\u76c8\u5229\u80fd\u529b\u7684\u8ff9\u8c61\u3002\n  \u4eca\u5929\u65e9\u4e9b\u65f6\u5019\u65e9\u4e9b\u65f6\u5019\u5d07\u62dc\u7684\u9769\u547d\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u516c\u53f8\u6709\u4e00\u4e2a\u6df1\u6df1\u7684\u65e0\u5229\u53ef\u56fe2020.\u4f46\u662f\u5982\u679c\u6211\u4eec\u6316\u6398\u5176\u5b63\u5ea6\u7ed3\u679c\uff0c\u5c31\u4f1a\u627e\u5230\u597d\u6d88\u606f\u3002\u7ebd\u4f2f\u514b\u65af\u7ec8\u4e8e\u53ef\u4ee5\u6210\u672c\u6210\u672c\u7ed3\u6784\u3002\n \u6240\u4ee5\u4eca\u5929\uff0c\u6211\u4eec\u5c06\u89e3\u6790\u5173\u952e\u7684\u6212\u5f8b\u8d22\u52a1\u7ed3\u679c\uff0c\u770b\u770b\u6211\u4eec\u53ef\u4ee5\u4eceStarling\u548cMonzo\u6316\u6398\u7684\u5185\u5bb9\u3002\u4e5f\u8bb8\u6765\u81ea\u7f57\u798f\u7279\u7684\u6709\u70b9\u826f\u597d\u7684\u8d22\u52a1\u65b0\u95fb\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u65b0\u7684\u8bfa\u8d1d\u514b\uff1f \n\u5982\u679c\u60a8\u60f3\u8981\u66f4\u5e7f\u6cdb\u7684\u955c\u5934\uff0c\u6211\u4eec\u81ea\u5df1\u7684Romain Dillet\u5728\u8fd9\u91cc\u5e7f\u6cdb\u770b\u51fa\u7f57\u798f\u7279\u7684\u4e1a\u52a1\u3002 \u6211\u4eec\u76ee\u524d\u53ea\u5173\u5fc3\u5176\u539f\u59cb\u8d22\u52a1\u7ed3\u679c\u3002  \u8fd9\u4e9b\u6570\u5b57\u7684\u4e3b\u65e8\u662f\u8be5\u516c\u53f8\u7684\u6536\u5165\u589e\u957f\u662f\u575a\u5b9e\u7684\uff0c\u4f46\u63d0\u9ad8\u6bdb\u5229\u7387\u5141\u8bb8\u5176\u6bdb\u5229\u4e8e2020\u5e74\u98d9\u5347\u3002 ", "note_en": "Building a consumer-facing fintech company is expensive. And if you want to build one in a sector crowded by both incumbent companies and richly funded startups, it can be  super expensive.\n That was the  lesson we learned in late 2020 by examining operating results from a number of neobanks.\n Neobanks are essentially software layers atop banking infrastructure, offering consumers digital-first, mobile-friendly and often lower-fee banking services. The push to rethink consumer banking is a global effort, with neobanks cropping up in essentially every market you can think of. Private investors have shown up in droves to fund competing neobanks because they have the potential to secure users \u2014 customers \u2014 that generate revenues for long periods of time.\n  The Exchange explores startups, markets and money.   Read it  every morning on Extra Crunch or get  The Exchange newsletter every Saturday.\n  Investors have proven more than willing to fund huge investments in growth and product at many neobanks, leading to steeply negative operating results at the unicorns. In short, while American consumer fintech Chime has  disclosed positive EBITDA \u2014 an adjusted profitability metric \u2014 many neobanks that we\u2019ve seen numbers from have demonstrated a stark inability to paint a path to profitability.\n  Recent results from Revolut that  TechCrunch covered earlier this morning show that the company had a deeply unprofitable 2020. But if we dig into its quarterly results, there\u2019s good news to be found. Neobanks could be maturing into their cost structure at last.\n So today we\u2019ll parse the key Revolut financial results and look at what we can dig up from Starling and Monzo. Perhaps the somewhat good financial news from Revolut is not merely to be found at just one neobank?\n  Our own  Romain Dillet has a broad look at Revolut\u2019s business  here, if you would like a wider lens. We only care about its raw financial results at the moment.\n   The gist of these figures is that the company\u2019s revenue growth was solid, but improving gross margins allowed its gross profit to spike in 2020.", "posttime": "2021-06-21 23:16:04", "source_domain": "slashdot.org", "source_name": "slashdot", "tags": "\u4f11\u606f,neobanks", "title": "\u4e5f\u8bb8\u6bd5\u7adf\u5c3c\u591a\u514b\u65af\u5c06\u4f11\u606f ", "title_en": "\n\t\t\t\tMaybe neobanks will break even after all\t\t\t", "transed": 1, "url": "https://techcrunch.com/2021/06/21/maybe-neobanks-will-break-even-after-all/", "via": "", "real_tags": ["\u4f11\u606f", "neobanks"]}, {"category": "", "categoryclass": "", "imagename": "3326f328847d7f21123b91a7aa35a2c9.jpg", "infoid": 1067216, "ip": "", "isanchordig": 1, "ischecked": 1, "isdelete": 0, "isneo": 1, "mark": "", "name": "", "note": "\u9996\u5c14 - \u5b66\u751f\u4eec\u5728\u9ed8\u9ed8\u5730\u5403\u5348\u9910\uff0c\u7136\u540e\u6536\u96c6\u5728\u4e00\u4e2a\u6697\u6de1\u7684\u623f\u95f4\u91cc\u88c5\u6ee1\u4e86\u9ad8\u529f\u7387\u7684\u7535\u8111\u3002\u5728\u90a3\u91cc\uff0c\u6559\u7ec3\u5e2e\u52a9\u4ed6\u4eec\u5728\u4e00\u4e2a\u6570\u5b57\u5e7b\u60f3\u4e16\u754c\u4e2d\u5b66\u4e60\u4e86\u8d8a\u91ce\u7684\u5bf9\u624b\uff0c\u5145\u6ee1\u4e86\u4f0f\u51fb\u548c\u602a\u7269\u3002\u5b66\u6821\u8fbe\u52305\u70b9\uff0c\u4f46\u4e2a\u4eba\u7ec3\u4e60\u6301\u7eed\u826f\u597d\uff0c\u591c\u665a - \u6240\u6709\u5728\u97e9\u56fd\u8bb8\u591a\u7535\u5b50\u4f53\u80b2\u9662\u6821\u7684\u5b66\u751f\u7684\u8270\u96be\u5de5\u4f5c\u3002\n \u201c\u6211\u6bcf\u5929\u53ea\u7761\u89c9\u4e09\u5230\u56db\u4e2a\u5c0f\u65f6\uff0c\u201dKim Min-Soo\uff0c17\u5c81\u7684\u5b66\u751f\u8bf4\uff0c\u4e00\u540d\u53f3\u624b\u56f4\u7ed5\u7740\u4e00\u6761\u62ec\u53f7\u6765\u51cf\u5c11\u8fd9\u4e48\u591a\u6e38\u620f\u7684\u75db\u82e6\u3002 \u201c\u4f46\u6211\u60f3\u6210\u4e3a\u4e00\u4e2a\u660e\u661f\u3002\u6211\u68a6\u60f3\u7740\u4e00\u4e2a\u4e0e\u7c89\u4e1d\u4e00\u8d77\u5305\u88c5\u7684\u7535\u5b50\u7ade\u6280\u573a\uff0c\u6240\u6709\u4eba\u90fd\u4e3a\u6211\u800c\u751f\u3002\u201c\n \u50cfMin-Soo\u8fd9\u6837\u7684\u5b66\u751f\u5e26\u6765\u4e86\u4e0e\u97e9\u56fd\u6559\u80b2\u76f8\u540c\u7684\u5f3a\u70c8\u7ade\u4e89\u80fd\u6e90\uff0c\u4ee5\u5728\u7535\u5b50\u4f53\u80b2\u9662\u6821\u57f9\u8bad\u3002\u97e9\u56fd\u88ab\u8ba4\u4e3a\u662f\u7535\u5b50\u8fd0\u52a8\u7684\u8bde\u751f\u5730\uff0c\u4f46\u5728\u8be5\u56fd\u7684\u8bb8\u591a\u4eba\u4ecd\u7136\u4ee4\u4eba\u7f9e\u8fb1\u7684\u662f\u9ad8\u5ea6\u9009\u62e9\u6027\u7684\u591a\u4ebf\u7f8e\u5143\u4ea7\u4e1a\u3002\u5b66\u9662\u81f4\u529b\u4e8e\u6539\u53d8\u8fd9\u79cd\u5f62\u8c61\uff0c\u8ba9\u6210\u5343\u4e0a\u4e07\u7684\u5e74\u8f7b\u4eba\u6709\u673a\u4f1a\u5728\u6e38\u620f\u957f\u671f\u88ab\u89c6\u4e3a\u4e00\u79cd\u751f\u6d3b\u65b9\u5f0f\u7684\u5730\u65b9\u8ffd\u6c42\u804c\u4e1a\u751f\u6daf\u3002\n  \u201c\u5728\u97e9\u56fd\uff0c\u73a9\u5bb6\u5fc5\u987b\u5728\u73a9\u6e38\u620f\u4e4b\u524d\u505a\u4f5c\u4e1a\uff0c\u56e0\u4e3a\u5982\u679c\u4ed6\u4eec\u6270\u4e71\u4e86\u4ed6\u4eec\u56e2\u961f\u7684\u6548\u7387\uff0c\u4ed6\u4eec\u5c31\u53ef\u4ee5\u88ab\u9a71\u9010\u51fa\u5934\uff0c\u201d\u97e9\u56fd\u7f8e\u56fd\u7535\u5b50\u6e38\u620f\u5f00\u53d1\u5546\u66b4\u98ce\u96ea\u5a31\u4e50\u97e9\u56fd\u8d1f\u8d23\u4eba\u8868\u793a\uff0c\u4ed6\u4eec\u8bf4\u3002\u201c\u5728\u6700\u8fd1\u5728\u9996\u5c14\u7684\u8bba\u575b\u671f\u95f4\u3002 \u201c\u97e9\u56fd\u6e38\u620f\u73a9\u5bb6\u81f4\u547d\u3002\u201d\n \u5728\u7ebf\u6e38\u620f\u5728\u97e9\u56fd\u8d8a\u65e9\u8d77\u98de\uff0c\u800c\u4e0d\u662f\u4e16\u754c\u4e0a\u4efb\u4f55\u5730\u65b9\u3002\u5f53\u8be5\u56fd\u572820\u4e16\u7eaa90\u5e74\u4ee3\u672b\u5f00\u59cb\u5f15\u5165\u9ad8\u901f\u4e92\u8054\u7f51\u65f6\uff0c\u5b83\u770b\u5230\u4e8624\u5c0f\u65f6\u6e38\u620f\u5496\u5561\u9986\u7684\u6269\u6563\uff0c\u79f0\u4e3aPC Bangs\u3002\n \u8fd9\u4e9b\u9ed1\u6697\uff0c\u5f80\u5f80\u662f\u5730\u4e0b\u7684\u5730\u65b9\u6210\u4e3a\u6e38\u620f\u6587\u5316\u7684\u6e29\u5e8a\uff0c\u6700\u7ec8\u4e3e\u529e\u975e\u6b63\u5f0f\u9526\u6807\u8d5b\u3002\u52302000\u5e74\uff0c\u97e9\u56fd\u6709\u7ebf\u7535\u89c6\u9891\u9053\u662f\u4e16\u754c\u4e0a\u64ad\u653e\u5728\u7ebf\u6e38\u620f\u6bd4\u8d5b\u7684\u4e16\u754c\u7b2c\u4e00\u3002\n \u6839\u636e\u6559\u80b2\u90e8\u53bb\u5e74\u7684\u4e00\u9879\u8c03\u67e5\uff0cE-Sports\u73b0\u5728\u662f\u97e9\u56fd\u5b66\u751f\uff0c\u5728\u8fd0\u52a8\u5458\uff0c\u533b\u751f\uff0c\u6559\u5e08\u548c\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u8005\u4e4b\u540e\u7684\u7b2c\u4e94\u6b21\u6d41\u884c\u7684\u672a\u6765\u5de5\u4f5c\u3002\u8fd9\u5c06\u5f88\u5feb\u6210\u4e3a2022\u5e74\u4e9a\u8fd0\u4f1a\u7684\u4e00\u90e8\u5206\u3002 \n\u50cfLee Sang-hyeok\u8fd9\u6837\u7684\u9876\u7ea7\u7403\u5458\uff0c\u7531\u6e38\u620f\u540d\u79f0\u717d\u52a8\u8005\uff0c\u8d5a\u53d6\u4e0eK-Pop\u5076\u50cf\u4e00\u6837\u591a\u7684\u540d\u58f0\u548c\u8d22\u5bcc\u3002\u6570\u767e\u4e07\u770b\u7740\u4ed6\u4eec\u5728Livestream\u4e0a\u73a9\u800d\u3002\u5728\u5927\u6d41\u884c\u4e4b\u524d\uff0c\u7c89\u4e1d\u5305\u88c5\u6210e-sports arenas\uff0c\u770b\u8d77\u6765\u50cf\u6447\u6eda\u97f3\u4e50\u4f1a\u548c\u4eb2\u6454\u8de4\u4f53\u80b2\u573a\u4e4b\u95f4\u7684\u5341\u5b57\u67b6\u3002\n  \u8bf1\u60d1\u53ef\u80fd\u5f88\u96be\u62b5\u6297\u3002\u7236\u6bcd\u62d6\u7740\u5b69\u5b50\u4eec\u54a8\u8be2\u6e38\u620f\u6210\u763e\u6216\u5eb7\u590d\u8bad\u7ec3\u8425\u3002\u5f53\u5c3d\u804c\u8d23\u4efb\u7684\u76ee\u6807\u8981\u6c42\u514d\u4e8e\u97e9\u56fd\u5f3a\u5236\u6027\u519b\u4e8b\u670d\u52a1\u65f6\uff0c\u5b98\u5458\u5c06\u8c03\u67e5\u4ed6\u4eec\u662f\u5426\u626e\u6f14\u6d89\u53ca\u67aa\u652f\u548c\u66b4\u529b\u7684\u5728\u7ebf\u6e38\u620f\u3002\n \u6210\u7ee9\u79cb\u5929\u3002\u6709\u65f6\u5b66\u751f\u8f8d\u5b66\u5c06\u82b1\u66f4\u591a\u7684\u65f6\u95f4\u6e38\u620f\u3002\u4f46\u73cd\u8d35\u7684\u5c11\u6570\u4eba\u4f1a\u6709\u673a\u4f1a\u8ba9\u5b83\u53d8\u5927\u3002\n \u8fd9\u5ea7\u7279\u8bb8\u7ecf\u8425\u7684\u4e13\u4e1a\u7535\u5b50\u7ade\u6280\u961f\u5728\u97e9\u56fd\u7ade\u4e89\u4f20\u8bf4\u4e2d\u6700\u53d7\u6b22\u8fce\u7684\u6e38\u620f\uff0c\u79df\u7528200\u540d\u7403\u5458\u3002\u90a3\u4e9b\u6ca1\u6709\u5207\u5272\u7684\u4eba\u6709\u5f88\u5c11\u7684\u66ff\u4ee3\u54c1\u3002\n \u7f3a\u4e4f\u826f\u597d\u7684\u6210\u7ee9 - \u901a\u5e38\u9ad8\u4e2d\u6587\u51ed - \u6e38\u620f\u73a9\u5bb6\u4f1a\u53d1\u73b0\u81ea\u5df1\u6709\u9650\u7684\u5de5\u4f5c\u524d\u666f\u3002\u4e0e\u4e00\u4e9b\u7f8e\u56fd\u5927\u5b66\u4e0d\u540c\uff0c\u97e9\u56fd\u5b66\u6821\u4e0d\u6839\u636e\u7535\u5b50\u4f53\u80b2\u6280\u80fd\u63d0\u4f9b\u5f55\u53d6\u3002\n \u5f53\u52a0\u5229\u798f\u5c3c\u4e9a\u5dde\u7684\u7535\u5b50\u4f53\u80b2\u516c\u53f8Gen.G\u57282019\u5e74\u9996\u5c14\u5f00\u8bbe\u4e86Elite Esports Academy\uff0c\u5b83\u5e0c\u671b\u89e3\u51b3\u4e00\u4e9b\u6311\u6218\uff0c\u56e0\u4e3a\u201c\u8fd9\u662f\u5927\u591a\u6570\u4eba\u624d\u6240\u5728\u7684\u6311\u6218\uff0c\u201dJoseph Baek\u8bf4\uff0cGen.g Academy\u7684\u8ba1\u5212\u603b\u76d1\u3002 \u201c\u97e9\u56fd\u4ecd\u88ab\u8ba4\u4e3a\u662f\u7535\u5b50\u8fd0\u52a8\u7684\u9ea6\u52a0\u3002\u201d\n \u5b66\u6821\u8bad\u7ec3\u5e74\u8f7b\u7684\u97e9\u56fd\u4eba\u548c\u5176\u4ed6\u5b66\u751f\u5982\u4f55\u8f6c\u52a8\u4e13\u4e1a\u4eba\u58eb\uff0c\u5e76\u5e2e\u52a9\u6e38\u620fBuff\u627e\u5230\u5783\u573e\uff0c\u8425\u9500\u4eba\u5458\u548c\u6570\u636e\u5206\u6790\u5e08\u7684\u673a\u4f1a\u3002\u5b83\u4e0e\u6559\u80b2\u516c\u53f8Elite Open School\u4e00\u8d77\u5f00\u8bbe\u4e86\u4e00\u4e2a\u4ec5\u9650\u82f1\u8bed\u7684\u8ba1\u5212\uff0c\u4e3a\u5b66\u751f\u63d0\u4f9b\u4e86\u83b7\u5f97\u7f8e\u56fd\u9ad8\u4e2d\u6587\u51ed\u7684\u673a\u4f1a\uff0c\u4ee5\u4fbf\u5728\u7f8e\u56fd\u7533\u8bf7\u7f8e\u56fd\u7684\u5927\u5b66\u5bf9\u7535\u5b50\u4f53\u80b2\u5956\u5b66\u91d1\u3002 \n\u5728\u6700\u8fd1\u4e00\u5929\u65e9\u4e0a\uff0c\u7761\u7720\u8d2b\u56f0\u7684\u9752\u5c11\u5e74\u5f52\u6863\u4e3a\u7cbe\u82f1\u5f00\u653e\u7684\u6821\u957f\u9762\u5177\u548c\u54c1\u724cT\u6064\u548c\u8fde\u5e3d\u886b\u3002\u5206\u4e3a\u54e5\u4f26\u6bd4\u4e9a\uff0cM.I.T\u7b49\u7f8e\u56fd\u5927\u5b66\u4ee5\u514b\u7684\u6559\u5ba4\u3002\u548c\u675c\u514b\uff0c\u4ed6\u4eec\u5b66\u4e60\u82f1\u8bed\uff0c\u7f8e\u56fd\u5386\u53f2\u548c\u5176\u4ed6\u6240\u9700\u7684\u79d1\u76ee\u3002\u6709\u4e9b\u4eba\u6bcf\u5929\u65e9\u4e0a\u4e24\u4e2a\u5c0f\u65f6\u5230\u5b66\u6821\u3002\n \u201c\u6211\u7684\u6311\u6218\u662f\u5982\u4f55\u8ba9\u4ed6\u4eec\u5728\u8bfe\u5802\u4e0a\u4fdd\u6301\u6e05\u9192\u548c\u8ba2\u5a5a\uff0c\u201d\u82f1\u8bed\u8001\u5e08\u8428\u59c6\u82cf\u82cf\u8bf4\u3002\n \u771f\u6b63\u7684\u5de5\u4f5c\u5f00\u59cb\u4e8e\u4e0b\u5348\uff0c\u5f53\u65f6\u4e24\u8f86\u516c\u5171\u6c7d\u8f66\u5c06\u5e74\u8f7b\u7684\u6e38\u620f\u73a9\u5bb6\u5e26\u5230\u4e00\u4e2a\u8c26\u865a\u7684\u6df7\u51dd\u571f\u5efa\u7b51\uff0c\u5728\u5c45\u4f4f\u533a\u8fdb\u884c\u53e6\u4e00\u4e2a\u6fc0\u70c8\u7684\u57f9\u8bad\u8bfe\u7a0b\u3002\n \u6765\u81ea\u6cd5\u56fd\u768422\u5c81\u7684\u524d\u4efbGeng.g\u5b66\u9662\u5b66\u751fAnthony Bazire\u8868\u793a\uff0c\u4ed6\u9009\u62e9\u4e86\u97e9\u56fd\u4f5c\u4e3a\u4ed6\u7684\u8bad\u7ec3\uff0c\u56e0\u4e3a\u4ed6\u77e5\u9053\u8fd9\u4e2a\u56fd\u5bb6\u6709\u4e00\u4e9b\u6700\u597d\u7684\u7403\u5458\u3002\u4eca\u5929\uff0c\u4f20\u8bf4\u4e2d\u7684\u5927\u5956\u8d5b\uff0c\u900f\u6c14\u548c\u661f\u9645\u4e89\u9738II\u5927\u591a\u662f\u97e9\u56fd\u4eba\u3002\n \u201c\u5f53\u4f60\u770b\u5230\u4eba\u4eec\u52aa\u529b\u5de5\u4f5c\u65f6\uff0c\u5b83\u4f1a\u63a8\u52a8\u4f60\u52aa\u529b\u5de5\u4f5c\uff0c\u201d\u4ed6\u8bf4\u3002\n  Gen.g\u8ba1\u5212\uff0c\u5b83\u5728\u97e9\u56fd\u7684\u7b2c\u4e00\u4e2a\u65b9\u6848\u751a\u81f3\u5e2e\u52a9\u4e86\u4e00\u4e9b\u5b66\u751f\u8bf4\u670d\u7236\u6bcd\u4ed6\u4eec\u505a\u51fa\u4e86\u806a\u660e\u7684\u804c\u4e1a\u53d1\u5c55\u3002\n 2019\u5e74\uff0c\u4ed6\u5728\u9ad8\u4e2d\u7684\u7b2c\u4e8c\u5e74\uff0c\u91d1\u8d6b\u7fc1yeong\u6bcf\u5929\u64ad\u653e\u4f20\u8bf410\u5c0f\u65f6\u3002\u4ed6\u7684\u6280\u80fd\u968f\u7740\u4ed6\u901a\u8fc7\u6570\u5b57\u5e7b\u60f3\u4e16\u754c\u7684\u65b9\u5f0f\u800c\u6539\u5584\u3002\u90a3\u4e2a\u590f\u5929\uff0c\u4ed6\u51b3\u5b9a\u6210\u4e3a\u4e00\u4e2a\u4e13\u4e1a\u8fd0\u52a8\u5458\u548c\u8f9e\u804c\u7684\u5b66\u6821\u3002 \n\u201c\u6211\u7684\u7236\u6bcd\u5b8c\u5168\u53cd\u5bf9\u5b83\uff0c\u201d\u91d1\u5148\u751f\uff0c19\u5c81\u3002\u201c\u6211\u544a\u8bc9\u4ed6\u4eec\u6211\u4f1a\u9057\u61be\uff0c\u56e0\u4e3a\u8fd9\u662f\u6211\u60f3\u8981\u5728\u6211\u751f\u547d\u4e2d\u5c1d\u8bd5\u7684\u4e00\u4ef6\u4e8b\uff0c\u6254\u8fdb\u6211\u6240\u62e5\u6709\u7684\u4e00\u5207\u3002\u201d\n \u4ed6\u7684\u6bcd\u4eb2Lee Ji-Eun\uff0c46\u5c81\uff0c\u5979\u8eba\u5728\u5e8a\u4e0a\u547b\u541f\u7740\u3002\u674e\u5973\u58eb\u6700\u7ec8\u51b3\u5b9a\u5728\u4ed6\u95ee\u5979\u6709\u4e00\u5929\u4e4b\u540e\u652f\u6301\u5979\u7684\u513f\u5b50\uff1a\u201c\u5988\u5988\uff0c\u5f53\u4f60\u662f\u6211\u7684\u5e74\u9f84\u65f6\uff0c\u4f60\u6709\u4ec0\u4e48\u68a6\u60f3\uff1f\u4f60\u6709\u6ca1\u6709\u751f\u6d3b\u8fc7\u8fd9\u4e2a\u68a6\u60f3\uff1f\u201c\n  \u91d1\u5148\u751f\u7814\u7a76\u4e86Gen.G\u8ba1\u5212\uff0c\u8be5\u8ba1\u5212\u6bcf\u5e74\u536025,000\u7f8e\u5143\uff0c\u5e76\u4f7f\u4ed6\u7684\u6bcd\u4eb2\u7ed9\u5b66\u9662\u8bf4\u670d\u5979\uff0c\u4ed6\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u540d\u7535\u5b50\u7ade\u6280\u4e13\u4e1a\u4eba\u58eb\u627e\u5230\u6210\u529f\u3002\u4ed6\u901a\u8fc7\u57fa\u4e8e\u4ed6\u7684\u7f51\u7edc\u6e38\u620f\u6280\u80fd\u8fdb\u5165\u80af\u5854\u57fa\u5927\u5b66\uff0c\u901a\u8fc7\u8d62\u5f97\u5f55\u53d6\u6765\u6e05\u9664\u4eca\u5e74\u7684\u68a6\u60f3\u5bf9\u4ed6\u7684\u68a6\u60f3\u8fdb\u884c\u4e86\u4e00\u5927\u969c\u788d\u3002\n \u6cd5\u56fd\u6e38\u620f\u73a9\u5bb6Bazire\u5148\u751f\u52a0\u5165\u4e863\u6708\u4efd\u4f5c\u4e3a\u4e00\u540d\u5b9e\u4e60\u8fd0\u52a8\u5458\u7684Legends of Legends\u56e2\u961f\u3002\u4ed6\u548c\u5176\u4ed6\u5b66\u5458\u5728\u9996\u5c14\u7684\u5171\u540c\u516c\u5bd3\u4e2d\u83b7\u5f97\u4e86\u8c26\u865a\u7684\u5de5\u8d44\u548c\u98df\u7269\u548c\u4f4f\u5bbf\u3002\u4ed6\u8bf4\uff0c\u4ed6\u4eec\u6bcf\u5929\u7ec3\u4e60\u957f\u8fbe18\u5c0f\u65f6\uff0c\u6bd4\u4ed6\u5728\u6cd5\u56fd\u77e5\u9053\u7684\u6bd4\u8d5b\u8005\u591a60\uff05\u81f370\uff05\u3002\n \u4f46\u6210\u4e3a\u4e00\u540d\u5b9e\u4e60\u751f\u7684\u65f6\u95f4\u4e0d\u4ec5\u4ec5\u662f\u56fa\u5b9a\u811a\u8dbe\u3002\u5b66\u5458\u5fc5\u987b\u5feb\u901f\u5730\u901a\u8fc7\u7b2c\u4e8c\u90e8\u95e8\u5230\u4e3b\u8981\u8054\u76df\uff0c\u5176\u4e2d\u4e13\u4e1a\u7684\u4f20\u8bf4\u7403\u5458\u6bcf\u5e74\u652f\u4ed82\u4e07\u7f8e\u5143\u7684\u5e73\u5747\u5de5\u8d44\uff0c\u5956\u91d1\u548c\u8d5e\u52a9\u4f18\u60e0\u3002\n \u968f\u7740\u5e74\u8f7b\u548c\u4e91\u5cf0\u7684\u5929\u8d4b\u4e0d\u65ad\u8d76\u4e0a\uff0c\u5728\u97e9\u56fd\u6700\u7ec826\u5c81\u4e4b\u524d\uff0c\u5927\u591a\u6570\u7535\u5b50\u8fd0\u52a8\u8fd0\u52a8\u5458\u7684\u804c\u4e1a\u751f\u6daf\u7ed3\u675f\uff0c\u5468\u56f4\u97e9\u56fd\u7537\u5b50\u572820\u591a\u5c81\u7684\u65f6\u5019\u611f\u53d7\u5230\u5f00\u59cb\u4ed6\u4eec\u7684\u5f3a\u5236\u6027\u519b\u4e8b\u670d\u52a1\u3002\n Min-Soo\uff0c\u68a6\u60f3\u6210\u4e3a\u7535\u5b50\u8fd0\u52a8\u660e\u661f\u7684\u5b66\u751f\uff0c\u9996\u5148\u5728\u4e2d\u5b66\u65f6\u611f\u53d7\u5230\u7535\u5b50\u4f53\u80b2\u821e\u53f0\u7684\u7535\u6c14\u5316\u6c1b\u56f4\u3002\u81ea2019\u5e74\u4ee5\u6765\uff0c\u4ed6\u6bcf\u5929\u4e0a\u53486\u70b9\u9192\u6765\uff0c\u6bcf\u5929\u90fd\u6709\u4e24\u5c0f\u65f6\u7684\u516c\u5171\u6c7d\u8f66\u548c\u5730\u94c1\u5230Gen.G\u5b66\u9662\u3002\u4ed6\u5728\u4e0b\u534811:30\u56de\u5230\u5bb6\u3002\u7136\u540e\u505a\u66f4\u591a\uff0c\u5f88\u5c11\u5728\u4e0a\u53483\u70b9\u4e4b\u524d\u7761\u89c9\u3002 \n\u4eca\u5e74\uff0c\u4ed6\u7ec8\u4e8e\u88ab\u8ba4\u4e3a\u8db3\u591f\u597d\uff0c\u5f00\u59cb\u8003\u8bd5\u6210\u4e3a\u4e13\u4e1a\u56e2\u961f\u7684\u5b9e\u4e60\u751f\u3002  \u201c\u8fd9\u662f\u4e00\u4e2a\u8270\u96be\u800c\u5b64\u72ec\u7684\u751f\u6d3b\uff0c\u56e0\u4e3a\u4f60\u5fc5\u987b\u653e\u5f03\u5176\u4ed6\u4e00\u5207\uff0c\u5c31\u50cf\u670b\u53cb\u4e00\u6837\uff0c\u201d\u4ed6\u8bf4\u3002 \u201c\u4f46\u6211\u6700\u5f00\u5fc3\uff0c\u56e0\u4e3a\u6211\u6b63\u5728\u505a\u6211\u6700\u559c\u6b22\u7684\u4e8b\u60c5\u3002\u201d ", "note_en": "SEOUL \u2014 The students ate lunch in silence before gathering in a dimly lit room packed with high-powered computers. There, coaches helped them learn to outmaneuver opponents in a digital fantasy world fraught with ambushes and monsters. School was over by 5 p.m., but individual practice continued well into the night \u2014 all in a hard day\u2019s work for the students at one of South Korea\u2019s many e-sports academies.\n \u201cI sleep only three or four hours a day,\u201d said Kim Min-soo, 17, a student who wore a brace around his right hand to lessen the pain from so much gaming. \u201cBut I want to become a star. I dream of an e-sports arena packed with fans all rooting for me.\u201d\n Students like Min-soo have brought the same intense competitive energy often associated with South Korean education to their training at e-sports academies. South Korea is considered a birthplace of e-sports, but the highly selective multibillion-dollar industry is still frowned upon by many in the country. The academies have worked to change that image and give thousands of young people a chance to pursue careers in a place where gaming has long been seen as a way of life.\n  \u201cIn South Korea, players must do homework on their game before playing it, because if they disrupt the efficiency of their \u200bteam, they can be expelled,\u201d said Jeon Dong-jin, Korea head of the American video game developer Blizzard Entertainment, during  a recent forum in Seoul. \u201cSouth Korean gamers are deadly serious.\u201d\n Online gaming took off sooner and faster in South Korea than anywhere else in the world. When the country began introducing high-speed internet in the late 1990s, it saw the proliferation of 24-hour gaming cafes called PC bangs.\n These dark, often underground parlors became hotbeds for gaming culture, eventually hosting informal tournaments. By 2000, South Korean cable channels were the first in the world to broadcast online gaming competitions.\n E-sports is now the fifth-most popular future job among South Korean students, after athletes, doctors, teachers and digital content creators, according to a survey by the Education Ministry last year. It will soon be a part of the Asian Games in 2022.\n Top players like  Lee Sang-hyeok, who goes by the gaming name Faker, earn as much fame and fortune as K-pop idols. Millions watch them play over livestream. Before the pandemic, fans packed into  e-sports arenas that looked like a cross between a rock concert and pro-wrestling stadium.\n  The allure can be hard to resist. Parents have dragged children to counseling for  gaming addiction or to rehabilitation  boot camps. When  conscientious objectors ask to be exempted from South Korea\u2019s mandatory military service, officials will investigate whether they play online games involving guns and violence.\n Grades fall. Sometimes students drop out of school to spend more time gaming. Yet precious few will get the chance to make it big.\n The 10 franchised professional e-sports teams in South Korea competing in League of Legends, the most popular game here, hire only 200 players total. Those who do not make the cut have few alternatives.\n Lacking good grades \u2014 and often high school diplomas \u2014 gamers will find themselves with limited job prospects. And unlike some American universities, South Korean schools do not offer admission based on e-sports skills.\n When  Gen.G, a California-based e-sports company, opened its  Gen.G Elite Esports Academy in Seoul in 2019, it wanted to address some of those challenges because \u201cthis is where most of the talent is,\u201d said Joseph Baek, program director at the Gen.G academy. \u201cSouth Korea is still considered the mecca of e-sports.\u201d\n The school trains young South Koreans and other students on how to turn pro and helps gaming buffs find opportunities as streamers, marketers and data analysts. Together with the educational company  Elite Open School, it opened an English-only program that offers students a chance to earn an American high school diploma so they can apply to universities in the United States on e-sports scholarships.\n  On a recent morning, the sleep-deprived teenagers filed into Elite Open School wearing masks and branded T-shirts and hoodies. Divided into classrooms named after American universities like Columbia, M.I.T. and Duke, they studied English, American history and other required subjects. Some commuted two hours each morning to school.\n \u201c\u200bMy challenge is how to keep them awake and engaged during class,\u201d said Sam Suh, an English teacher.\n The real work began in the afternoon, when two buses carried the young gamers to a modest concrete building in a residential area for another intense training session at the Gen.G academy.\n Anthony Bazire, a 22-year-old former Gen.G academy student from France, said he had chosen South Korea as his training ground because he knew the country had some of the best players. Today, top prize winners in  League of Legends,  Overwatch and  StarCraft II are mostly South Koreans.\n \u201cWhen you see people working hard, it pushes you to work hard,\u201d he said.\n  The Gen.G program, the first of its kind in South Korea, has even helped some students convince their parents that they made a smart career move.\n In 2019, his second year in high school, Kim Hyeon-yeong played League of Legends for 10 hours a day. \u200bHis skills improved as he romped his way through the digital fantasy world\u200b. That summer, he decided to become a pro e-sports player, and quit school.\n \u201cMy parents were totally against it,\u201d said Mr. Kim, 19. \u201cI told them that I would have no regrets, because this was the one thing I wanted to try in my life, throwing in everything I got.\u201d\n His mother, Lee Ji-eun, 46, was so distressed that she lay in bed moaning. Ms. Lee eventually decided to support her son after he asked her one day: \u201cMom, what dream did you have when you were my age? Have you lived that dream?\u201d\n  Mr. Kim researched the Gen.G program, which costs $25,000 a year, and led his mother to the academy to convince her that he could find success as an e-sports professional. He cleared a big hurdle to his dream this year by winning admission, based on his online game skills, into the University of Kentucky.\n Mr. Bazire, the French gamer, joined Gen.G\u2019s League of Legends team as a trainee player in March. He and other trainees receive modest salaries along with food and lodging at a shared apartment in Seoul. They practice up to 18 hours a day, 60 to 70 percent more than players he knew in France, he said.\n But becoming a trainee is little more than securing a toehold. Trainees must climb fast through the second division to the main league, where professional League of Legends players are paid an average salary of $200,000 a year, and prize money and sponsorship deals.\n With younger and nimbler talents catching up constantly, the careers of most e-sports athletes in South Korea end before they turn 26, around the time when Korean men in their late 20s feel pressure to begin their mandatory military service.\n Min-soo, the student who dreams of becoming an e-sports star, first felt the electrifying vibe of an e-sports arena when he was in middle school. Since 2019, he has woken up at 6 a.m. every day, taking a two-hour bus and subway ride to the Gen.G academy. He returns home at 11:30 p.m. and then practices more, seldom going to bed before 3 a.m.\n This year, he was finally considered good enough to start taking tests to become a trainee on a pro team.\n \u201cIt\u2019s a hard and lonely life, because you have to give up everything else, like friends,\u201d he said. \u201cBut I am happiest because I am doing what I enjoy the most.\u201d", "posttime": "2021-06-21 23:06:38", "source_domain": "slashdot.org", "source_name": "slashdot", "tags": "sports,\u97e9\u56fd,rise,\u6e38\u620f", "title": "\u770b\u770b\u97e9\u56fd\u7ade\u6280\u573a\u7684\u5de8\u5927\u5d1b\u8d77\uff0c\u6210\u5343\u4e0a\u4e07\u7684\u5e74\u8f7b\u4eba\u7ade\u4e89\u4e8e\u4e13\u4e1a\uff0c\u5e76\u52aa\u529b\u5728\u8be5\u884c\u4e1a\u4e2d\u521b\u9020\u53ef\u6301\u7eed\u5de5\u4f5c ", "title_en": "A look at the huge rise of esports in South Korea, where thousands of young people compete to go pro, and efforts to create sustainable jobs in the industry", "transed": 1, "url": "https://www.nytimes.com/2021/06/19/world/asia/south-korea-esports.html", "via": "", "real_tags": ["sports", "\u97e9\u56fd", "rise", "\u6e38\u620f"]}, {"category": "", "categoryclass": "", "imagename": "cda9e9d41d93a93666ef59f881ad4bb7.png", "infoid": 1067215, "ip": "", "isanchordig": 1, "ischecked": 1, "isdelete": 0, "isneo": 1, "mark": "", "name": "", "note": "Shopify\uff0c\u5b83\u901a\u8fc7\u5411\u5546\u5bb6\u63d0\u4f9b\u5728\u7ebf\u5de5\u5177\u5df2\u7ecf\u6210\u4e3a\u7535\u5b50\u5546\u52a1\u7684\u4e3b\u8981\u529b\u91cf\uff0c\u63a8\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u670d\u52a1\uff0c\u5141\u8bb8\u5185\u5bb9\u53d1\u5e03\u8005\u94fe\u63a5\u5230\u5546\u5e97\u7f51\u7edc\u4e2d\u7684\u5546\u5bb6\uff0c\u6839\u636e\u4eba\u4eec\u6362\u53d6\u4ea4\u6613\u6536\u5165\u7684\u524a\u51cf\u719f\u6089\u8fd9\u79cd\u60c5\u51b5\u3002\n \u65b0\u751f\u8fdb\u5165\u88ab\u79f0\u4e3a\u8054\u76df\u8425\u9500\u7684\u533a\u57df\u65e8\u5728\u4e3a\u4e9a\u9a6c\u900a\u548c\u6c83\u5c14\u739b\u7b49\u4e92\u8054\u7f51\u96f6\u552e\u5de8\u5934\u63d0\u4f9b\u7684\u7535\u5b50\u5546\u52a1\u5408\u4f5c\u4f19\u4f34\u63d0\u4f9b\u51fa\u7248\u5546\u548c\u96f6\u552e\u5546\uff0c\u8fd9\u4e24\u8005\u90fd\u6709\u5f88\u957f\u7684\u7f51\u7ad9\uff0c\u8fd9\u4e9b\u7f51\u7ad9\u5c06\u4e1a\u52a1\u53d1\u9001\u4ed6\u4eec\u7684\u4e00\u5c0f\u90e8\u5206\u90a3\u4e9b\u9500\u552e\u3002 BuzzFeed\u5df2\u7ecf\u7b7e\u7f72\u4e86\u65b0\u8ba1\u5212\u5e76\u5c06\u5176\u96c6\u6210\u5230\u5176\u73b0\u573a\u3002\u719f\u6089\u6b64\u4e8b\u7684\u4eba\u4eec\u719f\u6089\uff0cShopify\u5df2\u7ecf\u63a5\u8fd1\u4e86\u4e00\u4e9b\u5176\u4ed6\u6570\u5b57\u5a92\u4f53\u516c\u53f8\uff0c\u5305\u62ec\u590d\u6742\u7684\u7f51\u7edc\u548cVOX\u5a92\u4f53\u3002\n    Shopify\uff0c\u5b83\u901a\u8fc7\u5411\u5546\u5bb6\u63d0\u4f9b\u5728\u7ebf\u5de5\u5177\u5df2\u7ecf\u6210\u4e3a\u7535\u5b50\u5546\u52a1\u7684\u4e3b\u8981\u529b\u91cf\uff0c\u63a8\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u670d\u52a1\uff0c\u5141\u8bb8\u5185\u5bb9\u53d1\u5e03\u8005\u94fe\u63a5\u5230\u5546\u5e97\u7f51\u7edc\u4e2d\u7684\u5546\u5bb6\uff0c\u6839\u636e\u4eba\u4eec\u6362\u53d6\u4ea4\u6613\u6536\u5165\u7684\u524a\u51cf\u719f\u6089\u8fd9\u79cd\u60c5\u51b5\u3002\u65b0\u751f\u8fdb\u5165\u79f0\u4e3a\u8054\u76df\u8425\u9500\u7684\u533a\u57df\u65e8\u5728\u63d0\u4f9b\u53d1\u5e03\u5546\u548c... ", "note_en": "Shopify, which has become a major force in e-commerce by providing online tools to merchants, is rolling out a new service that will allow content publishers to link to merchants in the Shopify network in exchange for a cut of transaction revenue, according to people familiar with the situation.\n The nascent move into an area known as affiliate marketing is designed to offer publishers and retailers looking for an e-commerce partner an alternative to internet retail giants like Amazon and Walmart, both of which have long given websites that send business their way a small portion of those sales. BuzzFeed already has signed on to the new program and integrated it into its site. Shopify has approached a number of other digital media companies, including Complex Networks and Vox Media, people familiar with the matter said.\n    Shopify, which has become a major force in e-commerce by providing online tools to merchants, is rolling out a new service that will allow content publishers to link to merchants in the Shopify network in exchange for a cut of transaction revenue, according to people familiar with the situation. The nascent move into an area known as affiliate marketing is designed to offer publishers and...", "posttime": "2021-06-21 23:05:53", "source_domain": "slashdot.org", "source_name": "slashdot", "tags": "\u4ea4\u6613,shopify,\u63d0\u4f9b", "title": "\u6765\u6e90\uff1aShopify\u6b63\u5728\u63a8\u51fa\u4e00\u9879\u670d\u52a1\uff0c\u8ba9Buzzfeed\u548c\u5176\u4ed6\u5185\u5bb9\u51fa\u7248\u5546\u94fe\u63a5\u5230\u5546\u5bb6\u4ee5\u6362\u53d6\u4ea4\u6613\u6536\u5165\u7684\u524a\u51cf ", "title_en": "Sources: Shopify is rolling out a service that will let BuzzFeed and other content publishers link to merchants in exchange for a cut of transaction revenue", "transed": 1, "url": "https://www.theinformation.com/articles/shopify-seeks-to-challenge-amazon-through-deals-with-buzzfeed-other-sites", "via": "", "real_tags": ["\u4ea4\u6613", "shopify", "\u63d0\u4f9b"]}, {"category": "", "categoryclass": "", "imagename": "a15c83e7e8abc15a76ac2b5c4fb126b3.png", "infoid": 1067214, "ip": "", "isanchordig": 1, "ischecked": 1, "isdelete": 0, "isneo": 1, "mark": "", "name": "", "note": "\uff08\u8def\u900f\u793e\uff09 - \u7531\u4ebf\u4e07\u5bcc\u7fc1\u6295\u8d44\u8005\u4e39\u5c3c\u5c14\u00b7\u52d2\u5e03\u7684\u5bf9\u51b2\u57fa\u91d1\u7b2c\u4e09\u70b9\u652f\u6301\u7684\u7f51\u7edc\u5b89\u5168\u516c\u53f8Sentinelone Inc\uff0c\u65e8\u5728\u6839\u636e\u5468\u4e00\u7684\u76d1\u7ba1\u7533\u8bf7\uff0c\u4f30\u503c\u8d85\u8fc770\u4ebf\u7f8e\u5143\u7684\u7f8e\u56fd\u9996\u6b21\u516c\u5f00\u53d1\u884c\uff08IPO\uff09\u3002\n Sentinelone\uff0c\u5176\u5176\u4ed6\u6295\u8d44\u8005\u5305\u62ec\u98ce\u9669\u6295\u8d44\u8d44\u672c\u652f\u6301\u864e\u5168\u7403\uff0c\u7ea2\u6749\u8d44\u672c\u548c\u6d1e\u5bdf\u529b\u4f19\u4f34\uff0c\u8ba1\u5212\u51fa\u552e3200\u4e07\u80a1\uff0c\u6bcf\u80a126\u7f8e\u5143\u81f329\u7f8e\u5143\uff0c\u5728\u8be5\u8303\u56f4\u7684\u9876\u7aef\u7b79\u96c6\u4e869.28\u4ebf\u7f8e\u5143\u3002\n SentineLone\u6210\u7acb\u4e8e2013\u5e74\uff0c\u901a\u8fc7\u4f7f\u7528\u4eba\u5de5\u667a\u80fd\u6280\u672f\u6765\u4fdd\u62a4\u7b14\u8bb0\u672c\u7535\u8111\u548c\u624b\u673a\u514d\u53d7\u5b89\u5168\u6f0f\u6d1e\uff0c\u4ee5\u786e\u5b9a\u4f01\u4e1a\u7f51\u7edc\u4e2d\u7684\u5f02\u5e38\u884c\u4e3a\u3002\u7531\u4e8e\u5927\u591a\u6570\u5458\u5de5\u5728Covid-19\u5927\u6d41\u884c\u671f\u95f4\u5f00\u59cb\u5728\u5bb6\u5de5\u4f5c\uff0c\u5176\u4e1a\u52a1\u6709\u6240\u63a8\u52a8\u3002\n  \u5c71\u666f\uff0c\u57fa\u4e8e\u52a0\u5229\u798f\u5c3c\u4e9a\u5dde\u7684\u5c71\u666f\uff0c\u4ece\u5305\u62ec\u864e\u5168\u7403\u548c\u7ea2\u6749\u7684\u6295\u8d44\u8005\u7b79\u96c6\u4e862.67\u4ebf\u7f8e\u5143\uff0c\u4f30\u503c\u8d85\u8fc730\u4ebf\u7f8e\u5143\uff0c\u51e0\u4e4e\u4e09\u500d\u4e8e2020\u5e74\u7684\u4ef7\u503c\u3002\n Sentinelone\u5728\u7533\u8bf7\u4e2d\u8868\u793a\uff0c\u4e0eTiger Global\uff0cInsight Fordure Partners\uff0c\u7b2c\u4e09\u70b9\u4f01\u4e1a\u548cSequoia Capital\u7684\u7b2c\u4e09\u70b9\u4f01\u4e1a\u548cSequoia Capital\u96b6\u5c5e\u4e8e\u8d2d\u4e70\u4e86\u4e00\u4e9b\u80a1\u7968\u7684\u80a1\u7968\uff0c\u603b\u4ef7\u7ea6\u4e3a5000\u4e07\u7f8e\u5143\u3002\n Sentinelone\u7684IPO\u8ba1\u5212\u662f\u534e\u5c14\u8857\u7684\u80a1\u7968\u5e02\u573a\u7684\u5f55\u5236\u7ade\u8d5b\uff0c\u6d6e\u9009\u663e\u793a\u6ca1\u6709\u901f\u5ea6\u653e\u7f13\u7684\u8ff9\u8c61\u3002\u6839\u636eDealoGic\u7684\u6570\u636e\uff0c\u5728\u4eca\u5e74\u7684\u7ed3\u675f\uff0c\u7f8e\u56fdIPO\u5df2\u7ecf\u603b\u8ba11710\u4ebf\u7f8e\u5143\uff0c\u7f8e\u56fdIPO\u5df2\u7ecf\u8fbe\u5230\u4e86171\u4ebf\u7f8e\u5143\uff0c\u9eef\u7136\u5931\u8272\u52df\u80a1\uff0c\u8be5\u65e5\u5e38\u7eaa\u5f55\u4e3a1.68\u4ebf\u7f8e\u5143\u3002\n  Sentinelone\u5c06\u5728\u7b26\u53f7\u201cS\u201d\u4e0b\u5217\u51fa\u7ebd\u7ea6\u8bc1\u5238\u4ea4\u6613\u6240\u7684\u80a1\u7968\u3002 ", "note_en": "(Reuters) - Cybersecurity firm SentinelOne Inc, backed by billionaire investor Daniel Loeb\u2019s hedge fund Third Point, is aiming for a valuation of over $7 billion in its U.S. initial public offering (IPO), according to a regulatory filing on Monday.\n SentinelOne, whose other investors include venture capital backers Tiger Global, Sequoia Capital and Insight Venture Partners, plans to sell 32 million shares priced at between $26 and $29 per share, raising $928 million at the top end of the range.\n Founded in 2013, SentinelOne protects laptops and mobile phones from security breaches by using artificial intelligence technology to identify unusual behavior in enterprise networks. Its business had a boost as most employees started working from home during the COVID-19 pandemic.\n  The Mountain View, California-based company raised $267 million in November from investors including Tiger Global and Sequoia at a valuation of more than $3 billion, almost three times what it was valued in February 2020.\n Entities affiliated with Tiger Global, Insight Venture Partners, Third Point Ventures and Sequoia Capital, have separately agreed to purchase a number of shares with an aggregate price of about $50 million, SentinelOne said in the filing.\n SentinelOne\u2019s IPO plans comes as Wall Street\u2019s record-breaking run for stock market flotations shows no sign of slowing down. With more than six months until the year ends, U.S. IPOs have already totaled $171 billion, eclipsing the 2020 record of $168 billion, according to data from Dealogic.\n  SentinelOne will list its stock on the New York Stock Exchange under the symbol \u201cS\u201d.", "posttime": "2021-06-21 23:05:36", "source_domain": "slashdot.org", "source_name": "slashdot", "tags": "\u4ea4\u6613,\u7f51\u7edc\u5b89\u5168,floats,sentinelone", "title": "\u5f52\u6863\uff1a\u7f51\u7edc\u5b89\u5168\u516c\u53f8Sentinelone\u5c06\u4ee526-29\u7f8e\u5143\u7684\u4ef7\u683c\u51fa\u552e3200\u4e07\u7f8e\u5143\uff0c\u7784\u51c67B +\u4f30\u503c\u5e76\u5728\u9876\u7aef\u7b79\u96c692.8\u7c73\uff0c\u5f53\u5b83\u6f02\u6d6e\u5728\u7ebd\u7ea6\u8bc1\u5238\u4ea4\u6613\u6240 ", "title_en": "Filing: cybersecurity company SentinelOne will sell 32M shares at $26-$29, aiming for a $7B+ valuation and raising $928M at the top end, when it floats on NYSE", "transed": 1, "url": "https://www.reuters.com/article/us-sentinelone-ipo-idUSKCN2DX14I", "via": "", "real_tags": ["\u4ea4\u6613", "\u7f51\u7edc\u5b89\u5168", "floats"]}, {"category": "", "categoryclass": "", "imagename": "f1fb5f5de744f8e6aa9e74825c969084.png", "infoid": 1067213, "ip": "", "isanchordig": 1, "ischecked": 1, "isdelete": 0, "isneo": 1, "mark": "", "name": "", "note": "\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\uff06\uff0339; LL\u5c55\u793a\u4e86\u5982\u4f55\u5fae\u8c03\u4e24\u79cd\u4e0d\u540c\u7684\u53d8\u538b\u5668\u6a21\u578b\uff0cBERT\u548cDistilbert\uff0c\u4e24\u4e2a\u4e0d\u540c\u7684NLP\u95ee\u9898\uff1a\u60c5\u7eea\u5206\u6790\u548c\u91cd\u590d\u7684\u95ee\u9898\u68c0\u6d4b\u3002\n \u60a8\u53ef\u4ee5\u5728\u6211\u4eec\u7684COLAB\u7b14\u8bb0\u672c\u4e2d\u770b\u5230\u4e00\u4e2a\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528HuggingFace\u4e0a\u7684\u57f9\u8bad\u6a21\u578b\u3002\u8ba9\uff06\uff0339;\u8df3\u5165\uff01\n  \u7531\u4e8e\u9996\u5148\u5728\u5173\u6ce8\u4e2d\u5f00\u53d1\u548c\u53d1\u5e03\u7684\u662f\uff0c\u60a8\u9700\u8981\u7eb8\u5f20\u53d8\u538b\u5668\u5df2\u5b8c\u5168\u91cd\u65b0\u5b9a\u4e49\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\uff08NLP\uff09\u5728\u4f17\u591a\u4efb\u52a1\u4e0a\u8bbe\u7f6e\u6700\u5148\u8fdb\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u95ee\u9898\u5e94\u7b54\uff0c\u8bed\u8a00\u751f\u6210\u548c\u547d\u540d - \u5b9e\u4f53\u8bc6\u522b\u3002\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u8d62\u5f97\u4e86\uff06\uff0339; t\uff06\uff0339;\u5bf9\u4e8e\u4e00\u4e2a\u53d8\u538b\u5668\u662f\u4ec0\u4e48\uff0c\u800c\u662f\u5982\u4f55\u7533\u8bf7\u548c\u8bad\u7ec3\u4ed6\u4eec\u6765\u5e2e\u52a9\u5b9e\u73b0\u4e00\u4e9b\u4efb\u52a1\u3002\u5728\u6982\u5ff5\u4e0a\u81f4\u529b\u4e8e\u53d8\u5f62\u91d1\u521a\u7684\u4e3b\u8981\u4e8b\u9879\u662f\u4ed6\u4eec\u771f\u7684\u64c5\u957f\u5904\u7406\u987a\u5e8f\u6570\u636e\uff08\u6587\u672c\uff0c\u8bed\u97f3\u7b49\uff09\uff0c\u5b83\u4eec\u5145\u5f53\u7f16\u7801\u5668 - \u89e3\u7801\u5668\u6846\u67b6\uff0c\u5176\u4e2d\u6570\u636e\u88ab\u6620\u5c04\u5230\u7f16\u7801\u5668\u4e4b\u524d\u7684\u67d0\u4e9b\u4ee3\u8868\u7a7a\u95f4\u7136\u540e\u901a\u8fc7\u89e3\u7801\u5668\u6620\u5c04\u5230\u8f93\u51fa\uff0c\u5e76\u4e14\u5b83\u4eec\u975e\u5e38\u826f\u597d\u5730\u6269\u5c55\u5230\u5e76\u884c\u5904\u7406\u786c\u4ef6\uff08GPU\uff09\u3002\n \u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u53d8\u538b\u5668\u5df2\u7ecf\u63a5\u53d7\u5927\u91cf\u6587\u672c\u6570\u636e\u57f9\u8bad\uff0c\u5176\u5141\u8bb8\u4ed6\u4eec\u975e\u5e38\u4e86\u89e3\u8bed\u8a00\u7684\u8bed\u6cd5\u548c\u8bed\u4e49\u3002\u4f8b\u5982\uff0c\u5728BookScorpus\u57f9\u8bad\u4e86\u901a\u8fc7\u751f\u6210\u9884\u57f9\u8bad\u63d0\u9ad8\u8bed\u8a00\u7406\u89e3\u7684\u539f\u59cbGPT\u6a21\u578b\uff0c\u8d85\u8fc77,000\u591a\u672c\u8eab\u3002\u540c\u6837\u5730\uff0c\u7740\u540d\u7684BERT\u6a21\u578b\u5728\u7eb8\u4f2f\u7279\u91ca\u653e\uff1a\u5bf9\u8bed\u8a00\u7406\u89e3\u7684\u6df1\u53cc\u5411\u53d8\u538b\u5668\u7684\u9884\u8bad\u7ec3\u662f\u5728\u4e66\u67b6\u548c\u82f1\u8bed\u7ef4\u57fa\u767e\u79d1\u7684\u57f9\u8bad\u3002\u5bf9\u4e8e\u5bf9\u6f5c\u5165\u53d8\u538b\u5668\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u8bfb\u8005\uff0c\u539f\u59cb\u7eb8\u5f20\u548c\u6240\u793a\u7684\u53d8\u538b\u5668\u662f\u4e24\u5927\u8d44\u6e90\u3002\n \u53d8\u5f62\u91d1\u521a\u80cc\u540e\u7684\u4e3b\u8981\u798f\u5229\uff0c\u4ee5\u53ca\u6211\u4eec\u5c06\u5728\u6574\u4e2a\u535a\u5ba2\u4e2d\u770b\u4e00\u4e0b\uff0c\u8fd9\u662f\u4e00\u65e6\u9884\u5148\u8bad\u7ec3\u7684\u53d8\u538b\u5668\u53ef\u4ee5\u5feb\u901f\u5fae\u8c03\u4f17\u591a\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u4e14\u7ecf\u5e38\u4ece\u6846\u4e2d\u6267\u884c\u5f97\u5f88\u597d\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u53d8\u5f62\u91d1\u521a\u5df2\u7ecf\u4e86\u89e3\u4e86\u8bed\u8a00\uff0c\u8fd9\u5141\u8bb8\u57f9\u8bad\u4e13\u6ce8\u4e8e\u5b66\u4e60\u5982\u4f55\u505a\u95ee\u9898\u5e94\u7b54\uff0c\u8bed\u8a00\u751f\u6210\uff0c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff0c\u6216\u8005\u67d0\u4eba\u4e3a\u5176\u6a21\u578b\u7684\u601d\u60f3\u8fdb\u884c\u4efb\u4f55\u5176\u4ed6\u76ee\u6807\u3002\n    \u7b2c\u4e00\u4e2a\u4efb\u52a1\u6a21\u578b\u5c06\u53d7\u8fc7\u57f9\u8bad\uff0c\u662f\u60c5\u611f\u5206\u6790\u3002\u60c5\u7eea\u5206\u6790\u662fNLP\u9886\u57df\u7684\u957f\u671f\u57fa\u51c6\uff0c\u5176\u76ee\u6807\u662f\u80fd\u591f\u68c0\u6d4b\u5230\u67d0\u4e9b\u6587\u672c\u662f\u5426\u4e3a\u6b63\uff0c\u8d1f\u9762\u6216\u4e4b\u95f4\u7684\u67d0\u4e2a\u5730\u65b9\u3002\u8fd9\u6709\u8bb8\u591a\u7528\u4f8b\uff0c\u4f8b\u5982\u68c0\u6d4b\u4ea7\u54c1\u662f\u5426\u4ee5\u987e\u5ba2\u8bc4\u8bba\u6216\u8d1f\u9762\u65b9\u5f0f\u4ee5\u6b63\u9762\u6216\u8d1f\u9762\u65b9\u5f0f\u67e5\u770b\u6216\u5019\u9009\u8005\u57fa\u4e8e\u63a8\u6587\u5177\u6709\u9ad8\u6216\u4f4e\u6279\u51c6\u7b49\u7ea7\u3002\u6211\u4eec\u5c06\u7528\u4e8e\u57f9\u8bad\u7684\u6570\u636e\u96c6\u662f\u57f9\u8bad\u60c5\u7eea\u5206\u6790\u6a21\u578b\u662f\u5305\u542b11,855\u4e2a\u7535\u5f71\u8bc4\u8bba\u53e5\u5b50\u7684\u65af\u5766\u798f\u60c5\u7eea\u6811\u6728\u94f6\u884cV2\uff08SST2\uff09\u6570\u636e\u96c6\u3002\u6b64\u4efb\u52a1\u548c\u6570\u636e\u96c6\u662f\u666e\u901a\u8bed\u8a00\u7406\u89e3\u8bc4\u4f30\uff08\u80f6\u6c34\uff09\u57fa\u51c6\u7684\u4e00\u90e8\u5206\uff0c\u8fd9\u662f\u7528\u4e8e\u57f9\u8bad\uff0c\u8bc4\u4f30\u548c\u5206\u6790\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7cfb\u7edf\u7684\u8d44\u6e90\u96c6\u5408\u3002\n \u4ee5\u4e0b\u662f\u6765\u81ea\u6b64\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u793a\u4f8b\uff0c\u5176\u4e2d\u6570\u5b57\u66f4\u63a5\u8fd10\u8868\u793a\u8d1f\u9762\u60c5\u7eea\uff0c\u5e76\u4e14\u66f4\u63a5\u8fd11\u8868\u793a\u6b63\uff1a \n\u5ca9\u77f3\u6ce8\u5b9a\u8981\u6210\u4e3a21\u4e16\u7eaa\uff06\uff0339;\u65b0\u7684\uff06\uff0334;\u67ef\u5357\uff06\uff0334;\u5e76\u4e14\u4ed6\u5c06\u751a\u81f3\u5927\u4e8e\u963f\u8bfa\u5fb7\u65bd\u74e6\u8f9b\u683c\uff0c\u8ba9\u514b\u62c9\u592b\u00b7\u74e6\u59c6\u6885\u6216\u53f2\u8482\u6587\u00b7\u585e\u54e5\u5c14\u7684\u98de\u6e85\u751a\u81f3\u5927\u3002\n \u534e\u4e3d\u7cbe\u5fc3\u5236\u4f5c\u7684\u5ef6\u7eed\uff06\uff0334;\u6212\u6307\u7684\u4e3b\uff06\uff0334; Trilogy\u662f\u5982\u6b64\u5e9e\u5927\u7684\u662f\u4e00\u680f\u7684\u5355\u8bcd\u65e0\u6cd5\u5145\u5206\u63cf\u8ff0\u5171\u540c\u4f5c\u5bb6/\u8463\u4e8b\u5f7c\u5f97\u6770\u514b\u900a\uff06\uff0339; S\u6269\u5927\u4e86J.R.R.\u7684\u5c55\u671b\u3002 Tolkien\uff06\uff0339;\u4e2d\u571f\u3002\n \u4e00\u5f00\u59cb\u96f7\u9e23\uff0c\u7eaf\u7cb9\u7684\u7cbe\u5fc3\u822c\u7684\u8282\u594f\u5f88\u5c11\uff0c\u8ddd\u79bb;\u4ed6\u4eec\u7684\u77ed\u7f3a\u5f25\u8865\u4e86\u5176\u4ed6\u53ef\u656c\u7684\u884c\u52a8\u7684\u6548\u529b\u3002\n  \u7b2c\u4e8c\u4e2a\u4efb\u52a1\u6a21\u578b\u5c06\u88ab\u57f9\u8bad\uff0c\u4ee5\u8fdb\u884c\u91cd\u590d\u7684\u95ee\u9898\u68c0\u6d4b\u3002\u540c\u6837\uff0c\u6b64\u4efb\u52a1\u8fd8\u5177\u6709\u5404\u79cd\u7528\u4f8b\uff0c\u4f8b\u5982\u4eceQuora\u5e73\u53f0\u4e2d\u5220\u9664\u7c7b\u4f3c\u7684\u95ee\u9898\uff0c\u4ee5\u9650\u5236\u7528\u6237\u4e4b\u95f4\u7684\u6df7\u6dc6\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u7684\u6570\u636e\u96c6\u8fdb\u884c\u91cd\u590d\u7684\u95ee\u9898\u68c0\u6d4b\u6a21\u578b\u662fQuora\u95ee\u9898\u5bf9\u6570\u636e\u96c6\u3002\u6b64\u4efb\u52a1/\u6570\u636e\u96c6\u4e5f\u662f\u80f6\u6c34\u57fa\u51c6\u7684\u4e00\u90e8\u5206\u3002\n \u6765\u81ea\u6b64\u6570\u636e\u96c6\u7684\u8bb8\u591a\u793a\u4f8b\uff0c\u5176\u4e2d0\u8868\u793a\u672a\u91cd\u590d\u548c1\u8868\u793a\u91cd\u590d\u9879\uff1a\n   \u4e24\u4e2a\u4e0d\u540c\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u67b6\u6784\u5c06\u63a5\u53d7\u4e0a\u8ff0\u4efb\u52a1/\u6570\u636e\u96c6\u7684\u57f9\u8bad\u3002\u9884\u5148\u63a5\u53d7\u7684\u578b\u53f7\u5c06\u4eceHuggingFace\u53d8\u538b\u5668\u6062\u590d\uff0c\u5176\u4e2d\u5305\u542b60\u591a\u79cd\u4e0d\u540c\u7684\u7f51\u7edc\u7c7b\u578b\u3002 HuggingFace Model\u96c6\u7ebf\u5668\u4e5f\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u8d44\u6e90\uff0c\u5728\u5404\u79cd\u5404\u6837\u7684\u4efb\u52a1\u4e0a\u5305\u542b\u8d85\u8fc710,000\u79cd\u4e0d\u540c\u7684\u9884\u8bad\u7ec3\u53d8\u538b\u5668\u3002\n   \u6211\u4eec\u5c06\u57f9\u8bad\u7684\u7b2c\u4e00\u4e2a\u67b6\u6784\u662f\u5728Distilbert\u5f00\u653e\u7684ZERT\u7684\u84b8\u998f\u548c\u53d1\u5e03\u7684\u7b2c\u4e00\u4e2a\u67b6\u6784\uff0c\u4f2f\u7279\u84b8\u998f\u51fa\u6765\uff1a\u8f83\u5c0f\uff0c\u66f4\u5feb\uff0c\u66f4\u4fbf\u5b9c\uff0c\u66f4\u8f7b\u3002\u8be5\u53d8\u538b\u5668\u6bd4\u4f2f\u7279\u5c0f40\uff05\uff0c\u540c\u65f6\u4fdd\u7559\u4e8697\uff05\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u4e5f\u66f4\u5feb\u5730\u4e3a60\uff05\u3002\u6211\u4eec\u5c06\u4e3aSST2\u548cQQP\u6570\u636e\u96c6\u57f9\u8bad\u6b64\u67b6\u6784\u3002 \n\u6211\u4eec\u5c06\u57f9\u8bad\u7684\u7b2c\u4e8c\u4e2a\u67b6\u6784\u662fBERT\u5728BERT\u53d1\u5e03\u7684\uff1a\u7528\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u6df1\u53cc\u5411\u53d8\u538b\u5668\u7684\u9884\u8bad\u7ec3\u3002\u8fd9\u662f\u7b2c\u4e00\u4e2a\u901a\u8fc7\u5728\u91ca\u653e\u65f6\u4e3a11\u4e2a\u4e0d\u540c\u7684NLP\u4efb\u52a1\u8bbe\u7f6e\u65b0\u7684\u6700\u5148\u8fdb\u7684\u65b0\u6280\u672f\uff0c\u8fd9\u662f\u4e00\u4e2a\u771f\u6b63\u5728NLP\u57df\u4e2d\u5c55\u793a\u4e86\u8fd9\u79cd\u6a21\u578b\u7c7b\u578b\u7684\u6743\u529b\u3002\n   \u5728\u8fd9\u4e2a\u80cc\u666f\u4e0b\uff0c\u8ba9\uff06\uff0339;\u73b0\u5728\u770b\u770b\u4ee3\u7801\u548c\u706b\u8f66/\u5fae\u8c03\u8fd9\u4e9b\u578b\u53f7\uff01\u5728\u8fd9\u91cc\uff0c\u6211\u4eec\u4f7f\u7528Pytorch\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u4e14\u4ec5\u5305\u62ecSST2\u6570\u636e\u96c6\u7684\u4ee3\u7801\u3002\u8981\u8fd0\u884c\u6b64\u4ee3\u7801\uff0c\u60a8\u53ef\u4ee5\u968f\u65f6\u67e5\u770b\u6211\u4eec\u7684COLAB\u7b14\u8bb0\u672c\uff0c\u53ef\u4ee5\u8f7b\u677e\u7f16\u8f91\u4ee5\u5bb9\u7eb3QQP\u6570\u636e\u96c6\u3002\n  \u9996\u5148\u8ba9\uff06\uff0339; s\u4e3aSST2\u521b\u5efa\u6211\u4eec\u7684Pytorch DataSet\u7c7b\u3002\u6b64\u7c7b\u5b9a\u4e49\u4e86\u4ee5\u4e0b\u4e09\u4e2a\u91cd\u8981\u529f\u80fd\uff0c\u4ee5\u4e0b\u76ee\u7684\uff1a\n  #libraries\u4eceorch.utils.data\u5bfc\u5165\u6570\u636e\u96c6#pytorch dataset\u7c7bclass sst_dataset\uff08dataset\uff09\uff1a#name\uff1a__init__ #purpose\uff1ainit\u51fd\u6570\u52a0\u8f7ddataSet #inputs\uff1a\u6570\u636e\u96c6 - \uff06gt; DataSet #Outputs\uff1anone def __init __\uff08self\uff0cdataSet\uff09\uff1aself .dataset = dataSet return #name\uff1a__len__ #purpose\uff1a\u83b7\u53d6\u6570\u636e\u96c6\u7684\u957f\u5ea6#inputs\uff1anone #outputs\uff1alength  -  lengts-\uff06gt; DataSet DEF\u7684\u957f\u5ea6__len __\uff08self\uff09\uff1a\u8fd4\u56delen\uff08self .dataset\uff09#name\uff1a__getitem__ #purpose\uff1a\u4ece\u6570\u636e\u96c6#inputs\u83b7\u53d6\u4e00\u4e2a\u968f\u673a\u6587\u672c\u6bb5\u53ca\u5176\u6807\u7b7e\uff1aIDX  - \uff06gt;\u968f\u673a\u6587\u672c\u6bb5\u7684\u7d22\u5f15\u52a0\u8f7d#ourputs\uff1atext  - \uff06gt;\u6587\u672c\u6bb5\uff03\u6807\u7b7e - \uff06gt;\u60c5\u7eea\u5206\u6570def __getItem __\uff08self\uff0cIdx\uff09\uff1atext = self .dataset [idx] [\uff06\uff0339;\u53e5\u5b50\uff06\uff0339;]\u6807\u7b7e=\u706b\u70ac\u3002ractch .zeros\uff082\uff09\u6807\u7b7e[round\uff08self .dataset [idx] [\uff06\uff0339 ;\u6807\u7b7e\uff06\uff0339;]\uff09] = 1\u8fd4\u56de\u6587\u672c\uff0c\u6807\u7b7e\n  \u4e0b\u4e00\u4e2aLet\uff06\uff0339; S\u521b\u5efa\u4e86\u4e00\u5bf9\u592b\u59bb\u8f85\u52a9\u529f\u80fd\u6765\u505a\u50cfGET GPU\uff0c\u5c06\u6570\u636e\u8f6c\u79fb\u5230\u5b83\u7b49\u7b49\u3002\u795e\u7ecf\u7f51\u7edc\uff0c\u5c24\u5176\u662f\u53d8\u538b\u5668\u57fa\u4e8e\u53d8\u538b\u5668\uff0c\u51e0\u4e4e\u603b\u662f\u5728\u52a0\u901f\u5668\u786c\u4ef6\u7b49\u52a0\u901f\u5668\u786c\u4ef6\u4e0a\u8bad\u7ec3\uff0c\u6240\u4ee5\u53d1\u9001\u81f3\u5173\u91cd\u8981\u5982\u679c\u5b83\u6709\u7528\u4e8e\u5904\u7406\u7684\u6a21\u578b\u548c\u6570\u636e\uff06\uff0339;\u53ef\u7528\u3002\u8fd9\u5141\u8bb8\u663e\u7740\u7684\u8bad\u7ec3\u52a0\u901f\uff0c\u56e0\u4e3a\u53ef\u4ee5\u4f7f\u7528\u5e76\u884c\u5904\u7406\u80fd\u529b\u3002\n #name\uff1aget_gpu #purpose\uff1a\u68c0\u67e5gpu\u8bbe\u5907\u662f\u5426\u662f\u53ef\u7528\u7684#input\uff1anone #output\uff1agpu  - \uff06gt; GPU\u8bbe\u5907\u5982\u679c\u9002\u7528\uff0c\u5982\u679c\u4e0d\u662fdef get_gpu\uff08\uff09\uff1a#check\u5982\u679c\u662favaliable\uff0c\u5982\u679c\u662f\u8fd9\u6837\u7684\u8bdd\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5982\u679c\u662f\u8fd9\u6837\uff0c\u5982\u679ctorch .cuda .is_available\uff08\uff09\uff1a\u6253\u5370\uff08\uff06\uff0334;\u4f7f\u7528gpu\uff06\uff0334;\uff09gpu =\u706b\u70ac.Device\uff08\uff06\uff0334; cuda\uff06\uff0334;\uff09else\uff1aprint\uff08\uff06\uff0334;\u6ca1\u6709gpu\u8bbe\u5907\u53ef\u7528\uff01\u4f7f\u7528cpu\uff06\uff0334;\uff09return gpu #name\uff1atransfer_device #purpose\uff1a\u5c06\u6a21\u578b/\u6570\u636e\u8f6c\u79fb\u5230GPU Devie\u5982\u679c\u662f#Inputs\uff1aGPU  - \uff06gt; GPU\u8bbe\u5907\u5982\u679c\u9002\u7528\uff0c\u5982\u679c\u4e0d\u662f\uff03data  - \uff06gt;\u6570\u636e\u4f20\u8f93#output\uff1adata  - \uff06gt;\u5df2\u8f6c\u79fb\u7684\u6570\u636e\uff08\u5982\u679c\u9002\u7528DEF Transfer_Device\uff08GPU\uff09\uff0c\u6570\u636e\uff09\uff1aif\uff08gpu\uff01= none\uff09\uff1adata = data .ta\uff08gpu\uff09\u8fd4\u56de\u6570\u636e#name\uff1acount_correct #purpose\uff1a\u8ba1\u7b97\u6279\u6b21\u4e2d\u6b63\u786e\u7684\u6a21\u578b\u9884\u6d4b\u6570\u91cf#inputs\uff1a\u9884\u6d4b - \uff06gt;\u6a21\u578b\u9884\u6d4b\uff03\u76ee\u6807 - \uff06gt;\u76ee\u6807\u6807\u7b7e#OUTPUTS\uff1a\u6b63\u786e - \uff06gt;\u6b63\u786e\u6a21\u578b\u9884\u6d4bDef Count_correct\uff08\u9884\u6d4b\uff0c\u76ee\u6807\uff09\uff1a#Create\u53d8\u91cf\u5b58\u50a8\u6b63\u786e\u7684\u9884\u6d4b\u6570\u91cf\u4ee5\u53ca\u6279\u91cf\u9884\u6d4b\u7684\u7d22\u5f15= 0\u7d22\u5f15= 0 #Loop\u8de8\u6279\u5904\u7406\u4e2d\u7684\u6240\u6709\u9884\u6d4b\u5e76\u8ba1\u7b97\u6570\u91cf\u6b63\u786e\uff08index\uff06len\uff08\u9884\u6d4b\uff09\uff09\uff1a#convert\u9884\u6d4b\u548c\u76ee\u6807\u5217\u51fa\u9884\u6d4b=\u5217\u8868\uff08\u9884\u6d4b[index]\uff09target = list\uff08targets [index]\uff09#get\u6700\u5927\u503c\u6307\u793a\u771f\u503c\u503c\u9884\u6d4b\u548c\u76ee\u6807\u9884\u6d4b_INDEX =\u9884\u6d4b.index\uff08max\uff08\u9884\u6d4b\uff09\uff09target_index = target .index\uff08max\uff08target\uff09\uff09\uff03\u5982\u679c\u6700\u5927\u7d22\u5f15\u662f\u76f8\u540c\u7684\uff0c\u5982\u679c\uff08prediction_index == target_index\uff09\uff1a\u6b63\u786e+ = 1\u7d22\u5f15+ = 1\u8fd4\u56de\u6b63\u786e\n  \u73b0\u5728\u6211\u4eec\u5c06\u5b9a\u4e49\u635f\u5931\u51fd\u6570......\u56e0\u4e3a\u6211\u4eec\u6b63\u5728\u8bad\u7ec3\u4e00\u4e2a\u5206\u7c7b\u5668\u6765\u9884\u6d4b\u53e5\u5b50\u662f\u5426\u5177\u6709\u6b63\u9762\u6216\u8d1f\u9762\u60c5\u7eea\uff0c\u6216\u8005\u5982\u679c\u4e24\u4e2a\u95ee\u9898\u662f\u91cd\u590d\u7684\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u635f\u5931\u529f\u80fd\u3002\u8fd9\u79cd\u635f\u5931\u80cc\u540e\u7684\u6570\u5b66\u662f\uff1a \n\u8fd9\u91cc\u662f\u771f\u6b63\u7684\u6807\u7b7e\uff080\u62161\uff09\uff0c\u800cP\uff08y\uff09\u662f\u6211\u4eec\u7684\u6a21\u578b\u9884\u6d4b\u3002\u901a\u8fc7\u6700\u5c0f\u5316\u8fd9\u79cd\u4ef7\u503c\u6211\u4eec\u7684\u7f51\u7edc\u5b66\u4f1a\u4e86\u89e3\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u3002\n #name\uff1abinary_cross_entropy #purpose\uff1a\u5b9a\u4e49\u4e8c\u8fdb\u5236\u4ea4\u53c9\u71b5\u4e22\u5931\u51fd\u6570#inputs\uff1a\u9884\u6d4b - \uff06gt;\u6a21\u578b\u9884\u6d4b\uff03\u76ee\u6807 - \uff06gt;\u76ee\u6807\u6807\u7b7e#OUTPUTS\uff1a\u635f\u5931 - \uff06gt;\u635f\u5931\u503cdef binary_cross_entropy\uff08\u9884\u6d4b\uff0c\u76ee\u6807\uff09\uff1a\u635f\u5931=  - \uff08\u76ee\u6807*\u706b\u70ac.log\uff08\u9884\u6d4b\uff09+\uff081  - \u76ee\u6807\uff09*\u706b\u70ac.log\uff081  - \u9884\u6d4b\uff09\uff09\u635f\u5931=\u706b\u70ac.mean\uff08\u635f\u5931\uff09\u56de\u62a5\u635f\u5931\n  \u63a5\u4e0b\u6765\u8ba9\u6211\u4eec\u5c06\u6838\u5fc3\u57f9\u8bad/\u8bc4\u4f30\u903b\u8f91\u5199\u5165\u5fae\u8c03\u548c\u6d4b\u8bd5\u6211\u4eec\u7684\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u542b3\u4e2a\u4e3b\u8981\u529f\u80fd\uff1a\n  TRATE_MODEL\u529f\u80fd\u9996\u5148\u5728\u9a8c\u8bc1\u96c6\u4e0a\u8bc4\u4f30\u9884\u5148\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5e76\u5728\u53d1\u751f\u4efb\u4f55\u57f9\u8bad\u4e4b\u524d\u8ba1\u7b97\u6027\u80fd\u3002\u7136\u540e\uff0c\u6b64\u529f\u80fd\u5728\u57f9\u8bad\u8bad\u7ec3\u96c6\u4e0a\u57f9\u8bad\u6a21\u578b\u5e76\u5728\u9a8c\u8bc1\u96c6\u4e2d\u8bc4\u4f30\u5176\u6027\u80fd\u65f6\u5faa\u73af\u5faa\u73af\u3002 epoch\u57fa\u672c\u4e0a\u662f\u67d0\u4e9b\u6570\u636e\u96c6\u4e2d\u6240\u6709\u6570\u636e\u7684\u5faa\u73af\u3002\n \u5217\u8f66\u529f\u80fd\u901a\u8fc7\u4e3a\u65f6\u4ee3\u57f9\u8bad\u6a21\u578b\u6765\u8fd0\u884c\u3002\u8bf7\u6ce8\u610f\uff0c\u5728\u8fdb\u884c\u4efb\u4f55\u57f9\u8bad\u4e4b\u524d\uff0c\u6211\u4eec\u7684\u6a21\u578b\u88ab\u6295\u5165\u5230\u57f9\u8bad\u6a21\u5f0f\uff0c\u6307\u793aPytorch\uff0c\u9700\u8981\u4e3a\u53c2\u6570\u66f4\u65b0\u5b58\u50a8\u68af\u5ea6\u3002\u7136\u540e\u901a\u8fc7\u8fed\u4ee3Pytorch DataLoader\u6765\u5faa\u73af\u65f6\u4ee3\u4e2d\u7684\u6240\u6709\u6279\u6b21\u3002\u7136\u540e\uff0c\u6bcf\u6279\u5904\u7406\u90fd\u4f1a\u901a\u8fc7\u9500\u552e\u5668\uff0c\u5141\u8bb8\u8fd9\u4e9b\u4ee4\u724c\u5c06\u5176\u53d1\u9001\u5230\u6a21\u578b\u4ee5\u8fdb\u884c\u60c5\u611f\u8bc4\u5206\u9884\u6d4b\u3002\u5728De Facto Pytorch\u8bad\u7ec3\u5faa\u73af\u8bbe\u7f6e\u4e4b\u540e\uff0c\u8ba1\u7b97\u635f\u8017\u503c\uff0c\u4f18\u5316\u5668\u88ab\u5f52\u96f6\uff0c\u635f\u8017\u5bfc\u51fa\u68af\u5ea6\uff0c\u5e76\u4e14\u901a\u8fc7\u91c7\u7528\u4f18\u5316\u5668\u6b65\u9aa4\u66f4\u65b0\u6a21\u578b\u3002\n \u9664\u4e86\u6700\u7ec8\u4f18\u5316\u5668\u5f52\u96f6\u4e4b\u5916\uff0c\u8bc4\u4f30\u51fd\u6570\u5177\u6709\u7c7b\u4f3c\u7684\u8bbe\u7f6e\uff0c\u9664\u4e86\u6700\u7ec8\u4f18\u5316\u5668\u5f52\u96f6\uff0c\u68af\u5ea6\u5bfc\u51fa\u548c\u4f18\u5316\u7a0b\u5e8f\u6b65\u9aa4\uff0c\u56e0\u4e3a\u6a21\u578b\u4e0d\u5e94\u5728\u9a8c\u8bc1\u96c6\u4e0a\u57f9\u8bad\u3002\u8fd9\u4e24\u4e2a\u51fd\u6570\u4e4b\u95f4\u7684\u5176\u4ed6\u5dee\u5f02\u662f\uff0c\u5728\u6b64\uff0c\u6211\u4eec\u7684\u6a21\u578b\u8bbe\u7f6e\u4e3a\u8bc4\u4f30\u6a21\u5f0f\uff0c\u5176\u5141\u8bb8\u66f4\u5feb\u63a8\u8bba\uff0c\u56e0\u4e3a\u6e10\u53d8Don\uff06\uff0339; t\u9700\u8981\u5b58\u50a8\u3002\n \u5efa\u7acb\u5728\u5217\u8f66\u548c\u8bc4\u4f30\u529f\u80fd\u4e2d\u662f\u5bf9Count_Correct\u7684\u8c03\u7528\uff0c\u4ece\u800c\u8ba1\u7b97\u6bcf\u4e2a\u6279\u91cf\u7684\u6b63\u786e\u60c5\u7eea\u8bc4\u5206\u9884\u6d4b\u7684\u6570\u91cf\uff0c\u5141\u8bb8\u5728\u6574\u4e2a\u6570\u636e\u96c6\u4e2d\u6d3e\u751f\u6700\u7ec8\u7cbe\u5ea6\u5206\u6570\u3002\u53e6\u8bf7\u6ce8\u610f\uff0cSoftMax\u5728\u6a21\u578b\u4e0a\u8c03\u7528\uff06\uff0339; s\u8f93\u51fa\u6620\u5c04\u5206\u6570\u5230\u6982\u7387\u3002 \nWarning: Can only detect less than 5000 characters\n\u5728\u672c\u535a\u5ba2\u4e2d\uff0c\u6211\u4eec\u5b66\u4e60\u4e86\u5982\u4f55\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u53d8\u6362\u5668\uff0c\u7279\u522b\u662f\u60c5\u611f\u5206\u6790\u548c\u91cd\u590d\u7684\u95ee\u9898\u68c0\u6d4b\u3002 \u901a\u8fc7\u5fae\u8c03\u9884\u5148\u8bad\u7ec3\u7684\u53d8\u538b\u5668\uff0c\u53ef\u4ee5\u8282\u7701\u5927\u91cf\u65f6\u95f4  ...... ", "note_en": "In this tutorial, we&#39;ll show how you to fine-tune two different transformer models, BERT\u00a0and DistilBERT, for two different NLP\u00a0problems:\u00a0Sentiment Analysis, and Duplicate Question\u00a0Detection.\n You can see a complete working example in our  Colab Notebook, and you can play with the trained models on  HuggingFace. Let&#39;s jump in!\n  Since being first developed and released in the  Attention Is All You Need paper  Transformers have completely redefined the field of  Natural Language Processing (NLP) setting the state-of-the-art on numerous tasks such as question answering, language generation, and named-entity recognition. Here we won&#39;t go into too much detail about what a Transformer is, but rather how to apply and train them to help achieve some task at hand. The main things to keep in mind conceptually about Transformers are that they are really good at dealing with sequential data (text, speech, etc.), they act as an encoder-decoder framework where data is mapped to some representational space by the encoder before then being mapped to the output by way of the decoder, and they scale incredibly well to parallel processing hardware (GPUs).\n Transformers in the field of Natural Language Processing have been trained on massive amounts of text data which allow them to understand both the syntax and semantics of a language very well. For example, the original GPT model published in  Improving Language Understanding by Generative Pre-Training was trained on BooksCorpus, over 7,000 unique unpublished books. Likewise, the famous BERT model released in the paper  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding was trained on both BooksCorpus and English Wikipedia. For readers interested in diving into the neural network architecture of a Transformer,  the original paper and  The Illustrated Transformer are two great resources.\n The main benefit behind Transformers, and what we will take a look at throughout the rest of this blog, is that once pre-trained Transformers can be quickly fine-tuned for numerous downstream tasks and often perform really well out of the box. This is primarily due to the fact that the Transformer already understands language which allows training to focus on learning how to do question answering, language generation, named-entity recognition, or whatever other goal someone has in mind for their model.\n    The first task models will be trained for is sentiment analysis. Sentiment analysis is a long-standing benchmark in the field of NLP with the goal in mind to be able to detect whether some text is positive, negative, or somewhere in between. This has many use cases such as detecting if a product is viewed in a positive or negative manner based on customer reviews or if a candidate has a high or low approval rating based on tweets. The dataset we will use to train a sentiment analysis model is the  Stanford Sentiment Treebank v2 (SST2) dataset which contains 11,855 movie review sentences. This task and dataset is part of the  General Language Understanding Evaluation (GLUE) Benchmark which is a collection of resources for training, evaluating, and analyzing natural language understanding systems.\n Here are some examples from this dataset where numbers closer to 0 represent negative sentiment and numbers closer to 1 represent positive:\n The Rock is destined to be the 21st Century&#39;s new &#34;Conan&#34; and that he&#39;s going to make a splash even greater than Arnold Schwarzenegger, Jean-Claud Van Damme or Steven Segal.\n The gorgeously elaborate continuation of &#34;The Lord of the Rings&#34; trilogy is so huge that a column of words can not adequately describe co-writer/director Peter Jackson&#39;s expanded vision of J.R.R. Tolkien&#39;s Middle-earth.\n A thunderous ride at first, quiet cadences of pure finesse are few and far between; their shortage dilutes the potency of otherwise respectable action.\n  The second task models will be trained for is duplicate question detection. Likewise, this task also has various use cases such as removing similar questions from the Quora platform to limit confusion amongst users. The dataset we will use to train a duplicate question detection model is the  Quora Question Pairs dataset. This task/dataset is also part of the  GLUE Benchmark.\n A number of examples from this dataset where 0 represents non-duplicates and 1 represents duplicates are:\n   Two different Transformer based architectures will be trained for the tasks/datasets above. Pre-trained models will be loaded from the  HuggingFace Transformers Repo which contains over 60 different network types. The  HuggingFace Model Hub is also a great resource which contains over 10,000 different pre-trained Transformers on a wide variety of tasks.\n   The first architecture we will train is  DistilBERT which was open sourced and released in  DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter. This Transformer is 40% smaller than BERT while retaining 97% of the language understanding capabilities and also being 60% faster. We will train this architecture for both the SST2 and QQP\u00a0datasets.\n   The second architecture we will train is  BERT published in  BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. This was the first Transformer that really showed the power of this model type in the NLP domain by setting a new state-of-the-art on eleven different NLP tasks at the time of its release.\n   With this background in mind let&#39;s now take a look at the code and train/fine-tune these models! Here we use the  PyTorch deep learning framework and only include code for the SST2 dataset. To run this code yourself feel free to check out our  Colab Notebook which can be easily edited to accomodate the QQP dataset as well.\n  First let&#39;s create our  PyTorch Dataset class for SST2. This class defines three important functions with the following purposes:\n  #Libraries needed import  torch from  torch.utils.data  import Dataset #PyTorch dataset class class  SST_Dataset(Dataset):\t #Name: \t\t__init__\t #Purpose: \tinit function to load the dataset\t #Inputs: \tdataset -&gt; dataset\t #Outputs: \tnone\t def  __init__( self, dataset):\t\t self .dataset  = dataset\t\t return\t #Name: \t\t__len__\t #Purpose: \tget the length of the dataset\t #Inputs: \tnone\t #Outputs: \tlength -&gt; length of the dataset\t def  __len__( self):\t\t return  len( self .dataset)\t #Name: \t\t__getitem__\t #Purpose: \tget a random text segment and its label from the dataset\t #Inputs: \tidx -&gt; index of the random text segment to load\t #Outputs: \ttext -&gt; text segment\t # \t\t\tlabel -&gt; sentiment score\t def  __getitem__( self, idx):\t\ttext  =  self .dataset[idx][ &#39;sentence&#39;]\t\tlabel  = torch .zeros( 2)\t\tlabel[ round( self .dataset[idx][ &#39;label&#39;])]  =  1\t\t return text, label\n  Next let&#39;s create a couple helper functions to do things like get the GPU, transfer data to it, etc. Neural networks, especially Transformer based ones, nearly always train faster on accelerator hardware such as GPUs so it is critical to send both the model and data there for processing if it&#39;s available. This allows for a significant training speedup as parallel processing capabilities can be utilized.\n #Name: \t\tget_gpu #Purpose: \tchecks if a GPU device is avaliable #Input: \tnone #Output: \tGPU -&gt; GPU device if applicable, none if not def  get_gpu():\t #Check if a GPU is avaliable and if so return it\tGPU  =  None\t if torch .cuda .is_available():\t\t print( &#34;Using GPU&#34;)\t\tGPU  = torch .device( &#34;cuda&#34;)\t else:\t\t print( &#34;No GPU device avaliable! Using CPU&#34;)\t return GPU #Name: \t\ttransfer_device #Purpose: \ttransfers model / data to the GPU devie if present #Inputs: \tGPU -&gt; GPU device if applicable, none if not # \t\t \tdata -&gt; data to transfer #Output: \tdata -&gt; data that has been transferred if applicable def  transfer_device(GPU, data):\t if(GPU  !=  None):\t\tdata  = data .to(GPU)\t return data #Name: \t\tcount_correct #Purpose: \tcount the number of correct model predictions in a batch #Inputs: \tpredictions -&gt; model predictions #\t\t \ttargets -&gt; target labels #Outputs: \tcorrect -&gt; number of correct model predictions def  count_correct(predictions, targets):\t #Create variables to store the number of correct predictions along with the index of the prediction in the batch\tcorrect  =  0\tindex  =  0 \t #Loop across all predictions in the batch and count the number correct\t while(index  &lt;  len(predictions)):\t\t #Convert the prediction and target to lists\t\tprediction  =  list(predictions[index])\t\ttarget  =  list(targets[index]) \t\t #Get the max index indicating the truth value from the prediction and target\t\tprediction_index  = prediction .index( max(prediction))\t\ttarget_index  = target .index( max(target)) \t\t #If the max indices are the same increment correct\t\t if(prediction_index  == target_index):\t\t\tcorrect  +=  1\t\tindex  +=  1\t return correct\n  Now we will define the loss function... Since we are training a classifier to predict whether a sentence has positive or negative sentiment, or if two questions are duplicates, we will use the binary cross entropy loss function. The math behind this loss is:\n  Here  y is the true label (0 or 1) whereas  p(y) is our model prediction. Through the minimization of this value our network learns to make more accurate predictions.\n #Name: \t\tbinary_cross_entropy #Purpose: \tdefines binary cross entropy loss function #Inputs: \tpredictions -&gt; model predictions # \t\t\ttargets -&gt; target labels #Outputs: \tloss -&gt; loss value def  binary_cross_entropy(predictions, targets):\tloss  =  -(targets  * torch .log(predictions)  + ( 1  - targets)  * torch .log( 1  - predictions))\tloss  = torch .mean(loss)\t return loss\n  Next lets write the core training/evaluation logic to fine-tune and test our model which consists of 3 primary functions:\n  The  train_model function works by first evaluating the pre-trained model on the validation set and calculating the performance before any training has taken place. This function then loops over three epochs while training the model on the training set and evaluating its performance on the validation set. An epoch is essentially a loop over all the data in some dataset.\n The  train function operates by training the model for an epoch. Note that before any training takes place our model is put into training mode indicating to PyTorch that gradients need to be stored for parameter updates. All batches in an epoch are then looped over by iterating over the  PyTorch Dataloader. Each batch is then passed through the tokenizer allowing these tokens to then be sent to the model for sentiment score predictions. Following the de facto PyTorch training loop setup, a loss value is computed, the optimizer is zeroed out, gradients are derived on the loss, and the model is updated by taking an optimizer step.\n The  evaluate function has a similar setup to  train except the final optimizer zeroing out, gradient derivation, and optimizer step are removed since the model should not be trained on a validation set. Other differences between these two functions is that here our model is set to evaluation mode which allows for faster inference since gradients don&#39;t need to be stored.\n Built into both the  train and  evaluate function is a call to  count_correct which computes the number of correct sentiment score predictions per batch allowing a final accuracy score to be derived across the entire dataset. Also note that  softmax is called over the model&#39;s output mapping scores to probabilities.\n import  torch.nn.functional  as  F  #Name: \t\ttrain_model #Purpose: \ttrain the model while evaluating its performance #Inputs: \tGPU -&gt; GPU device to train / evaluate on # \t\t\ttrain_dataloader -&gt; training set dataloader # \t\t\tdev_dataloader -&gt; development set dataloader # \t\t\ttokenizer -&gt; text tokenizer for model # \t\t\tmodel -&gt; model to train / evaluate # \t\t\toptimizer -&gt; optimizer to use to update model parameters # \t\t\tcriterion -&gt; criterion to use to compute loss values #Outputs: \tmodel -&gt; model after training def  train_model(GPU, train_dataloader, dev_dataloader, tokenizer, model, optimizer, criterion):\t #Evaluate the performance of the model before training\tvalid_loss, valid_accuracy  = evaluate(GPU, dev_dataloader, tokenizer, model, criterion)\t print( &#34;Pre-training validation loss: &#34; + str(valid_loss) + &#34; --- Accuracy: &#34; + str(valid_accuracy))\t print()\t #Train the model across 3 epochs and evaluate its performance\t for epoch  in  range( 3):\t\tmodel, train_loss, train_accuracy  = train(GPU, train_dataloader, tokenizer, model, optimizer, criterion)\t\tvalid_loss, valid_accuracy  = evaluate(GPU, dev_dataloader, tokenizer, model, criterion)\t\t #Print performance stats\t\t print( &#34; &#34;, end = &#34; \\r &#34;)\t\t print( &#34;Epoch: &#34; + str(epoch + 1))\t\t print( &#34;Training loss: &#34; + str(train_loss) + &#34; --- Accuracy: &#34; + str(train_accuracy))\t\t print( &#34;Validation loss: &#34; + str(valid_loss) + &#34; --- Accuracy: &#34; + str(valid_accuracy))\t\t print()\t return model\n #Name: \t\ttrain #Purpose: \ttrain the model over 1 epoch #Inputs: \tGPU -&gt; GPU device to train on # \t\t\tdataloader -&gt; dataloader # \t\t\ttokenizer -&gt; text tokenizer for model # \t\t\tmodel -&gt; model to train # \t\t\toptimizer -&gt; optimizer to use to update model parameters # \t\t\tcriterion -&gt; criterion to use to compute loss values #Outputs: \tmodel -&gt; model after training over the epoch # \t\t\taverage_loss -&gt; average loss over the epoch # \t\t\taccuracy -&gt; accuracy over the epoch def  train(GPU, dataloader, tokenizer, model, optimizer, criterion):\t #Place the network in training mode, create a variable to store the total loss, and create a variable to store the total number of correct predictions\tmodel .train()\ttotal_loss  =  0\ttotal_correct  =  0 \t #Loop through all batches in the dataloader\t for batch_number, (texts, labels)  in  enumerate(dataloader):\t\t #Tokenize the text segments, get the model predictions, compute the loss, and add the loss to the total loss\t\ttokenized_segments  = tokenizer(texts, return_tensors = &#34;pt&#34;, padding = True, truncation = True)\t\ttokenized_segments_input_ids, tokenized_segments_attention_mask  = tokenized_segments .input_ids, tokenized_segments .attention_mask\t\tmodel_predictions  = F .softmax(model(input_ids =transfer_device(GPU, tokenized_segments_input_ids), attention_mask =transfer_device(GPU, tokenized_segments_attention_mask))[ &#39;logits&#39;], dim = 1)\t\tloss  = criterion(model_predictions, transfer_device(GPU, labels))\t\ttotal_loss  += loss .item() \t\t #Count the number of correct predictions by the model in the batch and add this to the total correct\t\tcorrect  = count_correct(model_predictions .cpu() .detach() .numpy(), labels .numpy())\t\ttotal_correct  += correct \t\t #Zero the optimizer, compute the gradients, and update the model parameters\t\toptimizer .zero_grad()\t\tloss .backward()\t\toptimizer .step()\t\t print( &#34;Training batch index: &#34; + str(batch_number) + &#34;/&#34; + str( len(dataloader)) +  &#34; ( &#34; + str(batch_number / len(dataloader) * 100) + &#34;% )&#34;, end = &#39; \\r &#39;) \t #Compute the average loss and accuracy across the epoch\taverage_loss  = total_loss  /  len(dataloader)\taccuracy  = total_correct  / dataloader .dataset .__len__()\t return model, average_loss, accuracy\n #Name: \t\tevaluate #Purpose: \tevaluate the model over 1 epoch #Inputs: \tGPU -&gt; GPU device to evaluate on # \t\t\tdataloader -&gt; dataloader # \t\t\ttokenizer -&gt; text tokenizer for model # \t\t\tmodel -&gt; model to evaluate # \t\t\tcriterion -&gt; criterion to use to compute loss values #Outputs: \taverage_loss -&gt; average loss over the epoch # \t\t\taccuracy -&gt; accuracy over the epoch def  evaluate(GPU, dataloader, tokenizer, model, criterion):\t #Place the network in evaluation mode, create a variable to store the total loss, and create a variable to store the total number of correct predictions\tmodel .eval()\ttotal_loss  =  0\ttotal_correct  =  0 \t #Loop through all batches in the dataloader\t for batch_number, (texts, labels)  in  enumerate(dataloader):\t\t #Tokenize the text segments, get the model predictions, compute the loss, and add the loss to the total loss\t\ttokenized_segments  = tokenizer(texts, return_tensors = &#34;pt&#34;, padding = True, truncation = True)\t\ttokenized_segments_input_ids, tokenized_segments_attention_mask  = tokenized_segments .input_ids, tokenized_segments .attention_mask\t\tmodel_predictions  = F .softmax(model(input_ids =transfer_device(GPU, tokenized_segments_input_ids), attention_mask =transfer_device(GPU, tokenized_segments_attention_mask))[ &#39;logits&#39;], dim = 1)\t\tloss  = criterion(model_predictions, transfer_device(GPU, labels))\t\ttotal_loss  += loss .item() \t\t #Count the number of correct predictions by the model in the batch and add this to the total correct\t\tcorrect  = count_correct(model_predictions .cpu() .detach() .numpy(), labels .numpy())\t\ttotal_correct  += correct\t\t print( &#34;Evaluation batch index: &#34; + str(batch_number) + &#34;/&#34; + str( len(dataloader)) +  &#34; ( &#34; + str(batch_number / len(dataloader) * 100) + &#34;% )&#34;, end = &#39; \\r &#39;) \t #Compute the average loss and accuracy across the epoch\taverage_loss  = total_loss  /  len(dataloader)\taccuracy  = total_correct  / dataloader .dataset .__len__()\t return average_loss, accuracy\n  Now that we have defined all the functions needed to train our model, we can finally fine-tune it and see what happens! Note that SST2 is one of many dataset stored in  HuggingFace Datasets making it incredibly easy to load and use.\n from  datasets  import load_dataset from  torch.utils.data  import DataLoader from  transformers  import AdamW from  transformers  import DistilBertTokenizer, DistilBertForSequenceClassification #Get the GPU device if it exists, load the SST-2 dataset, and create PyTorch datasets and dataloaders for the training and validation setsGPU  = get_gpu()sst2_dataset  = load_dataset( &#34;sst&#34;,  &#34;default&#34;)train_dataset  = SST_Dataset(sst2_dataset[ &#39;train&#39;])valid_dataset  = SST_Dataset(sst2_dataset[ &#39;validation&#39;])train_dataloader  = DataLoader(train_dataset, batch_size = 32, shuffle = True, num_workers = 4)valid_dataloader  = DataLoader(valid_dataset, batch_size = 32, shuffle = False, num_workers = 4)  #Create the tokenizer, model, optimizer, and criteriontokenizer  = DistilBertTokenizer .from_pretrained( &#39;distilbert-base-uncased&#39;)model  = transfer_device(GPU, DistilBertForSequenceClassification .from_pretrained( &#39;distilbert-base-uncased&#39;))optimizer  = AdamW(model .parameters(), lr = 2e-5)criterion  = binary_cross_entropy  #Train and save the modelmodel  = train_model(GPU, train_dataloader, valid_dataloader, tokenizer, model, optimizer, criterion)torch .save({\t &#39;tokenizer&#39;: tokenizer,\t &#39;model_state_dict&#39;: model .state_dict()},\tmodel + &#34;.pt&#34;) return\n     After fine-tuning both DistilBERT and BERT on the SST2 dataset for 3 epochs their performance was evaluated on the validation and test sets. Numbers below are accuracy scores averaged across 8 separate model training runs:\n   Although code for training on QQP is not shown in this blog, our  Colab Notebook can easily be modified to accomodate this data. The primary changes to make are editing the PyTorch dataset to handle two text inputs to a model, question 1 and question 2, as well as adjusting the input to the tokenizer. Results from fine-tuning DistilBERT on QQP for 3 epochs with performance being evaluated on the validation set can be seen below. Note that the accuracy score is averaged across 8 separate model training runs:\n   In this blog we learned how to fine-tune Transformers on downstream tasks, specifically sentiment analysis and duplicate question detection. By fine-tuning pre-trained Transformers significant time can be saved w\n......", "posttime": "2021-06-21 23:03:26", "source_domain": "news.ycombinator.com", "source_name": "Hacker News", "tags": "nlp,tuning,\u6a21\u578b", "title": "\u7528\u4e8eNLP\u7684\u5fae\u8c03\u53d8\u538b\u5668 ", "title_en": "Fine-Tuning Transformers for NLP", "transed": 1, "url": "https://www.assemblyai.com/blog/fine-tuning-transformers-for-nlp", "via": "", "real_tags": ["nlp", "tuning", "\u6a21\u578b"]}]