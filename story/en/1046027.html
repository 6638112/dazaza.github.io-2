<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为神经网络通用理论构建的基础（2019） Foundations Built for a General Theory of Neural Networks (2019)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Foundations Built for a General Theory of Neural Networks (2019)<br/>为神经网络通用理论构建的基础（2019） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-24 03:38:47</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/1/cf8c135f0462753e65a7d5bf04d5322e.jpg"><img src="http://img2.diglog.com/img/2021/1/cf8c135f0462753e65a7d5bf04d5322e.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>When we design a skyscraper we expect it will perform to specification: that the tower will support so much weight and be able to withstand an earthquake of a certain strength.</p><p>当我们设计摩天大楼时，我们希望它会达到规格：该塔将承受如此之大的重量，并能够承受一定强度的地震。</p><p> But with one of the most important technologies of the modern world, we’re effectively building blind. We play with different designs, tinker with different setups, but until we take it out for a test run, we don’t really know what it can do or where it will fail.</p><p> 但是，借助现代世界上最重要的技术之一，我们正在有效地建立盲目地位。我们采用不同的设计，采用不同的设置，但是直到我们将其取出进行测试运行之前，我们才真正知道它可以做什么或将在哪里失败。</p><p> This technology is the neural network, which underpins today’s most advanced artificial intelligence systems. Increasingly, neural networks are moving into the core areas of society: They determine what we learn of the world through our social media feeds, they help  doctors diagnose illnesses, and they even  influence whether a person convicted of a crime will spend time in jail.</p><p> 这项技术就是神经网络，它是当今最先进的人工智能系统的基础。神经网络越来越多地进入社会的核心领域：它们通过我们的社交媒体提要来确定我们对世界的了解，它们可以帮助医生诊断疾病，甚至可以影响被判有罪的人是否会在监狱中度过。</p><p>  Yet “the best approximation to what we know is that we know almost nothing about how neural networks actually work and what a really insightful theory would be,” said  Boris Hanin, a mathematician at Texas A&amp;M University and a visiting scientist at Facebook AI Research who studies neural networks.</p><p>  然而，“我们所知道的最好的近似结果是，我们几乎不了解神经网络的实际工作原理以及真正有见地的理论。”德克萨斯农工大学的数学家，Facebook AI的客座科学家Boris Hanin说道。研究谁研究神经网络。</p><p> He likens the situation to the development of another revolutionary technology: the steam engine. At first, steam engines weren’t good for much more than pumping water. Then they powered trains, which is maybe the level of sophistication neural networks have reached. Then scientists and mathematicians developed a theory of thermodynamics, which let them understand exactly what was going on inside engines of any kind. Eventually, that knowledge took us to the moon.</p><p> 他将这种情况比喻为另一种革命性技术的发展：蒸汽机。最初，蒸汽机的优点不仅仅在于抽水。然后他们为火车提供动力，这也许已经达到了成熟的神经网络的水平。然后，科学家和数学家开发了热力学理论，使他们可以准确地了解任何类型的发动机内部发生的情况。最终，这些知识使我们登上了月球。</p><p> “First you had great engineering, and you had some great trains, then you needed some theoretical understanding to go to rocket ships,” Hanin said.</p><p> 哈宁说：“首先，您拥有出色的工程技术，并且拥有一些出色的火车，然后需要一定的理论理解才能上火箭船。”</p><p> Within the sprawling community of neural network development, there is a small group of mathematically minded researchers who are trying to build a theory of neural networks — one that would explain how they work and guarantee that if you construct a neural network in a prescribed manner, it will be able to perform certain tasks.</p><p> 在庞大的神经网络开发社区中，有一小群具有数学思维的研究人员正在尝试建立一种神经网络理论，该理论将解释它们的工作原理，并保证如果您以规定的方式构建神经网络，它将能够执行某些任务。 </p><p> This work is still in its very early stages, but in the last year researchers have produced several papers which elaborate the relationship between form and function in neural networks. The work takes neural networks all the way down to their foundations. It shows that long before you can certify that neural networks can drive cars, you need to prove that they can multiply.</p><p>这项工作仍处于初期阶段，但去年研究人员发表了几篇论文，阐述了神经网络中形式与功能之间的关系。这项工作使神经网络一直深入到其基础。它表明，在您可以证明神经网络可以驾驶汽车之前，您需要证明它们可以繁殖。</p><p>  Neural networks aim to mimic the human brain — and one way to think about the brain is that it works by accreting smaller abstractions into larger ones. Complexity of thought, in this view, is then measured by the range of smaller abstractions you can draw on, and the number of times you can combine lower-level abstractions into higher-level abstractions — like the way we learn to distinguish dogs from birds.</p><p>  神经网络旨在模仿人的大脑，而思考大脑的一种方法是，它通过将较小的抽象吸收到较大的抽象中来起作用。在这种情况下，思想的复杂性由您可以借鉴的较小抽象的范围以及可以将较低级别的抽象组合为较高级别的抽象的次数（例如我们学会区分狗与鸟的方式）来衡量。</p><p> “For a human, if you’re learning how to recognize a dog you’d learn to recognize four legs, fluffy,” said  Maithra Raghu, a doctoral student in computer science at Cornell University and a member of  Google Brain. “Ideally we’d like our neural networks to do the same kinds of things.”</p><p> 康奈尔大学计算机科学专业的博士生，谷歌大脑的成员Maithra Raghu说：“对于人类来说，如果您正在学习如何识别狗，就必须学会识别四只毛茸茸的腿。” “理想情况下，我们希望我们的神经网络做同样的事情。”</p><p>  Abstraction comes naturally to the human brain. Neural networks have to work for it. As with the brain, neural networks are made of building blocks called “neurons” that are connected in various ways. (The neurons in a neural network are inspired by neurons in the brain but do not imitate them directly.) Each neuron might represent an attribute, or a combination of attributes, that the network considers at each level of abstraction.</p><p>  抽象自然地出现在人脑中。神经网络必须为此而努力。与大脑一样，神经网络由称为“神经元”的构建块组成，这些构建块以各种方式连接。 （神经网络中的神经元受到大脑中神经元的启发，但并不直接模仿它们。）每个神经元可能代表网络在每个抽象级别考虑的一个属性或属性组合。</p><p> When joining these neurons together, engineers have many choices to make. They have to decide how many layers of neurons the network should have (or how “deep” it should be). Consider, for example, a neural network with the task of recognizing objects in images. The image enters the system at the first layer. At the next layer, the network might have neurons that simply detect edges in the image. The next layer combines lines to identify curves in the image. Then the next layer combines curves into shapes and textures, and the final layer processes shapes and textures to reach a conclusion about what it’s looking at: woolly mammoth!</p><p> 将这些神经元连接在一起时，工程师可以做出很多选择。他们必须决定网络应具有多少层神经元（或应具有多少“深度”）。例如，考虑一个神经网络，其任务是识别图像中的对象。图像在第一层进入系统。在下一层，网络可能具有简单地检测图像边缘的神经元。下一层结合线以识别图像中的曲线。然后，下一层将曲线组合成形状和纹理，最后一层处理形状和纹理，以得出关于其外观的结论：羊毛猛ma象！</p><p> “The idea is that each layer combines several aspects of the previous layer. A circle is curves in many different places, a curve is lines in many different places,” said  David Rolnick, a mathematician at the University of Pennsylvania.</p><p> “想法是，每一层都结合了上一层的几个方面。宾夕法尼亚大学的数学家戴维·罗尼克（David Rolnick）说：“在很多地方，圆是曲线，在很多地方，曲线是直线。”</p><p> Engineers also have to decide the “width” of each layer, which corresponds to the number of different features the network is considering at each level of abstraction. In the case of image recognition, the width of the layers would be the number of types of lines, curves or shapes it considers at each level.</p><p> 工程师还必须确定每个层的“宽度”，该宽度与网络在每个抽象级别考虑的不同功能的数量相对应。在图像识别的情况下，层的宽度将是它在每个级别考虑的线，曲线或形状的类型数。 </p><p> Beyond the depth and width of a network, there are also choices about how to connect neurons within layers and between layers, and how much weight to give each connection.</p><p>除了网络的深度和宽度，还存在关于如何在层内和层之间连接神经元以及如何赋予每个连接多少权重的选择。</p><p> So if you have a specific task in mind, how do you know which neural network architecture will accomplish it best? There are some broad rules of thumb. For image-related tasks, engineers typically use “convolutional” neural networks, which feature the same pattern of connections between layers repeated over and over. For natural language processing — like speech recognition, or language generation — engineers have found that “recurrent” neural networks seem to work best. In these, neurons can be connected to non-adjacent layers.</p><p> 因此，如果您有特定的任务，您如何知道哪种神经网络架构将最好地完成它？有一些广泛的经验法则。对于与图像相关的任务，工程师通常使用“卷积”神经网络，该网络具有反复重复的图层之间相同的连接模式。对于自然语言处理（例如语音识别或语言生成），工程师发现“递归”神经网络似乎效果最好。在这些神经元中，神经元可以连接到不相邻的层。</p><p>  Beyond those general guidelines, however, engineers largely have to rely on experimental evidence: They run 1,000 different neural networks and simply observe which one gets the job done.</p><p>  但是，除了这些一般性准则之外，工程师在很大程度上还必须依靠实验证据：他们运行1,000个不同的神经网络，并仅观察哪个神经网络可以完成工作。</p><p> “These choices are often made by trial and error in practice,” Hanin said. “That’s sort of a tough [way to do it] because there are infinitely many choices and one really doesn’t know what’s the best.”</p><p> 哈宁说：“这些选择通常是通过实践中的反复试验来做出的。” “这很难做到（因为这样做），因为有无数的选择，而且真的不知道什么是最好的。”</p><p> A better approach would involve a little less trial and error and a little more upfront understanding of what a given neural network architecture gets you. A few papers published recently have moved the field in that direction.</p><p> 更好的方法是减少试验和错误，并对给定的神经网络体系结构能给您带来更多的了解。最近发表的一些论文已经朝着这个方向发展了。</p><p> “This work tries to develop, as it were, a cookbook for designing the right neural network. If you know what it is that you want to achieve out of the network, then here is the recipe for that network,” Rolnick said.</p><p> “这项工作试图照原样开发设计正确的神经网络的食谱。如果您知道要从网络中实现的目标，那么这里就是该网络的秘诀。” Rolnick说。</p><p>  One of the earliest important theoretical guarantees about neural network architecture came three decades ago. In 1989, computer scientists proved that if a neural network has only a single computational layer, but you allow that one layer to have an unlimited number of neurons, with unlimited connections between them, the network will be capable of performing any task you might ask of it.</p><p>  关于神经网络体系结构的最早的重要理论保证之一是在三十年前。 1989年，计算机科学家证明，如果神经网络只有一个计算层，但是您允许该层具有无限数量的神经元，并且它们之间具有无限的连接，则该网络将能够执行您可能要求的任何任务它的。 </p><p> It was a sweeping statement that turned out to be fairly intuitive and not so useful. It’s like saying that if you can identify an unlimited number of lines in an image, you can distinguish between all objects using just one layer. That may be true in principle, but good luck implementing it in practice.</p><p>这是一个笼统的陈述，事实证明它相当直观，但没有那么有用。这就像在说，如果您可以识别图像中无限多的线，则可以仅使用一层就可以区分所有对象。从原则上讲这可能是正确的，但在实践中祝您好运。</p><p> Researchers today describe such wide, flat networks as “expressive,” meaning that they’re capable in theory of capturing a richer set of connections between possible inputs (such as an image) and outputs (such as descriptions of the image). Yet these networks are extremely difficult to train, meaning it’s almost impossible to teach them how to actually produce those outputs. They’re also more computationally intensive than any computer can handle.</p><p> 今天，研究人员将如此宽泛的扁平网络描述为“表达性”，这意味着它们在理论上能够捕获可能的输入（例如图像）和输出（例如图像的描述）之间的更丰富的连接。但是，这些网络很难训练，这意味着几乎不可能教他们如何实际产生这些输出。它们的计算量也比任何一台计算机都无法承受。</p><p>  More recently, researchers have been trying to understand how far they can push neural networks in the other direction — by making them narrower (with fewer neurons per layer) and deeper (with more layers overall). So maybe you only need to pick out 100 different lines, but with connections for turning those 100 lines into 50 curves, which you can combine into 10 different shapes, which give you all the building blocks you need to recognize most objects.</p><p>  最近，研究人员一直在尝试了解它们可以将神经网络推向另一个方向–通过使它们更窄（每层更少的神经元）和更深（整体上更多的层）。因此，也许您只需要挑选100条不同的线，但是有了将这100条线变成50条曲线的连接，您可以将它们组合成10种不同的形状，从而为您提供识别大多数对象所需的所有构造块。</p><p> In a  paper completed last year, Rolnick and  Max Tegmark of the Massachusetts Institute of Technology proved that by increasing depth and decreasing width, you can perform the same functions with exponentially fewer neurons. They showed that if the situation you’re modeling has 100 input variables, you can get the same reliability using either 2 100 neurons in one layer or just 2 10 neurons spread over two layers. They found that there is power in taking small pieces and combining them at greater levels of abstraction instead of attempting to capture all levels of abstraction at once.</p><p> 在去年完成的一篇论文中，麻省理工学院的Rolnick和Max Tegmark证明，通过增加深度和减小宽度，可以用更少的神经元执行相同的功能。他们表明，如果要建模的情况具有100个输入变量，则可以使用一层中的2 100个神经元或仅分布在两层中的2 10个神经元来获得相同的可靠性。他们发现，可以将小片段合并为更高的抽象级别，而不是尝试一次捕获所有抽象级别。</p><p> “The notion of depth in a neural network is linked to the idea that you can express something complicated by doing many simple things in sequence,” Rolnick said. “It’s like an assembly line.”</p><p> Rolnick说：“神经网络中的深度概念与您可以通过顺序执行许多简单操作来表达复杂内容的想法相关联。” “这就像一条装配线。”</p><p> Rolnick and Tegmark proved the utility of depth by asking neural networks to perform a simple task: multiplying polynomial functions. (These are just equations that feature variables raised to natural-number exponents, for example  y =  x 3 + 1.) They trained the networks by showing them examples of equations and their products. Then they asked the networks to compute the products of equations they hadn’t seen before. Deeper neural networks learned the task with far fewer neurons than shallower ones.</p><p> Rolnick和Tegmark通过要求神经网络执行一个简单的任务：乘以多项式函数，证明了深度的实用性。 （这些只是方程式，其变量具有升至自然数指数的值，例如y = x 3 +1。）他们通过向网络展示方程式及其乘积的示例来训练网络。然后，他们要求网络计算以前从未见过的方程式的乘积。较深的神经网络所学习的任务要比浅层的神经元少得多。</p><p> And while multiplication isn’t a task that’s going to set the world on fire, Rolnick says the paper made an important point: “If a shallow network can’t even do multiplication then we shouldn’t trust it with anything else.”</p><p> Rolnick说，尽管乘法并不是要让世界着火的任务，但论文指出了一个重要观点：“如果浅层网络甚至无法进行乘法，那么我们就不应再以其他任何方式来信任它。” </p><p>  Other researchers have been probing the minimum amount of width needed. At the end of September,  Jesse Johnson, formerly a mathematician at Oklahoma State University and now a researcher with the pharmaceutical company Sanofi, proved that at a certain point,  no amount of depth can compensate for a lack of width.</p><p>其他研究人员一直在探索所需的最小宽度。 9月底，杰西·约翰逊（Jesse Johnson）曾在俄克拉荷马州立大学任数学家，现在在赛诺菲（Sanofi）制药公司任研究员，他证明，在一定程度上，没有任何深度可以弥补宽度的不足。</p><p> To get a sense of his result, imagine sheep in a field, except these are punk-rock sheep: Their wool has been dyed one of several colors. The task for your neural network is to draw a border around all sheep of the same color. In spirit, this task is similar to image classification: The network has a collection of images (which it represents as points in higher-dimensional space), and it needs to group together similar ones.</p><p> 为了了解他的结果，想象一下田野里的绵羊，除了这些是朋克摇滚绵羊：它们的羊毛已经被染成几种颜色之一。您的神经网络的任务是在所有相同颜色的绵羊周围绘制边框。从本质上讲，此任务类似于图像分类：网络具有图像集合（它表示为高维空间中的点），并且需要将相似的图像分组在一起。</p><p> Johnson proved that a neural network will fail at this task when the width of the layers is less than or equal to the number of inputs. So for our sheep, each can be described with two inputs: an  x and a  y coordinate to specify its position in the field. The neural network then labels each sheep with a color and draws a border around sheep of the same color. In this case, you will need three or more neurons per layer to solve the problem.</p><p> 约翰逊证明，当层的宽度小于或等于输入数时，神经网络将无法完成此任务。因此，对于我们的绵羊，可以用两个输入来描述它们：x和y坐标以指定其在字段中的位置。然后，神经网络用颜色标记每只绵羊，并在相同颜色的绵羊周围绘制边框。在这种情况下，您将需要每层三个或更多神经元来解决问题。</p><p> More specifically, Johnson showed that if the width-to-variable ratio is off, the neural network won’t be able to draw closed loops — the kind of loops the network would need to draw if, say, all the red sheep were clustered together in the middle of the pasture. “If none of the layers are thicker than the number of input dimensions, there are certain shapes the function will never be able to create, no matter how many layers you add,” Johnson said.</p><p> 更具体地说，约翰逊（Johnson）表明，如果宽度与可变比不正确，则神经网络将无法绘制闭环-如果所有的红羊都聚在一起，则网络将需要绘制这种环。一起在牧场中间约翰逊说：“如果任何一层都不比输入尺寸的厚度要厚，那么无论您添加多少层，功能都无法创建某些形状。”</p><p>  Papers like Johnson’s are beginning to build the rudiments of a theory of neural networks. At the moment, researchers can make only very basic claims about the relationship between architecture and function — and those claims are in small proportion to the number of tasks neural networks are taking on.</p><p>  约翰逊（Johnson）之类的论文开始建立起神经网络理论的基础。目前，研究人员只能对架构和功能之间的关系做出非常基本的声明，而这些声明与神经网络正在执行的任务数量不成比例。</p><p> So while the theory of neural networks isn’t going to change the way systems are built anytime soon, the blueprints are being drafted for a new theory of how computers learn — one that’s poised to take humanity on a ride with even greater repercussions than a trip to the moon.</p><p> 因此，尽管神经网络理论不会在短期内改变系统的构建方式，但蓝图正在起草一种有关计算机如何学习的新理论的蓝图，该理论有望带给人类更大的反响。登月之旅。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.quantamagazine.org/foundations-built-for-a-general-theory-of-neural-networks-20190131/">https://www.quantamagazine.org/foundations-built-for-a-general-theory-of-neural-networks-20190131/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/通用/">#通用</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/built/">#built</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>