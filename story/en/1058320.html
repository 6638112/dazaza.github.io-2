<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>欧盟规划以风险为基础的AI规则，将罚款设置为高达全球营业额的4％，每次泄露草案 
				EU plan for risk-based AI rules to set fines as high as 4% of global turnover, per leaked draft			</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">
				EU plan for risk-based AI rules to set fines as high as 4% of global turnover, per leaked draft			<br/>欧盟规划以风险为基础的AI规则，将罚款设置为高达全球营业额的4％，每次泄露草案 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-14 21:28:48</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/96c4345b957aacb047cc4dd696da17ed.jpg"><img src="http://img2.diglog.com/img/2021/4/96c4345b957aacb047cc4dd696da17ed.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>The plan to regulate AI has been on the cards for a while. Back in  February 2020 the European Commission published a white paper, sketching plans for regulating so-called “high risk” applications of artificial intelligence.</p><p>调节AI的计划一直在卡上。回到2020年2月，欧盟委员会出版了一份白皮书，素描计划的人工智能调节所谓的“高风险”应用。</p><p> At the time EU lawmakers were toying with a sectoral focus — envisaging certain sectors like energy and recruitment as vectors for risk. However that approach appears to have been rethought, per the leaked draft — which does not limit discussion of AI risk to particular industries or sectors.</p><p> 在欧盟立法者正在与部门焦点进行到目前为止 - 设想某些行业，如能量和招聘等载体。然而，根据泄露的草案，这种方法似乎已经受到了重新要求 - 这不会限制对特定行业或部门的风险的讨论。</p><p>  Instead, the focus is on compliance requirements for high risk AI applications, wherever they may occur (weapons/military uses are specifically excluded, however, as such use-cases fall outside the EU treaties). Although it’s not abundantly clear from this draft exactly how ‘high risk’ will be defined.</p><p>  相反，重点是对高风险AI应用的合规性要求，无论它们都可能发生（武器/军用用途都被特别排除在外，因为这种用例落在欧盟条约之外）。虽然它从这个草案中没有大量清楚，但完全如何定义“高风险”。</p><p> The overarching goal for the Commission here is to boost public trust in AI, via a system of compliance checks and balances steeped in “EU values” in order to encourage uptake of so-called “trustworthy” and “human-centric” AI. So even makers of AI applications not considered to be ‘high risk’ will still be encouraged to adopt codes of conduct — “to foster the voluntary application of the mandatory requirements applicable to high-risk AI systems”, as the Commission puts it.</p><p> 委员会的总体目标是通过遵守“欧盟价值观”的合规性检查和余额来提高AI的公众信任，以鼓励吸收所谓的“值得信赖”和“以人为本”的AI。因此，即使委员会仍然需要采用“高风险”的AI申请，未被视为“高风险”的制造商仍然被鼓励采用“适用于高风险AI系统的强制性要求”的行为守则。</p><p> Another chunk of the regulation deals with measures to support AI development in the bloc — pushing Member States to establish regulatory sandboxing schemes in which startups and SMEs can be proritized for support to develop and test AI systems before bringing them to market.</p><p> 该监管的另一个块涉及支持Bloc推动会员国中AI开发的措施，以建立监管沙箱计划，其中可以追溯起动和中小企业，以便在将其推向市场之前支持开发和测试AI系统。</p><p> Competent authorities “shall be empowered to exercise their discretionary powers and levers of proportionality in relation to artificial intelligence projects of entities participating the sandbox, while fully preserving authorities’ supervisory and corrective powers,” the draft notes.</p><p> 主管当局“应赋予与参与沙箱的实体人工智能项目的酌情权力和比例杠杆行使，同时完全保护当局的监督和纠正能力”，说明。</p><p>  Under the planned rules, those intending to apply artificial intelligence will need to determine whether a particular use-case is ‘high risk’ and thus whether they need to conduct a mandatory, pre-market compliance assessment or not.</p><p>  根据计划的规则，那些打算应用人工智能的人需要确定特定用例是否是“高风险”，从而需要进行强制性，预先市场的合规性评估。 </p><p> “The classification of an AI system as high-risk should be based on its intended purpose — which should refer to the use for which an AI system is intended, including the specific context and conditions of use and — and be determined in two steps by considering whether it may cause certain harms and, if so, the severity of the possible harm and the probability of occurrence,” runs one recital in the draft.</p><p>“AI系统的分类为高风险应基于其预期目的 - 这应该是指使用AI系统的用途，包括特定的上下文和使用条件和 - 并且由两个步骤确定考虑到是否可能造成某些危害，如果是这样，如果是这样，可能危害的严重程度和发生的可能性，“在草案中运行了一个争论。</p><p> “A classification of an AI system as high-risk for the purpose of this Regulation may not necessarily mean that the system as such or the product as a whole would necessarily be considered as ‘high-risk’ under the criteria of the sectoral legislation,” the text also specifies.</p><p> “AI系统的分类作为本规例的目的的高风险可能并不一定意味着在部门立法的标准下，整个系统的系统或产品的整体将被视为”高风险“， “该文本还指定。</p><p> Examples of “harms” associated with high-risk AI systems are listed in the draft as including: “the injury or death of a person, damage of property, systemic adverse impacts for society at large, significant disruptions to the provision of essential services for the ordinary conduct of critical economic and societal activities, adverse impact on financial, educational or professional opportunities of persons, adverse impact on the access to public services and any form of public assistance, and adverse impact on [European] fundamental rights.”</p><p> 与高风险AI系统相关的“危害”的例子在选秀中列出：“人的伤害或死亡，财产损害，社会的全身不利影响，对提供基本服务的重要中断普通的经济和社会活动的普通行为，对金融，教育或专业机遇的不利影响，对公共服务的获取和任何形式的公共援助的影响，以及对[欧洲]基本权利的不利影响。“</p><p> Several examples of high risk applications are also discussed — including recruitment systems; systems that provide access to educational or vocational training institutions; emergency service dispatch systems; creditworthiness assessment; systems involved in determining taxpayer-funded benefits allocation; decision-making systems applied around the prevention, detection and prosecution of crime; and decision-making systems used to assist judges.</p><p> 还讨论了高风险应用的几个例子 - 包括招聘系统;提供对教育或职业培训机构的访问的系统;紧急服务调度系统;信誉评估;涉及确定纳税人资助的福利分配的系统;在预防，检测和起诉犯罪周围申请决策系统;和用于协助法官的决策系统。</p><p> So long as compliance requirements — such as establishing a risk management system and carrying out post-market surveillance, including via a quality management system — are met such systems would not be barred from the EU market under the legislative plan.</p><p> 只要合规要求 - 如建立风险管理系统并开展市场后监督，包括通过质量管理体系 - 达到此类系统，这些系统不会被禁止在立法计划下的欧盟市场。</p><p> Other requirements include in the area of security and that the AI achieves consistency of accuracy in performance — with a stipulation to report to “any serious incidents or any malfunctioning of the AI system which constitutes a breach of obligations” to an oversight authority no later than 15 days after becoming aware of it.</p><p> 其他要求包括在安全领域，并且AI在绩效方面取得了一致性 - 以规定向“违反义务”而非违反义务的任何严重事件或任何故障，违反义务“的概况意识到15天后。</p><p> “High-risk AI systems may be placed on the Union market or otherwise put into service subject to compliance with mandatory requirements,” the text notes.</p><p> “高风险的AI系统可以放在Union Market上或以其他方式投入到符合强制要求的情况下，”文本说明。 </p><p> “Mandatory requirements concerning high-risk AI systems placed or otherwise put into service on the Union market should be complied with taking into account the intended purpose of the AI system and according to the risk management system to be established by the provider.</p><p>“应遵守AI系统的预期目的，并根据提供者建立的风险管理系统，遵守或以其他方式投入或以其他方式投入或以其他方式投入使用的高风险AI系统或以其他方式投入使用。</p><p> “Among other things, risk control management measures identified by the provider should be based on due consideration of the effects and possible interactions resulting from the combined application of the mandatory requirements and take into account the generally acknowledged state of the art, also including as reflected in relevant harmonised standards or common specifications.”</p><p> “除此之外，提供商确定的风险控制管理措施应基于适当考虑由强制性要求的合并应用所产生的效果和可能的相互作用，并考虑到普遍承认的本领域，也包括反映在相关的统一标准或普通规范中。“</p><p>   Certain AI “practices” are listed as prohibited under Article 4 of the planned law, per this leaked draft — including (commercial) applications of mass surveillance systems and general purpose social scoring systems which could lead to discrimination.</p><p>   某些AI“实践”列为根据计划法第4条禁止的，根据计划法的第4条，包括（商业）大规模监测系统和可能导致歧视的通用社会评分系统。</p><p> AI systems that are designed to manipulate human behavior, decisions or opinions to a detrimental end (such as via dark pattern design UIs), are also listed as prohibited under Article 4; as are systems that use personal data to generate predictions in order to (detrimentally) target the vulnerabilities of persons or groups of people.</p><p> 旨在操纵人类行为，决策或意见的AI系统（如通过黑暗模式设计UIS），也列为根据第4条禁止;与使用个人数据生成预测的系统是（不利地）针对人物或人群群体的漏洞。</p><p> A casual reader might assume the regulation is proposing to ban, at a stroke, practices like behavioral advertising based on people tracking — aka the business models of companies like Facebook and Google. However that assumes adtech giants will accept that their tools have a detrimental impact on users.</p><p> 休闲读者可能会假设规定禁止行为广告等行为广告的行为广告（基于人员跟踪） -  AKA像Facebook和Google等公司的商业模式。然而，假设Adtech Giants将接受他们的工具对用户产生了有害影响。</p><p> On the contrary, their regulatory circumvention strategy is based on claiming the polar opposite; hence Facebook’s talk of “relevant” ads. So the text (as written) looks like it will be a recipe for (yet) more long-drawn out legal battles to try to make EU law stick vs the self-interested interpretations of tech giants.</p><p> 相反，他们的监管规避策略是基于声称对方的极地;因此，Facebook的谈论“相关”广告。所以文本（书面形式）看起来像是（又人）更加长的法律战斗的食谱，以试图让欧盟法律坚持与科技巨头的自我诠释。</p><p> The rational for the prohibited practices is summed up in an earlier recital of the draft — which states: “It should be acknowledged that artificial intelligence can enable new manipulative, addictive, social control and indiscriminate surveillance practices that are particularly harmful and should be prohibited as contravening the Union values of respect for human dignity, freedom, democracy, the rule of law and respect for human rights.”</p><p> 禁止做法的理性在草案的早期协奏曲中总结 - 哪些国家：“应该承认人工智能可以实现特别有害的新操纵，上瘾，社会控制和不分青红皂白的监督实践，并且应该被禁止违反尊重人权，自由，民主，法治和尊重人权的联盟价值观。“ </p><p> It’s notable that the Commission has avoided proposing a ban on the use of facial recognition in public places — as it had apparently been considering, per a leaked draft  early last year, before last year’s White Paper steered away from a ban.</p><p>值得注意的是，委员会避免提出禁止在公共场所的使用面部承认 - 由于去年年初的泄漏草案，在去年的白皮书之前，它显然已经考虑了，而是从禁令的禁止被引导之前。</p><p> In the leaked draft “remote biometric identification” in public places is singled out for “stricter conformity assessment procedures through the involvement of a notified body” — aka an “authorisation procedure that addresses the specific risks implied by the use of the technology” and includes a mandatory data protection impact assessment — vs most other applications of high risk AIs (which are allowed to meet requirements via self-assessment).</p><p> 在公共场所的泄露草案“远程生物识别”草案中，以“通过通知机构的参与更严格的符合性评估程序” -  AKA一个“授权程序解决了该技术使用所暗示的具体风险”并包括强制性数据保护影响评估 - 对高风险AIS的大多数其他应用（允许通过自我评估达到要求）。</p><p> “Furthermore the authorising authority should consider in its assessment the likelihood and severity of harm caused by inaccuracies of a system used for a given purpose, in particular with regard to age, ethnicity, sex or disabilities,” runs the draft. “It should further consider the societal impact, considering in particular democratic and civic participation, as well as the methodology, necessity and proportionality for the inclusion of persons in the reference database.”</p><p> “此外，授权当局应考虑评估由针对特定目的的系统的不准确性造成的伤害的可能性和严重程度，特别是在年龄，种族，性别或残疾方面，违反了草案。 “考虑到尤其是民主和公民参与，还应考虑社会影响，以及在参考数据库中包含人员的方法，必要性和比例。”</p><p> AI systems “that may primarily lead to adverse implications for personal safety” are also required to undergo this higher bar of regulatory involvement as part of the compliance process.</p><p> AI系统“可能主要导致个人安全的不利影响”，也需要作为合规过程的一部分进行这种更高的监管兼容。</p><p> The envisaged system of conformity assessments for all high risk AIs is ongoing, with the draft noting: “It is appropriate that an AI system undergoes a new conformity assessment whenever a change occurs which may affect the compliance of the system with this Regulation or when the intended purpose of the system changes.”</p><p> 设想的所有高风险AIS的符合性评估系统正在进行，草案注意：“无论何时发生变化，AI系统都会妥善进行新的符合性评估，这可能会影响系统与本规则的遵守情况或当系统的预期目的发生了变化。“</p><p> “For AI systems which continue to ‘learn’ after being placed on the market or put into service (i.e. they automatically adapt how functions are carried out) changes to the algorithm and performance which have not been pre-determined and assessed at the moment of the conformity assessment shall result in a new conformity assessment of the AI system,” it adds.</p><p> “对于在被置于市场或投入服务之后继续”学习“的AI系统（即，他们自动调整函数如何进行）变化，算法和尚未预先确定和评估的性能符合性评估应导致AI系统的新一致性评估，“它增加了。</p><p> The carrot for compliant businesses is to get to display a ‘CE’ mark to help them win the trust of users and friction-free access across the bloc’s single market.</p><p> Carrot for兼容的业务是为了展示'CE'标志，以帮助他们赢得Bloc的单一市场的用户信任和无摩擦接入。 </p><p> “High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the Union,” the text notes, adding that: “Member States should not create obstacles to the placing on the market or putting into service of AI systems that comply with the requirements laid down in this Regulation.”</p><p>“高风险的AI系统应承担CE标记，以表明他们与本规定的符合性，以便他们可以在联盟内自由移动，”文本说明，补充说：“”会员国不应该为市场上的障碍或者造成障碍或者投入符合本规定规定要求的AI系统的服务。“</p><p>   As well as seeking to outlaw some practices and establish a system of pan-EU rules for bringing ‘high risk’ AI systems to market safely — with providers expected to make (mostly self) assessments and fulfil compliance obligations (such as around the quality of the data-sets used to train the model; record-keeping/documentation; human oversight; transparency; accuracy) prior to launching such a product into the market and conduct ongoing post-market surveillance — the proposed regulation seeks shrink the risk of AI being used to trick people.</p><p>   并寻求对某些做法进行禁止建立一个泛欧盟规则，以安全地将“高风险”AI系统带入市场 - 与预期的提供商（主要是自我）评估和履行合规义务（如围绕质量）用于训练模型的数据集;记录保存/文件;人类监督;透明度;准确性在推出这样的产品进入市场并进行正在进行的市场后监督 - 所提出的监管旨在缩小AI的风险曾经欺骗人。</p><p> It does this by suggesting “harmonised transparency rules” for AI systems intended to interact with natural persons (aka voice AIs/chat bots etc); and for AI systems used to generate or manipulate image, audio or video content (aka deepfakes).</p><p> 它通过建议旨在与自然人互动的AI系统（AKA语音AIS / CHAT BOTS等）的“统一透明度规则”来实现这一点;对于用于生成或操纵图像，音频或视频内容（AKA DeepFakes）的AI系统。</p><p> “Certain AI systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the requirements and obligations for high-risk AI systems,” runs the text.</p><p> “旨在与自然人互动或产生内容的某些AI系统可能会对冒险或欺骗的特定风险造成特定的风险，而不管他们是否有资格作为高风险。因此，在某些情况下，这些系统的使用应受特定的透明度义务，而不会妨碍高风险AI系统的要求和义务，“营业。</p><p> “In particular, natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use. Moreover, users, who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a reasonable person to be authentic, should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin.</p><p> “特别是，应该向自然人通知他们与AI系统进行交互，除非这是明显的情况和使用的背景。此外，使用AI系统生成或操纵图像，音频或视频内容的用户明显类似于现有人，地点或事件，并且将被错误地出现到合理的人是真实的，应该披露内容已经人工创建或通过标记人工智能产出并披露其人工来源来操纵。</p><p> “This labelling obligation should not apply where the use of such content is necessary for the purposes of safeguarding public security or for the exercise of a legitimate right or freedom of a person such as for satire, parody or freedom of arts and sciences and subject to appropriate safeguards for the rights and freedoms of third parties.”</p><p> “此标签义务不应申请使用此类内容的使用是为了保障公共安全的目的，以便行使讽刺，模仿或艺术和科学自由等人的合法权利或自由等待适当的第三方权利和自由的保障措施。“</p><p>   While the proposed AI regime hasn’t yet been officially unveiled by the Commission — so details could still change before next week — a major question mark looms over how a whole new layer of compliance around specific applications of (often complex) artificial intelligence can be effectively oversee and any violations enforced, especially given ongoing weaknesses in the enforcement of the EU’s data protection regime (which begun being applied back in 2018).</p><p>   虽然建议的AI政权尚未被委员会正式揭晓 - 所以细节可能在下周之前仍然可以改变 - 一个主要的问号界限，关于如何全新的（通常复杂）人工智能的特定应用程序有效监督和任何强制执行的违规行为，特别是在执行欧盟的数据保护制度（2018年开始申请的持续缺陷）。 </p><p> So while providers of high risk AIs are required to take responsibility for putting their system/s on the market (and therefore for compliance with all the various stipulations, which also include registering high risk AI systems in an EU database the Commission intends to maintain), the proposal leaves enforcement in the hands of Member States — who will be responsible for designating one or more national competent authorities to supervise application of the oversight regime.</p><p>因此，虽然高风险AIS的提供者必须承担将其系统置于市场上的责任（因此，遵守所有各种规定，也包括在欧盟数据库中注册高风险AI系统，委员会打​​算维持） ，该提案在会员国手中留下执法 - 谁将负责指定一个或多个国家主管当局来监督监督制度的申请。</p><p> We’ve seen how this story plays out with the General Data Protection Regulation. The Commission itself has conceded  GDPR enforcement is not consistently or vigorously applied across the bloc — so a major question is how these fledgling AI rules will avoid the same  forum-shopping fate?</p><p> 我们已经看到了这个故事如何通过一般数据保护规范脱颖而出。委员会本身已经承认了GDPR执法，并不一致或大力应用于集团 - 所以一个主要的问题是这些Fledgling AI规则将如何避免同样的论坛购物命运？</p><p>  “Member States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement. For certain specific infringements, Member States should take into account the margins and criteria set out in this Regulation,” runs the draft.</p><p>  “会员国应采取一切必要措施，以确保实施该监管的规定，包括为其侵权制定有效，比例和孤立罚款。对于某些特定的侵权，会员国应考虑到本条例所载的利润率和标准，“载草稿。</p><p> The Commission does add a caveat — about potentially stepping in in the event that Member State enforcement doesn’t deliver. But there’s no near term prospect of a different approach to enforcement, suggesting the same old pitfalls will likely appear.</p><p> 委员会确实增加了一个警告 - 在会员国执法不提供的情况下可能介绍。但是，不同的执法方法没有近期前景，建议出现相同的旧陷阱。</p><p> “Since the objective of this Regulation, namely creating the conditions for an ecosystem of trust regarding the placing on the market, putting into service and use of artificial intelligence in the Union, cannot be sufficiently achieved by the Member States and can rather, by reason of the scale or effects of the action, be better achieved at Union level, the Union may adopt measures, in accordance with the principle of subsidiarity as set out in Article 5 of the Treaty on European Union,” is the Commission’s back-stop for future enforcement failure.</p><p> “由于本规定的目标，即为在市场上配售市场的基督信制的生态系统的情况下，投入使用和在工会中使用人工智能，不能通过成员国充分实现，而且可以通过理性地实现行动的规模或影响，在联盟水平上取得更好，联盟可以按照欧洲联盟条约第5条所载的子公司提出的辅助原则，“委员会的回到停止未来的执法失败。</p><p> The oversight plan for AI includes setting up a mirror entity akin to the GDPR’s European Data Protection Board — to be called the European Artificial Intelligence Board — which will similarly support application of the regulation by issuing relevant recommendations and opinions for EU lawmakers, such as around the list of prohibited AI practices and high-risk systems.</p><p> AI的监督计划包括将镜像实体设置为类似于GDPR的欧洲数据保护委员会 - 被称为欧洲人工智能委员会 - 这将通过发布欧盟立法者的相关建议和意见，同样支持对规定的应用禁止的AI实践和高风险系统名单。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://techcrunch.com/2021/04/14/eu-plan-for-risk-based-ai-rules-to-set-fines-as-high-as-4-of-global-turnover-per-leaked-draft/">https://techcrunch.com/2021/04/14/eu-plan-for-risk-based-ai-rules-to-set-fines-as-high-as-4-of-global-turnover-per-leaked-draft/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/规划/">#规划</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/plan/">#plan</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/系统/">#系统</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>