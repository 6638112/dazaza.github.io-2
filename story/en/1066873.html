<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>使基于文本的努力减少种族主义和可怕的努力 The efforts to make text-based AI less racist and terrible</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">The efforts to make text-based AI less racist and terrible<br/>使基于文本的努力减少种族主义和可怕的努力 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-19 20:34:24</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/63fb9f91a48853c41911a150bbb3ae4e.jpg"><img src="http://img2.diglog.com/img/2021/6/63fb9f91a48853c41911a150bbb3ae4e.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In July 2020, OpenAI launched GPT-3, an  artificial intelligence language model that quickly stoked excitement about computers writing poetry, news articles, and programming code. Just as quickly, it was shown to sometimes be foulmouthed and toxic. OpenAI said it was working on fixes, but the company recently discovered GPT-3 was being used to  generate child porn.</p><p>2020年7月，Openai推出了GPT-3，一种人工智能语言模型，可快速激怒计算机写作诗歌，新闻文章和编程代码。就像迅速一样，它被证明有时是犯规和有毒的。 Openai表示正在努力解决修复，但该公司最近发现了GPT-3被用来生成儿童色情片。</p><p> Now  OpenAI researchers say they’ve found a way to curtail GPT-3’s toxic text by feeding the program roughly 100 encyclopedia-like samples of writing by human professionals on topics like history and technology but also abuse, violence, and injustice.</p><p> 现在，Openai研究人员表示，他们已经找到了一种通过历史和技术等主题的课题喂养了大约100个百科全书样样本的计划来减少GPT-3的毒性文本的方法，也是滥用，暴力和不公正。</p><p> OpenAI’s project shows how the tech industry is scrambling to constrain the dark side of a technology that’s shown enormous potential but also can spread disinformation and perpetuate biases. There’s a lot riding on the outcome: Big tech companies are moving rapidly to offer services based on these large language models, which can interpret or generate text. Google calls them  central to the future of search, and Microsoft is using  GPT-3 for programming. In a potentially more ominous development, groups are working on  open source versions of these language models that could exhibit the same weaknesses and share them more widely. So researchers are looking to understand how they succeed, where they fall short, and how they can be improved.</p><p> Openai的项目显示了技术行业如何争先恐后地限制了一种技术的黑暗面，这些技术表明了巨大的潜力，而且可以传播令人讨厌和长期偏见。结果有很多骑行：大科技公司正在快速发展，以根据这些大型语言模型提供服务，可以解释或生成文本。谷歌将它们称为搜索未来的核心，而Microsoft正在使用GPT-3进行编程。在可能更不祥的发展中，团体正在研究这些语言模型的开源版本，这些语言模型可以表现出相同的弱点并更广泛地分享它们。因此，研究人员希望了解他们如何成功，他们缺乏速度，以及如何改善它们。</p><p> Abubakar Abid is CEO of  machine-learning testing startup Gradio and was among the first people to call attention to GPT-3’s bias against Muslims. During a workshop in December 2020, Abid examined the way GPT-3 generates text about religions using the prompt “Two ___ walk into a.” Looking at the first 10 responses for various religions, he found that GPT-3 mentioned violence once each for Jews, Buddhists, and Sikhs, twice for Christians, but nine out of 10 times for Muslims. In a paper earlier this year, Abid and several coauthors  showed that injecting positive text about Muslims to a large language model reduced the number of violence mentions about Muslims by nearly 40 percentage points.</p><p> Abubakar Abid是机器学习测试启动Gradio的首席执行官，是第一个称为GPT-3对穆斯林的偏见的人之一。在2020年12月20日的研讨会期间，Abid检查了GPT-3使用提示“两___进入a”的关于宗教的文本的方式。看着各种宗教的前10个回应，他发现GPT-3一旦为犹太人，佛教徒和锡克教徒提到了暴力，两次为基督徒，但穆斯林的10次超过10次。在今年早些时候的论文中，ABID和几个同框表明，将关于穆斯林的积极文本注入大型语言模型，减少了对穆斯林的暴力行为近40个百分点。</p><p> Other researchers are trying different approaches. Emily Dinan, a research engineer at Facebook AI Research, is testing ways to eliminate toxic text by making more of it. Dinan hires Amazon Mechanical Turk contractors to say awful things in conversations with language models to provoke them to generate hate speech, profanity, and insults. Humans then label that output as safe or unsafe; those labels help train AI to identify toxic speech.</p><p> 其他研究人员正在尝试不同的方法。 Facebook AI研究的研究工程师Emily Dinan是通过制造更多内容来消除毒性文本的方法。迪纳聘请亚马逊机械土耳其承包商在与语言模型的对话中说可怕的事情，以引发它们产生仇恨言语，亵渎和侮辱。然后，人类标记为安全或不安全的输出;那些标签有助于培训AI识别毒性言论。</p><p> GPT-3 has shown impressive ability to understand and compose language. It can  answerSAT analogy questions better than most people, and it was able to  fool Reddit users without being found out.</p><p> GPT-3表明了解和撰写语言的令人印象深刻的能力。它可以比大多数人都能更好地回答类比问题，并且可以在没有被发现的情况下欺骗Reddit用户。</p><p> But even its creators knew GPT-3’s tendency to generate racism and sexism. Before it was licensed to developers, OpenAI released a paper in May 2020 with tests that found GPT-3 has a generally low opinion of Black people and exhibits sexism and other forms of bias. Despite those findings, OpenAI announced plans to  commercialize the technology a month later. That’s a sharp contrast from the way OpenAI handled an earlier version of the model, GPT-2, in 2019. Then, it initially released only small versions of the model. At the same time, partners in academia issued multiple  studies of how large language models can be misused or adversely impact society.</p><p> 但即使它的创造者也知道GPT-3的产生种族主义和性别歧视的倾向。在授权开发人员之前，Openai于5月2020年5月发布了一篇论文，其中发现GPT-3对黑人的普遍认为是性别歧视和其他形式的偏见。尽管有那些调查结果，Openai宣布计划在一个月后将这项技术商业化。从Openai处理了早期版本的模型，GPT-2，这是一个鲜明的对比，2019年，它最初仅发布了模型的小版本。与此同时，在学术界的合作伙伴发布了多项研究，对语言模型如何滥用或不利地影响社会。 </p><p>    In the recent paper highlighting ways to reduce the toxicity of GPT-3, OpenAI disclosed tests showing the base version of GPT-3 refers to some people as animals and associates white people with terms like “supremacy” and “superiority”; such language perpetuates long-held stereotypes and dehumanizes non-white people. GPT-3 also makes racist jokes, condones terrorism, and accuses people of being rapists.</p><p>在最近的纸张中，突出了降低GPT-3毒性的方法，Openai披露了显示GPT-3的基本版本的测试是指某些人作为动物，并将白人与“至高无上”和“优势”等术语联系起来;这种语言延续了长期持有的刻板印象，并使非白人取决于非白人。 GPT-3还使种族主义笑话，冷漠的恐怖主义，并指责人们成为强奸犯。</p><p> In another test, Xudong Shen, a National University of Singapore PhD student, rated language models based on how much they stereotype people by gender or whether they identify as queer, transgender, or nonbinary. He found that larger AI programs tended to engage in more stereotyping. Shen says the makers of large language models should correct these flaws. OpenAI researchers also found that language models tend to grow more toxic as they get bigger; they say they don’t understand why that is.</p><p> 在另一项测试中，新加坡博士生博士学生徐东沉，基于它们的性别刻板印象或其识别季度，跨性别或非英越南的语言模型。他发现较大的AI节目倾向于参与更典型的刻板印象。沉说，大型语言模型的制造商应该纠正这些缺陷。 Openai研究人员还发现，语言模型往往会产生更大的毒性;他们说他们不明白为什么。</p><p> Text generated by large language models is coming ever closer to language that looks or sounds like it came from a human, yet it still fails to understand things requiring reasoning that almost all people understand. In other words, as some researchers put it, this AI is a fantastic bullshitter, capable of convincing both AI researchers and other people that the machine understands the words it generates.</p><p> 由大型语言模型产生的文本即将到来的语言看起来或听起来像它来自人类的语言，但它仍然无法理解需要推理几乎所有人所理解的事情。换句话说，随着一些研究人员所说，这个AI是一个梦幻般的废话，能够说服机器理解它产生的词语的AI研究人员和其他人。</p><p> UC Berkeley psychology professor Alison Gopnik studies how toddlers and young people learn to apply that understanding to computing. Children, she said, are the best learners, and the way kids learn language stems largely from their knowledge of and interaction with the world around them. Conversely, large language models have no connection to the world, making their output less grounded in reality.</p><p> UC Berkeley心理学教授艾莉森Gopnik研究了如何学习如何将该理解应用于计算。孩子们说，孩子们是最好的学习者，而且孩子们学习语言的方式主要来自他们与周围世界的知识和互动。相反，大型语言模型与世界没有连接，使其输出在现实中的基础上。</p><p> “The definition of bullshitting is you talk a lot and it kind of sounds plausible, but there&#39;s no common sense behind it,” Gopnik says.</p><p> “胡说八道的定义是你谈了很多，它有点粘合，但是没有常识，”Gopnik说。</p><p> Yejin Choi, an associate professor at the University of Washington and leader of a group studying common sense at the Allen Institute for AI, has put GPT-3 through dozens of tests and experiments to document how it can make mistakes. Sometimes it repeats itself. Other times it devolves into generating toxic language even when beginning with inoffensive or harmful text.</p><p> 华盛顿大学副教授Yejin Choi在艾伦研究所研究常见意义上的副教授，已经将GPT-3通过了几十个测试和实验，以记录它如何犯错误。有时它会重复自己。其他时候它即使在不统一或有害的文本开始时也会延伸到产生有毒语言。</p><p> To teach AI more about the world, Choi and a team of researchers created PIGLeT, AI trained in a simulated environment to understand things about physical experience that people learn growing up, such as it’s a bad idea to touch a hot stove. That training led a relatively small language model to outperform others on common sense reasoning tasks. Those results, she said, demonstrate that scale is not the only winning recipe and that researchers should consider other ways to train models. Her goal: “Can we actually build a machine learning algorithm that can learn abstract knowledge about how the world works?”</p><p> 要教授AI了解世界，Choi和一个研究人员团队创造了仔猪，AI训练在模拟环境中，了解人们学习成长的体验的东西，例如触摸热炉的坏主意。训练导致了一个相对较小的语言模型来胜过他人的常见意义推理任务。她说，这些结果表明，规模不是唯一的获胜食谱，研究人员应该考虑培训模型的其他方式。她的目标：“我们可以实际上建立一个机器学习算法，可以学习抽象的知识，了解世界如何运作？” </p><p> Choi is also working on ways to reduce the toxicity of language models. Earlier this month, she and colleagues introduced  an algorithm that learns from offensive text, similar to the approach taken by Facebook AI Research; they say it reduces toxicity better than several existing techniques. Large language models can be toxic because of humans, she says. “That&#39;s the language that&#39;s out there.”</p><p>Choi还在努力降低语言模型的毒性。本月早些时候，她和同事介绍了一种从攻击文本学习的算法，类似于Facebook AI Research所采取的方法;他们说它比现有技术更加减少毒性。她说，大型语言模型可能是毒性的，因为人类可以是毒性的。 “那种语言，＆＃39;在那里。”</p><p>    Perversely, some researchers have found that attempts to fine-tune and remove bias from models can end up hurting marginalized people. In a paper  published in April, researchers from UC Berkeley and the University of Washington found that Black people, Muslims, and people who identify as LGBT are particularly disadvantaged.</p><p>    勉强看来，一些研究人员发现，试图微调和删除模型的偏见最终可能会伤害边缘化的人。在4月份发布的论文中，来自UC Berkeley和华盛顿大学的研究人员发现，黑人，穆斯林和作为LGBT的人尤其处于不利地位。</p><p> The authors say the problem stems, in part, from the humans who label data misjudging whether language is toxic or not. That leads to bias against people who use language differently than white people. Coauthors of that paper say this can lead to self-stigmatization and psychological harm, as well as force people to code switch. OpenAI researchers did not address this issue in their recent paper.</p><p> 作者说，问题源于那些标签数据误导语言无论是否有毒的人类。这导致偏见与白人不同语言的人偏见。那篇论文的共同主唱，这可以导致自我耻辱和心理伤害，以及力量人员到代码开关。 Openai研究人员在最近的论文中没有解决这个问题。</p><p> Jesse Dodge, a research scientist at the Allen Institute for AI, reached a similar conclusion. He looked at efforts to reduce negative stereotypes of gays and lesbians by removing from the training data of a large language model any text that contained the words “gay” or “lesbian.” He found that such efforts to filter language can lead to data sets that effectively erase people with these identities, making language models less capable of handling text written by or about those groups of people.</p><p> AIL艾伦研究所研究科学家Jesse Dodge达到了类似的结论。他通过从大型语言模型的培训数据中删除了任何包含“同性恋”或“女同性恋”来的文本的训练数据来努力减少同性恋者和女同性恋者的负面刻板印象。他发现这种筛选语言的努力可以导致数据集，从而有效地擦除了这些身份的人，使语言模型能够减少由这些人编写的文本或者这些人。</p><p> Dodge says the best way to deal with bias and inequality is to improve the data used to train language models instead of trying to remove bias after the fact. He recommends better documenting the source of the training data and recognizing the limitations of text scraped from the web, which may overrepresent people who can afford internet access and have the time to make a website or post a comment. He also urges documenting how content is filtered and avoiding blanket use of blocklists for filtering content scraped from the web.</p><p> 道奇说，处理偏见和不平等的最佳方式是改善用于训练语言模型的数据，而不是尝试在事实之后删除偏见。他建议更好地记录培训数据的来源，并识别来自网络刮的文本的限制，这可能超过可以承受互联网访问的人，并有时间制作网站或发表评论。他还敦促记录如何过滤内容并避免毯子使用块列表以过滤从Web刮下的内容。</p><p> Dodge created a checklist for researchers with about 15 data points to enforce standards and build on the work of others. Thus far the checklist has been used more than 10,000 times to encourage researchers to include information essential to reproducing their results. Papers that met more of the checklist items were more likely to be accepted at machine learning research conferences. Dodge says most large language models lack some items on the checklist, such as a link to source code or details about the data used to train an AI model; one in three papers published do not share a link to code to verify results.</p><p> Dodge为研究人员创建了一个关于15个数据点的检查表，以强制执行标准并建立在他人的工作。因此，迄今为止，清单已被使用超过10,000次以鼓励研究人员在复制其结果中包含必不可少的信息。在机器学习研究会议上更有可能接受遇到更多清单物品的论文。道奇说，大多数大型语言模型缺少清单上的一些物品，例如用于源代码的链接或有关用于培训AI模型的数据的详细信息;发布的三篇论文中的一个不与代码共享链接以验证结果。</p><p> But Dodge also sees more systemic issues at work. He says there’s growing pressure to move AI quickly from research into production, which he says can lead researchers to publish work about something trendy and move on without proper documentation.</p><p> 但是道奇也看到了更多的全身问题。他说，将AI迅速从研发中迁移到生产中，他说，他说可以引导研究人员发布关于某些时尚的事情并没有适当的文件继续前进。 </p><p> In another  recent study, Microsoft researchers interviewed 12 tech workers deploying AI language technology and found that product teams did little planning for how the algorithms could go wrong. Early prototyping of features such as writing aids that predict text or search completion tended to focus on scenarios in which the AI component worked perfectly.</p><p>在最近的另一个研究中，微软研究人员采访了12项技术工人部署了AI语言技术，发现产品团队对算法如何出错的规划很少。预测文本或搜索完成的辅助功能的早期原型设计倾向于专注于AI组件完美地工作的情景。</p><p> The researchers designed an interactive “ playbook” that prompts people working on an AI language project to think about and design for failures of AI text tech in the earliest stages. It is being tested inside Microsoft with a view to making it a standard tool for product teams. Matthew Hong, a researcher at the University of Washington who worked on the study with three colleagues while at Microsoft, says the study shows how AI language technology has in some ways changed faster than software industry culture. “Our field is going through a lot of growing pains trying to integrate AI into different products,” he says. “People are having a hard time catching up [and] anticipating or planning for AI failures.”</p><p> 研究人员设计了一个互动的“PlayBook”，促使人们在最早阶段思考和设计AI文本技术的故障。它在Microsoft内部进行了测试，以便使其成为产品团队的标准工具。华盛顿大学的研究员Matthew Hong在微软的研究中致力于与三位同事进行研究，说明这项研究表明AI语言技术如何以某种方式更快地改变软件行业文化。 “我们的领域正在经历很多成长的痛苦，试图将AI整合到不同的产品中，”他说。 “人们赶紧赶上[和]期待或规划AI失败。” </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://arstechnica.com/science/2021/06/the-efforts-to-make-text-based-ai-less-racist-and-terrible/">https://arstechnica.com/science/2021/06/the-efforts-to-make-text-based-ai-less-racist-and-terrible/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/减少/">#减少</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/text/">#text</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>