<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习应该比这更好 Machine Learning Deserves Better Than This</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Machine Learning Deserves Better Than This<br/>机器学习应该比这更好 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-03 23:36:44</div><div class="page_narrow text-break page_content"><p>This is an excellent overview at  Stat on the current problems with machine learning in healthcare. It’s a very hot topic indeed, and has been for some time. There has especially been a flood of manuscripts during the pandemic, applying ML/AI techniques to all sorts of coronavirus-related issues. Some of these have been pretty far-fetched, but others are working in areas that everyone agrees that machine learning can be truly useful, such as image analysis.</p><p>这是在医疗保健机器学习当前问题上的统计数据概述。这确实是一个非常热门的话题，已经有一段时间了。在大流行期间，普遍一直在洪水，将ML / AI技术应用于各种相关的冠状病毒相关问题。其中一些已经非常远，但其他人在各地工作，每个人都同意机器学习可以真正有用，例如图像分析。</p><p> How about coronavirus pathology as revealed in lung X-ray data?  This new paper (open access) reviewed hundreds of such reports and focused in on 62 papers and preprints on this exact topic. On closer inspection,  none of these is of any clinical use at all. Every single one of the studies falls into clear methodological errors that invalidate their conclusions. These range from  failures to reveal key details about the training and experimental data sets, to not performing robustness or sensitivity analyses of their models, not performing any external validation work, not showing any confidence intervals around the final results (or not revealing the statistical methods used to compute any such), and many more.</p><p> Coronavirus病理学如何揭示肺X射线数据？这篇新的论文（开放式访问）审查了数百个此类报告，并专注于62篇论文和本准确主题的预先印刷品。在仔细检查时，这些都没有任何临床使用。每种研究中的每一项都属于明确的方法论误差，使他们的结论无效。这些范围从故障揭示有关培训和实验数据集的关键细节，而不是对其模型的稳健性或敏感性分析，而不是执行任何外部验证工作，而不是在最终结果周围显示任何置信区间（或不透露统计方法用于计算任何此类），还有更多。</p><p> A very common problem was the (unacknowledged) risk of bias right up front. Many of these papers relied on public  collections of radiological data, but these have not been checked to see if the scans marked as COVID-19 positive patients really were (or if the ones marked negative were as well). It also needs to be noted that many of these collections are very light on actual COVID scans compared to the whole database, which is not a good foundation to work from, either, even if everything actually is labeled correctly by some miracle. Some papers used the entire dataset in such cases, while others excluded images using criteria that were not revealed, which is naturally a further source of unexamined bias.</p><p> 一个非常常见的问题是（未被承认）偏向前方的风险。这些论文中的许多文件依赖于公共收集放射数据，但尚未检查这些扫描是否标记为Covid-19阳性患者的扫描真正是（或者如果标记为负面的患者也是如此）。还需要注意的是，与整个数据库相比，许多这些系列在实际的Covid扫描上非常轻，这也不是一个良好的工作基础，即使一切都是由一些奇迹正确标记的。有些论文在这种情况下使用了整个数据集，而其他文件则使用未透露的标准排除图像，这是自然是未审查偏见的进一步源。</p><p> In all AI/ML approaches, data quality is absolutely critical. “Garbage in, garbage out” is turbocharged to an amazing degree under these conditions, and you have to be really, really sure about what you’re shoveling into the hopper. “We took all the images from this public database that anyone can contribute to and took everyone’s word for it” is, sadly, insufficient. For example, one commonly used pneumonia dataset turns out to be a pediatric collection of patients between one and five, so comparing that to adults with coronavirus infections is problematic, to say the least. You’re far more likely to train the model to recognize children versus adults.</p><p> 在所有AI / ML方法中，数据质量绝对是至关重要的。 “垃圾进入，垃​​圾出局”是在这些条件下涡轮增压到惊人的程度，你必须真的，真的肯定你铲入料斗。 “我们从这个公共数据库中拍摄了所有图像，任何人都可以为其提供贡献并占据所有人的话”，可悲的是，不够。例如，一个常用的肺炎数据集被证明是一到五个患者的儿科收集，因此将其与冠状病毒感染的成年人相比是有问题的，可以说是有问题的。你更有可能训练模型来承认孩子与成年人。</p><p> That point is addressed in  this recent preprint, which shows how such radiology analysis systems are vulnerable to this kind of short-cutting. That’s a problem for machine learning  in general, of course: if your data include some actually-useless-but-highly-correlated factor for the system to build a model around, it will do so cheerfully. Why wouldn’t it? Our own brains pull stunts like that if we don’t keep a close eye on them. That paper shows that ML methods too often pick up on markings around the edges of the actual CT and X-ray images if the control set came from one source or type of machine and the disease set came from another, just to pick one example.</p><p> 这一点是在这个最近的预印刷品中解决了这种放射学分析系统如何容易受到这种短切口的影响。这是一般机器学习的问题，当然：如果您的数据包括系统建立模型的一些实际无用但高度相关的因素，那么它将乐于愉快。为什么不是吗？如果我们没有密切关注它们，我们自己的大脑就像那样的话。那篇论文显示，如果控制装置来自一个源或机器类型，并且疾病集中来自另一个源，则毫无于挑选，ML方法常常拾取实际CT和X射线图像周围的标记。</p><p> To return to the original  Nature paper, remember, all this trouble is after the authors had eliminated (literally)  hundreds of other reports on the topic, for insufficient documentation. They couldn’t even get far enough to see if something had gone wrong, or how, because these other papers did not provide details of how the imaging data were pre-processed, how the training of the model was accomplished, how the model was validated, or how the final “best” model was selected at all. These fall into Pauli’s category of “not even false”. A machine learning paper that does not go into such details is, for all real-world purposes, useless. Unless you count “putting a publication on the CV” as a real-world purpose, and I suppose it is.</p><p> 要返回原来的自然纸张，请记住，所有这些麻烦都是在作者被淘汰（字面上）关于该主题的数百个报告之后，因为文件不足。他们甚至无法到达足够的东西，看看是否出错了，或者如何，因为这些其他论文没有提供了如何预处理的成像数据的细节，如何完成模型的培训，模型是如何完成的验证，或者如何选择最终的“最佳”模型。这些落入Pauli的“甚至是假”的类别。对于所有真实的目的，没有进入这些细节的机器学习纸就是无用的。除非你将“在CV上发表出版”作为真实世界的目的，我想这是。</p><p> But if we want to use these systems for some slightly more exalted purposes, we have to engage in a lot more tire-kicking than most current papers do. I have a not-very-controversial prediction: in coming years, virtually all of the work that’s being published now on such systems is going to be deliberately ignored and forgotten about, because it’s of such low quality. Hundreds, thousands of papers are going to be shoved over into the digital scrap heap, where they most certainly belong, because they never should have been published in the state that they’re in. Who exactly does all this activity benefit, other than the CV-padders and the scientific publishers?</p><p> 但是，如果我们想使用这些系统，稍微举起一些稍微崇高的目的，我们必须比大多数当前的论文所做的更多轮胎踢。我有一个不太争议的预测：在未来几年内，几乎所有正在发布的这些系统都会出版的工作将被故意忽略和遗忘，因为它具有如此低的品质。数百个，数千篇论文将被推入数字废料堆，他们肯定属于，因为他们从未在他们进入的状态下发表过。究竟谁这一切都是所有这项活动的利益，而不是CV脚垫和科学出版商？ </p><p>  You have completely misunderstood the purpose of machine learning in academia (based on my experience in software engineering). Machine learning provides a means for people who don’t know anything about a subject to publish papers in the field. All that is needed is some data (it does not have to be much, see second link below), some button pressing, the ability to convincingly sprout techno-babble, and getting lucky with reviewers.</p><p>您完全误解了学术界机器学习的目的（根据我在软件工程中的经验）。机器学习为人们提供了一个关于在田间发布论文的主题的人的人的手段。所需要的只是一些数据（它不必很多，见下面的第二个链接），一些按钮按下，能够令人信服地宣传Techno-Babble，并与审稿人幸运。</p><p>  I work on improving this situation within the cancer center where I am employed. I regularly use the results of this PubMed query to illustrate the point of this post and the STAT article.</p><p>  我努力改善我所雇用的癌症中心内的这种情况。我经常使用此PubMed查询的结果来说明这篇文章和统计文章的重点。</p><p> ((“machine learning”[Title/Abstract] OR “artificial intelligence”[Title/Abstract] OR AI[Title/Abstract] OR NLP[Title/Abstract])) AND (cancer[Title/Abstract] OR oncology[Title/Abstract])</p><p> （（“机器学习”[标题/摘要]或“人工智能”[标题/摘要]或AI [标题/摘要]或NLP [标题/摘要]）和（癌症[标题/摘要]或肿瘤学[标题/抽象的]）</p><p> Today it returns 8661 results, including 1336 published in 2021! But there are incredibly few ML algorithms in daily clinical use in cancer, for all the reasons cited.</p><p> 今天它返回了8661个结果，其中包括在2021年发布的1336年！但由于引用的所有原因，癌症中每日临床用途具有令人难以置信的少数毫升算法。</p><p>  DICOM info is just too tempting for the AI to ignore. Think about it for COVID-19, the AI would get age and sex and that is a good start that I would not want my image processing AI to take advantage of. To prevent this all of the images must be converted to the same format (PNG would be my choice, but any lossless 16 bit per channel format would work). Make sure there is no meta data from the DICOM file makes it to the PNG file.</p><p>  Dicom Info对于AI忽略了太诱人了。想想它为Covid-19，AI会变得年龄和性，这是一个很好的开始，我不希望我的图像处理ai利用。为防止，所有图像必须转换为相同的格式（PNG将是我的选择，但每个通道格式的任何无损16位都可以工作）。确保DICOM文件中没有元数据，使其到PNG文件。</p><p> For a first pass, I would only use data from the same model of X-ray, CT or MRI devices. That way the AI could not just look at say resolution and determine that the device used in the COVID-19 ward is where COVID-19 patients’ images come from.</p><p> 对于第一次通过，我只会使用来自X射线，CT或MRI设备的相同模型的数据。这样的方式，AI不仅仅是看决议，并确定Covid-19病房中使用的设备是Covid-19患者的图像来自的地方。</p><p> An example of how things could go wrong. The clinic where I work has two OCT devices (Zeiss Cirrus 5000 and 6000) that produce images that look identical to me, but the output is slightly different. One is close to the retina clinic and one is not. If we feed images from both into an AI training session for finding early signs of AMD, it would find use the subtle differences between the devices to help it determine who was destined to be diagnosed with AMD.</p><p> 事情如何出错的例子。我工作的诊所有两个OCT设备（Zeiss Cirrus 5000和6000），产生看起来与我相同的图像，但输出略有不同。一个接近视网膜诊所，一个是不是。如果我们将图像从两者送入AI培训会议以查找AMD的早期迹象，它会发现设备之间的微妙差异来帮助它确定谁注定要诊断为AMD。 </p><p> I think AI for many diagnostic devices is going to be important in the future, but we are going to have to be very careful about how they are trained.</p><p>我认为，许多诊断设备的AI将来会很重要，但我们将不得不非常小心他们是如何训练的。</p><p> Normally I am out of my league on this blog. Not so much this time. I have been working with computer image processing for decades going back to programming Kontron systems back in the mid 80’s. I have only played with AI for a couple of projects, so I am not an AI image processing expert.</p><p> 通常我在这个博客上脱离了我的联盟。这次不是很多。我一直在使用计算机图像处理，几十年来返回到80年代中期重新编程Kontron Systems。我只与AI一起玩了几个项目，所以我不是AI图像处理专家。</p><p>  It’s very difficult to hide information from ML models, but one thing you can do to avoid bias is to provide all those categories during training and then train it to ignore them – eg instead of producing the most accurate classification for the training set, you have a goal that the classification must be the same for all values of the age/sex fields.</p><p>  隐藏ML模型中的信息非常困难，但您可以做的一件事是为了避免偏见是在训练期间提供所有这些类别，然后培训它来忽略它们 - 例如，而不是为培训集生产最准确的分类，而不是为培训集生产对于年龄/性别字段的所有值，分类必须相同的目标。</p><p> That way, even if it finds a way to detect them in the image, it won’t use it for anything.</p><p> 这样，即使它找到了一种方法来检测图像中的方法，它也不会用于任何东西。</p><p>  It is easy to hide when you are a pharmaceutical industry researcher. If only journals did their job and enforce reproducibility, we would not get so much snobbism coming from that direction, I am sure.</p><p>  当您是制药行业的研究员时，很容易隐藏。如果只有期刊做了工作并执行重现性，我们就不会得到那么多的势利来自那个方向，我相信。</p><p>  Yes, it’s very easy to hide as a corporate researcher – especially from all those downstream users of your work.</p><p>  是的，藏起作为企业研究员非常容易 - 特别是从您工作的所有下游用户的所有下游用户。</p><p> Fake the assay results? No problem – no one else ever uses what the chemists make. The biologists won’t notice the compound doesn’t bind, and certainly the rats won’t.</p><p> 假冒测定结果？没问题 - 没有其他人使用化学家制造的东西。生物学家不会注意到该化合物不会结合，并且当然大鼠不会。 </p><p> A harsh truth is that it is dramatically easier to hide ongoing fraud or incompetence in academia, because a great deal of work is funded without a real world downstream user. It just goes in to the void, and continues to be funded anyway. (There are many reasons for this, a lot of them not even bad – it’s impossible to tell for sure in advance what will be useful.)</p><p>严酷的事实是，在学术界隐藏着持续的欺诈或无能的情况下，由于大量工作而没有真正的世界下游用户，因此更容易。它只是进入空白，并继续资助。 （对此有很多原因，很多人都没有糟糕 - 这是不可能的事先提前有用。）</p><p> But in industry? It’s a hell of a lot harder to persistently do work no one else uses. And if you lie or just do shit work, you’ll usually get found out.</p><p> 但在工业中？这是一个难以持续的工作，没有其他人使用。如果你撒谎或只是做狗屎工作，你通常会被发现。</p><p> Get off your high horse and admit the real problems in academic research, rather than insulting industrial researchers. It doesn’t invalidate the enterprise to recognize it has flaws.</p><p> 脱掉你的高马，承认学术研究中的真正问题，而不是侮辱工业研究人员。它不会使企业识别它具有缺陷。</p><p>  Philip, I’m sure that at some point in the future AI will be capable of answering all kinds of questions. However, I think this is many years in the future.</p><p>  菲利普，我相信，在未来的某些时候，AI将能够回答各种问题。但是，我认为这是未来多年的。</p><p> The current AI successes have been the result of massive computational power being available to anyone with a credit card, with a suitable spending limit (previously the upfront cost of building the necessary system deterred most), and the availability of lots of domain specific data (e.g., cat pictures).</p><p> 目前的AI成功一直是用信用卡的任何人提供大规模计算能力的结果，具有合适的支出限制（以前建立必要系统的前期成本威慑了最多），以及许多域特定数据的可用性（例如，猫图片）。</p><p> Piekniewski’s blog provides a refreshing dose of reality in the AI field:  https://blog.piekniewski.info/</p><p> Piekniewski的博客在AI Field中提供了令人耳目一新的现实：https://blog.piekniewski.info/</p><p>  You use Google search every day, yet Google also “[fails] to reveal key details about the training and experimental data sets, to not performing robustness or sensitivity analyses of their models, not performing any external validation work, not showing any confidence intervals around the final results (or not revealing the statistical methods used to compute any such), and many more.”. It’s still a useful tool, even for doctors. What am I missing? Why the double standard?</p><p>  您每天使用Google搜索，但谷歌也“[失败]揭示有关培训和实验数据集的关键细节，而不是对其模型的鲁棒性或敏感性分析，而不是执行任何外部验证工作，而不是展示周围的任何置信区间最终结果（或者没有透露用于计算任何此类的统计方法），还有更多。“。即使是医生，它仍然是一个有用的工具。我错过了什么？为什么双重标准？ </p><p>  Google is a search tool. Of course its results are (or should be) judged to a higher standard than results published in a peer reviewed journal. If they aren’t then what’s the point of a peer reviewed journal?</p><p>谷歌是一个搜索工具。当然，它的结果是（或应该是）判断到比同行评审期刊中发表的结果更高的标准。如果他们不是同行评审期刊的那一点？</p><p>  I got that phrasing the wrong way round. Results in a peer reviewed journal should obviously be judged to a higher standard than Google.</p><p>  我得到了错误的方式。结果在同行评审期刊上应显然应该被判断到比谷歌更高的标准。</p><p>  It has always depressed me that I saw more rigorous work done to ensure clear coding criteria and achieving good inter-rater reliability in the Psych and English departments at Carnegie Mellon than I have seen in many a published science paper. When you try and run a meta-analysis and realize that out of 84 studies of a phenomenon, only 4 have actually covered all the common bases necessary for a rational assessment, ….aaaaagh.</p><p>  它一直郁闷我，我看到了更严格的工作，以确保在卡内基梅隆在许多发表的科学论文中看到的Carnegie Mellon在Carnegie Mellon的良好编码标准和达到良好的帧间间可靠性。当您尝试进行META分析并意识到84个对现象的研究中，只有4个实际上涵盖了理性评估所需的所有常见基础，...... .AAAAAGH。</p><p>  This really starts sounding like a verbatim copy of the outcome of the previous ML/AI hype about 25 years ago in the late 90s. At like any hype in data science it depends on the quality of data which quite often is a real problem. And yes, “putting a publication on the CV” as a real-world purpose :-).</p><p>  这真的开始听起来像25年前在90年代后期之前的先前ml / ai炒作的结果逐字副本。就像数据科学的任何炒作一样，这取决于数据的质量，这通常是一个真正的问题。是的，“在CV上发表出版物”作为真实世界的目的:-)。</p><p>  A friend of mine recently attended a ML/AI conference online. I asked her what she thought. After a pause she said, “It all seemed quite hopeless.”</p><p>  我最近的一位朋友在线参加了ML / AI会议。我问她想到了什么。暂停后，她说，“这一切似乎都是绝望的。”</p><p>  Don’t get me wrong, as scientists we should definitely be doing this sort of work. It doesn’t seem to be be worth much yet, but that will change.</p><p>  不要让我错了，因为科学家们肯定应该做这种工作。它似乎尚不重要，但这会改变。</p><p>  Given all the real world applications expert systems are already being used for (job candidate evaluation and parole determination serve as existing frightening examples) the real tragedy is how much damage is done before people get a true handle on these things (assuming we ever get a handle on it at all….)</p><p>  鉴于所有现实世界应用专家系统已经用于（求职评估和假释决定作为现有的可怕例子）真正的悲剧是在人们对这些事情上真正的句柄之前完成了多少伤害（假设我们曾经得到过在所有...上处理它......） </p><p>  Derek – thanks for pivoting from Covid-19 topics to “things that smell” 🙂</p><p>Derek  - 感谢您从Covid-19主题枢转到“闻到的东西”🙂</p><p>  For example, one commonly used pneumonia dataset turns out to be a pediatric collection of patients between one and five, so comparing that to adults with coronavirus infections is problematic, to say the least. You’re far more likely to train the model to recognize children versus adults.</p><p>  例如，一个常用的肺炎数据集被证明是一到五个患者的儿科收集，因此将其与冠状病毒感染的成年人相比是有问题的，可以说是有问题的。你更有可能训练模型来承认孩子与成年人。</p><p> FWIW, even generally AI is out of my league, but one thing seems intuitively obvious: without fiducial markers on images, using AI to recognize anything is not going to work well. Facial recognition AI works in part because there are naturally occurring fiducial markers in facial images. For example, the distance between the centers of eye pupils.</p><p> FWIW，甚至通常是艾都是脱离我的联盟，但有一件事似乎直观地明显：没有基准标记图像，使用AI识别任何事情都不会好转。面部识别AI部分原因是面部图像中存在自然发生的基准标记。例如，眼睛瞳孔中心之间的距离。</p><p> I don’t have the first clue what naturally occurring fiducial markers might be present in, say, chest x-ray images. But I would expect there would be some that physiologists could define.</p><p> 我没有第一个Clue在胸部X射线图像中可能存在自然发生的基准标记。但我期待有一些生理学家可以定义。</p><p> At least the presence of natural markers would allow scaling, so that the actual size of the subject could be eliminated and the AI could train on image characteristics relevant to whatever it is looking for.</p><p> 至少存在自然标记的存在允许缩放，从而可以消除对象的实际大小，并且AI可以培训与其寻找相关的图像特征。</p><p> But, as I said, AI diagnosis of clinical images is light years out of my league.</p><p> 但是，正如我所说，临床图像的AI诊断是我联盟的轻微青年。</p><p>  More will come with time. I suspect AI diagnosis will come from the device manufacturers. As I stated in an earlier post, limiting training to one device makes things easier.</p><p>  更多会随着时间而来的。我怀疑AI诊断将来自设备制造商。正如我在早期的帖子中所述，对一个设备的限制培训使事情变得更加容易。 </p><p>  I think there are plenty of parameters you could extract from a chest x-ray to guess age, gender etc as long as the imaging setup is standardised – something like the distance between the humeral heads would be a good analogue to using the pupils in facial recognition.</p><p>我认为有很多参数您可以从胸部X射线提取到猜测年龄，性别等，只要成像设置标准化，就像肱骨头之间的距离一样是在面部材料中使用瞳孔的良好类似物认出。</p><p> The main problem seems to be that the authors of these papers are too lazy to do even the most basic sanity checks on their data. As Derek says, getting some data (any data, from god knows where) and running it through an algorithm is easy. The hard graft is generating a trustworthy dataset in the first place…</p><p> 主要问题似乎是这些论文的作者甚至懒得做，即使最基本的理智检查他们的数据。正如德里克所说，获得一些数据（来自上帝的任何数据知道在哪里）并通过算法运行它很容易。硬移植物首先生成一个值得信赖的数据集......</p><p>    I will believe in AI power when one of them analyse others and come to the conclusion that they all are hokum. And then tweet it.</p><p>    当其中一个人分析他人并得出他们都是Hokum，我会相信AI力量。然后推特。</p><p>  The mouse-over on that one is (as usual) the icing on the cake… The guy is a global treasure…!</p><p>  那个鼠标上的那个是（像往常一样）锦上添花......这个家伙是一个全球珍宝......！</p><p>  There’s a lot wrong with how machine learning models are described in the literature, and what claims are made. My biggest pet peeve is a disregard for experimental error, such as model metrics reported to 4 significant figures using data with one or two sig figs (like IC50s). Machine learning people just do not generally have backgrounds where they have generated data themselves, and therefore do not have an appreciation for the limitations and pitfalls of that data.</p><p>  在文献中描述机器学习模型以及制造索赔的声明是有很多错误。我最大的宠物Peeve是对实验错误的忽略，例如使用具有一个或两个SIG图（如IC50）的数据报告为4个重要数字的模型度量。机器学习人员通常不会在它们本身生成数据的背景下，因此对该数据的局限性和陷阱没有欣赏。</p><p> That being said, academic machine learning papers are usually not about the production of industrially useful models. They’re about providing evidence about the potential of the application of a method. If an imaging company wants to make use of that technology, it’s their responsibility to do all of the due diligence in order to embed that method into their product.</p><p> 所说，学术机器学习论文通常不是关于工业上有用模型的生产。他们是关于提供有关应用方法的潜力的证据。如果一个成像公司想要利用该技术，他们责有所有尽职调查，以便将该方法嵌入其产品中。</p><p> I’m not excusing lazy or incomplete paper writing and refereeing. And the journals are chock full of trivial papers. But it’s too easy to ding papers that don’t have particular analyses, or don’t have all of the quality controls that you think are necessary. For instance, confidence limits are not really pertinent to image classification, where you’re just trying to assign a probability that an image belongs in the positive or negative bucket. Sensible cross-validation, for instance, is more important.</p><p> 我不是懒惰或不完整的纸质写作和裁判。期刊是充满了琐碎的论文的陪成。但是，丁论文没有特别分析的文件太容易，或者没有你认为是必要的所有质量控制。例如，置信范围与图像分类没有真正有关，您只需尝试分配图像所属的概率或负桶。例如，合理的交叉验证更为重要。 </p><p> All models are wrong. Some are useful. Doesn’t Google correctly identify most pictures of you as you? Do you think they vetted Fluffy’s cat photos?</p><p>所有型号都是错误的。有些是有用的。谷歌没有正确识别你的大多数图片吗？你认为他们审查了蓬松的猫照片吗？</p><p>  We have been here before… How not to develop a quantitative structure-activity or structure-property relationship (QSAR/QSPR) J C Dearden, M T D Cronin, K L E Kaiser DOI: 10.1080/10629360902949567 ..and the same points apply if you are doing some fancy schmancy ML or just linear regression. LeeH’s point about data is well made. It seems so many skipped the lessons on precision vs accuracy, and the software vendors are among them.</p><p>  我们以前一直在这里......如何没有发展定量的结构 - 活动或结构 - 财产关系（QSAR / QSPR）JC Dealden，MTD Cronin，Kle Kaiser Doi：10.1080 / 10629360902949567 ..如果你正在做一些，那么同样的积分花哨的施密ml或只是线性回归。 Leeh关于数据的点很好。似乎很多跳过了精确VS准确度的课程，并且软件供应商是其中的。</p><p>  This sounds like a terrible thing to say, but I think part of the problem is that the community has developed so many powerful open-source software packages for ML (keras, sklearn, pytorch, tensorflow, caret), that it has become too easy for people with no idea what they are doing to “train” models.</p><p>  这听起来像是一个可怕的话说，但我认为部分问题是社区已经开发了如此多强大的开源软件包for ml（keras，sklearn，pytorch，tensoRflow，张图书），即它变得太容易了对于不知道他们正在做什么的人来说，“火车”模型。</p><p> I have seen some really terrible ML papers published in biology journals — papers that used small training sets with P &gt;&gt;&gt;&gt; N and did not take even elementary precautions to ensure their model would generalize — and it’s definitely a little depressing. Maybe that is just a symptom of the broader state of scientific publishing though, I don’t know.</p><p> 我看到了一些在生物期刊中发表的一些非常可怕的ml文件 - 用p＆gt;＆gt;＆gt; n并没有采取甚至基本的预防措施来确保他们的模型会概括 - 它绝对是一点点令人沮丧。也许这只是虽然是更广泛的科学出版状态的症状，但我不知道。</p><p> ML is definitely a powerful tool for bioinformatics, DeepMind’s success with AlphaFold demonstrates that. But you have to spend the time and effort to properly evaluate a model, and as you say think carefully about what you are using to train it. Apparently an alarming number of people either don’t realize that is important, or don’t care because they are much too focused on getting a quick paper to pad their resume (not sure which is worse).</p><p> ML绝对是生物信息学的强大工具，DeepMind与AlphaFold的成功表明这一点。但是你必须花时间和精力妥善评估模型，就像你说仔细考虑你正在使用的东西训练它。显然，人数令人震惊的人要么没有意识到这一点是重要的，或者不在乎，因为它们太专注于获得快速纸张来垫上他们的简历（不确定哪个更糟糕）。</p><p>  There is a huge economic incentive to publish. Having a couple of ML papers puts a fresh PhD in the front of the queue for jobs with ~200K or more in salary. Especially if the papers makes it to one of the prestigious ML conferences .It has been studied and concluded that due to the volume of submissions and lack of reviewers the acceptance at these conferences is random ( http://blog.mrtz.org/2014/12/15/the-nips-experiment.html). It is purely a numbers game.</p><p>  发布巨大的经济动力。在薪水中的队列前面有几个ml纸，在薪水的前面投入新的博士学位。特别是如果论文将其成为一个着名的ML会议之一。已经研究过，得出结论，由于提交的数量和缺乏审阅者这些会议的接受是随机的（http://blog.mrtz.org/2014 /12/15/the-nips-experiment.html）。它纯粹是一个数字游戏。</p><p>   Save my name, email, and website in this browser for the next time I comment.</p><p>   下次评论下，在此浏览器中保存我的姓名，电子邮件和网站。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blogs.sciencemag.org/pipeline/archives/2021/06/02/machine-learning-deserves-better-than-this">https://blogs.sciencemag.org/pipeline/archives/2021/06/02/machine-learning-deserves-better-than-this</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>