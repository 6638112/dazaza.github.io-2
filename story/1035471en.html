<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>用递归神经网络查找不良的火烈鸟图Finding bad flamingo drawings with recurrent neural networks</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Finding bad flamingo drawings with recurrent neural networks<br/>用递归神经网络查找不良的火烈鸟图</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-21 12:04:10</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/87f8c217f441959ab81d552a6a499098.png"><img src="http://img2.diglog.com/img/2020/11/87f8c217f441959ab81d552a6a499098.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Have you played  Quick, Draw! yet? It’s basically Pictionary, played against a neural network, and it’s a lot of fun. Not long ago, Google released a  dataset of some of the millions of sketches people have drawn so far over 345 categories.</p><p>您玩过快速，平局！然而？它基本上是Pictionary，是与神经网络对战的，很有趣。不久前，Google发布了一个数据集，该数据集包含人们迄今为止绘制的超过345个类别的数百万个草图。</p><p> In this post, I’ll just be looking at sketches in the ‘flamingo’ category. You can browse a random sample  here. Some of them are lovely.</p><p>在这篇文章中，我将只看“火烈鸟”类别中的草图。您可以在此处浏览随机样本。其中一些很可爱。</p><p>      Along with the datasets, Google has released some  pre-trained models that use the Sketch-RNN architecture described in  this paper.</p><p>除了数据集，Google还发布了一些使用本文描述的Sketch-RNN架构的预训练模型。</p><p>   But we can also repurpose them to get some insight into existing human-generated sketches. As I talked about in an earlier post on  character-level RBMs, learning to generate new examples of X is basically the same as learning the probability distribution of X.</p><p>但是我们也可以重新调整它们的用途，以对现有的人工生成草图有所了解。正如我在较早的有关字符级RBM的文章中所谈到的那样，学习生成X的新示例与学习X的概率分布基本相同。</p><p> So in addition to sampling from the learned distribution to generate new sketches, we can take existing sketches, and ask the model how probable it thinks they are. It seems reasonable that the sketches the model assigns the lowest probabilities should be among the ‘worst’.</p><p>因此，除了从学习的分布中采样以生成新​​草图外，我们还可以采用现有草图，并询问模型它认为它们的可能性有多大。模型分配的最低概率的草图应该在“最差”之中是合理的。</p><p>  Before we roast Quickdraw’s least gifted flamingo-drawers, let’s look at some of the ‘best’ flamingo sketches - i.e. those assigned the highest probabilities:</p><p>在我们烤Quickdraw最天才的火烈鸟抽屉之前，让我们看一些“最好的”火烈鸟草图-即那些分配最高概率的草图：</p><p>  Audubon they ain’t, but these are at least mostly recognizable as flamingos. Because we’re ranking by probability, we should in some sense expect the most ‘typical’ sketches here, rather than the most beautiful.</p><p>它们不是Audubon，但至少在大多数情况下可以识别为火烈鸟。由于我们是按照概率进行排名的，因此从某种意义上讲，我们应该期待这里最“典型”的草图，而不是最美丽的草图。</p><p>  To quote Tolstoy,  happy flamingos are all alike, but every garbage flamingo is garbage in its own way.</p><p>用托尔斯泰的话来说，快乐的火烈鸟都是一样的，但是每个垃圾火烈鸟以自己的方式都是垃圾。</p><p>        In most cases, the artist started out with something flamingo-like, then scratched it out in frustration. The Quickdraw dataset encodes sketches as a sequence of pen strokes, rather than just a grid of pixels, so we can actually reconstruct the history of sketches like this:</p><p>在大多数情况下，艺术家从火烈鸟般的东西开始，然后无奈地将其划掉。 Quickdraw数据集将草图编码为笔划序列，而不仅仅是像素网格，因此我们实际上可以重建草图的历史记录，如下所示：</p><p>      There are some cases where the artist was pretty clearly drawing a different Quickdraw category like ‘pineapple’, or ‘compass’. It’s not clear whether it was a glitch that landed these images in the flamingo dataset, or user error (maybe the players didn’t realize they ran out of time and were given a new category?).</p><p>在某些情况下，艺术家很清楚地画了一个不同的Quickdraw类别，例如“菠萝”或“指南针”。目前尚不清楚是将这些图像放到火烈鸟数据集中是一个小故障，还是用户错误（也许玩家没有意识到他们用完了时间并被赋予了新的类别？）。</p><p>   More head-scratching examples  here. The ‘wrong category’ explanation doesn’t work for most of these, because they don’t look like any of Quickdraw’s other categories (for example, there are no categories for ‘beaver’, ‘sitar’, or ‘female skeleton doing aerobics’).</p><p>这里有更多令人头疼的例子。 “错误类别”的解释不适用于大多数此类项目，因为它们看起来与Quickdraw的其他类别都不一样（例如，没有针对“有氧运动”，“ sitar”或“做健美操的女性骨骼”的类别'）。</p><p>   Some scoundrels just treated Quickdraw like a bathroom wall. The uncensored version is  here (NSFW).</p><p>一些流氓只是把Quickdraw当作浴室的墙。未经审查的版本在这里（NSFW）。</p><p>   Most of what I found was garbage, but the three above are actually kind of gorgeous. Add some colour to #3 and it could be a New Yorker cover! What gives?</p><p>我发现的大部分是垃圾，但上面的三个实际上很漂亮。为＃3添加一些颜色，可能是《纽约客》的封面！是什么赋予了？</p><p> These are great drawings, and pretty recognizable as flamingos, but that doesn’t count for much. We’re not ranking by how clearly these images represent their subject (i.e.  P(category=flamingo | drawing)), but rather how likely someone is to come up with this when asked to draw a flamingo ( P(drawing | category=flamingo)).</p><p>这些是很棒的图纸，并且很容易认出是火烈鸟，但这并不重要。我们不是根据这些图像代表主题的清晰程度（即P（category = flamingo | drawing））进行排名，而是根据有人要求绘制火烈鸟的可能性（P（drawing | category = flamingo） ））。</p><p> Very few people would think to draw the legs and head, with the body out of frame, as #3 does. #1 uses unusually many short, disconnected lines, and #2 uses a unique stylized head shape.</p><p>很少有人会像第三条那样，画出腿和头，而身体却不在画框内。 ＃1使用异常多的短而断开的线，＃2使用独特的程式化头部形状。</p><p>  The drawing above is another good example. Flamingos do bend their heads down low like that, but when asked to draw a flamingo, almost no-one thinks to pose it with its head below the horizon.</p><p>上图是另一个很好的例子。火烈鸟的确如此低下了头，但是当被要求画火烈鸟时，几乎没有人认为它的头在地平线以下。</p><p> You can browse a gallery of all the bottom 400 flamingos   here (warning: contains a few NSFW sketches).</p><p>您可以在此处浏览所有底部400只火烈鸟的画廊（警告：包含一些NSFW草图）。</p><p>   Google described their dataset as being ‘individually moderated’, and you can scroll through the examples  here for a long time before hitting any graffiti (and if you do find any, you can even click it to ‘flag as inappropriate’). It seems they’ve already done a pretty good job of filtering out most of the irrelevant/malicious sketches that were surely rampant in the raw data.</p><p>Google将他们的数据集描述为“个人审核”，您可以在此处浏览示例很长时间，然后再找到任何涂鸦（如果发现任何涂鸦，甚至可以单击它以“标记为不适当”）。看来，他们已经做得相当不错，可以过滤掉肯定在原始数据中猖ramp的大多数不相关/恶意的草图。</p><p> But this approach was able to easily identify a subset of sketches containing a high density of overlooked graffiti, scribbles, and irrelevant sketches. And it’s quite ‘cheap’, since it uses an existing model trained for a different purpose. If we wanted to go further, this would be a good place to start to get some labelled examples to bootstrap a graffiti classifier.</p><p>但是这种方法能够轻松地识别出草图的子集，其中包含高密度的被忽略的涂鸦，涂鸦和无关的草图。而且它非常“便宜”，因为它使用了为不同目的而训练的现有模型。如果我们想走得更远，那么这将是开始获得一些带有标签的示例以引导涂鸦分类器的好地方。</p><p>   I’ve been talking about measuring the ‘probability’ a Sketch-RNN model assigns to each sketch, and ranking by those probabilities, but the truth is a little messier. There are a few complications we need to consider here.</p><p>我一直在谈论测量Sketch-RNN模型分配给每个草图的“概率”，并根据这些概率进行排名，但事实有点混乱。在这里我们需要考虑一些并发症。</p><p>  Sketch-RNN models the location of the pen as a continuous random variable. In this view, the probability of any stroke’s (x, y) position, and therefore of any sketch as a whole, is 0. So it makes more sense to talk about measuring the  probability density of sketches. If A’s density is twice B’s, then A is in some sense ‘twice as likely’. (Both sketches still have probability zero, but we can measure a non-zero probability for the set of similar sketches in some tiny neighbourhood of equal size around A and B, and the probability near A will be twice as high).</p><p>Sketch-RNN将笔的位置建模为连续随机变量。在此视图中，任何笔划（x，y）位置的概率，因此整个草图的概率为0。因此，谈论测量草图的概率密度更有意义。如果A的密度是B的两倍，那么A在某种意义上是“可能的两倍”。 （两个草图的概率仍然为零，但是我们可以在A和B周围大小相等的某个微小邻域中为一组相似草图测出非零概率，而A附近的概率将是它的两倍。）</p><p>  Sketch-RNN outputs probabilities (/densities) per stroke. The natural way to get the probability (density) for a sketch as a whole would be to multiply the values for each stroke. (Densities can be multiplied together much like probabilities.)</p><p>Sketch-RNN输出每个笔划的概率（/密度）。获得草图整体概率（密度）的自然方法是将每个笔划的值相乘。 （密度可以像概率一样相乘。）</p><p>   A priori, we’d expect longer sketches to be less probable - every time we multiply probabilities, we get a smaller number. In language modeling, we’d control for this by measuring the  perplexity per word of a sentence or longer text. We can do something similar here, and measure the density per stroke.</p><p>先验地，我们希望更长的草图不太可能-每次乘以概率，我们得到的数字都会更少。在语言建模中，我们可以通过测量句子或更长文本的每个单词的困惑度来控制。我们可以在此处执行类似的操作，并测量每个笔划的密度。</p><p> (An interesting twist is that sorting by non-normalized densities, the sketches with the lowest  and the highest values were biased toward having more strokes, which completely broke my intuition. The reason for this is that, unlike probabilities, densities can be greater than 1. If the strokes in a sketch mostly have densities greater than 1, then adding more strokes leads to a higher overall density. This doesn’t mean that strokes  S_1, S_2, S_3 are more likely than their prefix  S_1, S_2 (that’d be weird). It means that comparing the values of density functions with different dimensions is generally a bad idea.)</p><p>（一个有趣的变化是，按非归一化的密度进行排序，具有最低和最高值的草图偏向于具有更多笔触，这完全打破了我的直觉。其原因是，与概率不同，密度可以大于1.如果草图中的笔划大部分具有大于1的密度，则增加笔划会导致更高的总体密度。这并不意味着笔划S_1，S_2，S_3比其前缀S_1，S_2更有可能（ d是怪异的），这意味着比较具有不同维度的密度函数的值通常是个坏主意。）</p><p>  I sorted on formula (9) in  this paper - the reconstruction loss. This is basically the log density of the sketch (times the constant,  -1/N_max). A minor deviation from (9):  L_p, the loss with respect to pen state, is summed up to the number of strokes in the sketch, N_s, not the maximum strokes per sketch, N_max. (This is actually  a hidden feature of the code when running in evaluation mode. I’m not sure whether the loss figures they reported e.g. in Table 1 include this tweak, but I’d be surprised if it makes much difference.)</p><p>我在本文中对公式（9）进行了排序-重建损失。这基本上是草图的对数密度（乘以常数，-1 / N_max）。与（9）的微小偏差：L_p，即相对于笔状态的损失，总和等于草图中的笔划数N_s，而不是每个草图的最大笔划N_max。 （实际上，这是在评估模式下运行时代码的隐藏功能。我不确定他们报告的损失数字（例如表1）是否包含此调整，但如果有所不同，我会感到惊讶。）</p><p> The  Bad Flamingos page sorts by the loss function normalized by number of strokes. The examples on this page were mostly selected from the sketches with the worst unnormalized values. Qualitatively, both metrics gave pretty reasonable results with a high degree of overlap, but I ended up preferring the normalized version because its bottom N had a more representative distribution of lengths. The fact that unnormalized loss worked at all is basically a fluke and not something to rely on - if you rescaled the (x,y) values of pen positions to different units, it could drastically change the sort for unnormalized loss, but it wouldn’t affect  loss/nstrokes.</p><p>“不良火烈鸟”页面按通过笔划数归一化的损失函数进行排序。此页面上的示例大部分是从具有最差未归一化值的草图中选择的。定性地，这两个度量都给出了高度重叠的相当合理的结果，但是我最终更喜欢归一化的版本，因为其底部N具有更具有代表性的长度分布。归一化损失完全起作用的事实基本上是fl幸，而不是要依赖的事实-如果将笔位置的（x，y）值重新缩放为不同单位，它可能会极大地改变归一化损失的种类，但不会不会影响损失/突击。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://colinmorris.github.io/blog/bad_flamingos">https://colinmorris.github.io/blog/bad_flamingos</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/递归/">#递归</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/bad/">#bad</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/草图/">#草图</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>