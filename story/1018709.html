<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>卡帕西(氏)MinGPT</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">卡帕西(氏)MinGPT</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-08-18 02:29:53</div><div class="story_img_container"><a href="http://img.diglog.com/img/2020/8/2bc73935d2e0e15dcd53cbdf18210cdd.jpeg"><img src="http://img.diglog.com/img/2020/8/2bc73935d2e0e15dcd53cbdf18210cdd.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>A PyTorch重新实施GPT培训。MinGPT试图做到小巧、干净、易于解释和有教育意义，因为目前可用的大多数都有点杂乱无章。GPT不是一个复杂的模型，该实现大约有300行代码，包括样板和完全不必要的自定义因果自我关注模块。无论如何，所发生的一切都是将一系列索引放入一系列变压器块中，然后得出下一个索引的概率分布。剩下的复杂性只是巧妙地使用批处理(跨示例和超过序列长度)，以便训练是有效的。</p><p>核心minGPT&#34；库&#34；(Hah)是两个文件：mingpt/model.py包含实际的Transformer模型定义，而mingpt/traine.py是训练模型的(独立于GPT的)PyTorch样板。然后，随附的Jupyter笔记本显示如何使用&#34；库(Hah)来训练序列模型：</p><p>Play_math.ipynb训练专注于加法的GPT(灵感来自GPT-3白皮书中的加法部分)。</p><p>Play_char.ipynb将GPT训练为任意文本的字符级语言模型，类似于我以前的char-rnn，但使用的是转换器而不是RNN。</p><p>有了BPE编码器，分布式训练，也许还有FP16，这个实现也许能够重现GPT-1/GPT-2的结果，尽管我还没有尝试过$。GPT-3可能是遥不可及的，因为我的理解是它不适合GPU内存，需要更仔细的模型-并行处理。</p><p>这段代码非常简单，只需进行内联，而不是使用&#34；，但是当前的API类似于：</p><p>#您可以自己定义一个类，将单个示例作为PyTorch LongTensors从Torch返回。公用事业。数据导入数据集TRAIN_DATASET=MyDataset(...)。Test_dataset=MyDataset(...)#从mingpt构造GPT模型。Model import GPT，GPTConfig mconf=GPTConfig(vocab_size，block_size，n_layer=12，n_head=12，n_embd=768)#a GPT-1 model=gpt(Mconf)#从mingpt构建教练器。培训师导入培训师，TraineConfig tconf=TraineConfig(max_pechs=10，Batch_size=256)Trainer=Training(model，Train_Dataset，test_Dataset，tconf)Training er。列车()#(...。享受一段时间的表演吧..。)#模型中的样本([无，...]。和[0]从mingpt推送/弹出所需的虚拟批次维度)。工具导入样本x=Torch。张量([1，2，3]，dtype=火炬。Long)[无，...]#上下文条件y=SAMPLE(MODEL，x，Steps=30，Temperature=1.0，Sample=True，top_k=5)[0]print(Y)#我们的模型用另外30个可能的整数填充整数序列。</p><p>OpenAI/image-gpt的代码中有一些更现代的类似gpt-3的修改，也有很好的参考价值。</p><p>HuggingFace/Transers有一个语言建模的例子。它功能齐全，但因此追踪起来也有一定的挑战性。例如，一些大型函数在各种分支语句后面有高达90%的未使用代码，这在简单语言建模的默认设置中是不被使用的。</p><p>我们训练了一个12层的只有解码器的变压器，带有掩蔽的自我注意头(768个维度状态和12个注意头)。对于位置前馈网络，我们使用了3072维的内态。</p><p>LR衰减：在最初的2000年更新中从零线性增加，并使用余弦时间表退火到0。</p><p>我们在小批量的64个随机抽样的512个令牌的连续序列上训练100个纪元。</p><p>由于在整个模型中广泛使用LayerNorm，简单的权重初始化N(0，0.02)就足够了。</p><p>(37)中提出的L2正则化的修改版本，在所有非偏置或增益权重上w=0.01。</p><p>我们使用学习位置嵌入代替了原工作中提出的正弦形式。</p><p>对于微调：我们以0.1的速率向分类器添加丢弃。学习速率为6.25e-5，批量为32。3个时代。我们使用线性学习率衰减时间表，热身超过0.2%的训练。λ设置为0.5.</p><p>LayerNorm被移动到每个子块的输入，类似于激活前的残差网络。</p><p>使用修正的初始化，其考虑了具有模型深度的残差路径上的累积。我们在初始化时将残留层的权重缩放1/√N，其中N是残留层的数量。(很奇怪，因为在他们发布的代码中，我只能找到旧的0.02的简单用法……。在他们发布的image-gpt中，我发现它用于c_proj，即使那样也只用于attn，而不用于mlp。哈。Https://github.com/openai/image-gpt/blob/master/src/model.py)。</p><p>GPT-2使用了48层和d_model 1600(与原来的12层和d_model 768相比)。~1.542B参数。</p><p>我们使用与GPT-2相同的模型和体系结构，包括其中描述的修改后的初始化、预归一化和可逆令牌化。</p><p>我们在变压器的各层中交替使用密集和局部带状稀疏注意模式，类似于稀疏变压器。</p><p>我们始终具有四倍于瓶颈层大小的前馈层，dff=4∗dmodel。</p><p>ADAM，β1=0.9，β2=0.95，Eps=10−8。</p><p>所有模型都使用0.1的权重衰减来提供少量的正则化。(注：GPT-1使用0.01我相信，见上文)</p><p>前3.75亿令牌上的线性LR预热。然后使用余弦衰减将学习率降到其值的10%，超过2600亿个令牌。</p><p>根据模型大小，在前40-120亿个训练令牌上，将批大小从小值(32K令牌)线性增加到全值。</p><p>始终使用完整的2048大小的时间上下文窗口，并在文档标记的末尾使用特殊的分隔符</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/卡帕/">#卡帕</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/karpathy/">#karpathy</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/gpt/">#gpt</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>