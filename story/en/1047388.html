<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Deepfake探测器可以击败，计算机科学家首次展示 Deepfake Detectors Can Be Defeated, Computer Scientists Show for the First Time</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Deepfake Detectors Can Be Defeated, Computer Scientists Show for the First Time<br/>Deepfake探测器可以击败，计算机科学家首次展示 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-09 20:21:39</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/ed3a52e0c43f77d2fe150b38e09d7738.png"><img src="http://img2.diglog.com/img/2021/2/ed3a52e0c43f77d2fe150b38e09d7738.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Systems designed to detect deepfakes—videos that manipulate real-life footage via artificial intelligence—can be deceived, computer scientists showed for the first time at the WACV 2021 conference which took place online Jan. 5 to 9, 2021.</p><p>计算机科学家首次在2021年1月5日至9日在线举行的WACV 2021大会上展示了旨在检测​​深度欺诈的系统（即通过人工智能操纵真实镜头的视频）的系统。</p><p> Researchers showed detectors can be defeated by inserting inputs called adversarial examples into every video frame. The adversarial examples are slightly manipulated inputs which cause artificial intelligence systems such as machine learning models to make a mistake. In addition, the team showed that the attack still works after videos are compressed.</p><p> 研究人员表明，可以通过在每个视频帧中插入称为对抗示例的输入来击败检测器。对抗示例是经过稍微操纵的输入，这些输入导致诸如机器学习模型之类的人工智能系统出错。此外，该团队表明，在视频压缩后，攻击仍然有效。</p><p> &#34;Our work shows that attacks on deepfake detectors could be a real-world threat,&#34; said Shehzeen Hussain, a UC San Diego computer engineering Ph.D. student and first co-author on the WACV paper. &#34;More alarmingly, we demonstrate that it’s possible to craft robust adversarial deepfakes in even when an adversary may not be aware of the inner workings of the machine learning model used by the detector.&#34;</p><p> ＆＃34;我们的工作表明，对Deepfake检测器的攻击可能是现实世界中的威胁，＆＃34;加州大学圣地亚哥分校计算机工程博士学位的Shehzeen Hussain说。学生，也是WACV论文的第一作者。 ＆＃34;更令人震惊的是，我们证明，即使对手可能不知道检测器使用的机器学习模型的内部工作原理，也可以制作出强大的对抗性深渊仿冒品。</p><p> In deepfakes, a subject’s face is modified in order to create convincingly realistic footage of events that never actually happened. As a result, typical deepfake detectors focus on the face in videos: first tracking it and then passing on the cropped face data to a neural network that determines whether it is real or fake. For example, eye blinking is not reproduced well in deepfakes, so detectors focus on eye movements as one way to make that determination. State-of-the-art deepfake detectors rely on machine learning models for identifying fake videos.</p><p> 在伪造品中，对对象的脸部进行修改以创建从未发生过的令人信服的逼真的镜头。结果，典型的Deepfake探测器将注意力集中在视频中的面部：首先对其进行跟踪，然后将裁剪后的面部数据传递到确定其是真实还是伪造的神经网络。例如，眨眼在伪造品中无法很好地再现，因此检测器将注意力集中在眼睛的运动上，以此作为确定眼睛的一种方法。最先进的Deepfake检测器依靠机器学习模型来识别假视频。</p><p> The extensive spread of fake videos through social media platforms has raised significant concerns worldwide, particularly hampering the credibility of digital media, the researchers point out. &#34;If the attackers have some knowledge of the detection system, they can design inputs to target the blind spots of the detector and bypass it,&#34; said Paarth Neekhara, the paper’s other first coauthor and a UC San Diego computer science student.</p><p> 研究人员指出，假冒视频在社交媒体平台上的广泛传播引起了全球的广泛关注，尤其是阻碍了数字媒体的信誉。 ＆＃34;如果攻击者对检测系统有一定的了解，他们可以设计输入以瞄准检测器的盲点并绕过检测器，＆＃34;该论文的另一位第一作者，加州大学圣地亚哥分校的计算机科学专业学生Paarth Neekhara说。</p><p> Researchers created an adversarial example for every face in a video frame. But while standard operations such as compressing and resizing video usually remove adversarial examples from an image, these examples are built to withstand these processes. The attack algorithm does this by estimating over a set of input transformations how the model ranks images as real or fake. From there, it uses this estimation to transform images in such a way that the adversarial image remains effective even after compression and decompression. The modified version of the face is then inserted in all the video frames. The process is then repeated for all frames in the video to create a deepfake video. The attack can also be applied on detectors that operate on entire video frames as opposed to just face crops.</p><p> 研究人员为视频帧中的每个面孔创建了一个对抗性示例。但是，尽管诸如压缩和调整视频大小之类的标准操作通常会从图像中删除对抗性示例，但这些示例旨在承受这些过程。攻击算法通过估计一组输入转换来实现此目的，模型将模型如何将图像分类为真实或伪造。从那里开始，它使用这种估计来转换图像，以使对抗图像即使在压缩和解压缩之后也保持有效。然后将脸部的修改版本插入所有视频帧中。然后对视频中的所有帧重复此过程，以创建Deepfake视频。攻击还可以应用于在整个视频帧上运行的检测器，而不仅仅是面部作物。</p><p> The team declined to release their code so it wouldn’t be used by hostile parties.</p><p> 该小组拒绝发布其代码，因此敌对方不会使用该代码。 </p><p>  Researchers tested their attacks in two scenarios: one where the attackers have complete access to the detector model, including the face extraction pipeline and the architecture and parameters of the classification model; and one where attackers can only query the machine learning model to figure out the probabilities of a frame being classified as real or fake. In the first scenario, the attack’s success rate is above 99 percent for uncompressed videos. For compressed videos, it was 84.96 percent. In the second scenario, the success rate was 86.43 percent for uncompressed and 78.33 percent for compressed videos. This is the first work which demonstrates successful attacks on state-of-the-art deepfake detectors.</p><p>研究人员在两种情况下测试了他们的攻击：一种是攻击者可以完全访问检测器模型，包括面部提取管道以及分类模型的体系结构和参数。攻击者只能查询机器学习模型来找出一帧被归类为真假的可能性。在第一种情况下，未压缩视频的攻击成功率超过99％。对于压缩视频，比例为84.96％。在第二种情况下，未压缩的成功率为86.43％，压缩视频的成功率为78.33％。这是展示对最先进的Deepfake检测器进行成功攻击的第一项工作。</p><p> &#34;To use these deepfake detectors in practice, we argue that it is essential to evaluate them against an adaptive adversary who is aware of these defenses and is intentionally trying to foil these defenses,&#34; the researchers write. &#34;We show that the current state of the art methods for deepfake detection can be easily bypassed if the adversary has complete or even partial knowledge of the detector.&#34;</p><p> ＆＃34;要在实践中使用这些Deepfake检测器，我们认为有必要对那些了解这些防御并且有意试图挫败这些防御的适应性对手进行评估，这很重要。研究人员写道。 ＆＃34;我们表明，如果攻击者对检测器具有完全或什至部分的知识，则可以很容易地绕过深度检测的最新方法。</p><p> To improve detectors, researchers recommend an approach similar to what is known as adversarial training: during training, an adaptive adversary continues to generate new deepfakes that can bypass the current state of the art detector; and the detector continues improving in order to detect the new deepfakes.</p><p> 为了改进探测器，研究人员推荐了一种类似于对抗训练的方法：在训练过程中，自适应对手会继续产生新的深造假货，从而可以绕过当前先进的探测器；检测器继续改进以检测新的深层伪造。</p><p> Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples Shehzeen Hussain, Malhar Jere, Farinaz Koushanfar, Department of Electrical and Computer Engineering, UC San Diego Paarth Neekhara,Â  Julian McAuley, Department of Computer Science and Engineering, UC San Diego</p><p> 对抗性Deepfakes：评估Deepfake检测器对对抗性示例的漏洞Shehzeen Hussain，Malhar Jere，Farinaz Koushanfar，加州大学圣地亚哥分校电气和计算机工程系Paarth Neekhara，Julian McAuley，加州大学圣地亚哥分校计算机科学与工程系</p><p>         UC San Diego’s  Studio Ten 300 offers radio and television connections for media interviews with our faculty, which can be coordinated via  .(JavaScript must be enabled to view this email address). To connect with a UC San Diego faculty expert on relevant issues and trending news stories, visit  https://ucsdnews.ucsd.edu/media-resources/faculty-experts.</p><p>         加州大学圣地亚哥分校的Studio 10 300提供广播和电视连接，可与我们的教员进行媒体采访，可以通过进行协调。（必须启用JavaScript才能查看此电子邮件地址）。要与加州大学圣地亚哥分校的教职专家联系，讨论有关问题和趋势新闻的趋势，请访问https://ucsdnews.ucsd.edu/media-resources/faculty-experts。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://ucsdnews.ucsd.edu/pressrelease/defeating_deepfake_detectors">https://ucsdnews.ucsd.edu/pressrelease/defeating_deepfake_detectors</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/科学家/">#科学家</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/探测器/">#探测器</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/detectors/">#detectors</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/检测器/">#检测器</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>