<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>应该允许受人工智能决策影响最大的人在塑造关于算法治理和监管的对话中发挥核心作用</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">应该允许受人工智能决策影响最大的人在塑造关于算法治理和监管的对话中发挥核心作用</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-03 22:56:53</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/5ea50c11e87601962553e2d5ad6a8d28.jpg"><img src="http://img2.diglog.com/img/2020/11/5ea50c11e87601962553e2d5ad6a8d28.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Workers for  Shipt, the grocery-delivery platform owned by Target, are protesting the firm’s recent implementation of a new algorithm dictating workers’ schedules and wages. How the algorithm makes these decisions isn’t clear: Shipt has provided few details to their more than 200,000 employees, and the company refuses to share anything with the public, claiming that the system is “proprietary.” But even without access to the inner workings of the algorithm, workers feel its impacts. Since the system went live in September, they claim that their wages have decreased and that scheduling is more complicated and unpredictable, throwing their lives into precarity and financial uncertainty. As Shipt worker  Willy Solis put it: “This is my business, and I need to be able to make informed decisions about my time.”</p><p>塔吉特(Target)旗下杂货配送平台Shipt的工人抗议该公司最近实施的一项新算法，该算法规定了工人的时间表和工资。目前尚不清楚算法是如何做出这些决定的：Shipt向其20多万名员工提供的细节很少，该公司拒绝与公众分享任何东西，声称该系统是“专有的”。但是，即使没有接触到算法的内部工作原理，工人们也能感受到它的影响。自从9月份系统投入使用以来，他们声称自己的工资下降了，日程安排变得更加复杂和不可预测，将他们的生活推入了早熟和财务不确定之中。正如船工威利·索利斯(Willy Solis)所说：“这是我的事，我需要能够对我的时间做出明智的决定。”</p><p> Even as evidence of artificial intelligence’s unevenly distributed harms and benefits mount, the question of  when it is appropriate to allow algorithms to make important decisions persists. Often, the people asked this question are people like me: those who have expertise in technology, and are employed in privileged positions in industry and academia. Left unasked and unanswered is another fundamental query: Who should get to answer this question, and on what basis? People like Willy, whose lives and opportunities are shaped by these systems, are almost never included.</p><p>即使有越来越多的证据表明人工智能的危害和好处分布不均，什么时候允许算法做出重要决策的问题仍然存在。通常，被问到这个问题的人都是像我这样的人：那些在技术方面有专长，并在工业界和学术界担任特权职位的人。另一个根本的问题是：谁应该回答这个问题，又有什么依据？这是另一个没有被问到和没有回答的问题。像威利这样的人，他们的生活和机会都是由这些系统塑造的，几乎从来没有被包括在内。</p><p> This is true across the board. Those who are subject to these systems are generally ignored in conversations about algorithmic governance and regulation. And when they are included, it’s often as a token or stereotype, and not as someone whose expertise is understood as central to AI decision-making.</p><p>这在所有方面都是正确的。在关于算法治理和监管的对话中，那些受制于这些系统的人通常会被忽略。当他们被包括在内时，通常是作为一种象征或刻板印象，而不是作为一个被理解为人工智能决策核心的人。</p><p> In some sense, this shouldn’t be surprising. These systems are largely produced by private companies and sold to other companies, governments, and institutions. They’re beholden to the incentives of those who create them, and whatever else they might do, they’re ultimately designed to increase the profits, efficiency, and growth of those who use them. Put another way, these are the tools of the powerful, generally applied by those who have power over those who have less. Shipt’s own chief communications officer, Molly Snyder,  said it herself: “We believe the model we rolled out is the right one for the company.” Here we see a tacit acknowledgment that the goals of the company are separate from those of its workers. And only one side has the power to choose how, and whether, to use the algorithm.</p><p>从某种意义上说，这并不令人惊讶。这些系统主要由私人公司生产，并出售给其他公司、政府和机构。他们受制于创建者的激励，无论他们还能做什么，最终都是为了提高使用者的利润、效率和增长。换句话说，这些都是强者的工具，通常是那些有权势的人用来对付那些没有权势的人。Shipt自己的首席公关官莫莉·斯奈德(Molly Snyder)自己也说过：“我们相信我们推出的模式是适合公司的。”在这里，我们看到了一种默许，即公司的目标与员工的目标是分开的。而且只有一方有权选择如何以及是否使用算法。</p><p>  Shipt is but one example of algorithmic harms. Earlier this year, the British government deployed  a new grading algorithm that failed spectacularly along predictable racial and class lines. Trying to compensate for COVID-related interruptions in testing, the algorithm guesstimated the scores it assumed students would have achieved under normal conditions, basing this assumption on things like teacher estimates and the historical performance of a given school. In doing so, it reduced the scores of poor, Black, and brown students, while giving higher marks to students from elite schools and wealthy areas.  Cori Crider, a lawyer from London law firm Foxglove, whose case won the reversal of the faulty grades, said, “There’s been a refusal to have an actual debate about how these systems work and whether we want them at all.”</p><p>Shipt只是算法危害的一个例子。今年早些时候，英国政府部署了一种新的评分算法，但在可预见的种族和阶级界限上失败了。为了补偿考试中与COVID相关的中断，该算法基于教师估计和给定学校的历史表现等因素，猜测了它假设的学生在正常情况下会获得的分数。在这样做的过程中，它降低了贫困学生、黑人学生和棕色学生的分数，而给精英学校和富裕地区的学生打了更高的分。伦敦律师事务所Foxglove的律师科里·克里德(Cori Crider)表示：“人们拒绝就这些系统是如何运作的，以及我们是否想要它们进行实际辩论。”他的案件赢得了推翻不合格成绩的诉讼。</p><p> The British grading algorithm fit a familiar pattern. Details of these algorithms are rarely made transparent to the public,  even to so-called experts. These systems are routinely protected from scrutiny by claims of corporate secrecy and decisions by governments and institutions that limit access and transparency. What is known about them is often what’s written by marketing departments and public relations representatives, presented to the public without evidence or verification. In Britain, government officials said the grading algorithm “was meant to make the system more fair,” a line that makes good PR but gives us zero information about the suitability of the system for its task, nor even the definition of “fairness” they might be relying on. This government and corporate protectionism and lack of access mean it’s extremely difficult for the public to make an informed decision about when and whether the use of AI is appropriate.</p><p>英国的评分算法符合人们熟悉的模式。这些算法的细节很少对公众透明，甚至对所谓的专家也是如此。这些系统通常受到公司保密主张以及政府和机构限制访问和透明度的决定的审查。人们对他们的了解往往是营销部门和公关代表撰写的，在没有证据或核实的情况下呈现给公众。在英国，政府官员表示，评级算法“是为了让系统更公平”，这句话可以做好公关，但却没有给我们任何关于系统是否适合其任务的信息，甚至连他们可能依赖的“公平”的定义也没有。这种政府和企业保护主义以及缺乏准入意味着公众很难在知情的情况下决定何时以及是否适合使用人工智能。</p><p>  Those who do have the closely guarded information about the inner workings of these systems are also often poorly positioned to make these types of decisions. The tech industry is extraordinarily concentrated. In the United States, this means that a handful of firms in Silicon Valley are at the heart of AI development, including building algorithmic models they license to third parties or leasing the infrastructure for AI startups who build their own. And those developing this technology are a homogenous group: predominantly white and male, hailing from elite universities, and possessing rare technical training.</p><p>那些拥有关于这些系统内部运作的严密保密信息的人，在做出这类决策时往往也处于不利地位。科技行业异常集中。在美国，这意味着硅谷的少数公司处于人工智能开发的核心，包括构建它们授权给第三方的算法模型，或者为建造自己的人工智能初创企业租赁基础设施。而那些开发这项技术的人是一个同质的群体：主要是白人和男性，来自精英大学，拥有罕见的技术培训。</p><p> They represent the powerful and the elite, and while they may understand how to train a convolutional neural network, they are far removed from the contexts and communities in which their technology will be applied. Indeed, they are often unaware of how and where the technology they build is ultimately used. Technical know-how, whether in government or in the technology industry, cannot substitute for contextual understanding and lived experiences in determining whether it’s appropriate to apply AI systems in sensitive social domains. Especially given that these systems replicate and amplify the harms of structural racism and historical discrimination, which fall predominantly on Black, brown, and poor communities.</p><p>他们代表着有权势的人和精英，虽然他们可能知道如何</p><p>  So what does it look like when the people who bear the risks of algorithmic systems get to determine whether — and how — they’re used? It doesn’t look like a neat flowchart, or a set of AI governance principles, or a room full of experts and academics opining on hypothetical benevolent AI futures. It looks like those who will be subject to these systems getting the information they need to make informed choices. It looks like these communities sharing their experiences and doing the work to envision a world they want to live in. Which may or may not include these technologies or the institutions that use them.</p><p>那么，当承担算法系统风险的人决定是否以及如何使用这些系统时，会是什么样子呢？它看起来不像一个整洁的流程图，也不像一套人工智能治理原则，也不像一屋子专家和学者对假想的仁慈的人工智能未来发表意见。看起来，那些将受制于这些系统的人将获得他们所需的信息，以便做出明智的选择。看起来，这些社区分享他们的经验，并做工作，以设想一个他们想要生活的世界。这可能包括也可能不包括这些技术或使用它们的机构。</p><p> It looks like  Tawana Petty’s work to ban facial recognition in Detroit;  British students protesting the government’s grading algorithm; Shipt workers on strike; and parents and teachers at the  Lockport school district in New York pushing back against the district’s procurement of these systems. It looks like social movements and civic engagements. And ultimately it looks like those who have to live under these systems reclaiming the right to determine the forces that shape their lives and livelihoods, and the right to refuse to be governed by technology that serves the interests of the powerful at their expense.</p><p>看起来像是塔瓦娜·佩蒂(Tawana Petty)在底特律禁止面部识别；英国学生抗议政府的评分算法；Shipt工人罢工；纽约洛克波特学区的家长和教师反对该学区采购这些系统。看起来像是社会运动和公民参与。归根结底，那些不得不生活在这些制度下的人，看起来是在要求决定塑造他们生活和生计的力量的权利，以及拒绝接受以牺牲他们为代价服务于强国利益的技术的统治的权利。</p><p>  We won’t know the actual answer to when it is appropriate to use algorithms until the people who are most affected have the power to answer that very question. And experts like me can’t answer it for them.</p><p>在受影响最大的人有能力回答这个问题之前，我们不会知道什么时候使用算法是合适的。像我这样的专家不能替他们回答这个问题。</p><p> Meredith Whittaker is a cofounder of AI Now Institute and Minderoo research professor at New York University.</p><p>梅雷迪思·惠特克(Meredith Whittaker)是AI Now Institute的联合创始人，也是纽约大学(New York University)Minderoo研究教授。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.bostonglobe.com/2020/11/02/opinion/who-am-i-decide-when-algorithms-should-make-important-decisions/">https://www.bostonglobe.com/2020/11/02/opinion/who-am-i-decide-when-algorithms-should-make-important-decisions/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/人工智能/">#人工智能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/允许/">#允许</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/affected/">#affected</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1032364.html"><img src="http://img2.diglog.com/img/2020/10/thumb_358ec8b565c36520433f972ae2130006.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1032364.html">
根据欧盟计划，大型科技公司的“黑匣子”算法面临监管监督</a></div><span class="my_story_list_date">2020-10-31 3:34</span></div><div class="col-sm"><div><a target="_blank" href="/story/1031734.html"><img src="http://img2.diglog.com/img/2020/10/thumb_45c297044366c0accf410efdfd170e6f.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031734.html">一项对波士顿地区约57,000名肾病患者的研究发现，一种用于决定移植优先顺序的算法对黑人患者有偏见</a></div><span class="my_story_list_date">2020-10-28 18:36</span></div><div class="col-sm"><div><a target="_blank" href="/story/1031711.html"><img src="http://img2.diglog.com/img/2020/10/thumb_944c649a872d5bcad506adefd9480ef9.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1031711.html">使用新的压缩算法提高快照速度</a></div><span class="my_story_list_date">2020-10-28 13:45</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030258.html"><img src="http://img2.diglog.com/img/2020/10/thumb_18b6543ca958addeb58b9457547435ea.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030258.html">揭开共识算法的神秘面纱及其实现</a></div><span class="my_story_list_date">2020-10-21 14:1</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>