<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>神经细胞自动机的对抗性重新编程 Adversarial Reprogramming of Neural Cellular Automata</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Adversarial Reprogramming of Neural Cellular Automata<br/>神经细胞自动机的对抗性重新编程 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-07 04:49:10</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/5/e660ccf4513b47a6e3c104d17f8d60b3.jpg"><img src="http://img2.diglog.com/img/2021/5/e660ccf4513b47a6e3c104d17f8d60b3.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This article makes strong use of colors in figures and demos. Click  here to adjust the color palette.</p><p>本文强烈使用数字和演示中的颜色。单击此处调用颜色调色板。</p><p> In a complex system, whether biological, technological, or social, how can we discover signaling events that will alter system-level behavior in desired ways? Even when the rules governing the individual components of these complex systems are known, the inverse problem - going from desired behaviour to system design - is at the heart of many barriers for the advance of biomedicine, robotics, and other fields of importance to society.</p><p> 在复杂的系统中，无论是生物，技术还是社会，我们如何发现将以所需方式改变系统级行为的信令事件？即使是已知有关这些复杂系统的各个组成部分的规则，也是从期望的行为到系统设计的逆问题 - 是生物医学，机器人，机器人和社会重要领域的许多障碍的核心。</p><p> Biology, specifically, is transitioning from a focus on mechanism (what is required for the system to work) to a focus on information (what algorithm is sufficient to implement adaptive behavior). Advances in machine learning represent an exciting and largely untapped source of inspiration and tooling to assist the biological sciences. Growing Neural Cellular Automata   and Self-classifying MNIST Digits   introduced the Neural Cellular Automata (Neural CA) model and demonstrated how tasks requiring self-organisation, such as pattern growth and self-classification of digits, can be trained in an end-to-end, differentiable fashion. The resulting models were robust to various kinds of perturbations: the growing CA expressed regenerative capabilities when damaged; the MNIST CA were responsive to changes in the underlying digits, triggering reclassification whenever necessary. These computational frameworks represent quantitative models with which to understand important biological phenomena, such as scaling of single cell behavior rules into reliable organ-level anatomies. The latter is a kind of anatomical homeostasis, achieved by feedback loops that must recognize deviations from a correct target morphology and progressively reduce anatomical error.</p><p> 具体地，生物学正在从焦点上转换到机制（系统所需的内容），以焦点对信息（哪种算法足以实现自适应行为）。机器学习的进展代表了一种令人兴奋的，主要是未开发的灵感和工具来源，以协助生物科学。生长神经蜂窝自动机和自我分类的MNIST数字引入了神经蜂窝自动机（神经CA）模型，并证明了需要自组织的任务，例如模式生长和数字的自我分类，可以在端到端培训，可怜的方式。由此产生的模型对各种扰动具有鲁棒性：在损坏时，CA越来越多的CA表达了再生能力; Mnist CA响应于底层数字的变化，无论只要需要重新分类。这些计算框架代表了了解重要生物现象的定量模型，例如单个细胞行为规则的缩放到可靠的器官级解剖。后者是一种解剖稳态，通过反馈回路实现，必须识别与正确的目标形态的偏差，逐渐减少解剖误差。</p><p> In this work, we  train adversaries whose goal is to reprogram CA into doing something other than what they were trained to do. In order to understand what kinds of lower-level signals alter system-level behavior of our CA, it is important to understand how these CA are constructed and where local versus global information resides.</p><p> 在这项工作中，我们训练他们的目标是将CA重新编制CA重新编制的东西，而不是他们训练的事情。为了了解哪些较低级别的较低级别的信号，改变了我们CA的系统级别行为，了解如何构建这些CA以及本地与全局信息所在的位置。</p><p>  Individual cell states. States store information which is used for both diversification among cell behaviours and for communication with neighbouring cells.</p><p>  个别细胞状态。商店信息用于细胞行为之间的多样化和与相邻小区进行通信。</p><p> The model parameters. These describe the input/output behavior of a cell and are shared by every cell of the same family. The model parameters can be seen as  the way the system works.</p><p> 模型参数。这些描述了单元格的输入/输出行为，并由同一系列的每个单元格共享。模型参数可以看作系统工作的方式。</p><p> The perceptive field. This is how cells perceive their environment. In Neural CA, we always restrict the perceptive field to be the eight nearest neighbors and the cell itself. The way cells are perceived by each other is different between the Growing CA and MNIST CA. The Growing CA perceptive field is a set of weights fixed both during training and inference, while the MNIST CA perceptive field is learned as part of the model parameters.</p><p> 感知领域。这就是细胞感知环境的影响。在神经CA中，我们总是将感知场限制为八个最近的邻居和细胞本身。通过彼此感知电池的方式是不同的CA和MNIST CA之间的不同。不断增长的Ca感知场是在训练和推理期间固定的一组重量，而MNIST CA感知字段被学习为模型参数的一部分。 </p><p>  We will explore two kinds of adversarial attacks: 1) injecting a few adversarial cells into an existing grid running a pretrained model; and 2) perturbing the global state of all cells on a grid.</p><p>我们将探讨两种对抗性攻击：1）将少数对抗性细胞注入运行预制模型的现有网格中; 2）扰乱了网格上所有细胞的全局状态。</p><p> For the first type of adversarial attacks we train a new CA model that, when placed in an environment running one of the original models described in the previous articles, is able to hijack the behavior of the collective mix of adversarial and non-adversarial CA. This is an example of injecting CA with differing  model parameters into the system. In biology, numerous forms of hijacking are known, including viruses that take over genetic and biochemical information flow  , bacteria that take over physiological control mechanisms   and even regenerative morphology of whole bodies  , and fungi and toxoplasma that modulate host behavior  . Especially fascinating are the many cases of non-cell-autonomous signaling developmental biology and cancer, showing that some cell behaviors can significantly alter host properties both locally and at long range. For example, bioelectrically-abnormal cells can trigger metastatic conversion in an otherwise normal body (with no genetic defects)  , while management of bioelectrical state in one area of the body can suppress tumorigenesis on the other side of the organism  . Similarly, amputation damage in one leg initiates changes to ionic properties of cells in the contralateral leg  , while the size of the developing brain is in part dictated by the activity of ventral gut cells  . All of these phenomena underlie the importance of understanding how cell groups make collective decisions, and how those tissue-level decisions can be subverted by the activity of a small number of cells. It is essential to develop quantitative models of such dynamics, in order to drive meaningful progress in regenerative medicine that controls system-level outcomes top-down, where cell- or molecular-level micromanagement is infeasible  .</p><p> 对于第一种类型的对抗攻击我们训练一个新的CA模型，当放置在运行前一篇文章中描述的原始模型之一的环境中时，能够劫持对抗和非对抗性CA的集体混合的行为。这是将具有不同模型参数的CA注入系统的示例。在生物学中，许多形式的劫持形式是已知的，包括服用遗传和生化信息流量的病毒，接管生理控制机制的细菌以及整个身体的再生形态，以及调节宿主行为的真菌和弓形虫。特别是迷人是许多非细胞自主信号传播生物学和癌症的情况，表明一些细胞行为可以显着改变本地和长距离的宿主性质。例如，生物电解细胞可以在否则正常的身体（没有遗传缺陷）中触发转移性转化，而在体内一个区域中的生物电态的管理可以抑制生物体的另一侧的肿瘤发生。类似地，一条腿上的截肢损伤引发对对侧腿中细胞的离子性质的变化，而显影大脑的大小部分是腹肠细胞的活性决定。所有这些现象都利好了解细胞组如何使集体决策如何做出集体决策，以及如何通过少量细胞的活动来颠覆。必须开发这种动态的定量模型，以便在再生药物上驱动有意义的进程，以防止控制体系水平结果，其中细胞或分子水平微型和分子是不可行的。</p><p> The second type of adversarial attacks interact with previously trained growing CA models by  perturbing the states within cells. We apply a global state perturbation to all living cells. This can be seen as inhibiting or enhancing combinations of state values, in turn hijacking proper communications among cells and within the cell’s own states. Models like this represent not only ways of thinking about adversarial relationships in nature (such as parasitism and evolutionary arms races of genetic and physiological mechanisms), but also a roadmap for the development of regenerative medicine strategies. Next-generation biomedicine will need computational tools for inferring minimal, least-effort interventions that can be applied to biological systems to predictively change their large-scale anatomical and behavioral properties.</p><p> 第二种类型的对抗攻击通过扰乱细胞内的状态与先前训练的CA模型相互作用。我们对所有活细胞施加全球扰动。这可以被视为抑制或增强状态值的组合，反过来劫持细胞之间的适当通信以及细胞的自己的状态。这样的模型不仅可以思考对抗性关系的方式（例如寄生和生理机制的进化臂竞争），也是用于开发再生医学策略的路线图。下一代Biomedicine将需要计算工具，用于推断可以应用于生物系统以预测改变​​其大规模解剖和行为特性的最小的，最小努力干预。</p><p>  Recall how the Self-classifying MNIST digits task consisted of placing CA cells on a plane forming the shape of an MNIST digit. The cells then had to communicate among themselves in order to come to a complete consensus as to which digit they formed.</p><p>  回想一下自我分类的MNIST数字任务是如何在形成MNIST数字形状的平面上放置CA电池。然后，细胞必须在自己之间进行沟通，以便完全共识，以便他们形成的数字。</p><p>  Below we show examples of classifications made by the model trained in Self-classifying MNIST Digits.</p><p>  下面我们展示了模型在自我分类的MNIST数字中培训的分类示例。</p><p>  In this experiment,  the goal is to create adversarial CA that can hijack the cell collective’s classification consensus to always classify an eight. We use the CA model from   and freeze its parameters. We then train a new CA whose model architecture is identical to the frozen model but is randomly initialized. The training regime also closely approximates that of self-classifying MNIST digits CA. There are three important differences:</p><p>  在这个实验中，目标是产生可能劫持细胞集体的分类共识的对抗性CA，以始终分类八个。我们使用CA模型从并冻结其参数。然后，我们培训模型架构与冻结模型相同但随机初始化的新CA。培训制度也密切近似于自我分类的MNIST数字CA.有三种重要差异：</p><p> Regardless of what the actual digit is, we consider  the correct classification to always be an eight.</p><p> 无论实际数字如何，我们都认为正确的分类始终是八个。 </p><p> For each batch and each pixel, the CA is randomly chosen to be either the pretrained model or the new adversarial one. The adversarial CA is used 10% of the time, and the pre-trained, frozen, model the rest of the time.</p><p>对于每个批次和每个像素，将CA随机选择为预磨料模型或新的对抗性。对抗的CA是使用10％的时间，并且预先训练的，冻结，剩下的时间。</p><p> Only the adversarial CA parameters are trained, the parameters of the pretrained model are kept frozen.</p><p> 只培训对侵扰性CA参数，预制模型的参数保持冷冻。</p><p> The adversarial attack as defined here only modifies a small percentage of the overall system, but the goal is to propagate signals that affect all the living cells. Therefore, these adversaries have to somehow learn to communicate deceiving information that causes wrong classifications in their neighbours and further cascades in the propagation of deceiving information by ‘unaware’ cells. The unaware cells’ parameters cannot be changed so the only means of attack by the adversaries is to cause a change in the cells’ states. Cells’ states are responsible for communication and diversification.</p><p> 这里定义的对抗攻击仅修改了整个系统的小百分比，但目标是传播影响所有活细胞的信号。因此，这些对手必须以某种方式学会传达欺骗信息，这些信息在其邻居中导致错误的分类以及通过“不知道”细胞传播欺骗信息的传播中的进一步级联。不知道细胞的参数无法改变，因此对手的唯一攻击方式是导致细胞状态的变化。细胞的国家负责沟通和多样化。</p><p> The task is remarkably simple to optimize, reaching convergence in as little as 2000 training steps (as opposed to the two orders of magnitude more steps needed to construct the original MNIST CA). By visualising what happens when we remove the adversaries, we observe that the adversaries must be constantly communicating with their non-adversarial neighbours to keep them convinced of the malicious classification. While some digits don’t recover after the removal of adversaries, most of them self-correct to the right classification. Below we show examples where we introduce the adversaries at 200 steps and remove them after a further 200 steps.</p><p> 任务非常简单，优化，达到与2000年训练步骤的收敛相反（与构建原始MNIST CA所需的两个步骤相反）。通过可视化我们去除对手时会发生什么，我们观察到对手必须与他们的非对抗性邻国不断沟通，以防止他们信服恶意分类。虽然某些数字在删除对手后不恢复，但大多数人都自我纠正到正确的分类。下面我们展示了我们在200个步骤中介绍对手的示例，并在另外200步后删除它们。</p><p>  While we trained the adversaries with a 10-to-90% split of adversarial vs. non-adversarial cells, we observe that often significantly fewer adversaries are needed to succeed in the deception. Below we evaluate the experiment with just one percent of cells being adversaries.</p><p>  虽然我们培训了伴对逆势与非对抗细胞的10％至90％的对手，但我们观察到欺骗中需要较少的对手越来越少。下面我们评价实验，只有百分之一的细胞是对手的。</p><p>  We created a demo playground where the reader can draw digits and place adversaries with surgical precision. We encourage the reader to play with the demo to get a sense of how easily non-adversarial cells are swayed towards the wrong classification.</p><p>  我们创建了一个演示游乐场，读者可以用手术精度绘制数字并将对手绘制。我们鼓励读者与演示一起玩，以了解易受对抗细胞如何朝错误的分类摇曳的感觉。</p><p>  The natural follow up question is whether these adversarial attacks work on Growing CA, too. The Growing CA goal is to be able to grow a complex image from a single cell, and having its result be persistent over time and robust to perturbations. In this article, we focus on the lizard pattern model from Growing CA.</p><p>  自然的跟进问题是这些对抗攻击也适用于增长CA。不断增长的CA目标是能够从单个单元格展现复杂的图像，并使其结果随着时间的推移而持续存在，并且鲁棒到扰动。在本文中，我们专注于蜥蜴模式模型来自不断增长的CA. </p><p>  The goal is to have some adversarial cells change the global configuration of all the cells. We choose two new targets we would like the adversarial cells to try and morph the lizard into: a tailless lizard and a red lizard.</p><p>目标是具有一些逆势细胞改变所有细胞的全局配置。我们选择两个新的目标我们希望对抗细胞尝试和变形蜥蜴进入：无蜥蜴和一个红色的蜥蜴。</p><p>   Red lizard: converting a lizard from green to red would show a global change in the behaviour of the cell collective. This behavior is not present in the dynamics observed by the original model. The adversaries are thus tasked with fooling other cells into doing things they have never done before (create the lizard shape as before, but now colored in red).</p><p>   红色蜥蜴：将蜥蜴从绿色转换为红色将显示细胞集体行为的全局变化。原始模型观察到的动态中不存在此行为。因此，对手的任务是欺骗其他细胞，以做他们以前从未做过的事情（以前创造蜥蜴形状，但现在在红色中色）。</p><p> Tailless lizard: having a severed tail is a more localized change that only requires some cells to be fooled into behaving in the wrong way: the cells at the base of the tail need to be convinced they constitute the edge or silhouette of the lizard, instead of proceeding to grow a tail as before.</p><p> 尾部蜥蜴：尾巴被切断的尾巴是一个更局部的变化，只需要一些细胞以错误的方式愚弄表现：尾部基部的细胞需要确信它们构成蜥蜴的边缘或轮廓，而是构成蜥蜴的边缘或轮廓继续以以前举行尾巴。</p><p> Just like in the previous experiment, our adversaries can only indirectly affect the states of the original cells.</p><p> 就像在以前的实验中一样，我们的对手只能间接地影响原始细胞的状态。</p><p> We first train adversaries for the tailless target with a 10% chance for any given cell to be an adversary. We prohibit cells to be adversaries if they are outside the target pattern; i.e. the tail contains no adversaries.</p><p> 我们首先将任何给予的细胞成为对手的10％的机会训练TaiLess目标的对手。如果在目标模式之外，我们禁止细胞成为对手;即，尾巴没有对手。</p><p>  The video above shows six different instances of the same model with differing stochastic placement of the adversaries. The results vary considerably: sometimes the adversaries succeed in removing the tail, sometimes the tail is only shrunk but not completely removed, and other times the pattern becomes unstable. Training these adversaries required many more gradient steps to achieve convergence, and the pattern converged to is qualitatively worse than what was achieved for the adversarial MNIST CA experiment.</p><p>  上面的视频显示了六种不同的同一模型实例，具有不同的对手的随机放置。结果差异很大：有时，对手成功地去除尾巴，有时尾巴仅缩小但没有完全移除，而其他时间的模式变得不稳定。培训这些对手需要更多的渐变步骤来实现收敛，并且融合的模式比对对抗MNIST CA实验所达到的方式更差。</p><p> The red lizard pattern fares even worse. Using only 10% adversarial cells results in a complete failure: the original cells are unaffected by the adversaries. Some readers may wonder whether the original pretrained CA has the requisite skill, or ‘subroutine’ of producing a red output at all, since there are no red regions in the original target, and may suspect this was an impossible task to begin with. Therefore, we increased the proportion of adversarial cells until we managed to find a successful adversarial CA, if any were possible.</p><p> 红色蜥蜴图案差价差。只使用10％的对抗细胞导致完全失败：原始细胞不受对手的影响。有些读者可能怀疑原来的预磨损CA是否具有生产的必要技能或“子程序”，因为原始目标没有红色区域，并且可能怀疑这是一个不可能的任务。因此，我们增加了对抗性细胞的比例，直到我们设法找到成功的对抗性CA，如果有的话。 </p><p>  In the video above we can see how, at least in the first stages of morphogenesis, 60% of adversaries are capable of coloring the lizard red. Take particular notice of the “step 500”  The still-image of the video is on step 500, and the video stops for a bit more than a second on step 500., where we hide the adversarial cells and show only the original cells. There, we see how a handful of original cells are colored in red. This is proof that the adversaries successfully managed to steer neighboring cells to color themselves red, where needed.</p><p>在上面的视频中，我们可以看到如何，至少在一个形态发生的第一个阶段，60％的对手能够着色蜥蜴红色。特别注意“步骤500”视频的静止图像是在步骤500上，视频停止超过步骤500的秒钟，在那里我们隐藏对抗细胞并仅显示原始单元。在那里，我们看一下少量原始细胞是红色的。这证明了对手成功地设法使邻近的单元格以所需的方式为颜色为红色。</p><p> However, the model is very unstable when iterated for periods of time longer than seen during training. Moreover, the learned adversarial attack is dependent on a majority of cells being adversaries. For instance, when using fewer adversaries on the order of 20-30%, the configuration is unstable.</p><p> 然而，在训练期间的时间长时间迭代时，该模型非常不稳定。此外，学习的对抗性攻击取决于大多数细胞是对手的。例如，当使用大约20-30％的对手较少时，配置不稳定。</p><p> In comparison to the results of the previous experiment, the Growing CA model shows a greater resistance to adversarial perturbation than those of the MNIST CA. A notable difference between the two models is that the MNIST CA cells have to always be ready and able to change an opinion (a classification) based on information propagated through several neighbors. This is a necessary requirement for that model because at any time the underlying digit may change, but most of the cells would not observe any change in their neighbors’ placements. For instance, imagine the case of a one turning into a seven where the lower stroke of each overlap perfectly. From the point of view of the cells in the lower stroke of the digit, there is no change, yet the digit formed is now a seven. We therefore hypothesise MNIST CA are more reliant and ‘trusting’ of continuous long-distance communication than Growing CA, where cells never have to reconfigure themselves to generate something different to before.</p><p> 与先前实验的结果相比，不断增长的Ca模型显示出比MNIST CA的对抗扰动更大的抗性。两种模型之间的显着差异是MNIST CA电池必须始终准备好并能够基于通过几个邻居传播的信息来改变意见（分类）。这是该模型的必要要求，因为在任何时候底层数字可能会发生变化，但大多数单元格都不会观察其邻居展示位置的任何变化。例如，想象一个转向每个重叠的下划线的七个的情况。从数字较低行程的细胞的角度来看，没有变化，但是形成的数字现在是一个七个。因此，我们假设MNIST CA更依赖于连续的长距离连通的更依赖性，而不是生长的CA，其中细胞永远不必重新配置自己以产生与之前不同的东西。</p><p> We suspect that more general-purpose Growing CA that have learned a variety of target patterns during training are more likely to be susceptible to adversarial attacks.</p><p> 我们怀疑在训练期间学到了多种目标模式的更通用的生长CA更有可能易于对抗性攻击。</p><p>  We observed that it is hard to fool Growing CA into changing their morphology by placing adversarial cells inside the cell collective. These adversaries had to devise complex local behaviors that would cause the non-adversarial cells nearby, and ultimately globally throughout the image, to change their overall morphology.</p><p>  我们观察到，通过将对抗细胞放置在细胞集体内的对抗细胞，难以愚弄越来越大的CA来改变它们的形态。这些对手必须设计复杂的当地行为，这些行为将导致非对抗细胞附近，最终整个形象，改变其整体形态。</p><p> In this section, we explore an alternative approach: perturbing the global state of all cells without changing the model parameters of any cell.</p><p> 在本节中，我们探索替代方法：扰乱所有单元格的全局状态而不改变任何单元的模型参数。</p><p> As before, we base our experiments on the Growing CA model trained to produce a lizard. Every cell of a Growing CA has an internal state vector with 16 elements. Some of them are phenotypical elements (the RGBA states) and the remaining 12 serve arbitrary purposes, used for storing and communicating information. We can perturb the states of these cells to hijack the overall system in certain ways (the discovery of such perturbation strategies is a key goal of biomedicine and synthetic morphology). There are a variety of ways we can perform state perturbations. We will focus on  global state perturbations, defined as perturbations that are applied on every living cell at every time step (analogous to “systemic” biomedical interventions, that are given to the whole organism (e.g., a chemical taken internally), as opposed to highly localized delivery systems). The new goal is to discover a certain type of global state perturbation that results in a stable new pattern.</p><p> 如前所述，我们将我们的实验基于培训的CA模型，以产生蜥蜴。生长CA的每个单元格都有一个带有16个元素的内部状态向量。其中一些是表型元素（RGBA状态），其余12个用于存储和传送信息的任意目的。我们可以在某些方面扰乱这些细胞的州以劫持整个系统（这种扰动策略的发现是生物医学和合成形态的关键目标）。我们可以执行各种方式进行状态扰动。我们将重点关注全球扰动，定义为每次步骤（类似于“系统性”生物医学干预措施的每个活细胞上应用的扰动，所述生物医学干预措施（例如，内部的化学物质）而不是高度本地化的交付系统）。新的目标是发现某种类型的全球国家扰动，导致稳定的新模式。 </p><p>  We show 6 target patterns: the tailless and red lizard from the previous experiment, plus a blue lizard and lizards with various severed limbs and severed head.</p><p>我们展示了6个目标模式：从前一个实验的无尾和红色蜥蜴，以及蓝色蜥蜴和蜥蜴，各种切断的四肢和切断头。</p><p>  We decided to experiment with a simple type of global state perturbation: applying a symmetric        1 6 × 1 6 16\times16     1 6 × 1 6 matrix multiplication        A A     A to every living cell at every step  In practice, we also clip the state of cells such that they are bounded in        [ − 3 , + 3 ] [-3, +3]     [ − 3 , + 3 ]. This is a minor detail and it helps stabilise the model.. To give insight on why we chose this, an even simpler “state addition” mutation (a mutation consisting only of the addition of a vector to every state) would be insufficient because the value of the states of our models are unbounded, and often we would want to suppress something by setting it to zero. The latter is generally impossible with constant state additions, as a constant addition or subtraction of a value would generally lead to infinity, except for some fortunate cases where the natural residual updates of the cells would cancel out with the constant addition at precisely state value zero. However, matrix multiplications have the possibility of amplifying/suppressing combinations of elements in the states: multiplying a state value repeatedly for a constant value less than one can easily suppress a state value to zero. We constrain the matrix to be symmetric for reasons that will become clear in the following section.</p><p>  我们决定尝试一种简单的全球扰动类型：应用对称1 6×1 6 16 \ time16 1 6×1 6矩阵乘法AAA在每一步的实践中，我们也剪辑了细胞的状态它们界定在[ -  3，+ 3] [-3，+3] [ -  3，+ 3]中。这是一个次要的细节，它有助于稳定模型。要识别我们选择这一点，更简单的“状态加法”突变（仅为每个状态添加向量组成的突变）是不够的我们模型状态的价值是无限的，通常我们希望通过将其设置为零来抑制某些内容。后者通常不可能以恒定的状态添加，因为值的恒定添加或减法通常会导致无穷大，除了一些幸运的情况，其中细胞的自然残留更新将在精确的状态值零时持续添加到恒定的添加。然而，矩阵乘法具有放大/抑制状态中元素的组合的可能性：将状态值乘以常数值小于一个，可以容易地抑制零值为零。我们将矩阵限制为对称的原因是以下部分将变得清晰的原因。</p><p> We initialize        A A     A with the identity matrix        I I     I and train        A A     A just as we would train the original Growing CA, albeit with the following differences:</p><p> 我们用身份矩阵I初始化A A I I I I我并培训A A的正如我们将培训原始的CA，尽管如此：</p><p> We consider the set of initial image configurations to be both the seed state and the state with a fully grown lizard (as opposed to the Growing CA article, where initial configurations consisted of the seed state only).</p><p> 我们考虑一组初始图像配置，既是种子状态和具有完全生长的蜥蜴的状态（与不断增长的CA制品相反，其中初始配置仅由种子状态组成）。</p><p>  The video above shows the model successfully discovering global state perturbations able to change a target pattern to a desired variation. We show what happens when we stop perturbing the states (an out-of-training situation) at step 500 through step 1000, then reapplying the mutation. This demonstrates the ability of our perturbations to achieve the desired result both when starting from a seed, and when starting from a fully grown pattern. Furthermore it demonstrates that the original CA easily recover from these state perturbations once it goes away. This last result is perhaps not surprising given how robust growing CA models are in general.</p><p>  上面的视频显示了模型成功发现能够将目标模式改变为所需变化的全局状态扰动。我们展示当我们在步骤500通过步骤1000停止扰乱状态（训练情况外）时会发生什么，然后重新填充突变。这证明了我们在从种子开始时达到所需结果的能力，以及从完全生长的模式开始时。此外，它表明，一旦它消失，原来的CA很容易从这些状态扰动中恢复。考虑到增长CA模型一般有多稳健，这对最后结果可能并不令人惊讶。</p><p> Not all perturbations are equally effective. In particular, the headless perturbation is the least successful as it results in a loss of other details across the whole lizard pattern such as the white coloring on its back. We hypothesize that the best perturbation our training regime managed to find, due to the simplicity of the perturbation, was suppressing a “structure” that contained both the morphology of the head and the white colouring. This may be related to the concept of differentiation and distinction of biological organs. Predicting what kinds of perturbations would be harder or impossible to be done, before trying them out empirically, is still an open research question in biology. On the other hand, a variant of this kind of synthetic analysis might help with defining higher order structures within biological and synthetic systems.</p><p> 并非所有扰动都同样有效。特别是，无头扰动是最不成功的，因为它导致整个蜥蜴模式的其他细节丢失，例如背部的白色着色。我们假设由于扰动的简单性，我们的培训制度的最佳扰动，抑制了含有头部和白色着色的形态的“结构”。这可能与生物器官的分化和区别的概念有关。在经验上审视之前，预测预测什么类型的扰动是更难或不可能完成的，仍然是生物学中的开放研究问题。另一方面，这种合成分析的变体可能有助于在生物和合成系统中定义更高阶结构。</p><p>  Our choice of using a symmetric matrix for representing global state perturbations is justified by a desire to have compositionality. Every complex symmetric matrix        A A     A can be diagonalized as follows:</p><p>  我们选择使用对称矩阵来代表全球国家扰动是通过具有合成性的愿望是合理的。每个复杂的对称矩阵A A A可以是对角线的如下： </p><p>  where        Λ \Lambda     Λ is the diagonal eigenvalues matrix and        Q Q     Q is the unitary matrix of its eigenvectors. Another way of seeing this is applying a change of basis transformation, scaling each component proportional to the eigenvalues, and then changing back to the original basis. This should also give a clearer intuition on the ease of suppressing or amplifying combinations of states. Moreover, we can now infer what would happen if all the eigenvalues were to be one. In that case, we would naturally have        Q I  Q ⊺ = I Q I Q^\intercal = I     Q I  Q        ⊺ = I resulting in a no-op (no change): the lizard would grow as if no perturbation was performed. We can now decompose        Q Λ  Q ⊺ = Q ( D + I )  Q ⊺ Q \Lambda Q^\intercal = Q (D + I) Q^\intercal     Q Λ  Q        ⊺ = Q ( D + I )  Q        ⊺ where D is the  perturbation direction (       Λ − I \Lambda - I     Λ − I) in the “eigenvalue space”. Suppose we use a coefficient        k k     k to scale D:         A k = Q ( k D + I )  Q ⊺ A_k = Q (kD + I) Q^\intercal      A        k ​   = Q ( k D + I )  Q        ⊺. If        k = 1 k=1     k = 1, we are left with the original perturbation        A A     A and when        k = 0 k=0     k = 0, we have the no-op        I I     I. Naturally, one question would be whether we can explore other values for        k k     k and discover meaningful perturbations. Since</p><p>其中λ\ lambdaλ是对角线特征值矩阵，q q q是其特征向量的整体矩阵。看到这一点的另一种方法是应用基础转换的变化，将每个组件缩放与特征值成比例，然后将返回到原始基础。这还应该在易于抑制或放大各国的组合方面提供更清晰的直觉。而且，我们现在可以推断出所有特征值是一个人会发生什么。在这种情况下，我们自然有Q I Q = I Q I Q ^ \ Intercal = I Q I Q I Q =我导致NO-OP（没有变化）：蜥蜴会随着未执行扰动而增长。我们现在可以分解Qλq∞= q（d + i）q⊺q\ lambda q ^ \ intercal = q（d + i）q ^ \ intercal qλq⊺= q（d + i）q⊺其中d是“特征值空间”中的扰动方向（λ -  i \ lambda  -  iλ -  i）。假设我们使用系数k k k缩放d：a k = q（k d + i）q⊺a_k = q（kd + i）q ^ \ intercal a k = q（k d + i）q1。如果k = 1 k = 1 k = 1，我们留下了原始扰动AAA，当K = 0 k = 0 k = 0时，我们有NO-OP II I.自然，一个问题是我们是否可以探索KKK的其他值并发现有意义的扰动。自从</p><p> A k = Q ( k D + I )  Q ⊺ = k A + ( 1 − k ) I A_k = Q (kD + I) Q^\intercal = k A + (1-k) I     </p><p> k = q（k d + i）q = k a +（1  -  k）i a_k = q（kd + i）q ^ \ intercal = k a +（1-k）i</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://distill.pub/selforg/2021/adversarial">https://distill.pub/selforg/2021/adversarial</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/编程/">#编程</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/自动机/">#自动机</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/细胞/">#细胞</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>