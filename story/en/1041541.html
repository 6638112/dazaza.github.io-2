<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Google开发了可以同时学习国际象棋和吃豆人的AI Google develops an AI that can learn both chess and Pac-Man</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Google develops an AI that can learn both chess and Pac-Man<br/>Google开发了可以同时学习国际象棋和吃豆人的AI </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-24 21:10:53</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/97f117b5f2963c91cd7f76671cb606f6.jpg"><img src="http://img2.diglog.com/img/2020/12/97f117b5f2963c91cd7f76671cb606f6.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>The first major conquest of artificial intelligence was chess. The game has a dizzying number of possible combinations, but it was relatively tractable because it was structured by a set of clear rules. An algorithm could always have perfect knowledge of the state of the game and know every possible move that both it and its opponent could make. The state of the game could be evaluated just by looking at the board.</p><p>人工智能的第一个重大征服是国际象棋。该游戏可能组合的数量令人眼花，乱，但由于它是由一组清晰的规则构成的，因此它相对易于处理。一个算法总是可以完全掌握游戏的状态，并且知道它及其对手可以做出的每一个可能的动作。只需看一下棋盘就可以评估游戏的状态。</p><p> But many other games aren&#39;t that simple. If you take something like  Pac-Man, then figuring out the ideal move would involve considering the shape of the maze, the location of the ghosts, the location of any additional areas to clear, the availability of power-ups, etc., and the best plan can end up in disaster if Blinky or Clyde makes an unexpected move. We&#39;ve developed AIs that can tackle these games, too, but they have had to take a very different approach to the ones that conquered chess and Go.</p><p> 但是许多其他游戏并非如此简单。如果您使用类似吃豆人的东西，那么找出理想的举动将涉及考虑迷宫的形状，幽灵的位置，需要清理的其他区域的位置，加电装置的可用性等；以及如果Blinky或Clyde出人意料地采取了行动，最好的计划可能会陷入灾难。我们也开发了可以处理这些游戏的AI，但与征服国际象棋和围棋的AI相比，它们必须采取截然不同的方法。</p><p> At least until now. Today, however, Google&#39;s DeepMind division published a paper describing the structure of an AI that can tackle both chess and Atari classics.</p><p> 至少到现在为止。然而，今天，谷歌的DeepMind部门发表了一篇论文，描述了可以同时处理国际象棋和Atari经典游戏的AI的结构。</p><p>  The algorithms that have worked on games like chess and Go do their planning using a tree-based approach, in which they simply look ahead to all the branches that stem from different actions in the present. This approach is computationally expensive, and the algorithms rely on knowing the rules of the game, which allows them to project the current game status forward into possible future game states.</p><p>  在象棋和围棋之类的游戏中使用的算法使用基于树的方法来进行计划，在该算法中，它们仅展望当前源自不同动作的所有分支。这种方法在计算上是昂贵的，并且算法依赖于了解游戏规则，这允许他们将当前游戏状态向前投影到可能的未来游戏状态。</p><p> Other games have required algorithms that don&#39;t really care about the state of the game. Instead, the algorithms simply evaluate what they &#34;see&#34;—typically, something like the position of pixels on a screen for an arcade game—and choose an action based on that. There&#39;s no internal model of the state of the game, and the training process largely involves figuring out what response is appropriate given that information. There have been some attempts to model a game state based on inputs like the pixel information, but they&#39;ve not done as well as the successful algorithms that just respond to what&#39;s on-screen.</p><p> 其他游戏需要的算法并不真正在意游戏的状态。取而代之的是，这些算法只是评估它们所看到的内容（通常类似于街机游戏在屏幕上的像素位置），然后根据该结果选择一个动作。尚无游戏状态的内部模型，并且训练过程主要涉及弄清楚给出该信息后哪种反应合适。已经进行了一些尝试，例如基于像素信息等输入来对游戏状态进行建模，但是它们并没有像仅对屏幕上的内容做出响应的成功算法那样出色。</p><p>    The new system, which DeepMind is calling MuZero, is based in part on DeepMind&#39;s work with the  AlphaZero AI, which taught itself to master rule-based games like chess and Go. But MuZero also adds a new twist that makes it substantially more flexible.</p><p>    DeepMind称为MuZero的新系统部分基于DeepMind与AlphaZero AI的合作，AlphaZero AI自学了掌握基于规则的游戏，如国际象棋和围棋。但是MuZero还增加了新的功能，使其更加灵活。</p><p> That twist is called &#34;model-based reinforcement learning.&#34; In a system that uses this approach, the software uses what it can see of a game to build an internal model of the game state. Critically, that state isn&#39;t prestructured based on any understanding of the game—the AI is able to have a lot of flexibility regarding what information is or is not included in it. The reinforcement learning part of things refers to the training process, which allows the AI to learn how to recognize when the model it&#39;s using is both accurate and contains the information it needs to make decisions.</p><p> 这种扭曲称为“基于模型的强化学习”。在使用这种方法的系统中，软件使用它可以看到的游戏内容来构建游戏状态的内部模型。至关重要的是，该状态不是基于对游戏的任何了解而预先构造的-AI能够在其中包含或不包含哪些信息方面具有很大的灵活性。事物的强化学习部分是指训练过程，它允许AI学习如何识别模型所使用的模型既准确又包含决策所需的信息。 </p><p>  The model it creates is used to make a number of predictions. These include the best possible move given the current state and the state of the game as a result of the move. Critically, the prediction it makes is based on its internal model of game states—not the actual visual representation of the game, such as the location of chess pieces. The prediction itself is made based on past experience, which is also subject to training.</p><p>它创建的模型用于做出许多预测。这些包括在给定当前状态的情况下的最佳可能移动，以及由于移动而导致的游戏状态。至关重要的是，它所做的预测是基于其游戏状态的内部模型，而不是游戏的实际视觉表示，例如棋子的位置。预测本身是根据过去的经验做出的，也需要接受培训。</p><p> Finally, the value of the move is evaluated using the algorithms predictions of any immediate rewards gained from that move (the point value of a piece taken in chess, for example) and the final state of the game, such as the win or lose outcome of chess. These can involve the same searches down trees of potential game states done by earlier chess algorithms, but in this case, the trees consist of the AI&#39;s own internal game models.</p><p> 最后，使用算法预测从该动作获得的任何立即奖励（例如，下棋时棋子的点值）和游戏的最终状态（例如胜负结果）来评估该动作的价值象棋。这些可以包括通过较早的国际象棋算法完成的对潜在游戏状态树的相同搜索，但是在这种情况下，树由AI自己的内部游戏模型组成。</p><p> If that&#39;s confusing, you can also think of it this way: MuZero runs three evaluations in parallel. One (the policy process) chooses the next move given the current model of the game state. A second predicts the new state that results, and any immediate rewards from the difference. And a third considers past experience to inform the policy decision. Each of these is the product of training, which focuses on minimizing the errors between these predictions and what actually happens in-game.</p><p> 如果感到困惑，您也可以这样考虑：MuZero并行运行三个评估。给定当前游戏状态模型，一个（策略过程）选择下一步。一秒钟可以预测所产生的新状态，以及由此产生的任何立即收益。三分之一的人考虑过去的经验来指导政策决策。这些都是训练的产物，训练的重点是最大程度地减少这些预测与游戏中实际发生的错误。</p><p>     Obviously, the folks at DeepMind would not have a paper in Nature if this didn&#39;t work. MuZero took just under a million games against its predecessor AlphaZero in order to reach a similar level of performance in chess or shogi. For Go, it surpassed AlphaZero after only a half-million games. In all three of those cases, MuZero can be considered far superior to any human player.</p><p>     显然，如果不起作用，DeepMind的人们不会在《自然》杂志上发表论文。 MuZero与前身AlphaZero进行了将近一百万场比赛，以达到国际象棋或将棋的类似水平。对于Go来说，仅在五十万场比赛之后就超过了AlphaZero。在这三种情况下，MuZero都可以认为比任何人类玩家都优越。</p><p> But MuZero also excelled at a panel of Atari games, something that had previously required a completely different AI approach. Compared to the previous best algorithm, which doesn&#39;t use an internal model at all, MuZero had a higher mean and median score in 42 out of the 57 games tested. So, while there are still some circumstances where it lags behind, it&#39;s now made model-based AI&#39;s competitive in these games, while maintaining its ability to handle rule-based games like chess and Go.</p><p> 但是MuZero在Atari游戏方面也表现出色，以前需要完全不同的AI方法。与之前完全不使用内部模型的最佳算法相比，MuZero在测试的57场比赛中有42场均值和中位数得分更高。因此，尽管在某些情况下它仍然落后，但它现在使基于模型的AI在这些游戏中具有竞争力，同时保持了其处理基于规则的游戏（如国际象棋和围棋）的能力。</p><p> Overall, this is an impressive achievement and an indication of how AIs are growing in sophistication. A few years back, training AIs at just one task, like recognizing a cat in photos, was an accomplishment. But now, we&#39;re able to train multiple aspects of an AI at the same time—here, the algorithm that created the model, the one that chose the move, and the one that predicted future rewards were all trained simultaneously.</p><p> 总体而言，这是令人印象深刻的成就，并表明了AI的成熟度。几年前，仅在一项任务上训练AI，就像在照片中认出一只猫一样，是一项成就。但是现在，我们能够同时训练AI的多个方面，在这里，创建模型的算法，选择动作的算法以及预测未来收益的算法都同时得到了训练。</p><p> Partly, that&#39;s the product of the availability of greater processing power, that makes playing millions of games of chess possible. But partly it&#39;s a recognition that this is what we need to do if an AI is ever going to be flexible enough to master multiple, distantly related tasks.</p><p> 在某种程度上，这是具有更高处理能力的产品，这使得玩几百万国际象棋成为可能。但是部分地，这是如果AI能够足够灵活地完成多个遥远相关的任务，这就是我们需要做的。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://arstechnica.com/science/2020/12/google-develops-an-ai-that-can-learn-both-chess-and-pac-man/">https://arstechnica.com/science/2020/12/google-develops-an-ai-that-can-learn-both-chess-and-pac-man/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/开发/">#开发</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/develops/">#develops</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/游戏/">#游戏</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>