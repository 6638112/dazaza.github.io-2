<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>梯度下降模型是内核机器 Gradient Descent Models Are Kernel Machines</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Gradient Descent Models Are Kernel Machines<br/>梯度下降模型是内核机器 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-09 20:29:36</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/9fa832d85040f46c36517cd3bf9f34c8.png"><img src="http://img2.diglog.com/img/2021/2/9fa832d85040f46c36517cd3bf9f34c8.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This paper shows that models which result from gradient descent training (e.g., deep neural nets) can be expressed as a weighted sum of similarity functions (kernels) which measure the similarity of a given instance to the examples used in training. The kernels are defined by the inner product of model gradients in the parameter space, integrated over the descent (learning) path.  Roughly speaking, two data points x and x&#39; are similar, i.e., have large kernel function K(x,x&#39;), if they have similar effects on the model parameters in the gradient descent. With respect to the learning algorithm, x and x&#39; have similar information content. The learned model y = f(x) matches x to similar data points x_i: the resulting value y is simply a weighted (linear) sum of kernel values K(x,x_i).</p><p>本文表明，由梯度下降训练（例如深层神经网络）产生的模型可以表示为相似度函数（内核）的加权总和，这些函数可度量给定实例与训练中所用示例的相似度。内核由参数空间中模型梯度的内积定义，并在下降（学习）路径上进行积分。粗略地说，两个数据点x和x＆＃39;相似，即如果它们对梯度下降中的模型参数具有相似的影响，则具有较大的核函数K（x，x＆＃39;）。关于学习算法，x和x＆＃39;。具有相似的信息内容。学习的模型y = f（x）将x与相似的数据点x_i匹配：结果值y只是内核值K（x，x_i）的加权（线性）和。</p><p>  This result makes it very clear that without regularity imposed by the ground truth mechanism which generates the actual data (e.g., some natural process), a neural net is unlikely to perform well on an example which deviates strongly (as defined by the kernel) from all training examples. Given the complexity (e.g., dimensionality) of the ground truth model, one can place bounds on the amount of data required for successful training.</p><p>  该结果非常清楚，如果没有由生成实际数据的地面真理机制（例如某个自然过程）强加规律性，则神经网络不太可能在与（由内核定义）强烈偏离的示例上表现良好所有培训示例。鉴于基本事实模型的复杂性（例如维数），可以对成功训练所需的数据量进行限制。</p><p>  This formulation locates the nonlinearity of deep learning models in the kernel function. The superposition of kernels is entirely linear as long as the loss function is additive over training data.       Deep learning’s successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep networkweights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.</p><p>  该公式将深度学习模型的非线性定位在内核函数中。只要损失函数与训练数据相加，核的叠加就完全是线性的。深度学习的成功通常归因于其自动发现数据新表示的能力，而不是像其他学习方法那样依赖手工功能。但是，我们表明，通过标准梯度下降算法学习的深度网络实际上在数学上近似于内核机器，这是一种简单地存储数据并直接通过相似性函数（内核）将其用于预测的学习方法。通过阐明深层网络权重实际上是训练示例的叠加，可以大大增强深层网络权重的可解释性。网络体系结构将目标功能的知识整合到内核中。这种更好的理解应该导致更好的学习算法。</p><p> From the paper: ... Here we show that every model learned by this method, regardless of architecture, is approximately equivalent to a kernel machine with a particular type of kernel. This kernel measures the similarity of the model at two data points in the neighborhood of the path taken by the model parameters during learning. Kernel machines store a subset of the training data points and match them to the query using the kernel. Deep network weights can thus be seen as a superposition of the training data points in the kernel’s feature space, enabling their efficient storage and matching. This contrasts with the standard view of deep learning as a method for discovering representations from data. ...</p><p> 从论文中：...在这里，我们显示了通过这种方法学习的每个模型，无论使用哪种体系结构，都大致等效于具有特定内核类型的内核计算机。该内核在学习过程中在模型参数所采用的路径附近的两个数据点处测量模型的相似性。内核机器存储训练数据点的子集，并使用内核将它们与查询匹配。因此，深层网络权重可以看作是内核特征空间中训练数据点的叠加，从而可以有效地存储和匹配它们。这与深度学习作为一种从数据中发现表示的方法的标准观点相反。 ...</p><p> ... the weights of a deep network have a straightforward interpretation as a superposition of the training examples in gradient space, where each example is represented by the corresponding gradient of the model. Fig. 2 illustrates this. One well-studied approach to interpreting the output of deep networks involves looking for training instances that are close to the query in Euclidean or some other simple space (Ribeiro et al., 2016). Path kernels tell us what the exact space for these comparisons should be, and how it relates to the model’s predictions. ...</p><p> ……深层网络的权重可以直接解释为训练样本在梯度空间中的叠加，其中每个样本都由模型的相应梯度表示。图2对此进行了说明。研究深度网络输出的一种经过深入研究的方法涉及在欧几里得或其他简单空间中寻找与查询接近的训练实例（Ribeiro et al。，2016）。路径核告诉我们进行这些比较的确切空间是多少，以及它与模型的预测如何相关。 ...</p><p>See also  this video which discusses the paper.</p><p>另请参见讨论该论文的视频。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://infoproc.blogspot.com/2021/02/gradient-descent-models-are-kernel.html">https://infoproc.blogspot.com/2021/02/gradient-descent-models-are-kernel.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/下降/">#下降</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/descent/">#descent</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/内核/">#内核</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>