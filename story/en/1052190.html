<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>零3卸载：缩放DL型号到千万参数，没有代码更改 Zero-3 Offload: Scale DL models to trillion parameters without code changes</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Zero-3 Offload: Scale DL models to trillion parameters without code changes<br/>零3卸载：缩放DL型号到千万参数，没有代码更改 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-13 23:46:40</div><div class="page_narrow text-break page_content"><p>Today we are announcing the release of ZeRO-3 Offload, a highly efficient and easy to use implementation of ZeRO Stage 3 and ZeRO Offload combined, geared towards our continued goal of democratizing AI by making efficient large-scale DL training available to everyone. The key benefits of ZeRO-3 Offload are:</p><p>今天，我们宣布释放零3卸载，​​高效易于使用的零级3和零卸载的实施，通过为每个人提供高效的大规模DL培训，实现了民主化AI的持续目标。零3卸载的主要优点是：</p><p> Unprecedented memory efficiency to run very large models on a limited number of GPU resources - e.g., fine-tune models with over 40B parameters on a single GPU and over 2 Trillion parameters on 512 GPUs!</p><p> 前所未有的内存效率在有限数量的GPU资源上运行非常大的模型 - 例如，在单个GPU上具有超过40B参数的微调模型，在512 GPU上超过2万亿参数！</p><p>  Extremely Easy to use:  Scale to over a trillion parameters without the need to combine multiple parallelism techniques in complicated ways.</p><p>  非常易于使用：缩小到千万的参数，无需以复杂的方式组合多个并行技术。</p><p>  For existing DeepSpeed users, turn on ZeRO-3 Offload with just a few flags in DeepSpeed Config file.</p><p>  对于现有的DeepSpeed用户，在DeepSpeed Confif文件中只需几个标志，打开零3卸载。</p><p>  High-performance per-GPU throughput and super-linear scalability across GPUs for distributed training.  With 1 Trillion parameters, ZeRO-3 Offload sustains 25 PetaFlops in compute performance on 512 NVIDIA V100 GPUs, achieving 49 TFlops/GPU.</p><p>  PPU的高性能Per-GPU吞吐量和超线性可扩展性进行分布式训练。借助1万亿参数，零3卸载在512 NVIDIA V100 GPU上维持25个PETAFLOPS，实现49个TFLOPS / GPU。</p><p>  The Zero Redundancy Optimizer (abbreviated ZeRO) is a family of memory optimization technologies for large-scale distributed deep learning. Unlike data parallelism (that is efficient but can only support a limited model size) or model parallelism (that can support larger model sizes but requires significant code refactoring while adding communication overhead that limits efficiency), ZeRO allows fitting larger models in memory without requiring code refactoring while remaining very efficient. ZeRO does so by eliminating the memory redundancy that is inherent in data parallelism while limiting the communication overhead to a minimum.ZeRO removes the memory redundancies across data-parallel processes by partitioning the three model states (optimizer states, gradients, and parameters) across data-parallel processes instead of replicating them. By doing this, it boosts memory efficiency compared to classic data-parallelism while retaining its computational granularity and communication efficiency.There are three stages in ZeRO corresponding to three model states, as shown in the Figure 1: the first stage (ZeRO-1) partitions only the optimizer states, the second stage (ZeRO-2) partitions both the optimizer states and the gradients and the final stage (ZeRO-3) partitions all three model states (for more details see the ZeRO  paper).</p><p>  零冗余优化器（缩写零）是用于大规模分布式深度学习的内存优化技术系列。与数据并行性不同（高效但只能支持有限的模型大小）或型号并行性（这可以支持更大的模型大小，而是在添加限制效率的通信开销时需要重大代码重构），零允许在不需要代码的情况下拟合更大的模型重构留下非常有效。零是通过消除数据并行性中固有的内存冗余，同时将通信开销限制为最终的通信.zero通过划分数据（优化器状态，​​渐变和参数）划分数据并行处理跨数据并行进程的内存冗余 - 平行流程而不是复制它们。通过这样做，它将内存效率与经典数据并行相比提高，同时保持其计算粒度和通信效率。零中的三个阶段对应于三个模型状态，如图1所示：第一级（零1）仅分区优化器状态，​​第二级（零2）分区优化状态和渐变和最终阶段（零3）分区所有三个模型状态（有关更多详细信息，请参阅零纸）。</p><p>  In addition to these three stages, ZeRO family of technology also consists of ZeRO-2 Offload. ZeRO-2 Offload is a heterogenous DL training technology that works in conjunction with ZeRO-2 to offload partitioned optimizer states and gradients to CPU memory. ZeRO-2 Offload offers the full memory advantage of ZeRO-2 even on a single GPU, while at the same time offering great scalability of ZeRO-2 on multi-GPU setup. DeepSpeed library has been offering ZeRO-2 Offload since Sept 2020. For details, please see below:</p><p>  除了这三个阶段，零技术还包括零2卸载。零2卸载是一个异构DL训练技术，与零2一起配合，以将分区优化器状态和渐变卸载到CPU内存。零2卸载即使在单个GPU上也提供过零2的全内存优势，同时在多GPU设置上提供零2的巨大可扩展性。深度图书馆自9月2020年9月以来一直提供零2卸载。有关详细信息，请参阅以下： </p><p>   With today’s release of ZeRO-3 Offload, we are adding support for partitioning and offloading parameters in addition to optimizer states and gradients partitioning already supported by ZeRO-2 Offload in DeepSpeed. With parameter partitioning ZeRO-3 Offload implements the full set of features in the three stages of ZeRO, that allows for a linear growth in model size with the number of GPUs. In addition, ZeRO-3 Offload can also optionally offload all these model states to CPU to further reduce GPU memory consumption, leveraging both CPU and GPU to maximize memory and compute efficiency of the entire system.</p><p>随着当今Zero-3卸载的释放，除了优化器状态和DeepSpeed中的零2卸载已经支持的优化器状态和渐变分区之外，我们还会增加对分区和卸载参数的支持。参数分区零3卸载在零的三个阶段中实现了全套特征，这允许使用GPU的数量模型大小的线性增长。此外，零3卸载还可以选择将所有这些模型状态卸载到CPU，以进一步降低GPU存储器消耗，利用CPU和GPU来最大化内存和整个系统的计算效率。</p><p> We believe ZeRO-3 Offload offers a massive leap for large model training, in three regards:</p><p> 我们认为零3卸载提供了大型模型培训的大规模飞跃，三项问候：</p><p>     Unlike ZeRO-2 and ZeRO-Offload where the parameters have to fit in the memory of a single GPU, ZeRO-3 Offload can partition the parameters across GPUs, and offload them to CPU, supporting model sizes that are much larger than the memory on a single GPU. Furthermore, ZeRO-3 Offload goes beyond the state-of-the-art hybrid 3D-parallelism (data, model and pipeline parallelism combined). While 3D Parallelism is limited by the aggregate GPU memory, ZeRO-3 Offload can exploit both GPU and CPU memory, the latter of which is much larger and cheaper compared to GPU memory. This allows ZeRO-3 Offload to train larger model sizes with the given GPU and CPU resources than any other currently available technology.</p><p>     与零2和零卸载不同，其中参数必须适合单个GPU的存储器，零3卸载可以将参数分区GPU，并将其卸载到CPU，支持模型大小比内存要大得多单个GPU。此外，零3卸载超出了最先进的混合3DParallid（数据，模型和管道并行组合）。虽然3D并行性受到GPU存储器的总体内存的限制，但零3卸载可以利用GPU和CPU内存，后者与GPU存储器相比，后者更大，更便宜。这允许零3卸载与给定的GPU和CPU资源培训更大的模型大小，而不是任何其他当前可用的技术。</p><p> Model Scale on Single GPU: ZeRO-3 Offload can train models with over 40B parameters efficiently on a single GPU (e.g., 32GB V100 GPU + 1.5TB CPU memory). This is 3x larger than what is possible with ZeRO-2 Offload, the current state-of-the art.</p><p> 单个GPU上的模型刻度：零3卸载可以在单个GPU（例如，32GB V100 GPU + 1.5TB CPU存储器）上有效地培训具有超过40B参数的模型。这比零2卸载，目前最先进的卸载大于可能的3倍。</p><p> Model Scale on Multi-GPUs: With ZeRO-3 Offload you can train a trillion and two trillion parameter models on NVIDIA 32GB V100 DGX-2 cluster with 256 GPUs and 512 GPUs, respectively. In contrast, the state-of-art 3D Parallelism requires 800 GPUs, and 1600 GPUs, respectively, to fit the same sized models. This represents a 3x reduction in GPUs required to fit models with over a trillion parameters.</p><p> Multi-GPU上的模型刻度：使用零3卸载，​​您可以分别使用256GPus和512个GPU的NVIDIA 32GB V100 DGX-2集群培训万亿和两个亿万的参数模型。相比之下，最先进的3DParparationItm需要800GPus，分别为1600GPus，以适合相同的大小模型。这表示使用超过万亿参数的模型所需的GPU减少3倍。</p><p>  From a system perspective, training models with hundreds of billions and trillions of parameters is extremely challenging. Data parallelism cannot scale the model size much further beyond a billion parameters, model parallelism (with tensor slicing) cannot be used to scale model size efficiently beyond a single node boundary due to massive communication overheads, and pipeline parallelism cannot scale beyond the number of layers available in a model, which limits both the model size and the number of GPUs that it can scale to.</p><p>  从系统的角度来看，培训模型具有数百亿亿万节的参数是极具挑战性的。数据并行性无法扩展模型大小，超过十亿个参数，模型并行性（带有张量切片）不能用于缩放模型大小，由于大规模的通信开销导致的单个节点边界之外，而流水线并行不能超过层数。可在模型中提供，限制模型大小和可以缩放的GPU的数量。</p><p> The only existing parallel technology available that can scale to over a trillion parameters on massively parallel GPU clusters is the  3D parallelism that combines data, model and pipeline parallelism in complex ways. While such a system can be very efficient, it requires major model code refactoring from data scientists to split the model into load balanced pipeline stages. This also makes 3D parallelism inflexible in the type of models that it can support, since models with complex dependency graphs cannot be easily converted into a load balanced pipeline.</p><p> 唯一可以在大量平行GPU集群上缩放到万亿参数的唯一现有的并行技术是将数据，模型和管道并行性以复杂的方式组合的3DParessib。虽然这种系统可以非常有效，但它需要从数据科学家重构的主要模型代码重构，将模型分成负载平衡管道级。这也使3D并行性在它可以支持的模型类型中不灵活，因为具有复杂依赖图形的模型不能轻易转换为负载平衡管道。 </p><p>  i) With ground-breaking memory efficiency, ZeRO-3 and ZeRO-3 Offload are the only DL parallel technology that can efficiently scale to over a trillion parameters by itself, without requiring a hybrid parallelism strategy, greatly simplifying the system stack for DL training.</p><p>i）采用接地内存效率，零3和零3卸载是唯一可以自行衡量到万亿参数的DL并行技术，而无需混合并行策略，大大简化了系统堆栈进行DL培训。</p><p> ii) ZeRO-3 Offload requires virtually no model refactoring from model scientists, liberating data scientists to scale up complex models to hundreds of billions to trillions of parameters.</p><p> ii）Zero-3卸载需要模型科学家的实际上没有模型重构，从模型科学家解放数据科学家将复杂的模型扩展到数百亿亿千千万个参数。</p><p>  High-performance per-GPU throughput on multiple nodes: ZeRO-3 Offload offers excellent training efficiency for multi-billion and trillion parameter models on multiple nodes. It achieves a sustained throughput of up to 50 Tflops per GPU running on 32 DGX2 nodes comprising 512 NVIDIA V100 GPUs (see Figure 2). In comparison, the standard data parallel training with PyTorch can only achieve 30 TFlops per GPU for a 1.2B parameter model, the largest model that can be trained using data parallelism alone.</p><p>  多个节点上的高性能PER-GPU吞吐量：零3卸载为多亿和多亿兆比参数模型提供出色的培训效率。它在包含512个NVIDIA V100 GPU的32个DGX2节点上运行的每个GPU的持续吞吐量可达50 TFLOPS（参见图2）。相比之下，使用Pytorch的标准数据并行训练只能为1.2B参数模型实现30个TFLOPS，最大的模型可以单独使用数据并行性训练。</p><p>  ZeRO-3 Offload obtains high efficiency despite the 50% communication overhead of ZeRO Stage 3 compared to standard data parallel training for a fixed batch size. This is made possible through a communication overlap centric design and implementation, which allows ZeRO-3 Offload to hide nearly all of the communication volume with computation, while taking advantage of a larger batch size for improved efficiency resulting from better GPU memory efficiency.</p><p>  与标准数据并行训练相比，零3卸载获得高效率，尽管为固定批次大小的标准数据并行培训，零级3的通信开销。这是通过通信重叠的设计和实现可以实现的，这允许零3卸载隐藏几乎所有的通信体积，同时利用较大的批量尺寸，以提高由更好的GPU存储器效率导致的提高效率。</p><p> Efficient multi-billion parameter model training on a single GPU: ZeRO-3 Offload further democratizes AI by enabling efficient training of multi-billion parameter models on a single GPU. For single GPU training, ZeRO-3 Offload provides benefits over ZeRO-2 Offload along two dimensions. First, ZeRO-3 Offload increases the size of models trainable on a single V100 from 13B to 40B. Second, for ZeRO-3 Offload provides speedups (e.g., 2.3X for 13B) compared to ZeRO-2 Offload for model sizes trainable by both solutions. These results are summarized in Figure 3.</p><p> 在单个GPU上有效的多亿参数模型培训：通过在单个GPU上实现高亿参数模型的高近亿参数模型进行高效培训，从零3卸载进一步民主化AI。对于单个GPU培训，零3卸载沿两个维度提供超过零2卸载的好处。首先，零3卸载将模型的大小增加在13b至40b的单个V100上。其次，与零2卸载相比，零3卸载提供加速（例如，2.3B，13B），用于通过两种解决方案可训练的型号尺寸。这些结果总结在图3中。</p><p>  Super-Linear scalability across GPUs: Additionally, ZeRO-3 Offload also preserves the super-linear scalability characteristics that we have demonstrated with all our previous ZeRO technologies (ZeRO Stage 1, ZeRO Stage 2 and ZeRO Offload). ZeRO-3 Offload can exploit the aggregate PCI-E bandwidth between GPU and CPU across all the GPUs in multi-GPU training configuration, and at the same time, it can also exploit the aggregate CPU compute across all the nodes. As a result, the CPU-GPU-CPU communication time as well as the optimizer update time decreases linearly with number of GPUs and nodes, respectively, allowing ZeRO-3 Offload to exhibit super-linear scaling (see Figure 4).</p><p>  跨GPU的超线性可伸缩性：此外，零3卸载还保留了我们通过先前的零技术（零级1，零级2和零卸载）所展示的超线性可扩展性特性。零3卸载可以在多GPU训练配置中的所有GPU上利用GPU和CPU之间的聚合PCI-e带宽，同时，它还可以利用所有节点的聚合CPU计算。结果，CPU-GPU-CPU通信时间以及优化器更新时间分别与GPU和节点的数量线性降低，允许零3卸载表现出超线性缩放（参见图4）。</p><p>   As with many other existing DeepSpeed features, once the user model has been converted to use DeepSpeed, enabling ZeRO-3 Offload is as easy as turning on a couple of flags in DeepSpeed Config file. Supporting advanced features like weight sharing, or enabling extremely large models that requires to be partitioned across GPUs/nodes to fit in GPU/CPU memory, can be done with just a couple of additional lines of code change using the ZeRO-3 Offload API.</p><p>   与许多其他现有的DeepSpeed功能一样，一旦用户模型已转换为使用DeepSpeed，就可以在DeepSpeed配置文件中打开零3卸载等于开启几个标志。支持具有权重共享的高级功能，或实现需要跨GPU /节点划分的极大型号以适应GPU / CPU存储器，只需使用零3卸载API即可使用几行额外的代码更改。 </p><p> If you are already a DeepSpeed user, you can find our detailed tutorial on ZeRO-3 Offload below. If you are new to DeepSpeed, we recommend that you start at the getting started page before trying out our ZeRO-3 Offload Tutorial.</p><p>如果您已经是DeepSpeed用户，您可以在下面的零3卸载中找到详细的教程。 如果您是DeepSpeed的新手，我们建议您在尝试零3卸载教程之前从“入门”页面开始。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.deepspeed.ai/news/2021/03/07/zero3-offload.html">https://www.deepspeed.ai/news/2021/03/07/zero3-offload.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/代码/">#代码</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/缩放/">#缩放</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/scale/">#scale</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/gpu/">#gpu</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012 - 2021 diglog.com </div></div></body></html>