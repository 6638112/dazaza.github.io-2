<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Mozilla风向下深入开发，宣布拨款计划 Mozilla winds down DeepSpeech development, announces grant program</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Mozilla winds down DeepSpeech development, announces grant program<br/>Mozilla风向下深入开发，宣布拨款计划 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-13 22:44:05</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/ecc0a4b0d1ad8b9f7aeaeb33347065fd.png"><img src="http://img2.diglog.com/img/2021/4/ecc0a4b0d1ad8b9f7aeaeb33347065fd.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Join GamesBeat Summit 2021 this April 28-29.  Register for a free or VIP pass today.</p><p>加入GamesBeat Summit 2021 4月28日至29日。今天注册免费或VIP通过。</p><p> In 2017, Mozilla launched  DeepSpeech, an initiative incubated within the machine learning team at Mozilla Research focused on open sourcing an automatic speech recognition model. Over the next four years, the DeepSpeech team released newer versions of the model capable of transcribing lectures, phone conversations, television programs, radio shows, and other live streams with “human accuracy.” But in the coming months, Mozilla plans to cease development and maintenance of DeepSpeech as the company transitions into an advisory role, which will include the launch of a grant program to fund a number of initiatives demonstrating applications for DeepSpeech.</p><p> 2017年，Mozilla推出了DeepSpeech，这是一项潜入Mozilla Research的机器学习团队，专注于开放采购自动语音识别模型。在接下来的四年中，DeepSeech团队发布了更新的模型版本，能够转录讲座，电话交谈，电视节目，广播节目和“人类准确性”的其他现场流。但在未来几个月，Mozilla计划停止开发和维护DeepSpeech，因为公司转型为咨询作用，这将包括推出拨款计划，以资助展示深度申请的举措。</p><p> DeepSpeech isn’t the only open source project of its kind, but it’s among the most mature. Modeled after research papers published by Baidu, the model is an end-to-end trainable, character-level architecture that can transcribe audio in a range of languages. One of Mozilla’s major aims was to achieve a transcription word error rate of lower than 10%, and the newest versions of the pretrained English-language model achieve that aim, averaging around a 7.5% word error rate.</p><p> DeepSpeech不是唯一的开源项目，但它是最成熟的。百度发布的研究论文建模，该模型是最终的培训，性格级架构，可以在一系列语言中传输音频。 Mozilla的主要目标之一是实现转录词错误率低于10％，最新版本的预磨料英语模型实现了这一目标，平均左右的字错误率为7.5％。</p><p>  It’s Mozilla’s belief that DeepSpeech has reached the point where the next step is to work on building applications. To this end, the company plans to transition the project to “people and organizations” interested in furthering “use-case-based explorations.” Mozilla says it’s streamlined the continuous integration processes for getting DeepSpeech up and running with minimal dependencies. And as the company cleans up the documentation and prepares to stop Mozilla staff upkeep of the codebase, Mozilla says it’ll publish a toolkit to help people, researchers, companies, and any other interested parties use DeepSpeech to build voice-based solutions.</p><p>  Mozilla认为，DeepSpeech已经达到了下一步是在建设应用程序上工作的观点。为此，公司计划将该项目转变为有兴趣进一步“基于用例探索”的“人员和组织”。 Mozilla表示，它简化了持续的集成过程，以便使用最小依赖性获得深度跳跃和运行。随着公司清理文件并准备停止Mozilla员工保养的代码库，Mozilla表示它将发布工具包来帮助人们，研究人员，公司和任何其他有关方面使用DeepSpeech来构建基于语音的解决方案。</p><p>  Mozilla’s work on DeepSpeech began in late 2017, with the goal of developing a model that gets audio features — speech — as input and outputs characters directly. The team hoped to design a system that could be trained using Google’s TensorFlow framework via supervised learning, where the model learns to infer patterns from datasets of labeled speech.</p><p>  Mozilla在DeepSpeech的工作开始于2017年底，目的是开发一个型号的模型 - 语音 - 直接输入和输出字符。该团队希望设计一个系统可以通过监督学习使用Google的Tensorflow框架培训的系统，其中模型学会从标记语音数据集中推断模式。</p><p> The latest DeepSpeech model contains tens of millions parameters, or the parts of the model that are learned from historical training data. The Mozilla Research team started training it with a single computer running four Titan X Pascal GPUs but eventually migrated it to two servers with 8 Titan XPs each. In the project’s early days, training a high-performing model took about a week.</p><p> 最新的深度模型包含数百万参数，或从历史培训数据中学习的模型的部分。 Mozilla Research团队开始用一台运行四个Titan x Pascal GPU的单个计算机训练它，但最终将其迁移到两个服务器，每个服务器都有8个泰坦XPS。在该项目的早期，培训高性能模型大约需要一周。</p><p> In the years that followed, Mozilla worked to shrink the DeepSpeech model while boosting its performance and remaining below the 10% error rate target. The English-language model shrank from 188MB to 47MB and memory consumption dropped by 22 times. In  December 2019, the team managed to get DeepSpeech running “faster than real time” on a single core of a Raspberry Pi 4.</p><p> 在随后的几年中，Mozilla致力于缩小深度模型，同时提高其性能并剩下低于10％的错误率目标。英语模型从188MB缩小到47MB，内存消耗降至22次。 2019年12月，该团队在覆盆子PI 4的单个核心上设法获得“比实时更快的时间更快”。 </p><p>  Mozilla initially trained DeepSpeech using freely available datasets like  TED-LIUM and  LibriSpeech as well as paid corpora like  Fisher and  Switchboard, but these proved to be insufficient. So the team reached out to public TV and radio stations, language study departments in universities, and others they thought might have labeled speech data to share. Through this effort, they were able to more than double the amount of training data for the English-language DeepSpeech model.</p><p>Mozilla最初使用自由的数据集培训了DeepSeech，如Ted-lium和Librispeech以及有费勒和交换机等付费的语料库，但这些证明是不够的。因此，该团队与公共电视和广播电台，大学的语言学习部门以及他们认为的其他人可能会标记为言语数据分享。通过这项努力，他们能够为英语深度模型的培训数据量增加两倍。</p><p> Inspired by these data collection efforts, the Mozilla Research team collaborated with Mozilla’s Open Innovation team to launch the  Common Voice project, which seeks to collect and validate speech contributions from volunteers. Common Voice consists not only of voice snippets but of voluntarily contributed metadata useful for training speech engines, like speakers’ ages, sex, and accents. It’s also grown to include dataset target segments for specific purposes and use cases, like the digits “zero” through “nine” and the words “yes,” ” no,” ” hey,” and ” Firefox.”</p><p> 灵感来自这些数据收集努力，Mozilla研究团队与Mozilla的开放式创新团队合作，推出了普通的语音项目，旨在收集和验证志愿者的言语贡献。常见的声音不仅包括语音片段，而且对于培训语音引擎的自愿贡献元数据，如发言者年龄，性别和口音。它还生长为包括特定目的和用例的数据集目标段，如通过“九”的数字“零”和“是”，“是”，“否”，“嘿”和“Firefox”。</p><p> Today, Common Voice is one of the largest multi-language public domain voice corpora in the world, with more than 9,000 hours of voice data in 60 different languages including widely spoken languages and less-used ones,  like Welsh and Kinyarwanda. Over 164,000 people have contributed to the dataset to date.</p><p> 今天，常见的声音是世界上最大的多语言公共领域语音集团之一，具有超过90多小时的语音数据，其中60种不同的语言，包括广泛的语言和较少使用的语言，如威尔士和克里瓦达。迄今为止，超过164,000人为数据集做出了贡献。</p><p> To support the project’s growth, Nvidia today announced that it would invest $1.5 million in Common Voice to engage more communities and volunteers and support the hiring of new staff. Common Voice will now operate under the umbrella of the Mozilla Foundation as part of its  initiatives focused on making AI more trustworthy.</p><p> 为了支持项目的增长，NVIDIA今天宣布它将投入150万美元的共同声音，以实现更多社区和志愿者，并支持雇用新员工。常见的声音现在将在Mozilla基金会的伞下运作，作为其举措的一部分，专注于使AI更值得信赖。</p><p>  As it winds down the development of DeepSpeech, Mozilla says its forthcoming grant program will prioritize projects that contribute to the core technology while also showcasing its potential to “empower and enrich” areas that may not otherwise have a viable route toward speech-based interaction. More details will be announced in May, when Mozilla publishes a playbook to guide people on how to use DeepSpeech’s codebase as a starting point for voice-powered applications.</p><p>  随着它的风化，Mozilla表示，它即将举行的赠款计划将优先考虑为核心技术做出有助于核心技术的项目，同时也展示其可能没有其他可行途径的“授权和丰富”的区域，以其他方式展示了基于语音的互动。 Mozilla发布了一个Playbook以指导人们如何使用DeepSpeech的CodeBase作为语音供电应用的起点，将在5月份公布更多详细信息。</p><p> “We’re seeing mature open source speech engines emerge. However, there is still an important gap in the ecosystem: speech engines — open and closed — don’t work for vast swaths of the world’s languages, accents, and speech patterns,” Mark Surman, executive director of the Mozilla Foundation, told VentureBeat via email. “For billions of internet users, voice-enabled technologies simply aren’t usable. Mozilla has decided to focus its efforts this side of the equation, making voice technology inclusive and accessible. That means investing in voice data sets rather than our own speech engine. We’re doubling down on Common Voice, an open source dataset that focuses on languages and accents not currently represented in the voice tech ecosystem. Common Voice data can be used to feed [open speech] frameworks … and in turn to allow more people in more places to access voice technology. We’re [also] working closely with Nvidia to match up these two sides of the inclusive voice tech equation.”</p><p> “我们看到了成熟的开源语音引擎出现了。然而，生态系统仍然存在一个重要的差距：语音发动机 - 开放和封闭 - 不适用于世界语言，口音和言语模式的巨大条件，“Mozilla Foundation的执行董事Mark Surman，告诉Venturebeat通过电子邮件。 “对于数十亿个互联网用户来说，启用语音技术根本无法使用。 Mozilla决定将其努力集中在方程的这一侧，使语音技术包容和可访问。这意味着投资语音数据集而不是我们自己的语音引擎。我们正在向常见的声音递增，一个开源数据集专注于语言和目前在语音科技生态系统中所代表的口音。常见的语音数据可用于喂养[打开语音]框架......然后又允许更多人在更多的地方访问语音技术。我们[还]与NVIDIA密切合作，以匹配包容性语音技术方程的这两侧。“</p><p> VentureBeat&#39;s mission is to be a digital town square for technical decision-makers to gain knowledge about transformative technology and transact.Our site delivers essential information on data technologies and strategies to guide you as you lead your organizations. We invite you to become a member of our community, to access: gated thought-leader content and discounted access to our prized events, such as   Transform 2021: Learn More</p><p> VidtureBeat＆＃39; S使命是成为技术决策者的数字城市广场，以获得有关转型技术和Transact的知识。您网站提供有关数据技术和策略的基本信息，以指导您的领导您的组织。我们邀请您成为社区的成员，访问：门控思想领导者内容和对我们奖化事件的折扣访问，如转换2021：了解更多 </p><p> Become a member</p><p>成为会员 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://venturebeat.com/2021/04/12/mozilla-winds-down-deepspeech-development-announces-grant-program/">https://venturebeat.com/2021/04/12/mozilla-winds-down-deepspeech-development-announces-grant-program/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/开发/">#开发</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/winds/">#winds</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/语音/">#语音</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>