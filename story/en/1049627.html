<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>解释：香农极限（2010） Explained: The Shannon Limit (2010)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Explained: The Shannon Limit (2010)<br/>解释：香农极限（2010） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-27 01:19:51</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/8e2974561f0acd5bd6086781f6c3c191.jpg"><img src="http://img2.diglog.com/img/2021/2/8e2974561f0acd5bd6086781f6c3c191.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>It’s the early 1980s, and you’re an equipment manufacturer for the fledgling personal-computer market. For years, modems that send data over the telephone lines have been stuck at a maximum rate of 9.6 kilobits per second: if you try to increase the rate, an intolerable number of errors creeps into the data.  Then a group of engineers demonstrates that newly devised error-correcting codes can boost a modem’s transmission rate by 25 percent. You scent a business opportunity. Are there codes that can drive the data rate even higher? If so, how much higher? And what are those codes?  In fact, by the early 1980s, the answers to the first two questions were more than 30 years old. They’d been supplied in 1948 by Claude Shannon SM ’37, PhD ’40 in a groundbreaking paper that essentially created the discipline of information theory. “People who know Shannon’s work throughout science think it’s just one of the most brilliant things they’ve ever seen,” says David Forney, an adjunct professor in MIT’s Laboratory for Information and Decision Systems.   Shannon, who taught at MIT from 1956 until his retirement in 1978, showed that any communications channel — a telephone line, a radio band, a fiber-optic cable — could be characterized by two factors: bandwidth and noise. Bandwidth is the range of electronic, optical or electromagnetic frequencies that can be used to transmit a signal; noise is anything that can disturb that signal.  Given a channel with particular bandwidth and noise characteristics, Shannon showed how to calculate the maximum rate at which data can be sent over it with zero error. He called that rate the channel capacity, but today, it’s just as often called the Shannon limit.   In a noisy channel, the only way to approach zero error is to add some redundancy to a transmission. For instance, if you were trying to transmit a message with only three bits, like 001, you could send it three times: 001001001. If an error crept in, and the receiver received 001011001 instead, she could be reasonably sure that the correct string was 001.   Any such method of adding extra information to a message so that errors can be corrected is referred to as an error-correcting code. The noisier the channel, the more information you need to add to compensate for errors. As codes get longer, however, the transmission rate goes down: you need more bits to send the same fundamental message. So the ideal code would minimize the number of extra bits while maximizing the chance of correcting error.  By that standard, sending a message three times is actually a terrible code. It cuts the data transmission rate by two-thirds, since it requires three times as many bits per message, but it’s still very vulnerable to error: two errors in the right places would make the original message unrecoverable.  But Shannon knew that better error-correcting codes were possible. In fact, he was able to prove that for any communications channel, there must be an error-correcting code that enables transmissions to approach the Shannon limit.   His proof, however, didn’t explain how to construct such a code. Instead, it relied on probabilities. Say you want to send a single four-bit message over a noisy channel. There are 16 possible four-bit messages. Shannon’s proof would assign each of them its own randomly selected code — basically, its own serial number.  Consider the case in which the channel is noisy enough that a four-bit message requires an eight-bit code. The receiver, like the sender, would have a codebook that correlates the 16 possible four-bit messages with 16 eight-bit codes. Since there are 256 possible sequences of eight bits, there are at least 240 that don’t appear in the codebook. If the receiver receives one of those 240 sequences, she knows that an error has crept into the data. But of the 16 permitted codes, there’s likely to be only one that best fits the received sequence — that differs, say, by only a digit.  Shannon showed that, statistically, if you consider all possible assignments of random codes to messages, there must be at least one that approaches the Shannon limit. The longer the code, the closer you can get: eight-bit codes for four-bit messages wouldn’t actually get you very close, but two-thousand-bit codes for thousand-bit messages could.  Of course, the coding scheme Shannon described is totally impractical: a codebook with a separate, randomly assigned code for every possible thousand-bit message wouldn’t begin to fit on all the hard drives of all the computers in the world. But Shannon’s proof held out the tantalizing possibility that, since capacity-approaching codes must exist, there might be a more efficient way to find them.  The quest for such a code lasted until the 1990s. But that’s only because the best-performing code that we now know of, which was invented at MIT, was ignored for more than 30 years. That, however, is a story for the  next installment of Explained.</p><p>这是1980年代初期，您是新兴的个人计算机市场的设备制造商。多年来，通过电话线发送数据的调制解调器一直以每秒9.6 kb的最大速率被阻塞：如果您尝试提高速率，则数据中将出现无法容忍的错误。然后，一组工程师演示了新设计的纠错码可以将调制解调器的传输速率提高25％。您闻到一个商机。是否有代码可以使数据速率更高？如果是这样，要高多少？这些代码是什么？实际上，到1980年代初，前两个问题的答案已有30多年的历史了。 1948年，克劳德·香农（Claude Shannon）SM ’37，博士’40在一份开创性的论文中提供了这些信息，该论文实质上创建了信息论的学科。麻省理工学院信息与决策系统实验室的副教授戴维•福尼（David Forney）说：“了解香农在整个科学领域工作的人们认为，这只是他们所见过的最辉煌的事物之一。”香农（Shannon）从1956年开始在麻省理工学院任教，直到1978年退休。他的研究表明，任何通信渠道（电话线，无线电频段，光缆）都可以通过两个因素来表征：带宽和噪声。带宽是可用于传输信号的电子，光学或电磁频率的范围；噪声是任何会干扰该信号的东西。给定一个具有特定带宽和噪声特性的信道，Shannon展示了如何计算可以零误差在其上发送数据的最大速率。他称该速率为频道容量，但今天，它通常被称为香农极限。在嘈杂的信道中，接近零错误的唯一方法是为传输增加一些冗余。例如，如果您尝试仅发送三个位（例如001）的消息，则可以发送三遍：001001001。如果发生了错误，而接收方却收到001011001，则她可以合理地确定正确的字符串是001。将此类其他信息添加到消息中以便可以纠正错误的方法称为纠错码。通道越嘈杂，您需要添加更多信息以补偿错误。但是，随着代码变得更长，传输速率会下降：您需要更多的比特来发送相同的基本消息。因此，理想的代码将最大程度地减少纠错的机会，同时减少额外位数。按照该标准，发送消息三遍实际上是一个糟糕的代码。由于每条消息需要三倍的比特率，因此可以将数据传输速率降低三分之二，但是它仍然非常容易出错：正确位置的两次错误会使原始消息无法恢复。但是Shannon知道可以使用更好的纠错代码。实际上，他能够证明对于任何通信通道，都必须有一个纠错码，使传输能够接近香农极限。但是，他的证明并没有说明如何构造这样的代码。相反，它依赖于概率。假设您要在嘈杂的频道上发送一条四位消息。有16种可能的四位消息。香农的证明将为他们每个人分配自己的随机选择的代码-基本上是自己的序列号。考虑这样一种情况，其中信道的噪声足以使四位消息需要八位代码。接收方像发送方一样，将具有将16条可能的四位消息与16条八位代码相关联的密码本。由于共有256个8位的可能序列，因此至少有240个未出现在密码本中。如果接收器接收到这240个序列之一，则她知道错误已蔓延到数据中。但是在16个允许的代码中，很可能只有一个最适合接收的序列，例如相差仅一个数字。香农表示，从统计学上讲，如果您考虑对邮件分配所有可能的随机码，则必须至少有一种接近香农极限。代码越长，您越接近：四位消息的八位代码实际上并不能使您非常接近，而千位消息的两千位代码则可以。当然，Shannon所描述的编码方案是完全不切实际的：对于每个可能的千位消息，具有单独的，随机分配的代码的代码簿将无法适应世界上所有计算机的所有硬盘驱动器。但是Shannon的证明提供了一种诱人的可能性，即由于必须存在容量接近代码，因此可能会有更有效的方法来找到它们。对这种代码的追求一直持续到1990年代。但这仅仅是因为我们现在所知的性能最佳的代码（已由MIT发明）被忽略了30多年。但是，这是下一期《解释的故事》的故事。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://news.mit.edu/2010/explained-shannon-0115">https://news.mit.edu/2010/explained-shannon-0115</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/解释/">#解释</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/shannon/">#shannon</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/代码/">#代码</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>