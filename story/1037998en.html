<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>InstaHide令人失望地获得贝尔实验室奖，第二名 InstaHide Disappointingly Wins Bell Labs Prize, 2nd Place</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">InstaHide Disappointingly Wins Bell Labs Prize, 2nd Place<br/>InstaHide令人失望地获得贝尔实验室奖，第二名 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-06 11:42:17</div><div class="page_narrow text-break page_content"><p>[What follows are my thoughts on some recent research in machine learning privacy. These are my thoughts, and do not represent those of others.]</p><p>[以下是我对机器学习隐私的一些最新研究的看法。这些是我的想法，并不代表其他人的想法。]</p><p>  InstaHide (a recent method that claims to give a way to train neural networks while preserving training data privacy) was just awarded the 2nd place  Bell Labs Prize (an award for “finding solutions to some of the greatest challenges facing the information and telecommunications industry.”). This is a grave error.</p><p>  InstaHide（一种声称可以提供一种训练神经网络的方法，同时又保留了训练数据的隐私性的最新方法）刚刚获得了第二名的贝尔实验室奖（该奖项是“为解决信息和电信行业面临的一些最大挑战提供解决方案的奖项”。 ”）。这是一个严重的错误。</p><p>  Bell Labs brought to the world the very foundations of information theory, the transistor, the C programming language, and the UNIX operating system. The world today would certainly not be what it is without Bell Labs. So when InstaHide was awarded the 2nd place Bell Labs Prize earlier this week, I was deeply disappointed and saddened.</p><p>  贝尔实验室将信息理论，晶体管，C编程语言和UNIX操作系统的基础推向世界。没有贝尔实验室，今天的世界肯定不会是现在。因此，当本周早些时候InstaHide被授予贝尔实验室奖第二名时，我感到非常失望和悲伤。</p><p>  In case you&#39;re not deeply embedded in the machine learning privacy research community, InstaHide is a recent proposal to train a neural network while preserving training data privacy. It (ostensibly) allows someone to train a machine learning model on a bunch of sensitive training data, and then publish that model, without fear that the model leaks anything about the training data itself.</p><p>  如果您未深入机器学习隐私研究社区，InstaHide是最近提出的在保留训练数据隐私的同时训练神经网络的建议。它（表面上）允许某人在大量敏感的训练数据上训练机器学习模型，然后发布该模型，而不必担心该模型会泄漏有关训练数据本身的任何信息。</p><p>    Unfortunately, it turns out that InstaHide offers no privacy. It is not private for any reasonable definition of privacy, and given the output of InstaHide it is possible to completely recover the inputs that went in to it. We showed this in  a recent paper (and if you&#39;re reading this within the next few days, we&#39;ll be giving a talk on this paper at the PPML workshop at NeurIPS).</p><p>    不幸的是，事实证明InstaHide不提供任何隐私。对于隐私的任何合理定义，它不是私有的，并且鉴于InstaHide的输出，可以完全恢复输入到其中的输入。我们在最近的一篇论文中对此进行了展示（如果您在接下来的几天内阅读此书，我们将在NeurIPS的PPML研讨会上对该论文进行演讲）。</p><p>  It is a grave error that InstaHide was awarded this prize because of how fundamentally misguided InstaHide is---both the idea itself and the methodology of the paper. Drawn to the right is what we&#39;re able to do: given a set of encoded images that try to preserve some notion of privacy, we recover extremely high fideltiy reconstructions.</p><p>  InstaHide之所以被授予这一奖项是一个严重的错误，因为InstaHide的想法和论文的方法论从根本上被误导了。向右绘制是我们能够执行的操作：给定一组编码图像，这些图像试图保留一些隐私概念，我们将恢复极高的保真度。</p><p>  I was planning on leaving things as they were after we wrote  our attack paper. But after watching the award ceremony, and the response to it, I felt compelled to respond.   [a]   And no, I promise I&#39;m not bitter because this paper was awarded a prize even though it&#39;s broken and offers no security. I&#39;m used to that. In 2013 I  broke the  kBouncer defense, which had received the  Microsoft Blue Hat prize of $200,000. I didn&#39;t complain then because at least that attack required some technical novelty and interesting research, and because Microsoft didn&#39;t award the prize  after the attack was published.</p><p>  在编写攻击报告后，我正打算保留原样。但是在观看了颁奖典礼以及对颁奖典礼的回应之后，我感到不得不做出回应。 [a]不，我保证我不会痛苦，因为即使这篇论文被破坏并且没有任何安全性，它也被授予了奖项。我已经习惯了。 2013年，我打破了kBouncer防御程序，该防御程序获得了Microsoft Blue Hat的20万美元奖金。那时我没有抱怨，因为至少该攻击需要一些技术上的新颖性和有趣的研究，并且因为微软在攻击发布后没有颁发奖金。 </p><p>  The reason this award really gets to me is that InstaHide really is a culmination of all the weaknesses that machine learning papers tend to have, ranging from focusing on having a good story over a good technique, to the fact that the claims aren&#39;t refutable.</p><p>该奖项之所以真正给我，是因为InstaHide确实是机器学习论文往往具有的所有弱点的最终体现，从专注于讲述好故事而不是掌握好的技术，到主张不是事实。 t可辩驳的。</p><p>      While this paper was written by several authors, I place no blame on the first author of this paper, a early-career PhD student who executed on this idea with care and precision. This paper is exceptionally well written, and the experiments are performed carefully and rigorously. Having worked with the InstaHide implementation over the last month, the code is correct, easy to follow, and something that other researchers should strive for when doing their own code releases. The figures in this paper are clear and provide justifications for the claims. Everything a first author should be expected to do has been done flawlessly.</p><p>      虽然这篇论文是由多位作者撰写的，但我不应该将其归咎于本文的第一作者，即一位早期职业的博士生，他认真而精确地执行了这一想法。这篇论文写得非常好，实验是经过认真而严格的进行的。在上个月使用InstaHide实现工作之后，该代码是正确的，易于遵循的，并且其他研究人员在自己编写代码时也应努力做到这一点。本文中的数字清楚，并为权利要求提供了依据。第一作者应该做的所有事情都做得很完美。</p><p>  For the remainder of what follows, when I refer to “the InstaHide authors” I am talking about the senior authors on this paper who should know better. Among them, they have two Godel prizes, 80,000 citations, and i-10 index of over 400. The first is  completely without fault.</p><p>  在接下来的其余部分中，当我提到“ InstaHide作者”时，我指的是本文中的高级作者，他们应该更了解。其中，他们获得了两次戈德尔奖，8万次引用和400的i-10指数。第一个完全没有错误。</p><p>      I&#39;ll go into great detail below, but to briefly summarize, here&#39;s a short explanation of what goes wrong.</p><p>      我将在下面进行详细介绍，但在此简要概述一下出了什么问题。</p><p>  InstaHide makes no refutable claims. The key difference between science and pseudoscience is that claims in science are falsifiable. There must exist a way to disprove the claim. InstaHide makes no falsifiable privacy claims, and instead states their algorithm is private, without ever defining what this means.</p><p>  InstaHide没有任何可辩驳的声明。科学与伪科学之间的主要区别在于科学中的主张是可以证伪的。必须存在一种方法来驳斥索赔。 InstaHide不会伪造任何隐私声明，而是声明其算法是私有的，而从未定义这意味着什么。</p><p>  InstaHide moves the privacy goalposts. Even if the paper itself does not make refutable claims, the authors release a contest that can be solved. So we did just that: and broke the contest (completely). The author&#39;s response to this is to say that InstaHide wasn&#39;t intended to be a bullet-proof scheme anyway, and so it&#39;s not surprising their challenge can be solved.</p><p>  InstaHide移动了隐私目标。即使论文本身没有提出可辩驳的主张，作者也发表了可以解决的竞赛。因此，我们做到了：彻底打破了比赛。作者对此的回应是，无论如何InstaHide都不打算成为防弹方案，因此解决他们的挑战也就不足为奇了。</p><p>  InstaHide is complicated just for show. Instead of building the simplest possible scheme that might offer privacy, InstaHide builds a scheme that is complicated because it makes the visual pictures look prettier.</p><p>  InstaHide只是为了演示而复杂。 InstaHide没有构建可能提供隐私的最简单的方案，而是构建了一个复杂的方案，因为它使视觉图片看起来更漂亮。 </p><p>  InstaHide&#39;s fake complexity undermines the scheme. Normally artificial complexity is an effective no-op; in this case, because of an implementation bug, the fake complexity actually allows a complete attack on the scheme.</p><p>InstaHide的虚假复杂性破坏了该方案。通常，人为的复杂性是一种有效的禁止操作。在这种情况下，由于实现错误，伪造的复杂性实际上允许对该方案进行完全攻击。</p><p>  InstaHide has spurious theorems. Theorems of privacy are important, however InstaHide gives theorems that while mathematically true, are independent of the security or privacy of the scheme.</p><p>  InstaHide具有虚假的定理。隐私定理很重要，但是InstaHide给出的定理在数学上是正确的，但与方案的安全性或隐私无关。</p><p>  InstaHide disregards technical rigor. Statements that are very precisely defined in the literature, such as indistinguishability, are thrown around cavalierly. Arguing something is indistinguishible requires work, but the paper runs a single statistical test instead.</p><p>  InstaHide忽略了严格的技术要求。文献中非常精确地定义的语句（例如，不可区分性）变得轻而易举。争论不清是需要工作的，但是本文只运行了一次统计检验。</p><p>  The InstaHide authors continue to promote it. Sometimes defenses get broken. That&#39;s part of life in computer security. When this happens, it&#39;s necessary to admit it&#39;s broken, and try to fix it or move on to a new proposal. Instead, the InstaHide authors continue to promote their work as if it is effective, without regard to reality.</p><p>  InstaHide的作者继续对其进行推广。有时防御会被破坏。这是计算机安全的一部分。发生这种情况时，有必要承认它已损坏，然后尝试对其进行修复或继续进行新的提案。取而代之的是，InstaHide的作者继续宣传他们的工作，就好像它是有效的一样，而不考虑现实。</p><p>      Suppose some hospital wanted to train the world&#39;s greatest medical imaging scanner that can take a scan of your body, run a fancy machine learning model over it, and tell you all the problems you have. To do this, they&#39;d certainly need a lot of training data: images from individual people with their scans, and the list of problems they do (or don&#39;t) have.</p><p>      假设某家医院想训练世界上最伟大的医学成像扫描仪，该扫描仪可以对您的身体进行扫描，在上面运行一个漂亮的机器学习模型，并告诉您所有的问题。为此，他们当然需要大量的培训数据：来自具有扫描内容的个人的图像，以及他们遇到的（或没有）遇到的问题的列表。</p><p>  I wouldn&#39;t want to just give some random organization all the scans I&#39;ve ever had of my body. They&#39;re my private images. What if they got leaked?</p><p>  我不想只是随机组织我曾经对我的身体进行的所有扫描。它们是我的私人照片。如果他们泄漏了怎么办？</p><p>  Training data privacy schemes give a way for someone to train a machine learning model on this private data without having to worry that their training data will be leaked. Most schemes published today are  provably correct: there is a formal argument that, unless there is an error in the proof, states my data can never be leaked from the trained model.</p><p>  训练数据隐私方案为某人提供了一种在此私有数据上训练机器学习模型的方式，而不必担心他们的训练数据将被泄漏。今天发布的大多数方案都可以证明是正确的：有一个正式的论据，除非证明中有错误，否则我的数据永远不会从训练过的模型中泄漏出来。 </p><p>  (Yes, cryptosystems like RSA or AES are not provably secure and we use them anyway. However these systems explicitly aim to be as simple as possible to make analysis easy. This is fairly independent from what we&#39;re talking about, because privacy schemes do tend to be provably secure.))</p><p>（是的，诸如RSA或AES之类的密码系统不是可证明的安全性，我们还是要使用它们。但是，这些系统明确地旨在尽可能简单以使分析变得容易。这完全独立于我们在谈论的内容，因为隐私方案确实确实可以证明是安全的。））</p><p>  The problem is that these schemes are often slow, and usually degrade the accuracy of the final model. This is not ideal, but such is the cost of correctness.</p><p>  问题在于这些方案通常很慢，并且通常会降低最终模型的准确性。这不是理想的，但这是正确性的代价。</p><p>  InstaHide is a proposal for how to achieve this result with a fairly simple algorithm that does not increase model training time, and also does not cause huge accuracy drops. This is clearly an important end goal, and we should be excited by any scheme that tries to do this.</p><p>  InstaHide是关于如何使用相当简单的算法来实现此结果的建议，该算法不会增加模型训练时间，也不会引起巨大的精度下降。这显然是一个重要的最终目标，任何尝试做到这一点的方案都应使我们感到兴奋。</p><p>  The basic idea behind InstaHide is a simple two-step process. To encode any particular private image, combine it together with a bunch of other random images, and then randomly flip the signs of the pixels in the image.</p><p>  InstaHide的基本思想是一个简单的两步过程。要对任何特定的私人图像进行编码，请将其与一堆其他随机图像组合在一起，然后随机翻转图像中像素的符号。</p><p>      One of the core tenents of modern science (for, oh, the past hundred years) is that  claims should be refutable: if the claim is false, there must exist an experiment which could disprove it. This is what separates science from pseudoscience. Unfortunately, InstaHide does not make falsifiable claims. It claims to offer privacy without ever defining what this means. This is a common failing in many papers on the security and privacy of machine learning, but no recent paper exemplifies this better than InstaHide.</p><p>      现代科学的核心宗旨之一（在过去的一百年里）是主张应该是可辩驳的：如果主张是错误的，那么必须存在一个可以证明它的实验。这就是将科学与伪科学区分开来的原因。不幸的是，InstaHide并未提出虚假的声明。它声称提供隐私，而从未定义这意味着什么。这是许多关于机器学习的安全性和隐私性的常见故障，但是最近没有一篇论文比InstaHide更好地证明了这一点。</p><p>  A typical privacy definition will say something of the form “it is not possible to learn property X about the training dataset if Y holds true”. For example, differential privacy says that it is not possible to distinguish between the case that a particular user is or is not in the dataset. The InstaHide paper has  no refutable privacy claims. It defines an algorithm, and says it is private, without ever saying what that means.</p><p>  一个典型的隐私定义会说“如果Y成立，就不可能学习有关训练数据集的属性X”的形式。例如，差异性隐私表示不可能区分特定用户是否在数据集中的情况。 InstaHide纸没有可辩驳的隐私声明。它定义了一个算法，并说它是私有的，而从未说过这意味着什么。</p><p>  As a result, it&#39;s impossible to ever write a paper that claims to break it, because defining an attack necessarily requires a definition to break. The best one can do (and indeed, what we do in our paper) is to define potential definitions of what InstaHide  may mean by privacy, and show that it doesn&#39;t satisfy  those definitions. But it&#39;s always possible that there exists some definition of privacy that InstaHide does satisfy.</p><p>  结果，不可能写出声称破坏它的论文，因为定义攻击必然需要一个破坏的定义。最好的方法（实际上是我们在本文中所做的）是定义InstaHide隐私可能意味着的潜在定义，并表明它不满足这些定义。但是，总有可能存在InstaHide确实满足的一些隐私定义。 </p><p>  Fortunately, the authors do release the InstaHide Challenge: a contest where they ask researchers to try and break the privacy of InstaHide under the strongest possible settings. Here, breaking the privacy  is well defined: given the output of the InstaHide algorithm, we are asked to try and reconstruct the original dataset. Even if the algorithm isn&#39;t refutable, at least this contest is.</p><p>幸运的是，作者的确发布了InstaHide挑战赛：他们要求研究人员在尽可能强大的环境下尝试破坏InstaHide的隐私。在这里，定义隐私的定义很明确：给定InstaHide算法的输出，我们被要求尝试重建原始数据集。即使算法不可重演，至少这场比赛也是。</p><p>  This Challenge (and the source code for the algorithm) was released  four months after the initial paper was being presented and discussed in public. The fact that there was this huge delay meant that, for the first four months, it was impossible to say InstaHide is insecure with any certainty.</p><p>  该挑战（以及该算法的源代码）在最初的论文被公开介绍和讨论四个月后发布。如此巨大的延误这一事实意味着，在头四个月中，不可能肯定地说InstaHide是不安全的。</p><p>  If the authors had released the challenge on time, early on when the paper was still being considered for the Bell Labs prize, would it have gotten an award? I suspect not. (Now it&#39;s still inexcusable that it got an award given our attack came out a month before the prize. But I expect if it had been broken a few weeks after being published it wouldn&#39;t have received a prize at all.)</p><p>  如果作者按时发布了挑战，那么在仍被认为是贝尔实验室奖的论文发表之初，它会获得奖项吗？我怀疑不是。 （现在，由于我们的攻击是在奖品发布前一个月发生的，所以它获得奖项仍然是不可原谅的。但是我希望，如果它在出版后几周就被打破了，它将不会在所有。）</p><p>  So, the week after the authors released the challenge, we solved it. We proposed two attacks, one breaking the fundamental algorithm and another breaking the implementation of the challenge. So at least we know the challenge data is not private. But this brings us to the next issue...</p><p>  因此，在作者发布挑战的一周后，我们解决了挑战。我们提出了两种攻击方式，一种破坏基本算法，另一种破坏挑战的实现。因此，至少我们知道挑战数据不是私有的。但这将我们带入下一个问题...</p><p>    After publishing our attack, Sanjeev Arora (the senior author on the paper) responded in a  blog post that  “InstaHide was never intended to be a mission-critical encryption like RSA”. First: what?! Second: this was never mentioned in the paper, and was stated only after we broke it. Third: no algorithm should be designed to only offer “some” security in the best case. Algorithms are designed to be able to be (nearly) perfectly secure, and as soon as any weaknesses are discovered they are discarded for stronger approaches or fixed to maintain strong security.</p><p>    在发布我们的攻击后，Sanjeev Arora（该论文的高级作者）在博客文章中回复说：“ InstaHide绝不是像RSA这样的任务关键型加密”。第一：什么？！第二：本文从未提到过，只有在我们将其破解后才予以说明。第三：任何算法都不应设计为仅在最佳情况下提供“某种”安全性。算法被设计为能够（几乎）完全安全，并且一旦发现任何弱点，就会将其丢弃以采用更强大的方法，或者将其修复以保持强大的安全性。</p><p>  As soon as it was clear that DES, (or MD5, or SHA-1) had any weakness at all, cryptographers started designing completely new algorithms to replace them. These replacement algorithms were, obviously, designed to be completely secure. Even if the defender has only very limited compute, cryptographic algorithms still argue security against adversaries who are orders of magnitude more powerful. Just because the defender can only run on 1 watt of power doesn&#39;t meant the adversary can&#39;t run on a compute cluster.</p><p>  一旦发现DES（或MD5或SHA-1）根本没有任何弱点，密码学家便开始设计全新的算法来替代它们。显然，这些替换算法设计为完全安全的。即使防御者只有非常有限的计算量，加密算法仍然认为安全性可以抵抗功能强大几个数量级的对手。仅仅因为防御者只能以1瓦的功率运行，并不意味着对手就不能在计算集群上运行。</p><p>  Now to get technical for a second, I should really discus security parameters. This is the value that controls “how strong” a scheme is. Many algorithms (like RSA) can be made much less secure if you pick a low value of this parameter. However, importantly, that&#39;s easy to avoid---just pick a big one! InstaHide has no security parameters, and so it can not be made more secure by adjusting a few constants.</p><p>  现在要获得技术知识，我应该真正讨论安全性参数。这是控制方案“多么强大”的价值。如果选择此参数的低值，许多算法（例如RSA）的安全性可能会大大降低。但是，重要的是，这很容易避免-只需选择一个大的！ InstaHide没有安全参数，因此无法通过调整一些常量来使其更加安全。 </p><p>  Arguing that InstaHide is not intended for mission-critical encryption only after it is broken---and even then calling the attack  “not yet a cost-effective attack in the intended settings”---is nothing short of moving the goalposts. And this is what you can do when you don&#39;t make any claims the first time around. InstaHide was intended to be secure. It is not.</p><p>争辩说InstaHide仅在被破坏后才用于关键任务加密-甚至将攻击称为“在预期的环境下仍不具有成本效益的攻击”-简直就是在移动目标杆。这是您第一次不提出任何要求时可以做的事情。 InstaHide旨在确保安全。它不是。</p><p>  (Briefly, on to the claim that the attack is not cost effective. The attack takes roughly 12 hours of P100 GPU time. On AWS or GCP this costs less than 20 USD to rent. Now $20 isn&#39;t nothing, it&#39;s certainly more expensive than free. But when DES was first broken it cost $250,000 USD to build the machine, and a few days of compute time to break a single key. A P100 GPU is $2500 (exactly 100 times cheaper, not inflation adjusted) and the attack is at least a few times faster. But I digress.)</p><p>  （简而言之，该攻击的成本效益不高。该攻击大约需要12个小时的P100 GPU时间。在AWS或GCP上，这笔租金不到20美元。现在20美元已经算是什么了，它39; s肯定比免费软件贵。但是当DES第一次被破解时，它的造价为25万美元，并且需要几天的计算时间才能破解单个密钥。P100GPU的价格为2500美元（便宜100倍，不是通货膨胀的原因）。调整后），攻击速度至少快了几倍。但我离题了。）</p><p>    It is important for papers to have a compelling story behind the technique, not just introduce a technique that advances the state of the art. And often this means that by artificially introducing complexity where none is required, a paper can be made to look more significant than it is.</p><p>    对于论文而言，重要的是要使该技术背后具有引人入胜的故事，而不仅仅是引入一种先进技术。通常，这意味着通过在不需要的地方人为地引入复杂性，可以使论文看起来比实际意义更重要。</p><p>  InstaHide does exactly this. At some point in their algorithm, InstaHide has a list of numbers [1, 3, -5, 7, -2]. It needs to increase the privacy of this list of numbers, so it multiplies each value by either 1 or -1 (chosen at random). So for example if we select [-1, 1, 1, -1, -1] then we would get [-1, 3, -5, -7, 2]. The claim is this somehow now preserves the privacy of these numbers.</p><p>  InstaHide正是这样做的。在他们的算法中的某个时刻，InstaHide有一个数字列表[1、3，-5、7，-2]。它需要增加此数字列表的私密性，因此将每个值乘以1或-1（随机选择）。因此，例如，如果我们选择[-1，1，1，-1，-1]，那么我们将得到[-1，3，-5，-7，2]。现在的主张是这样可以保留这些数字的隐私。</p><p>    In practice InstaHide operates on images. So it takes this image here on the left of a cat that we might regard as private, and converts it to an “encoded” version of the image on the right where each pixel has been multiplied by either 1 or -1.</p><p>    实际上，InstaHide可在图像上运行。因此，它将这幅图像放在猫的左边（我们可能将其视为私有），然后将其转换为右边图像的“编码”版本，其中每个像素都乘以1或-1。</p><p>    The astute reader might observe that all we have done is just remove the sign information of the original list. Because we&#39;ve multiplied by either 1 or -1, uniformly at random, we have information theoretically completely removed the sign. Thus, InstaHide could have (more simply) just released the absolute value of each item in the list: [1, 3, 5, 7, 2]. This would reveal no more or less data than doing some weird sign flipping procedure.</p><p>    精明的读者可能会发现，我们所做的只是删除原始列表的标志信息。因为我们随机地乘以1或-1，所以从理论上讲我们有信息完全删除了该符号。因此，InstaHide可以（更简单地）释放列表中每个项目的绝对值：[1、3、5、7、2]。这不会比做一些怪异的符号翻转过程揭示更多或更少的数据。</p><p>  Why bother with this sign flipping, then? Well, it turns out there&#39;s something called a One Time Pad. This is a (provably secure) encryption algorithm that  also, in a different setting, relies on masking a random list of numbers by multiplying each entry by 1 or -1.</p><p>  那么，为什么还要打扰这个标志呢？好吧，事实证明，有一种叫做“一次性垫”的东西。这是一种（证明是安全的）加密算法，在不同的设置下，它也依赖于通过将每个条目乘以1或-1来掩盖数字的随机列表。 </p><p>  And so in order to say that InstaHide is like a perfect encryption algorithm, the authors have to introduce this crazy sign flipping procedure. Sure it would be simpler to describe it as taking the absolute value. But that wouldn&#39;t have a good story attached to it; it wouldn&#39;t be possible to call it something like an encryption algorithm. One time pads give a good story.</p><p>因此，为了说InstaHide就像是一种完美的加密算法，作者必须介绍这种疯狂的符号翻转过程。当然，将其描述为采用绝对值会更简单。但这并没有一个很好的故事。无法将其称为加密算法。一个时间垫可以讲一个好故事。</p><p>    What would have happened if InstaHide had instead used the absolute value instead of flipping signs? Well, this. Clearly still a cat. But would the paper have gotten attention if it claimed that taking the absolute value was secure? Probably not.</p><p>    如果InstaHide改用绝对值而不是翻转符号会发生什么？好吧显然还是猫。但是，如果这篇论文声称采用绝对值是安全的，会引起注意吗？可能不会。</p><p>  This is why fake complexity is dangerous. It takes something that&#39;s obviously a broken idea (taking the absolute value of a pixel in order to preserve its privacy somehow) and replaces it with something that looks sane. But sane it is not.</p><p>  这就是为什么伪造复杂性很危险的原因。它需要一个显然是一个破损的主意（采用像素的绝对值以某种方式保留其隐私权），并用看起来理智的东西代替它。但不是理智。</p><p>  Side note: how do I know that this complexity is actually fooling people? In the award ceramony, the Bell Labs researcher presenting the award explicitly said  he doesn&#39;t understand how InstaHide is secure, but, and I quote,  “it works nonetheless”. No! It does not. But this complexity is working as intended.</p><p>  旁注：我怎么知道这种复杂性实际上是在欺骗人们？在颁奖典礼上，颁奖的贝尔实验室研究员明确表示，他不了解InstaHide是如何安全的，但我引用“仍然有效”。没有！它不是。但是这种复杂性正在按预期进行。</p><p>    Now in an ironic turn of events, it turns out that this fake complexity with sign flipping that InstaHide uses actually happens to introduce a completely new vulnerability that wouldn&#39;t be there if the authors had just taken the absolute value of the list of numbers.</p><p>    现在具有讽刺意味的是，事实证明，InstaHide使用的这种带有标志翻转的虚假复杂性实际上恰好引入了一个全新的漏洞，如果作者只是采用了列表的绝对值，该漏洞就不会存在。数字。</p><p>  This implementation uses a standard psudorandom number generator (PRNG) to create the random sign flipping mask. It turns out that PRNGs have the property that, given a few outputs, it&#39;s possible to learn their  state and as a result can recover every subsequent output. Learning the state requires a little bit of work, but it&#39;s not all that hard (it takes roughly 100 CPU hours, at a cost of about $4 USD). So we did it.</p><p>  此实现使用标准伪随机数生成器（PRNG）来创建随机符号翻转掩码。事实证明，PRNG具有这样的属性：给定一些输出，就可以了解其状态，从而可以恢复每个后续输出。学习状态需要一点点工作，但并不是那么困难（大约需要100个CPU小时，花费约4美元）。所以我们做到了。</p><p>  This means that we can actually  undo the sign flipping procedure, and recover the original images. After doing this, InstaHide no longer offers any meaningful privacy. We can completely break the algorithm.</p><p>  这意味着我们实际上可以取消符号翻转过程，并恢复原始图像。完成此操作后，InstaHide不再提供任何有意义的隐私。我们可以完全打破算法。 </p><p>  Now, if InstaHide had just taken the absolute value, then the sign information would  actually have been destroyed. This attack would not work. But since the authors insisted on adding unnecessary complexity in order to make a show of things, they&#39;ve actually weakened the scheme. Being complicated is not good. It introduces room for additional errors.</p><p>现在，如果InstaHide刚取了绝对值，那么标志信息实际上将被破坏。这种攻击是行不通的。但是由于作者坚持要添加不必要的复杂性以展示事物，因此实际上削弱了该方案。变得复杂并不好。它引入了其他错误的余地。</p><p>    There&#39;s this thing that machine learning research papers do where they propose Algorithm A, say in words it works, and then prove Theorem T. The problem is that Theorem T is often completely disjoint from whether or not Algorithm A works at all.  Lipton and Steinhardt call these  spurious theorems.</p><p>    机器学习研究论文在提出算法A的地方做这件事，用言语说它起作用，然后证明定理T。问题是定理T常常与算法A根本不起作用完全脱节。 。立顿和斯坦哈特称这些伪定理。</p><p>  The way InstaHide makes this error is as follows. In order to justify the security of the proposed approach, authors prove that  “For worst-case pixel-vectors, finding the k-image set is hard.” That is, they say that there exists a dataset that, if encrypted by InstaHide, would be hard to invert.</p><p>  InstaHide导致此错误的方法如下。为了证明所提出方法的安全性，作者证明“对于最坏情况的像素矢量，很难找到k图像集。”也就是说，他们说存在一个数据集，如果使用InstaHide加密，则将很难进行转换。</p><p>  This claim is true. There does exist a dataset that is hard to invert. But that says nothing about what happens for standard images. It&#39;s not helpful to show that there exists a case where things are hard, we must show that the problem is always hard.</p><p>  这个说法是正确的。确实存在难以反转的数据集。但这并没有说明标准图像会发生什么。证明存在困难的情况并没有帮助，我们必须证明问题始终是困难的。</p><p>  Compare this to something like differential privacy, where the story is completely reversed. The proof here says that  for all datasets---no matter how pathological---privacy will be preserved. InstaHide claims only to be private on some pathological datasets.</p><p>  将其与差异性隐私之类的东西相比，故事完全相反。这里的证据表明，对于所有数据集-不管病理如何-都将保留隐私。 InstaHide声称仅在某些病理数据集上是私有的。</p><p>    The paper prominently states that  “Distributions of encryptions of different images are indistinguishable” in the caption of Figure 2 in their paper. This phrase might seem bland, but realize that  indistinguishability actually means something  very specific in computer security papers: it means that  no adversary can distinguish between two distributions with probability better than random guessing plus some exponentially small probability.</p><p>    该论文在其论文的图2的标题中突出指出“不同图像的加密分布是无法区分的”。这个短语看似平淡无奇，但是意识到不可分辨性实际上意味着在计算机安全性论文中有一个非常具体的含义：这意味着没有任何对手能够以比随机猜测加成倍的指数小概率更好的概率来区分两种分布。</p><p>  This paper asserts the distributions are indistinguishable not with a proof, but because upon visual inspection of a histogram the distributions look like they overlap, more or less, plus a statistical test in an appendix.</p><p>  本文断言这些分布不是无法用证据来区分的，而是因为在对直方图进行目视检查时，这些分布看起来或多或少地相互重叠，并在附录中进行了统计检验。 </p><p>  It would be absolutely insane if a paper proposing an encryption algorithm were to plot the frequency of 1s and 0s in an encrypted message and say “our algorithm outputs data indistinguishible from random, and you can tell because the probability of a 1 looks roughly equal to the probability of a 0” but this paper makes exactly this argument.</p><p>如果提出加密算法的论文在加密的消息中绘制1s和0s的频率，然后说“我们的算法输出的数据与随机数据没有区别，那您就可以知道，因为1的概率看起来大致等于”可能性为0”，但本文正是提出了这一论点。</p><p>    If everything up until this point were true, but the authors were to come out and say “we tried to introduce a scheme to increase training data privacy. it turns out our scheme doesn&#39;t work. we now understand this and are trying to build stronger schemes with privacy definitions.” then I wouldn&#39;t have written this post. The problem is that after seeing our break, this hasn&#39;t happened. They still argue that it&#39;s secure under some settings, didn&#39;t retract their submission from the Bell Labs award, and haven&#39;t yet (after a month) accepted our break on their Challenge leaderboard.</p><p>    如果到现在为止的一切都是真的，但是作者要出来说：“我们试图引入一种方案来增加训练数据的隐私性。事实证明我们的方案行不通。我们现在了解了这一点，并正在尝试使用隐私定义来构建更强大的方案。”那么我不会写这篇文章。问题是看到我们休息后，这还没有发生。他们仍然认为在某些情况下它是安全的，没有撤回贝尔实验室奖的提交，并且（一个月后）还没有接受我们在挑战排行榜上的表现。</p><p>      I really hope that we can achieve the end goals of InstaHide: privacy-preserving training that doesn&#39;t sacrifice accuracy or training time. This would be amazing.</p><p>      我真的希望我们能够实现InstaHide的最终目标：不牺牲准确性或培训时间的保护隐私的培训。这将是惊人的。</p><p>  InstaHide does not offer this, however. And not only does it not offer privacy, i</p><p>  但是，InstaHide不提供此功能。而且它不仅不提供隐私，</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://nicholas.carlini.com/writing/2020/instahide_disappointingly_wins_bell_labs_prize.html">https://nicholas.carlini.com/writing/2020/instahide_disappointingly_wins_bell_labs_prize.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/获得/">#获得</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/instahide/">#instahide</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>