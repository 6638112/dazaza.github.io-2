<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Mozilla的AI伦理倡导集团提出了在Bug Bounty程序上建模的算法偏置检测程序 Mozilla's AI ethics advocacy group proposes algorithmic bias detection program modeled on bug bounty programs</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Mozilla's AI ethics advocacy group proposes algorithmic bias detection program modeled on bug bounty programs<br/>Mozilla的AI伦理倡导集团提出了在Bug Bounty程序上建模的算法偏置检测程序 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-18 03:45:50</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/07d6e9d817da2b5c743eb0b0eb1d59be.jpg"><img src="http://img2.diglog.com/img/2021/3/07d6e9d817da2b5c743eb0b0eb1d59be.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>When it comes to detecting bias in algorithms, researchers are trying to learn from the information security field – and particularly, from the  bug bounty-hunting hackers who comb through software code to identify potential security vulnerabilities.</p><p>当涉及到检测算法中的偏差时，研究人员正试图从信息安全领域学习 - 尤其是来自Bug Bounty-Hunting黑客，他通过软件代码梳理潜在的安全漏洞。</p><p> The parallels between the work of these security researchers and the hunt for possible flaws in AI models, in fact, is at the heart of the work carried out by Deborah Raji, a research fellow in algorithmic harms for the Mozilla Foundation.</p><p> 这些安全研究人员的工作与AI模型的可能缺陷之间的相似之处，实际上是Deborah Raji的工作中心，该研究员是Mozilla基金会的算法危害。</p><p>    Presenting the research she has been carrying out with advocacy group the Algorithmic Justice League (AJL) during the annual Mozilla Festival, Raji explained how along with her team, she has been studying bug bounty programs to see how they could be applied to the detection of a different type of nuisance: algorithmic bias.</p><p>    提出了研究，她一直在进行宣传小组的算法正义联赛（AJL），在每年的Mozilla节期间，Raji解释了她的团队如何，她一直在研究Bug赏金计划，看看它们如何应用于检测一种不同类型的滋扰：算法偏差。</p><p>  Bug bounties, which reward hackers for discovering vulnerabilities in software code before malicious actors exploit them, have become an integral part of the information security field. Major companies such as Google, Facebook or Microsoft now all run bug bounty programs; the  number of these hackers is multiplying, and so are the financial rewards that corporations are ready to pay to fix software problems before malicious hackers find them.</p><p>  BUG啤酒奖励，其中恶意演员在恶意演员利用它们之前在软件代码中发现漏洞的黑客已成为信息安全字段的组成部分。谷歌，Facebook或Microsoft等主要公司全部运行Bug Bounty程序;这些黑客的数量正在乘以，因此，在恶意黑客找到它们之前，公司准备好的财务奖励是公司准备支付以修复软件问题。</p><p> &#34;When you release software, and there is some kind of vulnerability that makes the software liable to hacking, the information security community has developed a bunch of different tools that they can use to hunt for these bugs,&#34; Raji tells ZDNet. &#34;Those are concerns that we can see parallels to with respect to bias issues in algorithms.&#34;</p><p> ＆＃34;当你发布软件时，有一些脆弱性使软件使软件能够遗忘，信息安全社区已经开发出一堆不同的工具，他们可以用来寻找这些虫子，＆＃34; Raji告诉ZDNet。 ＆＃34;那些人担心，我们可以看到与算法中的偏见问题相比之下。＆＃34;</p><p> As part of a project called CRASH (the Community Reporting of Algorithmic System Harms), Raji has been looking at the ways that bug bounties work in the information security field, to see if and how the same model could apply to bias detection in AI.</p><p> 作为一个项目的一部分，称为崩溃（算法系统危害的社区报告），Raji一直在查看Bug奖励在信息安全领域工作的方式，以了解同一模型可以应用于AI中的偏置检测。</p><p>   Although AI systems are becoming more sophisticated – and pervasive – by the day, there is currently no common stance on the best way to check algorithms for bias. The potentially devastating effects of flawed AI models, so far, has only been revealed by specialized organizations or independent experts, with no connection to one another.</p><p>   虽然AI系统变得更加复杂 - 而普遍存在 - 当天，目前没有常见的姿态是检查偏差算法的最佳方法。缺陷AI模型的潜在破坏性效果仅被专门的组织或独立专家透露，没有相互联系。 </p><p> Examples include Privacy International digging out  the details of the algorithms driving the investigations led by the Department for Work and Pensions (DWP) against suspected fraudsters, to MIT and Stanford researchers  finding skin-type and gender biases in commercially released facial-recognition technologies.</p><p>示例包括隐私国际挖掘促进工作和养老金部（DWP）领导的调查的算法的细节，以解释和斯坦福研究人员在商业上发布的面部识别技术中找到皮肤型和性别偏见。</p><p> &#34;Right now, a lot of audits are coming from different disciplinary communities,&#34; says Raji. &#34;One of the goals of the project is to see how we can come up with resources to get people on some sort of level-playing field so they can engage. When people start participating in bug bounties, for example, they get plugged into a community of people interested in the same thing.&#34;</p><p> ＆＃34;现在，很多审计来自不同的纪律社区，＆＃34; raji说。 ＆＃34;该项目的一个目标之一是看看我们如何提出资源，让人们在某种级别播放领域，以便他们可以参与。例如，当人们开始参加Bug Bounties时，他们就会被插入一个对同一件事感兴趣的人群。＆＃34;</p><p> The parallel between bug bounty programs and bias detection in AI is evident. But as they dug further, Raji and her team soon found that defining the rules and standards of discovering algorithmic harms might be a bigger challenge than establishing what constitutes a software bug.</p><p> 错误赏金程序与AI中的偏置检测之间是明显的。但随着他们挖出的，Raji和她的团队很快发现，定义发现算法危害的规则和标准可能是一个更大的挑战，而不是建立一个软件错误的挑战。</p><p> The very first question that the project raises, that of defining algorithmic harm, already comes with multiple answers. Harm is intrinsically linked to individuals – who in turn, might have a very different perspective from that of the companies designing AI systems.</p><p> 项目提出的第一个问题，即定义算法危害，已经有多个答案。危害本质上与个人联系起来 - 又转过身来，可能与设计AI系统的公司具有截然不同的观点。</p><p> And even if a definition, and possibly a hierarchy, of algorithmic harms were to be established, there remains an entire methodology for bias detection that is yet to be created.</p><p> 即使要建立一个定义和可能是算法危害的定义和可能的层次结构，仍然存在尚未创建的偏差检测方法。</p><p> In the decades since the first bug bounty program was launched (by browser pioneer Netscape in 1995), the field has had the time to develop protocols, standards and rules, that ensure that bug detection remains beneficial to all parties. For example, one of the best-known bug bounty platforms, HackerOne, has  a set of clear guidelines surrounding the disclosure of a vulnerability, which include submitting confidential reports to the targeted company and allowing sufficient time to publish a remediation.</p><p> 在几十年来，自第一个Bug赏金计划启动（通过1995年通过浏览器Pioneer Netscape），该领域已经有时间开发协议，标准和规则，以确保错误检测对所有各方都有利于有益。例如，Hackerone最着名的Bug Bounty平台之一Hackerone具有围绕漏洞的披露的一组明确的指导，其中包括向目标公司提交机密报告，并允许足够的时间发布补救措施。</p><p>  &#34;Of course, they&#39;ve had decades to develop a regulatory environment,&#34; says Raji. &#34;But a lot of their processes are a lot more mature than the current algorithmic auditing space, where people will write an article or a Tweet, and it&#39;ll go viral.&#34;</p><p>  ＆＃34;当然，他们有几十年来制定监管环境，＆＃34; raji说。 ＆＃34;但是，他们的很多过程比当前的算法审计空间更加成熟，人们会写一篇文章或推文，它和＃39; ll去病毒。＆＃34; </p><p> &#34;If we had a harms discovery process that, like in the security community, was very robust, structured and formalized, with a clear way of prioritizing different harms, making the whole process visible to companies and the public, that would definitely help the community gain credibility – and in the eyes of companies as well,&#34; she continues.</p><p>＆＃34;如果我们有危害发现过程，就像在安全社区一样，非常强大，结构和正式，具有优先考虑不同危害的明确方式，使公司和公众可见的全部过程，这绝对是帮助社区获得信誉 - 以及公司的眼中，＆＃34;她继续。</p><p> Corporations are spending millions on bug bounty programs. Last year, for instance, Google  paid a record $6.7 million in rewards to 662 security researchers who submitted vulnerability reports.</p><p> 公司在Bug Bounty计划上花费了数百万美元。例如，去年，谷歌向提交漏洞报告的662名安全研究人员支付了662份奖励的660万美元。</p><p> But in the AI ethics space, the dynamic is radically different; according to Raji, this is due to a misalignment of interests between AI researchers and corporations. Digging out algorithmic bias, after all, could easily lead to having to redesign the entire engineering process behind a product, or even taking the product off the market altogether.</p><p> 但在AI道德空间，动态是完全不同的;根据Raji的说法，这是由于AI研究人员和公司之间的利益不存在。毕竟，挖掘算法偏见可以很容易地导致必须重新设计产品背后的整个工程过程，甚至完全从市场上脱离产品。</p><p>  Raji remembers  auditing Amazon&#39;s facial recognition software Rekognition, in a study that concluded that the technology exhibited gender and racial bias. &#34;It was a huge battle, they were incredibly hostile and defensive in their response,&#34; she says.</p><p>  Raji记得审计亚马逊＆＃39;在一项研究中，该技术得出结论，该技术表现出性别和种族偏见。 ＆＃34;这是一个巨大的战斗，他们的反应是令人难以置信的敌对和防守，＆＃34;她说。</p><p> In many cases, says Raji, the population affected by algorithmic bias are not paying customers – meaning that, unlike in the information security space, there is little incentive for companies to mend their ways if a flaw is found.</p><p> 在许多情况下，Raji表示，受算法偏见影响的人口不支付客户 - 这意味着，与信息安全空间不同，公司几乎没有激励，如果发现缺陷，公司可以修复他们的方式。</p><p> While one option would be to trust companies to invest in the space out of a self-imposed willingness to carry out ethical technology, Raji isn&#39;t all that confident. A more promising avenue would be to exert external pressure on corporations, in the form of regulation – but also thanks to public opinion.</p><p> 虽然一个选项是信任公司投资空间，从淫亵技术，raji＆＃39; t所有的信心。更有前途的大道是以监管的形式对公司的外部压力施加外部压力 - 但也归功于舆论。</p><p> Will fear of reputational damage unlock the possibility of future AI-bias bounty programs? For Raji, the answer is evident. &#34;I think that cooperation is only going to happen through regulation or extreme public pressure,&#34; she says.</p><p> 是否会担心声誉损害解锁未来AI-BIAS赏金计划的可能性？对于Raji，答案很明显。 ＆＃34;我认为合作只是通过规定或极端的公共压力来实现，＆＃34;她说。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.zdnet.com/article/preventing-bias-in-ai-is-hard-bug-bounties-could-point-the-way-forward/">https://www.zdnet.com/article/preventing-bias-in-ai-is-hard-bug-bounties-could-point-the-way-forward/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ai/">#ai</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>