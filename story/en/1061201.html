<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>研究人员详细介绍了一种新型攻击，可以通过向网络输入增加少量噪声来提高神经网络的能量消耗 Researchers detail a new type of attack that could increase the energy consumption of neural networks by adding small amounts of noise to a network's inputs</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Researchers detail a new type of attack that could increase the energy consumption of neural networks by adding small amounts of noise to a network's inputs<br/>研究人员详细介绍了一种新型攻击，可以通过向网络输入增加少量噪声来提高神经网络的能量消耗 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-07 07:09:23</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/5/8b8e94ef936e768e8fbc14fdeba5d4f8.jpg"><img src="http://img2.diglog.com/img/2021/5/8b8e94ef936e768e8fbc14fdeba5d4f8.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>The news:  A new type of attack could increase the energy consumption of AI systems.  In the same way a denial-of-service attack on the internet seeks to clog up a network and make it unusable, the new attack forces a deep neural network to tie up more computational resources than necessary and slow down its “thinking” process.</p><p>新闻：一种新型的攻击可以提高AI系统的能量消耗。以同样的方式对互联网的拒绝服务攻击寻求堵塞网络并使它无法使用，新攻击强制了深度神经网络，以符合必要的计算资源，并减慢其“思维”过程。</p><p>  The target: In recent years, growing concern over the costly energy consumption of large AI models has led researchers to design more efficient neural networks. One category, known as input-adaptive multi-exit architectures, works by splitting up tasks according to how hard they are to solve. It then spends the minimum amount of computational resources needed to solve each.</p><p>  目标：近年来，对大型AI型号的昂贵能源消耗的越来越关注导致研究人员设计更高效的神经网络。一个类别，称为自适应多退出架构，通过根据它们的努力来拆分任务来解决。然后它花费了每个所需的计算资源量。</p><p>  Say you have a picture of a lion looking straight at the camera with perfect lighting and a picture of a lion crouching in a complex landscape, partly hidden from view. A traditional neural network would pass both photos through all of its layers and spend the same amount of computation to label each. But an input-adaptive multi-exit neural network might pass the first photo through just one layer before reaching the necessary threshold of confidence to call it what it is. This  shrinks the model’s carbon footprint—but it also improves its speed and allows it to be deployed on small devices like smartphones and smart speakers.</p><p>  假设你有一张狮子的照片直接在相机上，完美的照明和狮子蹲在一个复杂的景观中，部分隐藏在景观中。传统的神经网络将通过其所有层次通过两张照片并花费与每个层都花费相同的计算。但是输入自适应多出口神经网络可能在达到必要的信心的必要阈值之前通过一层通过第一张照片来称呼它是什么。这缩小了模型的碳足迹 - 但它也提高了其速度，并允许它部署在智能手机和智能扬声器等小型设备上。</p><p>   The attack: But this kind of neural network means if you change the input, such as the image it’s fed, you can change how much computation it needs to solve it. This opens up a vulnerability that hackers could exploit, as the researchers from the Maryland Cybersecurity Center outlined in a new paper being presented at the  International Conference on Learning Representations this week. By adding small amounts of noise to a network’s inputs, they made it perceive the inputs as more difficult and jack up its computation.</p><p>   攻击：但这种神经网络意味着如果您更改输入，例如它的封装，可以更改它需要解决的计算量。这使得黑客可以利用的脆弱性，因为在本周的学习陈述国际会议上举行的新论文中概述的马里兰州网络安全中心的研究人员。通过向网络的输入增加少量噪声，它们使得它感知输入更困难并加起来其计算。</p><p>  When they assumed the attacker had full information about the neural network, they were able to max out its energy draw. When they assumed the attacker had limited to no information, they were still able to slow down the network’s processing and increase energy usage by 20% to 80%. The reason, as the researchers found, is that the attacks transfer well across different types of neural networks. Designing an attack for one image classification system is enough to disrupt many, says Yiğitcan Kaya, a PhD student and paper coauthor.</p><p>  当他们假设攻击者有关于神经网络的完整信息时，他们能够最大限度地消失。当他们认为攻击者没有信息时，他们仍然能够减缓网络的处理，并将能源使用增加20％至80％。正如研究人员发现的那样，这是攻击跨越不同类型的神经网络转移。博士学位学生和纸张同步恋说，设计一个图像分类系统的攻击足以破坏许多人。</p><p>  The caveat: This kind of attack is still somewhat theoretical. Input-adaptive architectures aren’t yet commonly used in real-world applications. But the researchers believe this will quickly change from the pressures within the industry to deploy lighter weight neural networks, such as for smart home and other IoT devices. Tudor Dumitraş, the professor who advised the research, says more work is needed to understand the extent to which this kind of threat could create damage. But, he adds, this paper is a first step to raising awareness: “What’s important to me is to bring to people’s attention the fact that this is a new threat model, and these kinds of attacks can be done.”</p><p>  警告：这种攻击仍然有点理论上。输入 - 自适应架构尚未在现实​​世界应用中使用。但研究人员认为，这将很快从行业内的压力转化，以部署更轻的重量神经网络，例如智能家居和其他物联网设备。教授认为这项研究的教授铎杜米特拉斯说，需要更多的工作来了解这种威胁可能会产生损害的程度。但是，他补充说，本文是提高意识的第一步：“对我来说重要的是引起人们的注意，这是一个新的威胁模型，这些类型的攻击可以完成。” </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.technologyreview.com/2021/05/06/1024654/ai-energy-hack-adversarial-attack/">https://www.technologyreview.com/2021/05/06/1024654/ai-energy-hack-adversarial-attack/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/网络/">#网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/detail/">#detail</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>