<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>RNN的句法表现力不合理</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">RNN的句法表现力不合理</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-05 20:17:16</div><div class="page_narrow text-break page_content"><p>We prove a result that demonstrates RNNs can exactly implement bounded-depth stacks to capture a building block of human language optimally efficiently.</p><p>我们证明了RNN能够准确地实现有界深度的堆栈，以最优的效率捕获人类语言的构建块，这一结果证明了RNN可以准确地实现有界深度的堆栈。</p><p>  In 2015, Andrej Karpathy posted a now famous blog post on   The Unreasonable Effectiveness of Recurrent Neural Networks.  1In it, he shared some of the wonder he felt at the empirical utility and learning behavior of RNNs.To summarize this sense of wonder, Karpathy emphasized:</p><p>2015年，安德烈·卡帕西发表了一篇关于递归神经网络不合理有效性的博客文章，现在已经很有名了。1在这篇文章中，他分享了他对RNN的经验效用和学习行为感到的一些惊奇。为了总结这种奇妙的感觉，卡帕西强调：</p><p> We’ll train RNNs to generate text character by character and ponder the question “how is that even possible?”</p><p>我们将训练RNN逐个字符生成文本，并思考这样一个问题：“这怎么可能？”</p><p> Re-reading Karpathy’s blog post recently, even in the era of large pre-trained transformers, I still found the effectiveness of modestly sized RNNs for learning highly structured output spaces fascinating.For example, Andrej shows samples from an RNN trained on Wikipedia, which learns to generate not just relatively grammatical English, but Wikipedia article structure and even valid XML code.</p><p>重读卡帕西最近的博客文章，即使是在大型预先培训的变压器的时代，我仍然发现中等大小的RNN对于学习高度结构化的输出空间的有效性很有趣。例如，Andrej展示了一个针对Wikipedia培训的RNN的样本，它不仅学习生成相对语法的英语，而且学习生成Wikipedia的文章结构甚至有效的XML代码。</p><p> In this blog post,  we describe a recent step towards understanding Karpathy’s question –  how is that even possible?</p><p>在这篇博客文章中，我们描述了理解卡帕西问题的最新进展--这怎么可能呢？</p><p>   Empirically, RNNs are pretty effective in learning the syntax of language!On the one hand, to really process hierarchical structure in general, it’s necessary emulate  pushing and popping elements from a stack, a fundamental operation in  parsing.On the other hand, there’s healthy skepticism about RNNs; they might simply be counting up statistics about likely (long) word sequences like an   n-gram model, and sharing some statistical strength due to word embeddings.</p><p>从经验上看，RNN在学习语言语法方面非常有效！一方面，要真正处理总体上的层次结构，有必要模拟堆栈中的推送和弹出元素，这是语法分析中的基本操作。另一方面，人们对RNN持健康的怀疑态度；他们可能只是像n元语法模型那样统计可能(长)的单词序列，并由于单词嵌入而分享一些统计强度。</p><p> So, which is it? The mushy hidden state of an RNN doesn’t seem amenable to stack-like behavior, and many models have been made to make up for this, from the Ordered Neuron variant of the LSTM  (Shen et al., 2019) to RNNs augmented with an  explicit, external stack  (Joulin and Mikolov, 2015).It’s even been  proven that RNNs can’t recognize hierarchical languages in general  (Merrill et al., 2020)! Things aren’t looking great for the stack story, which helps us clarify our  how is that possible question:  is stack-like behavior even possible in RNNs in a realistic setting?</p><p>那么，到底是哪一个呢？RNN的模糊隐藏状态似乎不适合堆栈一样的行为，已经建立了许多模型来弥补这一点，从LSTM的有序神经元变体(沈等人，2019年)到用显式外部堆栈扩展的RNN(Joulin和Mikolov，2015)。甚至已经证明，RNN一般不能识别层次化语言(Merrill等人，2020)！堆栈的情况看起来并不乐观，这有助于我们澄清这个问题是如何可能的：RNN中类似堆栈的行为在现实环境中是可能的吗？</p><p> We add one insight to this question: when processing natural language,  we can bound the maximum number of things we’ll need to store on the stack.Once you assume this, we were very surprised to prove a formal version of the following:</p><p>我们为这个问题增加了一个洞察力：在处理自然语言时，我们可以限制堆栈上需要存储的最大数量的东西。一旦你假设了这一点，我们非常惊讶地证明了以下内容的正式版本：</p><p> RNNs can turn their hidden states into bounded-capacity stacks so efficiently, they can generate a bounded hierarchical language using asymptotically optimally few hidden units of memory.</p><p>RNN可以如此高效地将它们的隐藏状态转换为有限容量的堆栈，以至于它们可以使用渐近最优的少量隐藏存储单元来生成一种有界的层次化语言。</p><p>  We prove that RNNs  can do this, not that they  learn to do this. However, our proof consists of explicit mechanisms by which RNNs can implement bounded-depth stacks, in fact, we also provide a separate, more efficient mechanism for LSTMs that solely uses its gates.</p><p>我们证明了RNN可以做到这一点，而不是说他们学会了这样做。然而，我们的证明包括RNN可以实现有界深度堆栈的显式机制，事实上，我们还为只使用其门的LSTM提供了一个单独的、更有效的机制。</p><p> Our result changed the way I think about RNNs.It’s true that RNNs can’t implement arbitrary-depth stacks (like a  pushdown automaton can).They have finite memory, so they’re  finite-state machines).However, they  can turn their hidden states into stack  structured memory, meaning they can process some (bounded, hierarchical!) languages exponentially more efficiently than one would think if they were learning the finite-state machine.We argue this means the  Chomsky Hierarchy isn’t the (only) thing we should be using to characterize neural models’ expressivity; even in the simplest (finite-state) regime, the structure of the memory of these networks is of interest.We hope that our proofs and constructions help people think about and analyze RNN behavior, and inspire future improvements in neural architectures.</p><p>我们的结果改变了我对RNN的看法。RNN确实不能实现任意深度的堆栈(就像下推自动机一样)。它们的内存是有限的，所以它们是有限状态机)。但是，它们可以将隐藏状态转换为堆栈结构的内存，这意味着它们可以处理一些(有界的，层次化的！)。语言的效率比人们在学习有限状态机时想象的要高得多。我们认为，这意味着乔姆斯基层次不是我们应该用来描述神经模型表达能力的(唯一)东西；即使在最简单的(有限状态)制度下，这些网络的记忆结构也很有趣。我们希望我们的证明和构造能帮助人们思考和分析RNN行为，并启发神经体系结构的未来改进。</p><p> This blog post is based off the paper   RNNs can generate bounded hierarchical languages with optimal memory, published at EMNLP 2020, joint work with  Michael Hahn,  Surya Ganguli,  Percy Liang, and  Chris Manning.</p><p>本文基于论文《RNNS可以生成具有最佳内存的有限层次化语言》，发表于EMNLP 2020，与Michael Hahn、Surya Ganguli、Percy Leung和Chris Manning联合发表。</p><p> Theoretical tl;dr: we introduce Dyck-\((k,m)\), the language of nested brackets of \(k\) types and nesting depth at most \(m\), and prove that RNNs can generate these languages in \(O(m\log k)\) hidden units, an exponential improvement over known constructions, which would use \(O(k^{m/2})\).We also prove that this is tight; using \(o(m\log k)\) is impossible.</p><p>理论tl；dr：我们引入了DYCK-((k，m))，这是一种嵌套深度至多为(M)的k型方括号语言，并证明了RNN可以在O(m\log k)个隐含单元中生成这些语言，比已有的使用O(k^{m/2})的结构有了指数的改进，我们还证明了这是紧的，使用o(m\log k)是不可能的。</p><p>  We’ll start with the second most interesting part of NLP: language  2.In human languages, the order in which words are spoken matters  [citation needed].But their order doesn’t provide the whole story off the bat; there’s rich structure in human languages that governs how the meanings of words and phrases are composed to form the meanings of larger bits of language.The insufficiency of the linear (that is, “as given”) order of language is clear due to hierarchical dependenices in language.For example, consider the following sentence:</p><p>我们将从自然语言处理的第二个最有趣的部分开始：语言2。在人类语言中，单词的发音顺序很重要[需要引用]。但它们的顺序并不能即刻提供整个故事；人类语言中有丰富的结构，它控制着单词和短语的意义是如何组成的，从而形成更多语言的意义。由于语言中的等级依赖，语言的线性(即，给定的)顺序的不足是显而易见的。例如，考虑一下下面的句子：</p><p>    Here, it’s still the chef that’s exceptional, not the stores, showing that the (chef, is) relationship is not based linear order.We can keep playing this game more or less for as long as we’d like:</p><p>在这里，例外的仍然是厨师，而不是商店，这表明(厨师，是)关系不是基于线性顺序。我们可以或多或少地继续玩这个游戏，只要我们愿意：</p><p>  The subject/verb agreement relationship in English, as with (chef, is), is a nice example of the hierarchical nature of language.More generally, the sentential hierarchical relationships of language are described in syntactic annotation, usually in the form of trees, like in the following phrases  3:</p><p>英语中的主谓一致关系，如(chef，is)，是语言等级性质的一个很好的例子。更普遍的是，语言的句子等级关系是以句法注释的形式描述的，通常是以树的形式，如下面的短语3：</p><p>  The two trees provide two distinct interpretations of the phrase  Natural Language Processing.In the leftmost tree,  language and  processing first combine. Then  language processing combines with  natural.So, it’s language processing that’s performed or characterized by its naturalness.Is this the intended reading of  NLP?I’d argue, no.In the rightmost tree, we have  natural combine with  language. then  natural language combines with  processing, meaning we’re processing this thing called  natural language.That’s more like it.</p><p>这两个树为短语自然语言处理提供了两种截然不同的解释。在最左边的树中，语言和处理首先结合在一起。然后，语言处理与自然结合。所以，语言处理是以其自然性为特征的。这是对自然语言的有意解读吗？我认为，不是。在最右边的树中，我们有自然与语言的结合。然后自然语言和处理结合在一起，这意味着我们正在处理一种叫做自然语言的东西。这更像是一种语言。</p><p> A common formalism for syntax is the  context-free grammar, both for natural language and for languages like XML.The example we gave above for the phrase  Natural Language Processing is informally in constituency form, but here’s a full example  4:</p><p>语法的一种常见形式是上下文无关文法，对于自然语言和像XML这样的语言都是如此。上面给出的短语自然语言处理(Natural Language Processing)的例子是非正式的，但这里有一个完整的例子4：</p><p>  The labels on the tree are very useful, but we won’t go into what they mean here.To see the connection between such a structure and a stack, consider a preorder traversal of the tree.</p><p>树上的标签非常有用，但我们不会深入讨论它们的含义。要查看这种结构和堆栈之间的联系，请考虑对树进行预排序遍历。</p><p> (S (NP (DT The ) (NN children)) (VP (VBD ate) (NP (DT the) (NN cake))) (PP (JJ with) (NP (DT a) (NN spoon))))</p><p>(s(np(Dt The)(NN个孩子))(VP(Vbd Ate)(np(Dt The)(nn个蛋糕)(PP(Jj With)(np(Dt A)(np(Dt A)(nn勺子)。</p><p> If one had access to this explicit linear representation of the tree, one could imagine pushing the  (S token, then the  (NP, then pushing (and popping) the  (DT and  The and  ), and so on, to keep track of what unfinished constituents were under construction.A key difficulty of processing real language, of course, is that the internal nodes (like  (S) are not provided; this is why parsing isn’t trivial for natural language.But even  with access to these internal nodes, one needs to be able to push and pop values from a stack to track syntactic structure; this is true in general, and so for the special case of RNNs.</p><p>当然，处理真实语言的一个关键困难是没有提供内部节点(如(S)；这就是为什么解析对于自然语言来说并不是微不足道的。但是，即使可以访问这些内部节点，人们也需要能够从内部节点推送和弹出值。)当然，处理真实语言的一个关键困难是没有提供内部节点(如(S)；这就是为什么解析对于自然语言来说并不是微不足道的。但是，即使访问这些内部节点，也需要能够从内部节点推送和弹出值)。当然，处理真实语言的一个关键困难是没有提供内部节点(如(S)；这就是为什么解析对于自然语言来说并不是微不足道的。但是，即使访问这些内部节点，也需要能够从内部节点推送和弹出值。一般来说，这是正确的，对于RNN的特殊情况也是如此。</p><p> So if it takes a stack to parse natural language, what are RNNs doing? Roughly, RNNs can be thought of as applying either of (or some combination of) two strategies:</p><p>那么，如果解析自然语言需要一个堆栈，那么RNN在做什么呢？粗略地说，RNN可以被认为是应用了两种策略中的一种(或某种组合)：</p><p> Surface statistics: By aggregating lots of statistics about how some words (like “the”) precede various groups of words (nouns), and what sort of nouns are likely to “eat” and such, networks may just learn a soft version of an  n-gram language model, sharing statistical strength between words and allowing for relatively large  n.</p><p>表面统计：通过汇总大量统计数据，了解一些单词(如“the”)如何出现在不同的单词(名词)组之前，以及哪些名词可能会“吃掉”等等，网络可能只是学习了一个软版本的n元语法语言模型，在单词之间共享统计强度，并允许相对较大的n。</p><p>  Stack-like behavior: By simulating a stack, RNNs might process language “the right way”, pushing and popping words or abstract elements as tokens are observed, and coming to an implicit parse of a sentence.</p><p>类似堆栈的行为：通过模拟堆栈，RNN可能会“以正确的方式”处理语言，将单词或抽象元素作为观察到的标记进行推送和弹出，并实现对句子的隐式解析。</p><p> In all likelihood, the truth is neither exactly the  surface statistics story nor exactly the  stack-like behavior story.What we’re able to show in this work, however, is that RNNs are  able to exactly simulate bounded-capacity stacks and we provide clear, exact specifications of their learnable parameters that provide that behavior.Not only this, but the stack simulation they perform is, in a strong sense, as efficient as it could possibly be (as a function of the number of hidden units needed to simulate a certain-size stack,) so such behavior isn’t just possible, it’s in the set of functions RNNs can express most efficiently.We hope that these insights guide both future empirical analyses of RNNs, as well as model-building efforts.</p><p>很可能，事实既不是表面的统计故事，也不是确切的堆栈行为故事。然而，我们在这项工作中能够展示的是，RNN能够准确地模拟有限容量的堆栈，并且我们提供了它们提供该行为的可学习参数的清晰、准确的规范。不仅如此，它们执行的堆栈模拟在很大程度上也是尽可能高效的(作为模拟特定大小的堆栈所需的隐藏单元的数量的函数)，所以这样的行为不仅仅是。这是RNN能够最有效地表达的一组功能。我们希望这些见解能够指导RNN未来的实证分析，以及建模工作。</p><p>   The computational power of neural networks depends heavily on the assumptions one makes about some “implementation details.”You may have heard, for example, that RNNs are Turing-complete!  5On the other hand, recent work states that RNNs don’t have the capacity to recognize the simplest stack-requiring languages.  6</p><p>神经网络的计算能力在很大程度上取决于人们对某些“实现细节”的假设。例如，您可能听说过RNN是图灵完全的！5另一方面，最近的研究表明RNN没有能力识别最简单的需要堆栈的语言。6个。</p><p>  These works differ in the assumptions they make about how RNNs are used. If you represent every hidden unit of an RNN by an unlimited number of bits, and allow it to read the whole input (sentence) and then unroll for as much longer as it needs to, then you arrive at the Turing-complete result.This doesn’t reflect how RNNs are used in practice, however.If you instead represent every hidden unit of an RNN by a number of bits that scales  logarithmically with the length of the input, and only unroll the RNN once per input symbol, the result is that RNNs cannot implement stacks in general.</p><p>这些工作的不同之处在于它们对如何使用RNN所做的假设。如果你用无限的位数表示RNN的每个隐藏单元，并允许它读取整个输入(句子)，然后根据需要将其展开尽可能长的时间，那么你就会得到图灵完成的结果。然而，这并不能反映RNN在实践中是如何使用的。如果你用随输入长度对数缩放的位数来表示RNN的每个隐藏单元，并且每个输入符号只展开RNN一次，那么结果就是RNN。但是，如果你用不限数量的位来表示RNN的每个隐藏单元，并且允许它读取整个输入(句子)，然后根据需要展开RNN，那么你就得到了图灵完成的结果。然而，这并不能反映RNN在实践中是如何使用的。</p><p> In our work, we consider an even more constrained setting: we assume that each hidden unit in an RNN is takes on a value specified by a finite number of bits – that doesn’t grow with the sequence length, or any measure of complexity of the language.As we’ll show, this has a pretty interesting effect on the types of properties we can study about the RNN!</p><p>在我们的工作中，我们考虑了一种更有约束的设置：我们假设RNN中的每个隐藏单元都具有一个由有限位数指定的值--该值不会随着序列长度或语言复杂性的任何度量而增长。正如我们将展示的那样，这对我们可以研究的RNN的属性类型有非常有趣的影响！</p><p> Danger: very theoretical paragraph  Finally, you’ll see us refer to most earlier work as evaluating RNNs’ ability to  recognize a language.This is formal language speak; a language is a set of strings \(\mathcal{L} \subseteq \Sigma^*\) from a vocabulary \(\Sigma\).To recognize a language \(\mathcal{L}\) means for any string, the RNN can read the whole string in \(\Sigma^*\), and at the end, produce a binary decision as to whether that string belongs to \(\mathcal{L}\).In this work, we’ll refer to RNNs as  generating a language \(\mathcal{L}\) instead.This is a new requirement we impose to make the theoretical setting more like how RNNs are used in practice: to predict an output token by token!Practically, this enforces a constraint that the hidden state of the RNN be readable at every timestep; if you’re interested in the details, look to the paper.</p><p>危险：非常理论性的段落最后，你会看到我们提到大多数早期的工作是评估RNN的识别语言的能力。这是正式的语言演讲；语言是词汇表\(\Sigma\)中的一组字符串\(\Mathcal{L}\subseteq\Sigma^*\)。为了识别任何字符串的语言\(\Mathcal{L}\)，RNN可以读取\(\Sigma^*\)中的整个字符串，并在最后对该字符串是否属于\(\Mathcal{L}\)做出二元判定。在这项工作中，RNN可以读取\(\Mathcal{L}\)中的整个字符串，并在最后产生关于该字符串是否属于\(\Mathcal{L})的二元决策。在这项工作中，我们将RNN称为生成一种语言\(\Mathcal{L}\)。这是我们强加的一个新要求，以使理论设置更像RNN在实践中的使用方式：逐个令牌预测输出令牌！实际上，这强制了一个约束，即RNN的隐藏状态在每个时间步都是可读的；如果你对细节感兴趣，可以看看报纸。</p><p>  In this section we introduce the language we’ll work with: Dyck-\((k,m)\).We start with Dyck-\(k\), the language of well-nested brackets of \(k\) types.Like how there’s hierarchical structure in language, as in this example highlighting the relationships between subjects and verbs in center-embedded clauses:</p><p>在这一节中，我们将介绍我们将要使用的语言：Dyck-\((k，m)\)。我们从Dyck-\(k\)开始，这是一种嵌套良好的\(k\)型方括号语言。类似于语言中的层次结构，如下例所示，突出了中心嵌入子句中主语和动词之间的关系：</p><p>    In fact, despite the simplicity of Dyck-\(k\), in a strong sense it’s a prototypical language of hierarchical structure.Thinking back to the  context-free languages, like those describable by constituency grammars, it turns out that Dyck-\(k\) is at the core of all of these languages.This is formalized in the  Chomsky-Schützenberger Theorem.</p><p>事实上，尽管Dyck-(k\)很简单，但在很大程度上，它是一种典型的层次化语言。回想那些上下文无关的语言，比如那些可以由选区文法描述的语言，结果发现Dyck-(k\)是所有这些语言的核心，这在Chomsky-Schützenberger定理中得到了形式化。</p><p> The connection that we’ve discussed between hierarchy and stacks is immediate in Dyck-\(k\): upon seeing an open bracket of type \(i\), memory of that bracket must be pushed onto a stack; upon seeing its corresponding close bracket of type \(i\), it is popped.</p><p>我们讨论的层次结构和堆栈之间的联系在Dyck-\(k\)中是直接的：在看到类型为\(i\)的左方括号时，必须将该方括号的内存推入堆栈；在看到相应的类型为\(i\)的右方括号时，它将被弹出。</p><p> But consider the fragment  Laws the lawmaker the reporter questions writes are, from the example above.Isn’t it a little… hard to read?If you take a second, you can work out that the reporter is questioning a lawmaker, who writes laws, and those laws are the subject of the sentence.But consider the following sentence:</p><p>但考虑到这位立法者所写的碎片化法律，从上面的例子来看，这不是有点…吗？难读？如果你花一秒钟的时间，你就会发现记者正在询问的是一名制定法律的立法者，而这些法律是判决的主题。但考虑一下下面的句子：</p><p>  I found this one easier to read.Part of the reason for this is that the first sentence has nested center-embedded clauses, which require you to keep a bunch of memory around to know how subjects pair with verbs.The second one isn’t as deeply nested; you have to keep  Laws around for a while before seeing  are, but you only need to keep two items around ( Laws,  the lawmaker ) before you see  wrote and can sort of relegate  the lawmaker to less precise memory.</p><p>我觉得这一句更容易读懂。部分原因是第一句有嵌套在中间的子句，这需要你保持大量的记忆，以了解主语和动词是如何配对的。第二句没有那么深入；在看到之前，你必须在周围保留一段时间的法律，但你只需要在周围保留两项(法律，立法者)，然后你就可以看到书面的内容，这样可以让立法者在某种程度上不那么精确地记忆。</p><p> There’s a connection between this center-embedding depth and the depth of stack you need in order to parse sentences.  7Intuitively, it’s about the amount of  memory you need in order to figure out the meaning.You only have so much working memory, and things get hard to understand when they test your limits. Dyck-\(k\) can be arbitrarily deeply nested, but human language rarely goes deeper than center-embedding depth 3.  8</p><p>这种中心嵌入深度与解析句子所需的堆栈深度之间存在联系。7实际上，你需要多少内存才能理解其中的含义。你只有那么多的工作记忆，当他们测试你的极限时，事情就很难理解了。Dyck-\(k\)可以任意深度嵌套，但人类语言的深度很少超过中心嵌入深度3.8。</p><p> So, instead of studying RNNs’ ability to generate Dyck-\(k\), we put a bound on the maximum depth of the stack of brackets, call it \(m\), and call it Dyck-\((k,m)\).If you’re a fan of the Chomsky Hierarchy (regular languages, context-free, context-senstive, recursively enumerable), this means that Dyck-\((k,m)\) is a  regular language, and so is recognizable by a finite-state machine (DFA).But it’s a regular language with special, stack-like states, like in the following example for Dyck-\((2,2)\):</p><p>因此，我们没有研究RNN生成Dyck-\(k\)的能力，而是对方括号堆栈的最大深度设置了一个界限，称之为\(M)，并称之为Dyck-((k，m)\)。如果您是Chomsky层次结构(规则语言、上下文无关、上下文敏感、递归可枚举)的粉丝，这意味着Dyck-((k，m)\)是一种规则语言，因此可以通过有限的。如下面的Dyck-\((2，2)\)示例所示：</p><p>   Under our finite-precision setting, it’s a known result that for any regular language, you can take the minimal deterministic finite state machine, and encode that machine in the RNN using finite precision.So it may seem confusing at first why we’re asking new theoretical questions about RNNs and their ability to generate Dyck-\((k,m)\).</p><p>在我们的有限精度设置下，众所周知，对于任何正则语言，都可以采用最小确定性有限状态机，并使用有限精度将其编码到RNN中。因此，一开始，我们为什么要提出关于RNN及其生成Dyck-\((k，m)\)能力的新理论问题似乎令人困惑。</p><p> It’s because using those encodings of DFAs in RNNs, we’d need about \(k^{m+1}\) hidden units in our RNNs in order to generate Dyck-\((k,m)\).This is  a lot; think about \(k=100{,}000\) as the vocabulary size and \(m=3\) as the stack depth; we’d need \(100{,}000^4=10^{20}\) hidden units!Using our construction for the LSTM, we’d need \(3m\lceil \log k \rceil -m=150\) hidden units.</p><p>这是因为在RNN中使用这些DFA编码，我们在RNN中需要大约(k^{m+1})个隐藏单元才能生成Dyck-((k，m)\)。这是很多的；考虑一下\(k=100{，}000\)作为词汇表大小，\(m=3\)作为堆栈深度；我们将需要\(100{，}000^4=10^{20})个隐藏单元！</p><p>    We’ll now go into how you could build a bounded stack in an RNN, starting with an extended model that can do things popular RNNs can’t do, for ease of explanation.Our extended model is  second-order RNNs, which can choose from recurrent matrices \(W_x\) depending on which input you see.</p><p>为了便于解释，我们现在将讨论如何在RNN中构建有界堆栈，首先是一个扩展模型，它可以做流行的RNN做不到的事情。我们的扩展模型是二阶RNN，它可以根据您看到的输入从递归矩阵\(W_x\)中进行选择。</p><p> Consider encoding a stack of up to \(m\) elements, each of dimensionality \(k\), using a \(km\)-dimensional vector:</p><p>考虑使用\(Km\)维向量对最多\(m\)个元素的堆栈进行编码，每个元素的维度为\(k\)：</p><p>  Think of each of the \(k\) possible bracket types as being represented by one of \(k\) one-hot vectors.We’d like the top of the stack to be in slot \(1\).So when we  push an element, we’ll write it directly to slot \(1\).Whatever was in slot \(1\), we want to shift away from slot \(1\) to make room, which we can do with a matrix \(W_{\text{push}}\), as follows:</p><p>将每种\(K)种可能的支架类型看作是由一个\(k\)个热向量表示的。我们希望堆栈的顶部位于插槽\(1\)中。因此，当我们推送一个元素时，我们会将其直接写入插槽\(1\)。无论插槽\(1\)中的内容是什么，我们都希望从插槽\(1\)移出以腾出空间，这可以使用一个矩阵\(W_{\text{Push}}\)来完成，如下所示：</p><p>  Notice how, if we had \(m\) elements on the stack already, the last one would be shifted off the hidden state.Next, if we’d like to  pop the top element from the stack, this is easy enough with a separate matrix \(W_{\text{pop}}\), as follows:</p><p>请注意，如果堆栈上已有\(m\)个元素，则最后一个元素将从隐藏状态移位。接下来，如果我们希望从堆栈中弹出顶部元素，使用单独的矩阵\(W_{\Text{POP}}\)可以很容易地完成此操作，如下所示：</p><p>  This works for our second-order RNN because if we see an open bracket, we can choose to apply \(W_{\text{push}}\) to make room for it at the top, and if we see a close bracket, we can choose to apply \(W_{\text{pop}}\) to forget its corresponding open bracket.</p><p>这适用于我们的二阶RNN，因为如果我们看到左方括号，我们可以选择应用\(W_{\text{ush}}\)来在顶部为其腾出空间，如果我们看到右方括号，我们可以选择应用\(W_{\text{op}}\)来忽略其相应的左方括号。</p><p> Why doesn’t this work for the Simple RNN that people use in practice?Because it only has one \(W\) matrix, and for that matter, the LSTM does as well.So we’ll need to get a bit clever.</p><p>为什么这不适用于人们在实践中使用的简单RNN？因为它只有一个\(W\)矩阵，而LSTM也是如此。所以我们需要更聪明一些。</p><p>  We’ll get around the fact that there’s only one \(W\) matrix by using a factor of \(2\) more space, that is ,\(2mk\) dimensions.</p><p>我们将通过使用一个多2个空间的因子，即2mk维，来解决只有一个W矩阵这一事实。</p><p> This corresponds to two \(mk\)-sized places where the stack could be stored.We’ll call one of them \(h_{\text{pop}}\) and the other \(h_{\text{push}}\), meaning where we write the stack when we apply a pop, and where we write the stack when we apply a push.Whichever place we  don’t write the stack we’ll make sure is empty (that is, equals 0).</p><p>这对应于可以存储堆栈的两个\(Mk)大小的位置。我们将其中一个称为\(h_{\text{POP}}\)，另一个称为\(h_{\text{Push}}\)，表示我们在应用弹出时写入堆栈，在应用推送时写入堆栈。无论在哪个位置不写入堆栈，我们都将确保为空(即等于0)。</p><p> With these guarantees, we can  read from both \(h_{\text{pop}}\) and \(h_{\text{push}}\) at once using a single \(W\) matrix.By stacking \(W_{\text{push}}\) and \(W_{\text{pop}}\) matrices, we can do this, and use the same matrix to write to both \(h_{\text{pop}}\) and \(h_{\text{push}}\) what  would happen if we saw each of pop or push:</p><p>有了这些保证，我们可以使用单个\(W\)矩阵同时读取\(h_{\text{op}}\)和\(h_{\text{ush}}\)。通过堆叠\(W_{\text{ush}}\)和\(W_{\text{op}}\)矩阵，我们可以做到这一点，并使用相同的矩阵同时写入\(h_{\text{op}}\)和\(h_{\text{op}}\)。</p><p>  So looking at \(Wh\), we’ve read from both \(h_{\text{pop}}\) and \(h_{\text{push}}\) and written to both as well.But we don’t want to write to both; we’d need only the one of \(h_{\text{pop}}\) or \(h_{\text{push}}\) that corresponds to the input we actually saw.For this we use the \(Ux_t\) term.Intuitively, when \(x_t\) is an open bracket, \(Ux_t\) adds a very negative value to \(h_{\text{pop}}\), which after the \(\sigma\) nonlinearity results in \(0\) values.When \(x_t\) is a close bracket, \(Ux_t\) does the same thing, but for \(h_{\text{push}}\).So, with \(Wh + Ux_t\), we’ve ensured that only one of \(h_{\text{pop}}\) or \(h_{\text{push}}\) is written to, and we read from both of them.</p><p>因此，查看\(wh\)，我们已经从\(h_{\text{op}}\)和\(h_{\text{ush}}\)中读取并写入这两个数据。但我们不想同时写入这两个数据；我们只需要与我们实际看到的输入相对应的\(h_{\text{POP}}\)或\(h_{\text{Push}}\)中的一个。为此，我们使用\(Ux_t\)术语。实际上，当\(x_t\)是左方括号时，\(Ux_t\)会将一个非常负值添加到\(h_{\text{op}}\)中，当x_t是右方括号时，\(Ux_t\)执行相同的操作，但对于\(h_{\text{Push}}\)。因此，对于\(Wh+Ux_t\)，我们确保只写入\(h_{\text{op}}\)或\(h_{\text{ush}}\)中的一个，并且</p><p> There you have it! It took an extra factor-\(2\) of memory, but it’s an exact way to push or pop from a bounded-depth stack in an RNN.</p><p>这就对了！它需要额外的内存系数-\(2\)，但这正是从RNN中的有限深度堆栈推送或弹出的一种方式。</p><p>  LSTMs were proposed to improve  learning, solving the vanishing gradient problem of RNNs.Again we’re considering  expressivity in this work, not learning, so we study whether the extra gating functions in LSTMs allow them to implement stacks more efficiently.</p><p>LSTM的提出是为了改进学习，解决RNN的零梯度问题。另外，我们在工作中考虑的是表现力，而不是学习，所以我们研究了LSTM中额外的门控函数是否能让它们更有效地实现堆栈。</p><p> We find that yes, though we needed \(2mk\) to implement our stack with the Simple RNN, we need \(mk\) (like for second-order RNNs) for LSTMs, a factor-two improvement.Interestingly, the mechanism by which we prove this is completely separate from the push/pop components we used in the Simple RNN; instead, it  relies entirely on the LSTM’s gates.</p><p>我们发现，是的，虽然我们需要\(2mk\)来使用简单RNN实现我们的堆栈，但是我们需要\(mk\)(就像对于二阶RNN一样)来实现LSTM，这是一个两倍的改进。有趣的是，我们用来证明这一点的机制完全独立于我们在简单RNN中使用的推/弹组件；相反，它完全依赖于LSTM的门。</p><p>  The exact mechanisms by which the constructions work are a bit involved, so we sketch out the main ideas here.Let’s start by walking through an example of how the LSTM’s memory behaves as it processes a sequence.</p><p>构造工作的确切机制有点复杂，所以我们在这里勾勒出主要思想。让我们从一个LSTM内存在处理序列时如何工作的示例开始。</p><p>  As is clear from the figure, the top of our LSTM’s stack isn’t always slot 1 like in the Simple RNN construction. Instead, the top of the stack is always the farthest non-empty slot from slot 1, and the bottom of the stack is slot 1.All brackets currently on the stack are stored in the cell state \(c_t\), but  only the top element of the stack is allowed through the output gate into the hidden state, \(h_t\).This is visualized in the above diagram as the state that is shaded grey.</p><p>从图中可以清楚地看出，我们的LSTM堆栈的顶部并不总是插槽1，就像在简单的RNN构造中那样。相反，堆栈的顶部始终是距离插槽1最远的非空槽，堆栈的底部是插槽1。当前堆栈上的所有托架都存储在单元状态\(c_t\)中，但只有堆栈的顶部元素被允许通过输出门进入隐藏状态\(h_t\)。这在上图中显示为灰色阴影状态。</p><p> Recall the LSTM equations, or skip past them if you’d prefer, and remember that the LSTM has an  input gate, an  output gate, a  forget gate, and a  new cell candidate:</p><p>回想一下LSTM等式，如果愿意也可以跳过它们，记住LSTM有一个输入门、一个输出门、一个遗忘门和一个新的单元候选：</p><p> $$i_t = \sigma(W_ih_{t-1} + U_ix_t + b_i)$$$$o_t = \sigma(W_oh_{t-1} + U_ox_t + b_o)$$$$f_t = \sigma(W_fh_{t-1} + U_fx_t + b_f)$$$$\tilde{c}_t = \text{tanh}( W_{\tilde{c}}h_{t-1} + U_{\tilde{c}}x_t + b_{\tilde{c}})$$$$c_t = i_t \cdot \tilde{c}_t + f_t \cdot c_{t-1}$$$$h_{t} = o_t \cdot \text{tanh}(c_t)$$</p><p>$$i_t=\sigma(W_ih_{t-1}+U_ix_t+b_i)$o=\sigma(W_oh_{t-1}+U_ox_t+b_o)$f_t=\sigma(W_fh_{t-1}+U_fx_t+b_f)$\tilde{c}_t=\text{tanh}(W_{。}}x_t+b_{\tilde{c}})$c_t=i_t\cot\tilde{c}_t+f_t\cot c_{t-1}$h_{t}=o_t\cot\text{tanh}(C_T)$$。</p><p> Intuitively, the input gate determines what new content (from \(\tilde{c}_t\)) is added to the memory cell \(c_t\), and what old content (from \(c_{t-1}\)) is copied into the memory cell \(c_t\).The output gate \(o_t\) determines what information flows from the cell \(c_t\) to the hidden state \(h_t\).</p><p>直观地，输入门确定将哪些新内容(来自{c}t\)添加到存储单元\(c_t\)，以及将哪些旧内容(来自\(c_{t-1}))复制到存储单元\(c_t\)。输出门\(o_t\)确定哪些信息从单元\(c_t\)流向隐藏状态\(h_t\)。</p><p> Let’s go through how each of the gates contributes to the memory management that we saw in our example.We’ll discuss the behavior of each gate under each of the two stack operations: push and pop.</p><p>让我们来看看每个门如何对我们在示例中看到的内存管理做出贡献。我们将讨论每个门在两种堆栈操作(Push和Pop)下的行为。</p><p>  To implement a  push, the new cell candidate \(\tilde{c}_t\) has the job of attempting to write an open bracket to the top of the stack.This is a bit difficult because, as we’ve said, the location where the new top of the stack should be written could be at any of the \(m\) slots of the memory.Thus, the new cell candidate, through \(U_{\tilde{c}_t}x_t\) actually tries to write the identity of the bracket specified by \(x_t\) to  all m stack slots.This works because the new cell candidate relies on the input gate being \(1\) only for the stack slot where the new element should be written, and \(0\) elsewhere, so all other slots are unaffecte</p><p>为了实现推送，新的单元候选单元\(\tilde{c}_t\)的工作是尝试向堆栈顶部写入左方括号。这有点困难，因为正如我们所说的，应该写入新的堆栈顶部的位置可能在存储器的任何\(m\)个槽上。因此，新的单元候选单元，通过\(U_{\tilde{c}_t}x_t\)实际上尝试将\(x_t\)指定的方括号的标识写入所有m个堆栈槽。这之所以有效，是因为新的候选单元格依赖于\(1\)仅用于应写入新元素的堆栈槽的\(1\)输入门，以及\(0\)其他位置的输入门，因此所有其他槽都不受影响。</p><p>......</p><p>.</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://nlp.stanford.edu/~johnhew/rnns-hierarchy.html">https://nlp.stanford.edu/~johnhew/rnns-hierarchy.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/句法/">#句法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/syntactic/">#syntactic</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/rnn/">#rnn</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>