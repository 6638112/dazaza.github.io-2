<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Kafka API很棒； 现在让我们快速 The Kafka API is great; now let's make it fast</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">The Kafka API is great; now let's make it fast<br/>Kafka API很棒； 现在让我们快速 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-24 21:27:30</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/f84a708f828fff14f092a76e6ba324d4.jpg"><img src="http://img2.diglog.com/img/2021/2/f84a708f828fff14f092a76e6ba324d4.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This blog post represents months of work and over 400+ hours of actualbenchmarking where we compared Redpandaand the latest 2.7 Kafka Release. We used the recommended productionsetup and environment from Confluent’s forkof the CNCF open messaging benchmark.</p><p>这篇博客文章代表了几个月的工作和超过400个小时的实际基准测试，我们将Redpanda和最新的2.7 Kafka版本进行了比较。我们使用了Confluent在CNCF开放式消息传递基准测试中的推荐的生产设置和环境。</p><p> Before we get started, the entirety of the actual results with the fullworkload distribution, saturation, latency and throughput are at the bottomof this interactive  blog post.</p><p> 在开始之前，完整的实际结果以及完整的工作负载分布，饱和度，延迟和吞吐量位于此交互式博客文章的底部。</p><p>  The  Kafka APIhas emerged as the lingua franca for streaming workloads. Developerslove the ecosystem and being able to turn sophisticated productsovernight. Just like Apache and Nginx have their own HTTPimplementations, gccgo and the Go compiler specify parsers for thelanguage, MySQL and Postgres implement SQL, Redpanda and Kafka implementthe Kafka API. Redpanda aims to bring operational simplicity to theexisting overwhelming complexity of standing up state-of-the-artstreaming systems. This manifests at its lowest level in no longerhaving to choose between data safety and performance.</p><p>  Kafka API已成为流工作负载的通用语言。开发人员喜欢生态系统，并且能够在一夜之间转换复杂的产品。就像Apache和Nginx有自己的HTTP实现一样，gccgo和Go编译器为语言指定了解析器，MySQL和Postgres实现了SQL，Redpanda和Kafka实现了Kafka API。 Redpanda旨在为现有的最先进系统的现有压倒性复杂性带来操作简便性。这体现在最低水平，不再需要在数据安全性和性能之间进行选择。</p><p> Let’s be clear, the reason tail latency matters in the world of big datais because Redpanda does not exist in isolation. It often sits betweenyour web servers, databases, internal microservices, data lakes, etc.Redpanda controls the information flow of how, when and where things arestored, transferred, accessed, mutated and eventually delivered. Thereason we obsess over tail latency is because the p99.99 in a messagingsystem happens often - it’s a simple function of the messages exchanged.As the volume of interactions and messages between systems using theKafka API increases, so does the probability that a single useroperation or API call is affected by latencies  above the 99.99thpercentile.</p><p> 让我们清楚一点，在大数据世界中尾部延迟很重要的原因是因为Redpanda并不是孤立存在的。它通常位于您的Web服务器，数据库，内部微服务，数据湖等之间。Redpanda控制着如何，何时何地存储，传输，访问，变异和最终交付事物的信息流。我们之所以担心尾部延迟，是因为消息传递系统中的p99.99经常发生-这是交换消息的简单功能。随着使用Kafka API的系统之间的交互和消息的数量增加，单个用户操作或API调用受99.99thpercentile以上的延迟影响。</p><p>   We present 18 workloads below. All workloads for both systems replicatethe data to 3 nodes in total with no lag (manually verified in thequorum system). The only difference is that Apache Kafka was run within-memory replication (using the page-cache) and flushing after everymessage. Redpanda can only be operated in safe mode (flushing afterevery batch) with acks=all. The benchmarks below are the same benchmarks  Alok Nikhil &amp; Vinoth Chandar fromConfluentperformed while comparing Pulsar and Kafka with a 1.2GB/s extension,using the same CNCF Open Messaging Benchmark suite.</p><p>   我们在下面介绍18个工作负载。两个系统的所有工作负载均将数据复制到3个节点，没有任何延迟（在仲裁系统中手动验证）。唯一的区别是Apache Kafka在内存内复制（使用页面缓存）中运行，并在每个消息后刷新。 Redpanda仅可在acks = all的安全模式下（每批后冲洗）运行。下面的基准与Alok Nikhil＆amp; Confluent的Vinoth Chandar使用相同的CNCF Open Messaging Benchmark套件将Pulsar和Kafka扩展速度为1.2GB / s进行了比较。</p><p> First, we note that we were only able to reproduce Confluent’s first 6results. For the other three workloads, the data shows that it is infact impossible to achieve sustained network throughput above 300MB/s onAWS on i3en.2xlarge instances. Please see our Benchmark Appendix sectionat the end for a detailed explanation. We also change the default 1minute warmup time to 30 minutes to account for   commonpitfallsin   Virtual Machinebenchmarking practices and focus entirely on steady state performance.We then ran each workload for 60 additional minutes, recorded theresults and repeated the steps, taking the best run of each system. Thatis, each time we ran the benchmarks it took over 54 hours to finish.</p><p> 首先，我们注意到我们只能复制Confluent的前6个结果。对于其他三个工作负载，数据显示，在i3en.2xlarge实例上的AWS上，要实现300MB / s以上的持续网络吞吐量实际上是不可能的。有关详细说明，请参见最后的“基准附录”部分。我们还将默认的1分钟预热时间更改为30分钟，以解决虚拟机基准测试实践中的常见缺陷，并完全专注于稳定状态性能，然后将每个工作负载再运行60分钟，记录结果并重复步骤，以使每个系统都能最佳运行。也就是说，每次我们运行基准测试时，都要花54个小时以上才能完成。</p><p> For all workloads, we used two m5n.8xlarge for the clients, with32-cores and with 25Gbps of guaranteed networking throughputand 128GB of memory to ensure the bottleneck would be on the serverside. The benchmark used three i3en.6xlarge 24-core instances with192GB of memory, 25Gbps guaranteed networking and two NVMe SSD devices.</p><p> 对于所有工作负载，我们为客户端使用了两个m5n.8xlarge，具有32核，并具有25Gbps的保证网络吞吐量和128GB内存，以确保瓶颈在服务器端。基准测试使用了三个i3en.6xlarge 24核心实例，这些实例具有192GB内存，25Gbps保证网络连接和两个NVMe SSD设备。 </p><p> We note that after spending several hundreds of hours benchmarks, we hadto scale up Confluent’s Kafka settings to keep up with larger instancesto num.replica.fetchers=16, message.max.bytes=10485760,replica.fetch.max.bytes=10485760, num.network.threads=16,num.io.threads=16, log.flush.interval.messages=1. Otherwise, the gapbetween Redpanda and Kafka would be much larger. This had theunfortunate effect that for lower percentiles, Kafka’s latency was alittle higher than using half as many threads as specified byConfluent’s Github repo.</p><p>我们注意到经过几百小时的基准测试，我们占用了Confluent的Kafka设置，以跟上较大的instancesto num.replica.fetchers = 16，message.max.bytes = 10485760，supplica.fetch.max.bytes = 10485760， num.threads = 16，num.io.threads = 16，log.flush.interval.messages = 1。否则，Redpanda和Kafka的Gapbetween将要大得多。这具有艰的效应，对于较低百分位，Kafka的延迟比使用一半的线程，以便使用指定的Github Repo指定的一半。</p><p> All the latencies below are the end-to-end p99.999 latency with 16producers and 16 consumers with 100 partitions on a single topic. Everymessage represents 1KB of data. We note that by and large Kafka is ableto keep up on throughput except for a couple of workloads with acks=allwhere Redpanda is better. The meaningful differences are in latency: howfast can each system go.</p><p> 下面的所有延迟都是端到端P99.999延迟，16个Producers和16个消费者在一个主题上有100个分区。 EveryMessage代表1KB的数据。我们注意到，By和大Kafka是Ableto，除了几个与Acks = Allwhere Redpanda更好的工作负载之外。有意义的差异在延迟：每个系统都可以去。</p><p>    In memory replication for Kafka (page cache &amp; no explicit flushes) vs.  Redpanda fsync()-ing after every batch</p><p>    在Kafka的内存复制中（页面缓存＆amp;没有显式刷新）vs. redpanda fsync（） - 每批次后</p><p>   We’ve said this ad nauseam:   hardware is theplatform.Modern hardware is capable of giving you both low latency and no dataloss (fsync). Let’s talk about most of the meaningful low-levelarchitectural differences that get us there</p><p>   我们已经说过这个adauseam：硬件是theplatform.modern硬件能够为您提供低延迟和没有数据档（fsync）。让我们谈谈大部分有意义的低级建筑差异，让我们在那里</p><p>  Before we dive deep into details, we encourage you to sign up for ourlive Twitch Stream where we’ll be going through every single claim madein this article, and are happy to answer your questions live. It will beme, emacs, you and questions. Perfect.</p><p>  在我们深入了解详细信息之前，我们鼓励您注册我们的Twitch Stream，我们将通过每一个索赔制作本文，并且很乐意回答您的问题。它会备注，emacs，你和问题。完美的。</p><p> When we started building Redpanda, the main driving factor was understandability. Above performance, we wanted a simple mentalmodel of what it meant to have 2 out of 3 replicas up and running, whichis how we ended with Raft - a protocol with a mathematical proof ofcorrectness &amp; log completeness and a focus on usability as part of itsdesign goals.</p><p> 当我们开始建立Redpanda时，主要的驱动因子是可理解的。上面的性能，我们想要一个简单的心理模型，它意味着在3次复制品中有2个，我们如何用筏结束 - 一种具有数学证明的协议＆amp;日志完整性和重点关注可用性，作为其指定目标的一部分。</p><p> However, once you get your replication model set, the rest of the lifeof the product is spent on predictability, and for big-data &amp; real timesystems, that means understandable, flat tail latencies. It is notenough to be fast. It is not enough to be safe. When trying to handlehundreds of terabytes per day of streaming you need to be predictablenot only in the way the product breaks in case of network partitions,bad disks, etc, but also in how performance degrades as a function ofhardware saturation. This is at the core of operational simplicity forstreaming systems.</p><p> 但是，一旦您获得复制模型集，产品的其余部分都花在了可预测性上，以及大数据和amp;实时系统，这意味着可以理解，扁平的尾随延迟。它是快速的。这是不够的安全性。当尝试使用每天流的Terabytes时，您只需在网络分区，磁盘等的情况下的产品中断的方式中只需要预测，而且还要在性能如何降低作为函数饱和度的函数。这是在运行简单的系统的核心。 </p><p> Modern hardware allows us to finally have nice things. It is not thecase anymore that you have to choose between safety (no data loss) andspeed (low latency). Furthermore, this predictability affords youaccurate planning for product launches. As a user, I understand how tobuy hardware. I will perform an `fio` test and have a decentunderstanding of what that specific hardware can do. Redpanda lets youtake these hardware saturation numbers and gives you a reasonable chanceof predicting how costly it is to develop a new product.</p><p>现代硬件让我们终于有好东西。它不再是您必须在安全性（无数据丢失）andspeed（低延迟）之间进行选择。此外，这种可预测性为产品发布提供了您准确的规划。作为用户，我了解如何运输硬件。我将执行一个“FIO`测试”，并有一个特定硬件可以做的事情的折磨。 RedPanda让您提供这些硬件饱和度号，并为您提供了一个合理的机会，预测开发新产品的成本如何。</p><p>   The page cache is an object in the Linux Kernel. It is maintained perfile with global locking semantics. It is a tried and true, genericscheduling mechanism with heuristics from a variety of production usecases that push and pull the design to a really good middle ground. Itaims to never be a bad choice if you need to do IO. However, for ourspecific use case - a log - we can do much better. We understandexactly, all the way to the user application, how much data is going tobe needed next, the access patterns which mostly move forward, updatefrequency, background operations and cache prioritization.</p><p>   页面缓存是Linux内核中的对象。它与全球锁定语义保持繁华。这是一种尝试和真实的，具有来自各种生产的启发式的易懂机制，该机制推动和将设计推向一个非常好的中间地面。如果你需要做io，那就不能成为一个糟糕的选择。但是，对于我们特种用例 - 日志 - 我们可以做得更好。我们一直到用户应用程序的解释，下一步需要多少数据，其中大多前进，更新频道，后台操作和高速缓存优先级的访问模式。</p><p> For us, the page cache introduces latency and nondeterministic IObehavior. For example, when loading data for a Kafka fetch request theLinux Kernel will trigger general-purpose read-ahead heuristics, andcache the bytes it read, take a global lock, and update indexes.Redpanda does not do general IO. It is a log, append only, with wellunderstood access patterns. We add data to the end file and haveaggressive write-behind strategies. When we read data, Redpanda reads inorder, which means we can in theory have perfect read-ahead and objectmaterialization that sits above the byte array style API of the pagecache, etc.</p><p> 对我们来说，页面缓存引入了延迟和非法确定的iobehavior。例如，当加载Kafka Fetch请求的数据时，Thelinux内核将触发通用读取的启发式，AndCache它读取的字节，拍摄全局锁定，并更新indexes.redpanda不进行常规IO。它是一个日志，仅附加，具有Wellunderstood访问模式。我们将数据添加到终端文件并收缩写作背后的策略。当我们读取数据时，RedPanda读取InOrder，这意味着我们可以在理论上具有完美的读取前方和objerMaterialization，它位于PageCache等字节数组样式API之上。</p><p> More fundamentally, bypassing the Kernel’s page cache allows us to bepredictable, with respect to both failure semantics and tail latency. Wecan detect and measure the rate and latency of IO and adjust our bufferpools accordingly. We can react to low memory pressure situations andhave a holistic view of our memory footprint. We have predictabilityover each filesystem operation that can actually affect correctness - asrecently evidenced by the PostgreSQL team with an fsync() bug that was undetected for 20 years.</p><p> 更基本上，绕过内核的页面缓存允许我们对故障语义和尾部延迟进行遗嘱。 WeCan检测并测量IO的速率和延迟并相应地调整我们的缓冲池。我们可以对低记忆压力情况做出反应，并且了解我们的存储器足迹的整体视图。我们有可预测的是每个文件系统操作，该文件实际上可以影响正确性 -  PostgreSQL团队与未检测到20年未被发现的FSYNC（）错误所证明的。</p><p>  The general-purpose heuristics are a lifetime of heuristics aggregatedby programmers over decades of experience with production systems fordoing IO that permeates almost every layer of the Kernel. In addition tobypassing the page cache,   Redpandacomes with a bundled in auto tuner,   rpk,that turns your regular linux box into an appliance by:</p><p>  通用启发式机构是一生的启发式聚合比编程师，几十年的生产系统经验，用于渗透到几乎每层内核的IO。此外，还有页面缓存，带有捆绑在自动调谐器中的RedPandacomes，RPK，将常规Linux框转换为设备：</p><p> Disabling the Linux Block-IO automatic merging of IO by setting  nomergesin /sys/block/*, disabling expensive checks, but moreimportantly, giving Redpanda deterministic memory footprint duringIO.</p><p> 通过设置Nomergesin / Sys / Block / *禁用IO的Linux Block-IO自动合并，禁用昂贵的检查，但对Redpanda确定型存储器占用期间，请讨价还价。</p><p>  Enforcing interrupt affinity for I/O, so that the kernel notifiesthe core that initially dispatched the request.</p><p>  对I / O执行中断关联，使得最初派出请求的内核不足的核心。 </p><p> These settings are especially useful in NVMe SSD devices that can haveup to 64K queues and each queue up to 64K slots. However, while thesesettings do provide anywhere from 10-30% improvement, the materialimprovements come from our architecture.</p><p>这些设置在NVMe SSD设备中特别有用，该设备最多可以有64K队列，每个队列最多有64K插槽。但是，尽管这些设置确实可以提供10％到30％的改进，但是实质性的改进来自于我们的体系结构。</p><p>  First, our   adaptivefallocationamortizes file metadata contention in the Linux Kernel (global operationfor updating the  size). Reducing metadata contention by debouncingfile size updates gives Redpanda a 20%+ performance improvement on taillatencies when nearing disk ops saturation. Instead of mutating a globalobject with every file write, we simply tell the kernel to give us a fewmore chunks of data and associate them with a particular file descriptorbecause we know that we are about to write them. So now when we appenddata to our log, there is no global metadata update.</p><p>  首先，我们的自适应分配在Linux内核（用于更新大小的全局操作）中分摊文件元数据争用。通过消除文件大小更新来减少元数据争用，当磁盘操作接近饱和时，Redpanda的拖尾延迟性能提高了20％以上。与其在每次写入文件时都改变全局对象，不如说让内核给我们更多的数据块并将它们与特定的文件描述符相关联，因为我们知道我们将要写入它们。因此，现在当我们将数据追加到日志中时，没有全局元数据更新。</p><p>  if  ( next_committed_offset ( )  + n  &gt; _fallocation_offset )  {  return  do_next_adaptive_fallocation ( ) . then (  [ this , buf , n ]  {  return  do_append (buf , n ) ;  } ) ;  }</p><p>  if（next_committed_offset（）+ n＆gt; _fallocation_offset）{返回do_next_adaptive_fallocation（）。然后（[this，buf，n] {return do_append（buf，n）;}）; }</p><p>  As we’ve  written before,our log segment writers use Direct Memory Access (DMA) which means thatwe manually align memory according to the layout of the filesystem andspend a great deal of effort ensuring that we flush as little as we canto the actual physical device, and instead try to spend as much time aspossible doing dma_write() to reduce serialization points.</p><p>  如我们之前所写，我们的日志段编写器使用直接内存访问（DMA），这意味着我们根据文件系统的布局手动对齐内存，并花费了大量的精力来确保尽可能少地刷新到实际的物理设备，并尝试花费尽可能多的时间进行dma_write（）来减少序列化点。</p><p> While streaming reads from the  append_entries() Raft RPC, wecontinuously dispatch the current buffer out of a pool of allocatedchunks with a  thread-local buffer pool of filesystem alignedbuffers. This is a fundamental departure from the page cache, not onlybecause we simply don’t use it but because we control exactly how muchmemory is allocated on a thread local pool, and used as a sharedresource between all the open file handles. This means that the mostdemanding partitions are allowed to borrow the most number of buffers incase of spikes.</p><p> 当流从append_entries（）Raft RPC读取流时，我们使用文件系统alignedbuffers的线程本地缓冲池从分配的内存池中连续调度当前缓冲区。这是与页面缓存的根本区别，不仅因为我们根本不使用它，还因为我们可以精确控制在线程本地池上分配了多少内存，并将其用作所有打开文件句柄之间的共享资源。这意味着在出现尖峰的情况下，允许最苛刻的分区借用最多数量的缓冲区。</p><p>   This technique was really inspired by instruction pipelining techniques.To us mere mortals, the hardware guys have figured things out and whenpossible, we borrow pages from their book :) When decoding a set ofoperations for Raft, we artificially debounce the writes for a couple ofmilliseconds. This allows us to skip many fdatasync() operations, savingus not only from using less IOPs but from introducing global barriers onthe file - flushes prevent any other writes in the background as fsync()is a point of synchronization.</p><p>   这项技术的真正灵感来自于指令流水线技术。对于我们这些凡人，硬件专家已经弄清楚了，在可能的情况下，我们从他们的书中借书了几页：）在为Raft解码一组操作时，我们人为地将写操作反跳了几毫秒。 。这使我们可以跳过许多fdatasync（）操作，不仅节省了使用较少的IOP的时间，而且还避免了在文件上引入全局障碍-刷新防止在后台进行任何其他写操作，因为fsync（）是同步点。</p><p>   Pipelining, batching, write-behind, out-of-order execution, Kerneltuning are the foundations for the material performance gains in thecontext of a   thread-per-corearchitecture in Redpanda.We haven’t even scratched the surface on our deep on-disk read-aheadstrategies, write-behind buffering, fragmented-buffer parsing, integerdecoding, streaming compressors, impact of cache-friendly look-upstructures, etc.</p><p>   管道，批处理，后写，无序执行，内核调整是Redpanda中每核心线程架构的情况下实质性性能提升的基础。我们甚至还没有在磁盘上深入研究过预读策略，后写缓冲，分段缓冲区解析，整数解码，流压缩器，对缓存友好的查找结构的影响等。 </p><p> We have given a brief exposé of the fundamental techniques to leveragemodern hardware Redpanda uses, and, if you have made it this far, youmight want to join us for our live Twitch stream where we’ll be goingover these benchmarks. If you want a live chat, please join our  slack and say hi.</p><p>我们已经简要介绍了利用Redpanda使用的现代硬件的基本技术，并且，如果您到目前为止已经做到这一点，则可能希望加入我们的实时Twitch流中，我们将通过这些基准测试。如果您想进行实时聊天，请加入我们的团队，打个招呼。</p><p>  We empirically show that it is impossible to deliver on a 3rd of theworkloads that exceed 300MB/s of sustained load on i3en.2xlargeinstances in AWS. We ran the 500MB/s and 1GB/s workloads 4 differenttimes on the original i3en.2xlarge instances as specified by theoriginal set of benchmarks, with the original settings produced byConfluent, and we were unable to produce more than 300MB/s consistently.It is impossible to achieve a steady state of anything above 300MB/s notbecause of any software limitation but because the underlying AWS fabriccaps you at around 300MB/s on these instance types.</p><p>  我们凭经验显示，在AWS中的i3en.2xlargeinstances上无法交付超过持续负载300MB / s的第3个工作负载。我们在原始i3en.2xlarge实例上运行了4次不同的500MB / s和1GB / s工作负载，这些实例是原始基准测试集指定的，而原始设置是由Confluent产生的，因此我们无法持续产生300MB / s以上的速度。不可能由于任何软件限制而无法达到高于300MB / s的稳定状态，而是因为在这些实例类型上基础AWS架构将您的速度限制在300MB / s左右。</p><p> We increased the workloads benchmark times because we could not getlatency stability below 30 minutes warm up with Kafka and we wanted toshowcase only steady state performance for both systems.</p><p> 我们增加了工作负载基准测试时间，因为在使用Kafka进行30分钟的预热后，我们无法获得延迟稳定性，并且我们仅想展示两个系统的稳态性能。</p><p>  For smaller test duration, we show that you can achieve the 1GB/sthroughput as stated on the original benchmarks with a dramatic cliffdrop to 300MB/s.</p><p>  对于较小的测试持续时间，我们表明您可以达到原始基准所规定的1GB /吞吐量，并且急剧下降到300MB / s。</p><p>  Additionally, and as mentioned in passing on the introduction, we werenot able to saturate higher boxes with the default number of IO threads(8). We ran these benchmarks for 100 hours with 8 threads and we werereliably unable to saturate at the 1GB/s and 1.25GB/s throughputs. Wehad to increase the number of threads to 16 which caused Kafka’s averagelatency to increase a little bit, but we finally managed to get Kafka tosaturate the hardware in most cases for throughput. Please see theinteractive charts below.</p><p>  此外，正如在引言部分中提到的那样，我们无法使用默认数量的IO线程来饱和更高的盒子（8）。我们使用8个线程将这些基准测试运行了100个小时，我们无法在1GB / s和1.25GB / s的吞吐量下达到饱和。我们不得不将线程数增加到16个，这导致Kafka的平均延迟有所增加，但是我们最终设法使Kafka在大多数情况下使硬件饱和以提高吞吐量。请参阅下面的交互式图表。</p><p> The last significant change on the open-messaging benchmarks was toincrease the number of consumers and producers to 16 which technicallyquadruples the original number of 4. We could not reliably produce andconsume enough data from the benchmarks otherwise. 16 was the magicnumber that allowed us to get steady state performance on the clientside.</p><p> 开放消息基准的最后一个重大变化是将消费者和生产者的数量增加到16，这在技术上是原始数量4的四倍。否则，我们将无法可靠地产生和消费来自基准的足够数据。 16是不可思议的数字，它使我们能够在客户端获得稳定的性能。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://vectorized.io/blog/fast-and-safe/">https://vectorized.io/blog/fast-and-safe/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/kafka/">#kafka</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/api/">#api</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>