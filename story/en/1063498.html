<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>神经演员：具有姿势控制的神经自由观察人体演员的合成 Neural Actor: Neural Free-View Synthesis of Human Actors with Pose Control</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Neural Actor: Neural Free-View Synthesis of Human Actors with Pose Control<br/>神经演员：具有姿势控制的神经自由观察人体演员的合成 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-04 21:13:53</div><div class="page_narrow text-break page_content"><p>We propose Neural Actor (NA), a new method for high-quality synthesis of humans from arbitrary viewpoints and under arbitrary controllable poses. Our method is built upon recent neural scene representation and rendering works which learn representations of geometry and appearance from only 2D images. While existing works demonstrated compelling rendering of static scenes and playback of dynamic scenes, photo-realistic reconstruction and rendering of humans with neural implicit methods, in particular under user-controlled novel poses, is still difficult. To address this problem, we utilize a coarse body model as the proxy to unwarp the surrounding 3D space into a canonical pose. A neural radiance field learns pose-dependent geometric deformations and pose- and view-dependent appearance effects in the canonical space from multi-view video input. To synthesize novel views of high fidelity dynamic geometry and appearance, we leverage 2D texture maps defined on the body model as latent variables for predicting residual deformations and the dynamic appearance. Experiments demonstrate that our method achieves better quality than the state-of-the-arts on playback as well as novel pose synthesis, and can even generalize well to new poses that starkly differ from the training poses. Furthermore, our method also supports body shape control of the synthesized results.</p><p>我们提出了神经演员（NA），从任意观点和任意可控姿势的高质量合成人类的新方法。我们的方法是基于最近的神经场景表示和渲染工作，从而从仅从2D图像学习几何形状和外观的表示。虽然现有的作品令人兴奋的静态场景渲染和动态场景的播放，具有神经隐含方法的照片 - 现实重建和人类的渲染，特别是在用户控制的新颖姿势下，仍然很困难。为了解决这个问题，我们利用一个粗体模型作为将周围的3D空间留到典型姿势的代理。神经辐射场从多视图视频输入中了解在规范空间中的姿势相关的几何变形和姿势和视图相关的外观效果。为了综合高保真动态几何和外观的新颖视图，我们利用身体模型上定义的2D纹理地图作为预测残余变形和动态外观的潜在变量。实验表明，我们的方法能够比播放的最先进以及新颖的姿势合成来实现更好的质量，并且甚至可以概括到新的姿势与训练姿势不同。此外，我们的方法还支持合成结果的体形控制。</p><p>    We present Neural Actor, a new method for free-view synthesis of human actors which allows arbitrary pose control. Given a sequence of driving poses as well as a virtual camera as input, our method can synthesize realistic animations of the actor with pose- and view-dependent dynamic appearance, sharp features and high-fidelity winkle patterns. We can freely change the viewpoint for rendering.</p><p>    我们展示了神经演员，一种用于自由观看人体参与者的新方法，允许任意姿势控制。考虑到一系列驱动姿势以及虚拟摄像头作为输入，我们的方法可以用姿势和视图依赖动态外观，尖锐的特征和高保真湿润模式来综合演员的现实动画。我们可以自由更改渲染的观点。</p><p>              Neural Actor generalizes well to unseen poses which starkly differ from the ones in the training. Here, we show a challenging dancing performance from the AIST dataset, which was not seen during training. Note that our method also generalizes well to novel views.</p><p>              神经演员概括到看不见的姿势，与训练中的那些毫无意义。在这里，我们从训练期间展示了来自AIST DataSet的具有挑战性的舞蹈表演。请注意，我们的方法还概括为新颖的观点。</p><p>        Since our method only requires a skeletal pose, we can apply the same pose to different people allowing effects like synchronous crowd dancing.</p><p>        由于我们的方法只需要一个骨骼姿势，我们可以向不同的人施加同样的姿势，允许同步人群跳舞的效果。</p><p>  Given a pose, we synthesize images by sampling points along camera rays near the posed SMPL mesh. For each sampled point $x$, we assign to it the skinning weights of its nearest surface point and predict a residual deformation to transform $x$ to the canonical space. We then learn the radiance field in the canonical pose space to predict the color and density for $x$ using multi-view 2D supervision. The pose-dependent residual deformation and color are predicted from the local coordinate of $x$ along with the latent variables extracted from a 2D texture map of the nearest surface point of $x$. At training time, we use the ground truth texture map generated from multi-view training images to extract latent variables. At test time, the texture map is predicted from the normal map, which is extracted from the posed SMPL mesh via an image translation network, which is trained separately with the ground truth texture map as supervision.</p><p>  给定姿势，我们通过沿着所构成的SMPL网格附近的相机光线采样点来综合图像。对于每次采样点$ x $，我们将其分配到其最近的表面点的剥皮权重，并预测剩余变形以将$ x $转换为规范空间。然后，我们学习规范姿势空间中的辐射场，以预测使用多视图2D监控的$ x $的颜色和密度。从$ x $的局部坐标预测姿势相关的残余变形和颜色以及从最近的$ x $的2d纹理映射提取的潜在变量。在培训时间，我们使用从多视图训练图像生成的地面真相纹理映射来提取潜在变量。在测试时间时，从正常地图预测纹理地图，该法线地图通过图像转换网络从构成的SMPL网格中提取，该图像转换网络与地面真理纹理图分开训练作为监督。</p><p>  @misc{liu2021neural, title={Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control},  author={Lingjie Liu and Marc Habermann and Viktor Rudnev and Kripasindhu Sarkar and Jiatao Gu and Christian Theobalt}, year={2021}, eprint={2106.02019}, archivePrefix={arXiv}, primaryClass={cs.CV}}</p><p>  @misc {liu2021neural， 标题= {神经演员：用姿势控制}的神经自由观点综合人体演员}， 作者= {Lingjie Liu和Marc Habermann和Viktor Rudnev和Kripasindhu Sarkar和Jigao Gu和Christian Theobalt}， 年= {2021}， ePrint = {2106.02019}， ArchivePrefix = {ARXIV}， PrimaryClass = {CS.cv}} </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="http://gvv.mpi-inf.mpg.de/projects/NeuralActor/">http://gvv.mpi-inf.mpg.de/projects/NeuralActor/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/演员/">#演员</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/姿势/">#姿势</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>