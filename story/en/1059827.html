<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>一路解析：写一个自主托管的解析器 Parsers all the way down: writing a self-hosting parser</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Parsers all the way down: writing a self-hosting parser<br/>一路解析：写一个自主托管的解析器 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-23 04:27:25</div><div class="page_narrow text-break page_content"><p>One of the things we’re working on in  my new programming language is aself-hosting compiler. Having a self-hosted compiler is a critical step in thedevelopment of (some) programming languages: it signals that the language ismature enough to be comfortably used to implement itself. While this isn’t rightfor some languages (e.g. shell scripts), for a systems programming language likeours, this is a crucial step in our bootstrapping plan. Our self-hosted parserdesign was completed this week, and today I’ll share some details about how itworks and how it came to be.</p><p>我们在新的编程语言中致力于托管的内容之一是托管编译器。拥有自主主机编译器是（某些）编程语言的关键步骤：它表示语言足够舒适地舒适地实现自己。虽然这不是正确的语言（例如shell脚本），但对于Systems编程语言，这是我们的引导计划中的重要步骤。我们本周完成了自托管的ParserDesign，今天我将分享有关ItWorks如何以及它如何成为的细节。</p><p> This is the third parser which has been implemented for this language. We wrotea sacrificial compiler prototype upfront to help inform the language design, andthat first compiler used  yacc for its parser. Using yacc was helpful atfirst because it makes it reasonably simple to iterate on the parser when thelanguage is still undergoing frequent and far-reaching design changes. Anothernice side-effect starting with a yacc parser is that it makes it quite easy toproduce a formal grammar when you settle on the design. Here’s a peek at some ofour original parser code:</p><p> 这是为此语言实施的第三解析器。我们wrotea牺牲编译器原型预先帮助通知语言设计，第一个编译器为其解析器使用了yacc。使用YACC有助于ATFirst，因为当TheLanguage仍然经常频繁和深远的设计变化时，它使它在解析器上迭代的价格合理简单。另一个以YACC解析器开始的副作用是，当您在设计上时，它使得它非常容易表达正式的语法。这是一些Oure Oround Parser代码的偷看：</p><p> struct_type	: T_STRUCT &#39;{&#39; struct_fields &#39;}&#39; {		$$.flags = 0;		$$.storage = TYPE_STRUCT;		allocfrom((void **)&amp;$$.fields, &amp;$3, sizeof($3));	}	| T_UNION &#39;{&#39; struct_fields &#39;}&#39; {		$$.flags = 0;		$$.storage = TYPE_UNION;		allocfrom((void **)&amp;$$.fields, &amp;$3, sizeof($3));	}	;struct_fields	: struct_field	| struct_field &#39;,&#39; { $$ = $1; }	| struct_field &#39;,&#39; struct_fields {		$$ = $1;		allocfrom((void **)&amp;$$.next, &amp;$3, sizeof($3));	}	;struct_field	: T_IDENT &#39;:&#39; type {		$$.name = $1;		allocfrom((void**)&amp;$$.type, &amp;$3, sizeof($3));		$$.next = NULL;	}	;</p><p> struct_type：t_struct＆＃39; {＆＃39; struct_fields＆＃39;}＆＃39; {$$。标志= 0; $$。Storage = type_struct; Allocfroom（（void **）＆amp; $$。字段，＆amp; 3美元，尺寸（3美元））; } | t_union＆＃39; {＆＃39; struct_fields＆＃39;}＆＃39; {$$。标志= 0; $$。Storage = type_union; Allocfroom（（void **）＆amp; $$。字段，＆amp; 3美元，尺寸（3美元））; }; struct_fields：struct_field | struct_field＆＃39;，＆＃39; {$$ = $ 1; } | struct_field＆＃39;，＆＃39; struct_fields {$$ = $ 1; Allocfroom（（void **）＆amp; $$。下一个，＆amp; 3美元，尺寸（3美元））; }; struct_field：t_ident＆＃39;：＆＃39;类型{$$。名称= 1美元; Allocfroom（（void **）＆amp; $$。类型，＆amp; $ 3，sizeof（$ 3））; $$。下一个= null; };</p><p> This approach has you writing code which is already almost a formal grammar inits own right. If we strip out the C code, we get the following:</p><p> 此方法您是否有写作已经几乎正式的语法inits itation的代码。如果我们删除C代码，我们得到以下内容：</p><p> struct_type	: T_STRUCT &#39;{&#39; struct_fields &#39;}&#39;	| T_UNION &#39;{&#39; struct_fields &#39;}&#39;	;struct_fields	: struct_field	| struct_field &#39;,&#39;	| struct_field &#39;,&#39; struct_fields	;struct_field	: T_IDENT &#39;:&#39; type	;</p><p> struct_type：t_struct＆＃39; {＆＃39; struct_fields＆＃39;}＆＃39; | t_union＆＃39; {＆＃39; struct_fields＆＃39;}＆＃39; ; struct_fields：struct_field | struct_field＆＃39;，＆＃39; | struct_field＆＃39;，＆＃39; struct_fields; struct_field：t_ident＆＃39;：＆＃39;类型	;</p><p> This gives us a reasonably clean path to writing a formal grammar (andspecification) for the language, which is what we did next.</p><p> 这为我们提供了一个合理的清洁道路，可以为语言编写正式的语法（andspecification），这就是我们接下来所做的。</p><p>  All of these samples describe a struct type. The following example shows whatthis grammar looks like in real code — starting from the word “struct” andincluding up to the “}” at the end.</p><p>  所有这些样本描述了结构类型。以下示例显示了在实际代码中的语法看起来像是 - 从“struct”一词开始，在最后一词上包括“}”。 </p><p>  In order to feed our parser tokens to work with, we also need a lexer, or a lexical analyzer. This turns a series of characters like “struct” into asingle token, like the T_STRUCT we used in the yacc code. Like the originalcompiler used yacc as a parser generator, we also used  lex as a lexergenerator. It’s simply a list of regexes and the names of the tokens that matchthose regexes, plus a little bit of extra code to do things like turning “1234”into an int with a value of 1234. Our lexer also kept track of line and columnnumbers as it consumed characters from input files.</p><p>为了养活我们的解析器令牌工作，我们还需要一个Lexer或词汇分析仪。这将一系列字符称为“struct”进入Asingle令牌，如在YACC代码中使用的t_struct。就像原始媒体一样使用YACC作为解析器发生器，我们也使用Lex作为lexergenerator。它只是一个正则表达式列表和令牌的令牌的名称，与正则表达式相匹配，加上一点额外的代码，以便将“1234”变成一个值为1234的int。我们的Lexer还保持线路和列向内的轨道它从输入文件中消耗了字符。</p><p> &#34;struct&#34;	{ _lineno(); return T_STRUCT; }&#34;union&#34;		{ _lineno(); return T_UNION; }&#34;{&#34;		{ _lineno(); return &#39;{&#39;; }&#34;}&#34;		{ _lineno(); return &#39;}&#39;; }[a-zA-Z][a-zA-Z0-9_]* {	_lineno();	yylval.sval = strdup(yytext);	return T_IDENTIFIER;}</p><p> ＆＃34; struct＆＃34; {_lineno（）;返回t_struct; }＆＃34;联盟＆＃34; {_lineno（）;返回t_union; }＆＃34; {＆＃34; {_lineno（）;返回＆＃39; {＆＃39 ;; }＆＃34;}＆＃34; {_lineno（）;返回＆＃39;}＆＃39 ;; } [A-ZA-Z] [A-ZA-Z0-9 _] * {_Lineno（）; yylval.sval = strdup（yytext）;返回t_identifier;}</p><p> After we settled on the design with our prototype compiler, which was able tocompile some simple test programs to give us a feel for our language design, weset it aside and wrote the specification, and, alongside it, a second compiler.This new compiler was written in C — the language was not ready toself-host yet — and uses a hand-written  recursive descent parser.</p><p> 在我们与我们的原型编译器中定居设计后，能够才能才能才能才能提供一些简单的测试程序，让我们了解我们的语言设计，将其删除，并写下规格，以及它，第二个编译器。这是新的编译器用c  - 语言没有准备好ToSelf-Host  - 并使用手写的递归下降解析器。</p><p> To simplify the parser, we deliberately designed a context-free LL(1) grammar,which means it (a) can parse an input unambiguously without needing additionalcontext, and (b) only requires one token of look-ahead. This makes our parserdesign a lot simpler, which was a deliberate goal of the language design. Ourhand-rolled lexer is  slightly more complicated: it requires two characters oflookahead to distinguish between the “.”, “..”, and “…” tokens.</p><p> 为了简化解析器，我们故意设计了一种无与伦比的LL（1）语法，意味着它（a）可以明确地解析输入的输入而不需要额外的文本，并且（b）只需要一个令牌的令牌。这使我们的ParserDesign更简单，这是语言设计的故意目标。滚动的Lexer略微复杂：它需要两个字符的oflokahead以区分“。”，“..”和“...”令牌。</p><p> I’ll skip going in depth on the design of the second parser, because the hostedparser is more interesting, and a pretty similar design anyway. Let’s start bytaking a look at our hosted lexer. Our lexer is initialized with an input source(e.g. a file) from which it can read a stream of characters. Then, each time weneed a token, we’ll ask it to read the next one out. It will read as manycharacters as it needs to unambiguously identify the next token, then hand it upto the caller.</p><p> 我会在第二个解析器的设计上跳过深入，因为主机的摊类更有趣，无论如何都是一个非常类似的设计。让我们开始看看我们托管的Lexer。我们的Lexer用输入源（例如，文件）初始化它可以读取字符流。然后，每次想到令牌时，我们都会要求它读完下一个。它将读为多种字符，因为它需要明确识别下一个令牌，然后将其交给呼叫者。</p><p>  A token is the smallest unit of meaning in the  **** grammar. The lexical analysis phase processes a UTF-8source file to produce a stream of tokens by matching the terminals with theinput text.</p><p>  令牌是****语法中最小的意义单位。词汇分析阶段通过将终端与incput文本匹配来处理UTF-8Source文件以生成令牌流。</p><p> Tokens may be separated by  white-space characters, which are defined as theUnicode code-points  U+0009 (horizontal tabulation),  U+000A (line feed), and U+0020 (space). Any number of whitespace characters may be inserted betweentokens, either to disambiguate from subsequent tokens, or for aestheticpurposes. This whitespace is discarded during the lexical analysis phase.</p><p> 令牌可以通过空白字符分隔，它被定义为“单极码 - 点U + 0009（水平标记），U + 000A（行馈送）和U + 0020（空间）。任何数量的空格字符都可以插入间位，无论是从后续代币歧视，还是为美学歧视。在词汇分析阶段丢弃该空格。 </p><p> Within a single token, white-space is meaningful. For example, thestring-literal token is defined by two quotation marks  &#34; enclosing anynumber of literal characters. The enclosed characters are considered part ofthe string-literal token and any whitespace therein is not discarded.</p><p>在一个令牌中，空白是有意义的。例如，脚轮文字令牌由两个引号＆＃34定义;包围任何字符字符。封闭的字符被认为是字符串文字令牌的一部分，并且没有丢弃其中的任何空格。</p><p> The lexical analysis process consumes Unicode characters from the source fileinput until it is exhausted, performing the following steps in order: it shallconsume and discard white-space characters until a non-white-space characteris found, then consume the longest sequence of characters which couldconstitute a token, and emit it to the token stream.</p><p> 词汇分析过程从源fileInput中消耗Unicode字符，直到它耗尽，按顺序执行以下步骤：它须知，直到找到非空白空间字符，然后消耗即可消耗最长的字符序列一个令牌，并将其发出到令牌流。</p><p> There are a few different kinds of tokens our lexer is going to need to handle:operators, like “+” and “-&#34;; keywords, like “struct” and “return”; user-definedidentifiers, like variable names; and constants, like string and numericliterals.</p><p> 有一些不同种类的令牌我们的Lexer需要处理：运营商，如“+”和“ - ＆＃34 ;;关键词，如“结构”和“返回”;用户redigeIdentifiers，如变量名称;和常量，如字符串和numericliterals。</p><p>     This way, our parser doesn’t have to deal with whitespace, or distinguishing“int” (keyword) from “integer” (identifier), or handling invalid tokens like“$”. To actually implement this behavior, we’ll start with an initializationfunction which populates a state structure.</p><p>     这样，我们的解析器不必处理空格，或区分“Integer”（Integer“（标识符）（标识符）或处理”$“的无效令牌。为了实际实现此行为，我们将从填充状态结构的初始化功能开始。</p><p> // Initializes a new lexer for the given input stream. The path is borrowed.  export  fn  init ( in :  * io :: stream ,  path :  str ,  flags :  flags ...)  lexer  =  {	 return  lexer  {		 in  =  in ,		 path  =  path ,		 loc  =  ( 1 ,  1 ),		 un  =  void ,		 rb  =  [ void ...],	 }; }; export  type  lexer  =  struct  {	 in :  * io :: stream ,	 path :  str ,	 loc :  ( uint ,  uint ),	 rb :  [ 2 ]( rune  |  io :: EOF  |  void ), };</p><p> //为给定输入流初始化新的Lexer。这条道路是借来的。导出fn init（in：* io :: stream，path：str，标志：标志...）Lexer = {return lexer {in = in，path = path，loc =（1,1），un = void，rb = [void ...]，}; };导出类型lexer = struct {in：* io :: stream，path：str，loc :( uint，uint），rb：[2]（rune | io :: eof | void），};</p><p>    // Returns the next token from the lexer.  export  fn  lex ( lex :  * lexer )  ( token  |  error ); // A single lexical token, the value it represents, and its location in a file.  export  type  token  =  ( ltok ,  value ,  location ); // A token value, used for tokens such as &#39;1337&#39; (an integer).  export  type  value  =  ( str  |  rune  |  i64  |  u64  |  f64  |  void ); // A location in a source file.  export  type  location  =  struct  {	 path :  str ,	 line :  uint ,	 col :  uint }; // A lexical token class.  export  type  ltok  =  enum  uint  {	 UNDERSCORE ,	 ABORT ,	 ALLOC ,	 APPEND ,	 AS ,	 // ... continued ... 	 EOF , };</p><p>    //从Lexer返回下一个令牌。出口FN LEX（​​LEX：* LEXER）（令牌|错误）; //一个单一词汇令牌，它代表的值，它在文件中的位置。导出类型令牌=（LTOK，值，位置）; //令牌值，用于令牌，如＆＃39; 1337＆＃39; （整数）。导出类型值=（str | rune | i64 | U64 | F64 |空白）; //源文件中的一个位置。导出类型位置= struct {path：str，行：uint，col：uint}; //一个词汇令牌课程。导出类型ltok = enum uint {下划线，中止，alloc，附加，as // ...继续... eof，};</p><p> The idea is that when the caller needs another token, they will call  lex, andreceive either a token or an error. The purpose of our lex function is to readout the next character and decide what kind of tokens it might be the start of,and dispatch to more specific lexing functions to handle each case.</p><p> 这个想法是，当来电者需要另一个令牌时，他们会致电Lex，Andreceive令牌或错误。我们的Lex函数的目的是读出下一个字符并决定它可能是什么样的令牌，并且派遣到更具体的LEXING函数来处理每种情况。 </p><p> export  fn  lex ( lex :  * lexer )  ( token  |  error )  =  {	 let  loc  =  location  {  ...  };	 let  rn :  rune  =  match  ( nextw ( lex ) ? )  {		 _ :  io :: EOF  =&gt;  return  ( ltok :: EOF ,  void ,  mkloc ( lex )),		 rl :  ( rune ,  location )  =&gt;  {			 loc  =  rl .1 ;			 rl .0 ;		 },	 };	 if  ( is_name ( rn ,  false ))  {		 unget ( lex ,  rn );		 return  lex_name ( lex ,  loc ,  true );	 };	 if  ( ascii :: isdigit ( rn ))  {		 unget ( lex ,  rn );		 return  lex_literal ( lex ,  loc );	 };	 let  tok :  ltok  =  switch  ( rn )  {		 *  =&gt;  return  syntaxerr ( loc ,  &#34;invalid character&#34; ),		 &#39;&#34;&#39; ,  &#39;\&#39;&#39;  =&gt;  {			 unget ( lex ,  rn );			 return  lex_rn_str ( lex ,  loc );		 },		 &#39;.&#39; ,  &#39;&lt;&#39; ,  &#39;&gt;&#39;  =&gt;  return  lex3 ( lex ,  loc ,  rn ),		 &#39;^&#39; ,  &#39;*&#39; ,  &#39;%&#39; ,  &#39;/&#39; ,  &#39;+&#39; ,  &#39;-&#39; ,  &#39;:&#39; ,  &#39;!&#39; ,  &#39;&amp;&#39; ,  &#39;|&#39; ,  &#39;=&#39;  =&gt;  {			 return  lex2 ( lex ,  loc ,  rn );		 },		 &#39;~&#39;  =&gt;  ltok :: BNOT ,		 &#39;,&#39;  =&gt;  ltok :: COMMA ,		 &#39;{&#39;  =&gt;  ltok :: LBRACE ,		 &#39;[&#39;  =&gt;  ltok :: LBRACKET ,		 &#39;(&#39;  =&gt;  ltok :: LPAREN ,		 &#39;}&#39;  =&gt;  ltok :: RBRACE ,		 &#39;]&#39;  =&gt;  ltok :: RBRACKET ,		 &#39;)&#39;  =&gt;  ltok :: RPAREN ,		 &#39;;&#39;  =&gt;  ltok :: SEMICOLON ,		 &#39;?&#39;  =&gt;  ltok :: QUESTION ,	 };	 return  ( tok ,  void ,  loc ); };</p><p>出口FN LEX（​​LEX：* LEXER）（令牌|错误）= {LET LOC =位置{...};让RN：rune =匹配（nextw（lex）？）{_：io :: eof =＆gt;返回（LTOK :: EOF，VOID，MKLOC（LEX）），RL :(符文，位置）=＆gt; {loc = rl .1; RL .0; }，}; if（is_name（rn，false））{UNEGE（LEX，RN）;返回lex_name（lex，loc，true）; }; if（ascii :: isdigit（rn））{UNEGE（LEX，RN）;返回lex_literal（Lex，LOC）; };让tok：ltok = switch（rn）{* =＆gt;返回syntaxerr（loc，＆＃34;字符无效＆＃34;），＆＃39;＆＃34;＆＃39; ，＆＃39; \＆＃39;＆＃39; =＆gt; {UNET（LEX，RN）;返回Lex_rn_str（Lex，LOC）; }，＆＃39;。＆＃39; ，＆＃39;＆＃39; ，＆＃39;＆gt;＆＃39; =＆gt;返回Lex3（Lex，Loc，RN），＆＃39; ^＆＃39; ，＆＃39; *＆＃39; ，＆＃39;％＆＃39; ，＆＃39; /＆＃39; ，＆＃39; +＆＃39; ，＆＃39;  - ＆＃39; ，＆＃39;：＆＃39; ，＆＃39;！＆＃39; ，＆＃39;＆amp;＆＃39; ，＆＃39; |＆＃39; ，＆＃39; =＆＃39; =＆gt; {返回Lex2（Lex，LOC，RN）; }，＆＃39;〜＆＃39; =＆gt; ltiok :: bnot，＆＃39;，＆＃39; =＆gt; LTOK ::逗号，＆＃39; {＆＃39; =＆gt; ltiok :: lbace，＆＃39; [＆＃39; =＆gt; ltiok :: lbracket，＆＃39;（＆＃39; =＆gt; ltiok :: lparen，＆＃39;}＆＃39; ltiok :: rbrace，＆＃39;]＆＃39; =＆gt ; ltiok :: rbracket，＆＃39;）＆＃39; =＆gt; ltiok :: rppen，＆＃39 ;;＆＃39; =＆gt; ltiok ::分号，＆＃39;？＆＃39; =＆gt; LTOK ::问题，};返回（tok，void，loc）; };</p><p> Aside from the EOF case, and simple single-character operators like “;”, both ofwhich this function handles itself, its role is to dispatch work to varioussub-lexers.</p><p> 除了eof案例之外，和简单的单个字符运算符，如“;”，这函数都处理自己，它的作用是向各种筹码发动机派遣工作。</p><p>    fn nextw(lex: *lexer) ((rune, location) | io::EOF | io::error) = {	for (true) {		let loc = mkloc(lex);		match (next(lex)) {			e: (io::error | io::EOF) =&gt; return e,			r: rune =&gt; if (!ascii::isspace(r)) {				return (r, loc);			} else {				free(lex.comment);				lex.comment = &#34;&#34;;			},		};	};	abort();};</p><p>    fn nettw（lex：* lexer）（（rune，location）| io :: eof | io ::错误）= {for（true）{let loc = mkloc（lex）;匹配（下一个（LEX））{E :( IO ::错误| IO :: EOF）=＆gt;返回e，r：rune =＆gt; if（！ascii :: Isspace（r））{return（r，loc）; }否则{免费（Lex.com）; Lex.commert =＆＃34;＆＃34 ;; }，}; }; abort（）;};</p><p> fn unget(lex: *lexer, r: (rune | io::EOF)) void = {	if (!(lex.rb[0] is void)) {		assert(lex.rb[1] is void, &#34;ungot too many runes&#34;);		lex.rb[1] = lex.rb[0];	};	lex.rb[0] = r;};</p><p> FN UNGE（LEX：* LEXER，R：（rune | io :: eof））void = {if（！（lex.rb [0]是void））{assert（lex.rb [1]是void，＆＃ 34; ungot太多符号＆＃34;）; lex.rb [1] = lex.rb [0]; }; lex.rb [0] = r;};</p><p> fn is_name(r: rune, num: bool) bool =	ascii::isalpha(r) || r == &#39;_&#39; || r == &#39;@&#39; || (num &amp;&amp; ascii::isdigit(r));</p><p> fn is_name（r：rune，num：bool）bool = ascii :: isalpha（r）|| r ==＆＃39; _＆＃39; || r ==＆＃39; @＆＃39; || （Num＆amp;＆amp; ASCII :: ISDIGIT（R））;</p><p> The sub-lexers handle more specific cases. The lex_name function handles thingswhich look like identifiers, including keywords; the lex_literal functionhandles things which look like literals (e.g. “1234”); lex_rn_str handles runeand string literals (e.g. “hello world” and ‘\n’); and lex2 and lex3respectively handle two- and three-character operators like “&amp;&amp;” and “&gt;&gt;=”.</p><p> 子输出者处理更具体的情况。 Lex_name函数处理它们看起来像标识符的东西，包括关键字; Lex_Literal函数手柄看起来像文字（例如“1234”）; Lex_rn_str Handles runeand字符串文字（例如，“Hello World”和'\ n'）;和Lex2和Lex3Respective处理像“＆amp;＆amp;＆amp;＆amp;＆amp;＆amp;”的两个和三个字符的运营商和“＆gt;＆gt; =”。</p><p> lex_name is the most complicated of these. Because the only thing whichdistinguishes a keyword from an identifier is that the former matches a specificlist of strings, we start by reading a “name” into a buffer, then binarysearching against a list of known keywords to see if it matches something there.To facilitate this, “bmap” is a pre-sorted array of keyword names.</p><p> Lex_name是最复杂的。因为唯一一个从标识符到标识符的关键字是前者匹配字符串的特定列表，所以我们首先读取一个“名称”到缓冲区，然后从已知关键字列表中读取，以查看它是否与某些东西匹配。此，“BMAP”是一个预先排序的关键字名称数组。 </p><p> const  bmap :  [ _ ] str  =  [	 // Keep me alpha-sorted and consistent with the ltok enum. 	 &#34;_&#34; ,	 &#34;abort&#34; ,	 &#34;alloc&#34; ,	 &#34;append&#34; ,	 &#34;as&#34; ,	 &#34;assert&#34; ,	 &#34;bool&#34; ,	 // ...  ]; fn  lex_name ( lex :  * lexer ,  loc :  location ,  keyword :  bool )  ( token  |  error )  =  {	 let  buf  =  strio :: dynamic ();	 match  ( next ( lex ))  {		 r :  rune  =&gt;  {			 assert ( is_name ( r ,  false ));			 strio :: appendrune ( buf ,  r );		 },		 _ :  ( io :: EOF  |  io :: error )  =&gt;  abort (),  // Invariant 	 };	 for  ( true )  match  ( next ( lex ) ? )  {		 _ :  io :: EOF  =&gt;  break ,		 r :  rune  =&gt;  {			 if  ( ! is_name ( r ,  true ))  {				 unget ( lex ,  r );				 break ;			 };			 strio :: appendrune ( buf ,  r );		 },	 };	 let  name  =  strio :: finish ( buf );	 if  ( ! keyword )  {		 return  ( ltok :: NAME ,  name ,  loc );	 };	 return  match  ( sort :: search ( bmap [.. ltok :: LAST_KEYWORD + 1 ],			 size ( str ),  &amp; name ,  &amp; namecmp ))  {		 null  =&gt;  ( ltok :: NAME ,  name ,  loc ),		 v :  * void  =&gt;  {			 defer  free ( name );			 let  tok  =  v :  uintptr  -  &amp; bmap [ 0 ] :  uintptr ;			 tok  /=  size ( str ) :  uintptr ;			 ( tok :  ltok ,  void ,  loc );		 },	 }; };</p><p>const BMAP：[_] str = [//让我保持alpha排序和与LTOK枚举一致。 ＆＃34; _＆＃34; ，＆＃34;中止＆＃34; ，＆＃34; alloc＆＃34; ，＆＃34;附录＆＃34; ，＆＃34;作为＆＃34; ，＆＃34;断言＆＃34; ，＆＃34; bool＆＃34; ，// ......]; fn lex_name（lex：* lexer，loc：location，locient：bool）（令牌）（错误）= {let buf = strio :: commynd（）;匹配（下一个（Lex））{R：rune =＆gt; {assert（is_name（r，false））; Strio :: Appendrune（Buf，R）; }，_ :( io :: eof | io ::错误）=＆gt; abort（），//不变}; for（true）匹配（下一个（lex）？）{_：io :: eof =＆gt;休息，r：rune =＆gt; {if（！is_name（r，true））{UNEGE（LEX，R）;休息 ; }; Strio :: Appendrune（Buf，R）; }，};让名称= strio :: finish（buf）; if（！关键字）{return（ltok :: name，name，loc）; };返回匹配（排序::搜索（BMAP [relok :: last_keyword + 1]，size（str），＆amp; name，＆amp; namecmp））{null =＆gt; （ltok :: name，name，loc），v：* void =＆gt; {推迟免费（姓名）;让tok = v：uintptr  - ＆amp; BMAP [0]：UINTPTR; tok / = size（str）：uintptr; （tok：ltok，void，loc）; }，}; };</p><p> The rest of the code is more of the same, but I’ve  put it up here ifyou want to read it.</p><p> 其余的代码更像是相同的，但我在这里把它放在这里，如果你想读它。</p><p> Let’s move on to parsing: we need to turn this one dimensional stream of tokensinto an structured form: the  Abstract Syntax Tree. Consider the followingsample code:</p><p> 让我们继续解析：我们需要将此一维TokensInto旋转为结构化表单：抽象语法树。考虑以下内容：</p><p>      We know at each step what kinds of tokens are valid in each situation. After wesee “let”, we know that we’re parsing a binding, so we look for a name (“x”)and a colon token, a type for the variable, an equals sign, and an expressionwhich initializes it. To parse the initializer, we see an identifier, “add2”,then an open parenthesis, so we know we’re in a call expression, and we canstart parsing arguments.</p><p>      我们在每一步都知道各种令牌在每种情况下有效。在Wesee“让”之后，我们知道我们解析了一个绑定，所以我们查找名称（“x”）和冒号令牌，变量的类型，等于符号和初始化的表达式。要解析初始化程序，我们会看到一个标识符，“add2”，然后是一个打开的括号，所以我们知道我们处于调用表达式，我们可以才能启动解析参数。</p><p> To make our parser code expressive, and to handle errors neatly, we’re going toimplement a few helper function that lets us describe these states in terms ofwhat the parser wants from the lexer. We have a few functions to accomplishthis:</p><p> 为了使我们的解析器代码表达，并整理地处理错误，我们将映射几个辅助功能，让我们以解析器从Lexer的要求描述这些状态。我们有一些职能来完成：</p><p> // Requires the next token to have a matching ltok. Returns that token, or an error.  fn  want ( lexer :  * lex :: lexer ,  want :  lex :: ltok ...)  ( lex :: token  |  error )  =  {	 let  tok  =  lex :: lex ( lexer ) ? ;	 if  ( len ( want )  ==  0 )  {		 return  tok ;	 };	 for  ( let  i  =  0 z ;  i  &lt;  len ( want );  i  +=  1 )  {		 if  ( tok .0  ==  want [ i ])  {			 return  tok ;		 };	 };	 let  buf  =  strio :: dynamic ();	 defer  io :: close ( buf );	 for  ( let  i  =  0 z ;  i  &lt;  len ( want );  i  +=  1 )  {		 fmt :: fprintf ( buf ,  &#34;&#39;{}&#39;&#34; ,  lex :: tokstr (( want [ i ],  void ,  mkloc ( lexer ))));		 if  ( i  +  1  &lt;  len ( want ))  {			 fmt :: fprint ( buf ,  &#34;, &#34; );		 };	 };	 return  syntaxerr ( mkloc ( lexer ),  &#34;Unexpected &#39;{}&#39;, was expecting {}&#34; ,		 lex :: tokstr ( tok ),  strio :: string ( buf )); }; // Looks for a matching ltok from the lexer, and if not present, unlexes the // token and returns void. If found, the token is consumed from the lexer and is // returned.  fn  try (	 lexer :  * lex :: lexer ,	 want :  lex :: ltok ... )  ( lex :: token  |  error  |  void )  =  {	 let  tok  =  lex :: lex ( lexer ) ? ;	 assert ( len ( want )  &gt;  0 );	 for  ( let  i  =  0 z ;  i  &lt;  len ( want );  i  +=  1 )  {		 if  ( tok .0  ==  want [ i ])  {			 return  tok ;		 };	 };	 lex :: unlex ( lexer ,  tok ); }; // Looks for a matching ltok from the lexer, unlexes the token, and returns // it; or void if it was not a ltok.  fn  peek (	 lexer :  * lex :: lexer ,	 want :  lex :: ltok ... )  ( lex :: token  |  error  |  void )  =  {	 let  tok  =  lex :: lex ( lexer ) ? ;	 lex :: unlex ( lexer ,  tok );	 if  ( len ( want )  ==  0 )  {		 return  tok ;	 };	 for  ( let  i  =  0 z ;  i  &lt;  len ( want );  i  +=  1 )  {		 if  ( tok .0  ==  want [ i ])  {			 return  tok ;		 };	 }; };</p><p> //需要下一个令牌有一个匹配的LTOK。返回该令牌或错误。 fn want（lexer：* lex :: lexer，想要：lex :: ltiok ...）（Lex ::令牌|错误）= {令唱tok = lex :: lex（Lexer）？ ; if（len（想要）== 0）{return tok; }; for（让我= 0 z; i＆lt; len（想要）; i + = 1）{if（tok .0 == wants [i]）{return tok; }; };让bef = strio ::动态（）;推迟IO ::关闭（BUF）; for（让我= 0 z; i＆lt; len（想要）; i + = 1）{fmt :: fprintf（buf，＆＃34;＆＃39; {}＆＃34;＆＃34;，Lex： ：tokstr（（想要[i]，空白，mkloc（lexer）））））））; if（i + 1＆lt; l; l lt; len（want））{fmt :: fprint（buf，＆＃34 ;,＃34;）; }; };返回syntaxerr（mkloc（lexer），＆＃34;意外＆＃39; {}＆＃39; {}＆＃39;，lex :: tokstr（tok），strio :: string（buf））; }; //查找来自Lexer的匹配LTOK，如果不存在，则ONLEXSS //令牌并返回void。如果发现，令牌从Lexer消耗，并且返回。 fn try（lexer：* lex :: lexer，想要：lex :: ltiok ...）（Lex ::令牌|错误| void）= {et ketk = lex :: lex（Lexer）？ ;断言（Len（想要）＆gt; 0）; for（让我= 0 z; i＆lt; len（想要）; i + = 1）{if（tok .0 == wants [i]）{return tok; }; }; Lex :: Unlex（Lexer，Tok）; }; //查找来自Lexer的匹配LTOK，TENLEXES令牌，并返回//它;或者如果不是ltok，则禁用。 Fn Peek（Lexer：* Lex :: Lexer，想要：Lex :: LTOK ...）（Lex ::令牌|错误| void）= {令唱tok = lex :: lex（Lexer）？ ; Lex :: Unlex（Lexer，Tok）; if（len（想要）== 0）{return tok; }; for（让我= 0 z; i＆lt; len（想要）; i + = 1）{if（tok .0 == wants [i]）{return tok; }; }; };</p><p> Let’s say we’re looking for a binding like our sample code to show up next. Thegrammar from the spec is as follows:</p><p> 假设我们正在寻找像我们的示例代码一样绑定以显示下一个。来自规范的CARMMAR如下： </p><p>   fn  binding ( lexer :  * lex :: lexer )  ( ast :: expr  |  error )  =  {	 const  is_static :  bool  =  try ( lexer ,  ltok :: STATIC ) ?  is  lex :: token ;	 const  is_const  =  switch  ( want ( lexer ,  ltok :: LET ,  ltok :: CONST ) ? .0 )  {		 ltok :: LET  =&gt;  false ,		 ltok :: CONST  =&gt;  true ,	 };	 let  bindings :  [] ast :: binding  =  [];	 for  ( true )  {		 const  name  =  want ( lexer ,  ltok :: NAME ) ? .1  as  str ;		 const  btype :  nullable  * ast :: _type  =			 if  ( try ( lexer ,  ltok :: COLON ) ?  is  lex :: token )  {				 alloc ( _type ( lexer ) ? );			 }  else  null ;		 want ( lexer ,  ltok :: EQUAL ) ? ;		 const  init  =  alloc ( expression ( lexer ) ? );		 append ( bindings ,  ast :: binding  {			 name  =  name ,			 _type  =  btype ,			 init  =  init ,		 });		 match  ( try ( lexer ,  ltok :: COMMA ) ? )  {			 _ :  void  =&gt;  break ,			 _ :  lex :: token  =&gt;  void ,		 };	 };	 return  ast :: binding_expr  {		 is_static  =  is_static ,		 is_const  =  is_const ,		 bindings  =  bindings ,	 }; };</p><p>fn绑定（Lexer：* Lex :: lexer）（AST :: expr |错误）= {const is_static：bool = try（lexer，ltok :: static）？是Lex ::令牌; const is_const = switch（想要（ltexer，ltok :: let，ltok :: const）？.0）{ltok :: let =＆gt; False，LTOK :: const =＆gt;真的 ，	 };让绑定：[] AST :: BINDING = []; for（true）{const name = wance（lexer，ltok :: name）？ .1作为str; const btype：nullable * ast :: _type = if（尝试（lexer，ltok :: colon）？lex ::令牌）{alloc（_type（lexer）？）; }否定;想要（Lexer，LTOK ::等于）？ ; const init = alloc（表达式（Lexer）？）;附加（绑定，AST ::绑定{name = name，_type = btype，init = init，}）;匹配（尝试（lexer，ltok ::逗号）？）{_：void =＆gt;休息，_：Lex ::令牌=＆gt;空白 ，		 }; };返回AST :: binding_expr {is_static = is_static，is_const = is_const，绑定=绑定，}; };</p><p> Hopefully the flow of this code is fairly apparent. The goal is to fill in thefollowing AST structure:</p><p> 希望这段规范的流程相当明显。目标是填补空调的AST结构：</p><p> // A single variable biding. For example: // // 	foo: int = bar  export  type  binding  =  struct  {	 name :  str ,	 _type :  nullable  * _type ,	 init :  * expr , }; // A variable binding expression. For example: // // 	let foo: int = bar, ...  export  type  binding_expr  =  struct  {	 is_static :  bool ,	 is_const :  bool ,	 bindings :  [] binding , };</p><p> //单个变量触点。例如：// // foo：int = bar导出类型绑定= struct {name：str，_type：nullable * _type，init：* expr，}; //一个变量绑定表达式。例如：// //让foo：int = bar，...导出类型binding_expr = struct {is_static：bool，is_const：bool，绑定：[]绑定，};</p><p> The rest of the code is pretty similar, though some corners of the grammar are abit hairier than others. One example is how we parse infix operators for binaryarithmetic expressions (such as “2 + 2”):</p><p> 其余的代码非常相似，但语法的一些角落比其他角落都是毛茸茸的。一个示例是我们如何解析用于二进制表达式的INVIX运算符（例如“2 + 2”）：</p><p> fn  binarithm (	 lexer :  * lex :: lexer ,	 lvalue :  ( ast :: expr  |  void ),	 i :  int , )  ( ast :: expr  |  error )  =  {	 // Precedence climbing parser 	 // https://en.wikipedia.org/wiki/Operator-precedence_parser 	 let  lvalue  =  match  ( lvalue )  {		 _ :  void  =&gt;  cast ( lexer ,  void ) ? ,		 expr :  ast :: expr  =&gt;  expr ,	 };	 let  tok  =  lex :: lex ( lexer ) ? ;	 for  ( let  j  =  precedence ( tok );  j  &gt;=  i ;  j  =  precedence ( tok ))  {		 const  op  =  binop_for_tok ( tok );		 let  rvalue  =  cast ( lexer ,  void ) ? ;		 tok  =  lex :: lex ( lexer ) ? ;		 for  ( let  k  =  precedence ( tok );  k  &gt;  j ;  k  =  precedence ( tok ))  {			 lex :: unlex ( lexer ,  tok );			 rvalue  =  binarithm ( lexer ,  rvalue ,  k ) ? ;			 tok  =  lex :: lex ( lexer ) ? ;		 };		 let  expr  =  ast :: binarithm_expr  {			 op  =  op ,			 lvalue  =  alloc ( lvalue ),			 rvalue  =  alloc ( rvalue ),		 };		 lvalue  =  expr ;	 };	 lex :: unlex ( lexer ,  tok );	 return  lvalue ; }; fn  precedence ( tok :  lex :: token )  int  =  switch  ( tok .0 )  {	 ltok :: LOR  =&gt;  0 ,	 ltok :: LXOR  =&gt;  1 ,	 ltok :: LAND  =&gt;  2 ,	 ltok :: LEQUAL ,  ltok :: NEQUAL  =&gt;  3 ,	 ltok :: LESS ,  ltok :: LESSEQ ,  ltok :: GREATER ,  ltok :: GREATEREQ  =&gt;  4 ,	 ltok :: BOR  =&gt;  5 ,	 ltok :: BXOR  =&gt;  6 ,	 ltok :: BAND  =&gt;  7 ,	 ltok :: LSHIFT ,  ltok :: RSHIFT  =&gt;  8 ,	 ltok :: PLUS ,  ltok :: MINUS  =&gt;  9 ,	 ltok :: TIMES ,  ltok :: DIV ,  ltok :: MODULO  =&gt;  10 ,	 *  =&gt;  - 1 , };</p><p> FN Binarithm（Lexer：* Lex :: Lexer，Lvalue :( ast :: expr | void），i：int，）（ast :: expr |错误）= {//优先级攀登parser // https：// en。 wikipedia.org/wiki/operator-precendence_parser让lvalue =匹配（lvalue）{_：void =＆gt;演员（Lexer，空白）？ ，expr：ast :: expr =＆gt; expr，};让Tok = Lex :: Lex（Lexer）？ ; for（让J =优先级（tok）; j＆gt; = i; j =优先级（tok））{const op = binop_for_tok（tok）;让Rvalue =施放（Lexer，void）？ ; Tok = Lex :: Lex（Lexer）？ ; for（让K =优先级（TOK）; K> K =优先级（TOK））{Lex :: Unlex（Lexer，Tok）; rvalue = binarithm（lexer，rvalue，k）？ ; Tok = Lex :: Lex（Lexer）？ ; };让expr = ast :: binarithm_expr {op = op，lvalue = alloc（lvalue），rvalue = alloc（rvalue），}; lvalue = expr; }; Lex :: Unlex（Lexer，Tok）;返回lvalue; }; fn优先级（tok：lex ::令牌）int = switch（tok .0）{ltok :: lor =＆gt; 0，ltok :: lxor =＆gt; 1，LTOK :: LAND =＆GT; 2，LTOK :: Lequal，Ltiok :: Nequal =＆gt; 3，ltok :: lell :: lesteq，ltok :: greater，ltiok :: greatereq =＆gt; 4，LTOK :: Bor =＆gt; 5，LTOK :: BXOR =＆GT; 6，LTOK :: Band =＆gt; 7，LTOK :: LSHIFT，LTOK :: Rshift =＆gt; 8，LTOK :: Plus，LTOK :: minus =＆gt; 9，ltok :: times，ltiok :: div，ltiok :: modulo =＆gt; 10，* =＆gt; -  1，};</p><p> I don’t really grok this algorithm, to be honest, but hey, it works. Whenever Iwrite a precedence climbing parser, I’ll stare at the Wikipedia page for 15minutes, quickly write a parser, and then immediately forget how it works. MaybeI’ll write a blog post about it someday.</p><p> 我真的不在这个算法，说实话，但嘿，它有效。每当IWRITE一个优先攀爬解析器时，我将盯着维基百科页面15分钟，快速写一个解析器，然后立即忘记它是如何工作的。 Moteli有一天会写一篇关于它的博客文章。</p><p> Anyway, ultimately, this code lives in our standard library and is used forseveral things, including our (early in development) self-hosted compiler.Here’s an example of its usage, taken from our documentation generator:</p><p> 无论如何，最终，这段代码生存在我们的标准库中，并用于致专用事物，包括我们（早期的开发）自主主机编译器。该模拟的一个例子是我们的文档生成器： </p><p> fn  scan ( path :  str )  ( ast :: subunit  |  error )  =  {	 const  input  =  match  ( os :: open ( path ))  {		 s :  * io :: stream  =&gt;  s ,		 err :  fs :: error  =&gt;  fmt :: fatal ( &#34;Error reading {}: {}&#34; ,			 path ,  fs :: strerror ( err )),	 };	 defer  io :: close ( input );	 const  lexer  =  lex :: init ( input ,  path ,  lex :: flags :: COMMENTS );	 return  parse :: subunit ( &amp; lexer ) ? ; };</p><p>fn扫描（路径：str）（ast :: subonit |错误）= {const输入=匹配（OS ::打开（路径））{s：* io :: stream =＆gt; s，err：fs :: error =＆gt; FMT ::致命（＆＃34;错误读数{}：{}＆＃34;，path，fs :: strerror（err）），};推迟IO ::关闭（输入）; Const Lexer = Lex :: init（输入，路径，Lex ::标志::评论）;返回解析::亚基（＆amp; lexer）？ ; };</p><p>  // A sub-unit, typically representing a single source file.  export  type  subunit  =  struct  {	 imports :  [] import ,	 decls :  [] decl , };</p><p>  //一个子单元，通常表示单个源文件。导出类型子单元= struct {imports：[]导入，depl：[] drev，};</p><p> Pretty straightforward! Having this as part of the standard library should makeit much easier for users to build language-aware tooling with the languageitself. We also plan on having our type checker in the stdlib as well. This issomething that I drew inspiration for from Golang — having a lot of theirtoolchain components in the standard library makes it really easy to writeGo-aware tools.</p><p> 非常直截了当！将其作为标准库的一部分，应该更容易地为用户构建语言感知的工具与luginiciteelf。我们还计划在STDLIB中拥有我们的类型检查器。这是我为来自Golang吸引启发的这个问题 - 在标准库中拥有大量的Hythoolchain组件使得它真的很容易写入感知工具。</p><p> So, there you have it: the next stage in the development of our language. I hopeyou’re looking forward to it!</p><p> 所以，你有它：下一阶段在我们的语言开发。我希望你期待着它！ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://drewdevault.com/2021/04/22/Our-self-hosted-parser-design.html">https://drewdevault.com/2021/04/22/Our-self-hosted-parser-design.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/托管/">#托管</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/writing/">#writing</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/ltok/">#ltok</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>