<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>如何智能地做多任务学习 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">如何智能地做多任务学习 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-25 01:06:14</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/2bdcdad40b8404768ec30cc3190a5847.jpg"><img src="http://img2.diglog.com/img/2021/6/2bdcdad40b8404768ec30cc3190a5847.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>在过去十年中，机器学习已经爆发了流行，现在正在应用于许多领域的问题。传统上，单机学习模型专门用于一项任务，例如一项任务。分类图像，称为单任务学习（STL）。然而，培训模型存在一些优点，以制造单个样本的多种预测，例如，图像分类和语义分割。这称为多任务学习（MTL）。在本文中，我们讨论了MTL的动机以及一些用例，困难和最近的算法进步。</p><p>  有各种原因，保证使用MTL。我们知道机器学习模型通常需要大量的培训数据。但是，我们经常最终有许多任务，其中各个数据集不足以实现良好的效果。在这种情况下，如果这些任务中的一些是相关的，例如，从患者的个人资料中预测许多疾病和结果，我们可以将功能和标签合并到一个较大的训练数据集中，以便我们可以利用相关任务的共享信息来构建一个足够大的数据集。</p><p> MTL还提高了模型的泛化。使用MTL，相关任务中学到的信息提高了模型学习数据的有用表示的能力，这减少了过度装备和增强了泛化。 MTL还可以减少培训时间，因为我们在多个任务上投资时间培训许多模型，我们训练一个型号。</p><p> MTL在某些情况下至关重要，例如当模型将部署在具有有限的计算能力的环境中。由于机器学习模型通常具有许多需要存储在存储器中的参数，对于计算功率受到限制的应用（例如，边缘设备），因此优选地具有具有一些共享参数的单个MTL网络，而不是多个STL模型做相关任务。例如，在自动驾驶汽车中，我们需要实时完成多个任务，包括对象检测和深度估计。拥有多个神经网络，以便单独执行这些任务，需要可能无法使用的计算能力。相反，使用具有MTL培训的单个模型可降低存储器要求并加速推断。</p><p>  尽管MTL的优势，但在某些情况下，该方法实际上可以损害性能。在培训MTL网络期间，任务可以彼此竞争，以实现更好的学习表示即，一个或多个任务可以主导培训过程。例如，当在MTL设置中，当与MTL设置中的语义分割（在图像中的每个单独对象分割图像中的每个单独对象的单独对象）进行培训时，除非一些任务平衡机制，否则后者任务通常会主导学习过程就业[1]。</p><p> 此外，由于多个总和损耗，MTL的损耗功能也可能更复杂，从而使优化更加困难。在这些情况下，存在对多个任务的合作的负面影响，并且在单个任务上培训的单个网络可能更好地执行。</p><p> 所以我们什么时候应该多任务？回答这个问题很困难，但在过去的几年里，有一系列重要的论文提出了学习应该在一起学习的算法和当应单独学习任务时的算法。以下是其中的三篇重要论文： </p><p>动机：通常在MTL中，使用两种方法中的一种。一个是硬参数共享，其中初始层被共享，直到某个点之后，网络分支出用于对各个任务进行预测。这种方法的问题在于它强制机器学习从业者指定要共享哪些层，这可能对手的任务可能不是最佳的。在软参数共享中，每个任务都是通过单独的型号和权重学习，但目标函数包括丢失术语，鼓励模型的参数是相似的。软参数共享的缺点是大量参数，特别是随着任务数量的增加。</p><p> 结合这些方法的好处，X. Sun等人。建议，“Adashare：学习分享有效的深度多任务学习”（2020）[2]。研究人员的主要目标是为MTL指定单层架构，并培训一个策略，该策略确定要通过多个任务共享哪些层，这些层将用于特定任务以及哪些图层在确保模型时跳过所有任务的图层以最紧凑的形式提供最高性能。</p><p>  方法：作者提出的方法共同优化了网络参数和二进制随机变量UL，对于每个层L和TK中的每个任务，其中TK代表一组K任务。这里，二进制随机变量或策略表示由特定块进行共享，跳过或单独完成多个任务的任务。由于策略变量是不可差异的，因此Gumbel-SoftMax采样[3]方法用于优化它。本文提出的损失函数是任务特定损失的总和，鼓励模型紧凑的稀疏性损失以及鼓励跨任务分享的共享损失。</p><p> 限制：所提出的方法的主要限制是它需要模型在图层之间跳过连接。虽然这些架构已经在事先工作（例如Resnets）中使用，但是所提出的方法不能直接推广到其他网络架构。</p><p>  动机：为了有效地进行MTL，网络需要与任务之间的输入功能共享相关信息，同时平衡各个任务的学习率。在“注意到与注意力的结束多任务学习”[4]，刘等人。介绍一个统一的方法，可以在学习过程中使用任务共享和任务平衡方案。</p><p>  方法：作者提出的这种方法将神经网络架构划分为两部分。第一部分是在所有功能和任务中培训的标准共享图层。在这些共享层之后，软关注机制从共享层收集特定于任务特征，允许以自我监督的方式将任务学习到结束。换句话说，这些注意力模块作为每个特定任务的特征选择器，它被馈送到网络的第二个任务专用部分中。</p><p> 此外，为了平衡不同任务的学习率，作者还提出了一种“动态重量平均”技术。在一开始，对于训练的前两个迭代，每个任务丢失的权重初始化为1.在每次迭代之后，首先将权重调整为对该任务的前两次迭代中的损耗与损失的比率。然后软最大化使它们在0和1之间。通过这种技术，权重适应，使得最难学习的任务在训练期间给予更多重量。 </p><p>限制：虽然这种方法看似有效，但是本文作者经营的实验（以及我们审查的其他两篇论文）主要限于少数计算机视觉任务。虽然作者在一个数据集上尝试了最多10个任务的方法，但结果与其他最先进的MTL方法没有比较。需要进一步评估来了解如何在越来越多的任务数量和多样性的情况下缩放。</p><p>  动机：在Trevor Standley等中的本文中。 [5]，作者不仅考虑如何将任务组合在一起，还要探讨应为每组任务分配多少计算预算。作者为多任务学习推出了一个新的学习框架，最大化了给定的计算预算中的任务的性能。</p><p>  方法：要研究任务集之间的关系，作者对Taskomaty DataSet的不同设置进行了模型性能的实证研究[6]。该研究的结果突出了网络容量，辅助任务以及培训数据的影响以及整体MTL性能的影响。</p><p> 基于该研究的结果，作者提出了三种MTL：最佳解决方案（OS），早期停止近似（ESA）和更高阶近似（HOA）的技术。第一种方法是基于分支和绑定算法，它使用组合方法基于性能和推理时间选择所有完全训练的网络任务组合的空间中的最佳解决方案。</p><p> 由于OS可以花费大量运行，因此后一种方法越快近似。 ESA通过从训练早期阶段估计任务关系来减少运行时，然后培训所选择的网络配置直到收敛。 HOA计算每个任务丢失估计（基于各个任务），并使用此估计来近似网络配置的性能。</p><p> 限制：由于最佳解决方案基于分支和绑定算法，因此它具有运行时可能是不可行的，因为任务数量增加。在ESA中，早期培训和最终培训表现之间的相关性并不一定高，因此为任务关系提供了误导性结果。 HOA完全忽略与分组任务一起关联的任务交互和非线性效果。因此，ESA和HOA都遭受预测性能的降级。</p><p>  由于多任务学习的重要性增加，已经提出了大量方法来自动了解应该共同学习哪些任务。然而，这些方法尚未被详尽地评估，特别是在大量任务和计算机视觉之外的域名上。可能需要新方法来扩展这些方法到数十或数百个任务以及多任务处理很重要的其他域，例如自然语言处理和生物医学数据。 </p><p>[1] Alex Kendall，Yarin Gal，Roberto Cipolla（2018）。多任务学习利用不确定性来称量场景几何和语义的损失。在计算机愿景和模式识别（CVPR）2018年IEEE会议中。</p><p> [2] Sun，X.，Panda，R.，Feris，R.，＆amp; Saenko，K。（2020）。 Adashare：学习如何分享高效的深度多任务学习。神经信息处理系统年会，神经潮端，12月6日至12日。</p><p> [3] Jang，E.，Gu，S.，＆amp; Poole，B.（2016）。 Gumbel-Softmax的分类Reparameterization。 Arxiv预印迹arxiv：1611.01144。</p><p> [4]刘，S.，约翰，E.，＆amp;达维森，A. J.（2019）。紧随关注的端到端多任务学习。在IEEE / CVF会议上关于计算机愿景和模式识别（PP.1871-1880）的诉讼程序。</p><p> [5] Standley，T.，Zamir，A.，Chen，D.，Guibas，L.，Malik，J.，＆amp; Savarese，S。（2020）。哪个任务应该在多任务学习中学到？在国际机会学习会议中（第9120-9132页）。 PMLR。</p><p> [6] Zamir，A. R.，SAX，A.，Shen，W. B.，Guibas，L. J.，Malik，J.和Savarese，S. Taskomy：解开任务转移学习。在计算机视觉和模式识别（CVPR）的IEEE会议中。 IEEE，2018。</p><p>  Aminul Huq是清华大学的硕士学位＆＃39; S学士学位。他的研究兴趣位于计算机视觉和对抗机器学习。 </p><p>Mohammad Hanan Gani是Harman International Inc.的ML工程师，他在R＆amp; D团队中工作，以建立AI解决方案来解决自动化领域的挑战性问题。他的研究兴趣位于无人监督的深度学习，很少有射击学习和计算机愿景中的多任务学习。</p><p> Ammar Sherif是尼罗大学的教学和研究助理。他正在进行与学习有效的研究，包括来自多任务学习和不确定性估计的主题。</p><p> Abubakar Abid是Gradio的首席执行官/ Cofounder，在那里他建造了探索和解释机器学习模型的工具。他还研究了机器学习到医学作为斯坦福大学研究员的应用。</p><p>     Aminul Huq，Mohammad Hanan Gani，Ammar Sherif，Abubakar Abid，＆＃34;如何智能地进行多任务学习＆＃34;，梯度，2021。</p><p>  @Article {aminulmultAstask2021，作者= {Huq，Aminul和Gani，Mohammad Hanan和Sherif，Ammar和Abid，Abubakar}，Title = {如何做多任务学习智能}，期刊= {梯度}，年= { 2021}，Hopublished = {https://thegradient.pub/how-to-do-multi-task-learning-intelligpless}，} </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://thegradient.pub/how-to-do-multi-task-learning-intelligently/">https://thegradient.pub/how-to-do-multi-task-learning-intelligently/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/多任务/">#多任务</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/task/">#task</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/任务/">#任务</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>