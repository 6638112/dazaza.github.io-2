<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么使用Android的神经网络API有时2乘3有时等于7？ Why can 2 times 3 sometimes equal 7 with Android's Neural Network API?</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Why can 2 times 3 sometimes equal 7 with Android's Neural Network API?<br/>为什么使用Android的神经网络API有时2乘3有时等于7？ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-24 04:18:41</div><div class="page_narrow text-break page_content"><p>Two times three equals six or at least that’s what I naively expected.</p><p>三分之二等于六，或者至少是我天真的期望。</p><p> It is well-known that floating point matrix multiplication can result in a variety of  surprises.But did you know that quantized neural networks are not deterministic as well?A quantized neural network uses integers tomultiply weights and activations in a neural network.Thus, the naive logic goes, two and three represented and multiplied as integers should always give the exact same result,namely six.</p><p> 众所周知，浮点矩阵乘法可能会导致各种意外情况，但您是否知道量化神经网络也不是确定性的？量化神经网络使用整数将神经网络中的权重和激活值相乘。幼稚的逻辑说，两个和三个表示并乘以整数的乘积始终应得出完全相同的结果，即六个。</p><p> However, according to the  Android NNAPI Driver Validation documentation, adriver is valid if multiplication of quantized tensors is within plus or minusone bit.</p><p> 但是，根据Android NNAPI驱动程序验证文档，如果量化张量的乘法在正或负位之内，则驱动程序有效。</p><p>  The non-determinism of quantized multiplication is a result of optimizing forthe most common neural network deployment use cases. Neural networks used forclassification and detection generally work well with small amounts ofnon-determinism due to the nature of the mathematical operations involved in neural networks. Furthermore, such networks often benefit from training withsome noise (e.g. batch normalization). Likewise, the input to many neuralnetworks comes from a camera that generates inherently variable images.</p><p>  量化乘法的不确定性是针对最常见的神经网络部署用例进行优化的结果。由于神经网络所涉及的数学运算的性质，用于分类和检测的神经网络通常在少量的不确定性下工作良好。此外，这样的网络通常受益于带有一些噪声的训练（例如，批量归一化）。同样，许多神经网络的输入来自生成固有可变图像的摄像机。</p><p> The main challenge associated with neural network deployment on the phoneis creating models that are sufficiently fast, power friendly, and small ondisk. Quantizing a neural network helps one improve in all of these categories.Quantized weights take up less space on disk, take less energy to transfer, andcan be moved more quickly between different levels of memory.</p><p> 与在电话上部署神经网络相关的主要挑战是创建足够快速，省电且磁盘空间较小的模型。量化神经网络有助于改善所有这些类别。量化权重占用磁盘上的空间更少，传递能量更少，并且可以在不同级别的内存之间更快地移动。</p><p> The last issue at play is that it is much cheaper (both in terms of computingcost and engineering effort) to quantize a pre-trained model than to train anexisting model from scratch. Consequently, the method developed in this Integer Inference papertargets quantizing existing models. The key idea is that a floating pointvalue can be approximated as a scale times a quantized value minus a zeropoint. (Also take a look at the  Tensorflow Quantization Spec).The scale and zero point can be chosen relatively easily by, forinstance, looking at therange of the weights and activations when evaluating the model on a number ofrepresentative inputs.</p><p> 最后一个问题是，量化预训练模型要比从头开始训练一个已有的模型便宜得多（在计算成本和工程工作方面）。因此，此Integer Inference论文中开发的方法旨在量化现有模型。关键思想是，浮点值可以近似为比例乘以量化值减去零点后的值。 （也可以看一下Tensorflow量化规范）。例如，在许多代表输入上评估模型时，可以通过查看权重和激活范围来相对容易地选择比例和零点。</p><p> The loss in predictive performance due to quantization in many practical use cases issmall and this abstraction is widely implemented and baked into hardware,for instance, by Qualcomm’  Snapdragon Neural Processing Engine SDK.[Strictly speaking, the drivers for this hardware could bechanged to ensure determinism, but that may result in a significant loss inspeed at inference time.]The SNPE was developed and subsequently, Android sought to simplify the processof deploying neural networks given the wide variety of availableaccelerators. The restrictions on the driver tests were relaxed after pushback fromchip manufacturers whose hardware and drivers did not produce deterministicquantized math.</p><p> 在许多实际使用案例中，由于量化导致的预测性能损失很小，这种抽象已被广泛实施并移植到硬件中，例如，通过Qualcomm的Snapdragon神经处理引擎SDK。[严格来说，可以更改此硬件的驱动程序以确保确定性，但可能会导致推理时速度显着损失。]开发了SNPE，随后，考虑到可用的加速器种类繁多，Android试图简化部署神经网络的过程。从硬件和驱动程序无法产生确定性的量化数学的芯片制造商推出后，对驱动程序测试的限制便放松了。 </p><p> Unfortunately, the NN API off by at most 1 (or 3 for a larger network) leaves alot of room for funny business that can change the results by small amounts.The hardware and driver vendors want to produce fast times and low energyusage numbers onvarious neural network benchmarks and are less concerned about reproducibility.This can lead to unexpected changes in neural network outputs.For instance, the zero point or scale of theweights might be changed for some reason by the hardware, and the quantizedvalues are then recomputed. Another thing that can happen is that the range mightbe slightly shifted to make zero exactly representable (see Case 3: Inputs areboth negative and positive in the  Snapdragon Neural Processing Engine SDK).Likewise, as the off by one error bound is on a per layer basis, largernetworks generate accumulating error. Practically speaking, the error bound isincreased to three for mobilenet in the NN API driver validation tests. Othernetworks could produce larger errors.</p><p>不幸的是，NN API最多减少1个（对于大型网络，最多减少3个）为有趣的业务留出了很大的空间，可以少量改变结果。硬件和驱动程序供应商希望在各种神经网络上产生快速，低能耗的数字网络基准测试，并且不太关注可重复性，这可能导致神经网络输出出现意想不到的变化，例如，由于某些原因，硬件可能会更改权重的零点或小数位数，然后重新计算量化值。可能发生的另一件事是，该范围可能会略微偏移以使零可以精确表示（请参阅案例3：在Snapdragon Neural Processing Engine SDK中输入既为负也为正）。在此基础上，较大的网络会产生累积误差。实际上，在NN API驱动程序验证测试中，移动网络的错误范围增加到三个。其他网络可能会产生更大的错误。</p><p>  The lack of determinism for quantized neural networks touches on the broaderissue of cross platform reproducibility and performance optimization for neural networks.As the neural networks get deployed more and more widely, these issues will grow in importance.</p><p>  量化神经网络缺乏确定性，触及了跨平台可再现性和神经网络性能优化的广泛问题。随着神经网络的部署越来越广泛，这些问题的重要性将日益提高。</p><p> The first major issue is that if you want to deploy a neural network acrossmany platforms, you don’t have any guarantees that you are going to get thesame results.Even if you manage to assemble a large team to test your networks on manydifferent platforms, the problem isn’t solved. The lack of determinism means that even if you test your networkon every platform, a driver update may unexpectedly change your results.This creates an ongoing engineering cost beyond the initial fixed cost ofvalidating the deployment on existing hardware.</p><p> 第一个主要问题是，如果您想在多个平台上部署神经网络，则无法保证将获得相同的结果。即使您设法组建了一支庞大的团队来在许多不同的平台上测试网络，问题没有解决。缺乏确定性意味着即使在每个平台上测试网络，驱动程序更新也可能会意外更改结果。这会产生持续的工程成本，超出了在现有硬件上验证部署的初始固定成本。</p><p> The next major issue is that some application areas have more stringentstandards in terms of reproducibility and auditability. Suppose that a neuralnetwork is used in a self-driving car that gets in an accident. For auditingpurposes, it is important to be able to replay the decisions made by the car’scontrol system exactly. In the best case, this requires a significantengineering effort to keep track of all of the driver versions and internaltesting to verify reproducibility for any deployment.</p><p> 下一个主要问题是，某些应用领域在可重复性和可审核性方面具有更严格的标准。假设在发生事故的自动驾驶汽车中使用了神经网络。对于审核目的，重要的是能够准确重放由汽车控制系统做出的决策。在最佳情况下，这需要大量的工程工作来跟踪所有驱动程序版本和内部测试，以验证任何部署的可重复性。</p><p> The final issue is that while classification and detection are not particularlysensitive to small changes in output, the lack of determinism makes it morechallenging to deploy neural network models which are deeper or more sensitive tochanges. For instance, imagine a language model that is unrolled over manyiterations to parse or generate a sentence. In that case, accumulating errorscould cause unexpected results.</p><p> 最终的问题是，尽管分类和检测对输出的微小变化并不特别敏感，但是缺乏确定性使得部署更具深度或更敏感变化的神经网络模型更具挑战性。例如，想象一下一种语言模型，该语言模型会展开许多迭代以解析或生成一个句子。在这种情况下，累积错误可能会导致意外结果。</p><p>  In the past couple years, there has been an explosion of experimentation inneural network deployment from software frameworks to hardwareaccelerators. As the field matures, questions about best practices will slowlybe answered. The ability to  train once, deploy anywhere will become easierand easier. In the particular case of determinism, the most reasonable nextstep is to have a driver test that ensures determinism and publish a list ofvendors that implement a deterministic API.</p><p>  在过去的几年中，从软件框架到硬件加速器的实验性神经网络部署激增。随着该领域的成熟，有关最佳实践的问题将慢慢得到解答。训练一次，部署到任何地方的能力将变得越来越容易。在确定性的特定情况下，最合理的下一步是进行驱动程序测试，以确保确定性并发布实现确定性API的供应商列表。</p><p> The question of determinism is only one small part of the puzzle of being ableto deploy anywhere. Another subtle but important issue is that differentoperations will run faster on different hardware. Even if a network can deployanywhere in a reproducible manner, an even more fundamental issue is whetheror not a network will be fast and power efficient on any platform. Indeed, while therehas been a great amount of progress in the field of neural networkdeployment, there is much to be done to make the ecosystem easier to use andmore mature.</p><p> 确定性问题只是能够部署到任何地方的难题的一小部分。另一个微妙但重要的问题是，不同的操作将在不同的硬件上更快地运行。即使网络可以以可重现的方式部署到任何地方，一个更根本的问题是网络在任何平台上是否都将快速高效地运行。确实，尽管在神经网络部署领域已经取得了很大的进步，但是要使生态系统更易于使用和更加成熟，还有很多工作要做。 </p><p> If you are interested in learning more about issues surrounding neural networkdeployment or how to be a better machine learning engineer,connect with me on  LinkedIn to see my future  blog posts! If you think that Ican help you grow professionally, check out my  coaching page.</p><p>如果您想了解有关神经网络部署或如何成为一名更好的机器学习工程师的更多信息，请在LinkedIn上与我联系，以查看我将来的博客文章！ 如果您认为我可以帮助您专业成长，请查看我的辅导页面。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="http://alexanderganderson.github.io/engineering/2021/01/23/integer_indeterminism.html">http://alexanderganderson.github.io/engineering/2021/01/23/integer_indeterminism.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/android/">#android</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/equal/">#equal</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>