<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么深度学习能奏效，尽管它不应该奏效</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">为什么深度学习能奏效，尽管它不应该奏效</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-20 22:43:10</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/feddd3c63be51cbfdf0157dd7226aed9.jpg"><img src="http://img2.diglog.com/img/2020/10/feddd3c63be51cbfdf0157dd7226aed9.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This is a big question, and I’m not a particularly big person. As such, these are all likely to be obvious observations to someone deep in the literature and theory. What I find however is that there are a base of unspoken intuitions that underlie expert understanding of a field, that are never directly stated in the literature, because they can’t be easily proved with the rigor that the literature demands. And as a result, the insights exist only in conversation and subtext, which make them inaccessible to the casual reader.</p><p>这是一个很大的问题，我也不是一个特别高大的人。因此，对于深谙文学和理论的人来说，这些都可能是显而易见的观察。然而，我发现，有一种潜移默化的直觉是专家对一个领域的理解的基础，这些直觉从来没有在文献中直接陈述过，因为它们不容易用文献要求的严谨性来证明。因此，洞察力只存在于对话和潜台词中，这使得普通读者无法接触到它们。</p><p> Because I have no need of rigor to post on the internet, (or even a need to be correct) I’m going to post some of those intuitions here as I understand them. If this is all obvious to you, skip to the section on “Suggestions for Research” because there are a lot of ways that I think typical papers ignore things that most researchers believe to be true.</p><p>因为我不需要在互联网上发帖的严谨(甚至不需要是正确的)，所以我会在这里贴出一些我理解的直觉。如果这对你来说都是显而易见的，跳到“研究建议”部分，因为我认为典型的论文有很多方式忽略了大多数研究人员认为是正确的事情。</p><p> In particular I find that people from a statistics background tend to throw up their hands at deep learning, because from a traditional statistics perspective, none of it can possibly work. This makes it very frustrating that it does. As a result they tend to have a much more dim view of its results and methods than their continued success warrants, so I hope here that I can bridge some of that gap.</p><p>特别是，我发现有统计学背景的人倾向于放弃深度学习，因为从传统的统计学角度来看，这些都不可能奏效。这让它变得非常令人沮丧。因此，他们往往对其结果和方法的看法比他们继续成功的保证要黯淡得多，所以我希望在这里我能弥合一些差距。</p><p> The key thing I’m going to try to intuitively explain is why models always get better when they are bigger and deeper, even when the amount of data they consume stays the same or gets smaller. Some of this might turn out to be wrong, but I think it’s much more likely to be incomplete than to be wrong. The effects I describe here likely matter, even though it’s possible they aren’t the dominant causes. There is going to be nothing terribly formal here, which will madden some people, and relieve others. If you find this all irritatingly hand wavy, go read papers about the  lottery ticket theory of deep learning instead, because I think that’s the closest thing to a formal theory that encapsulates most of this and is currently making progress.</p><p>我要试着直观地解释的关键是，为什么模型越大越深，即使它们消耗的数据量保持不变或变小，也总是会变得更好。其中一些可能会被证明是错误的，但我认为它更有可能是不完整的，而不是错误的。我在这里描述的影响可能很重要，即使它们可能不是主要原因。这里不会有什么特别正式的东西，这会让一些人发疯，也会让另一些人松一口气。如果你觉得这一切都令人恼火，那就去看看关于深度学习的彩票理论的论文吧，因为我认为这是最接近正式理论的东西，它概括了大部分内容，目前正在取得进展。</p><p>  If you start your parameters in a reasonable place, they’re already close to good ones, even though they’re totally random.</p><p>如果你从一个合理的位置开始你的参数，它们已经接近好的位置了，即使它们完全是随机的。</p><p> In high dimensional spaces, distance is a statistical concept. Squared euclidean distance is just a big sum, and statistics tells us what happens to all big sums. They  become normal distributions, and they become relatively tighter and tighter around their mean as the number of terms in the sum increases. This means that when there is any amount of well behaved randomness involved, all distances in high dimensions are about the same. In a model, with parameters that begin as random variables due to initialization, and end as random variables due to the nature of the data, the central limit theorem applies to these sums. So all sets of parameters in a high dimensional model are about equally close to/far from each other.</p><p>在高维空间中，距离是一个统计学概念。欧几里得距离的平方只是一个很大的和，统计数据告诉我们所有大和会发生什么。它们变成正态分布，并且随着总和中项数的增加，它们在平均值周围变得越来越紧。这意味着当涉及到任何数量的行为良好的随机性时，高维中的所有距离都大致相同。在模型中，参数由于初始化而以随机变量开始，由于数据的性质而以随机变量结束，中心极限定理适用于这些和。因此，高维模型中的所有参数集彼此之间的距离大致相等。</p><p> In the dimensions we live in, we’re used to the idea that some things are closer together than other things, so we mentally think of concepts like “regions” and think about things like bad regions and good regions for parameters. But high dimensional spaces are extremely well connected. You can get to anywhere with a short jump from anywhere else. There are no bad places to start. If the magnitudes of the random initialization are about right, all places are reasonably good. No matter where you start, you’re close to good parameters, and you’re as likely to be close to good parameters as to any others. The only assumption we need for this to be formally true is that the parameters of the final models come from roughly the same distribution as the parameters at initialization, which is pretty mild.</p><p>在我们生活的维度中，我们习惯了这样的想法，有些东西比其他东西更接近，所以我们会在脑海中想到像“区域”这样的概念，并考虑像参数的坏区域和好区域这样的东西。但是高维空间是非常紧密相连的。你可以从其他任何地方跳到任何地方。没有什么不好的地方可以开始。如果随机初始化的大小大致正确，则所有地方都相当好。无论您从哪里开始，您都接近良好的参数，而且与其他任何参数一样，您也很可能接近良好的参数。我们需要的唯一假设是，最终模型的参数来自与初始化时的参数大致相同的分布，这是相当温和的。</p><p> High dimensional spaces are unlikely to have local optima, and probably don’t have any optima at all.</p><p>高维空间不太可能有局部最优值，也可能根本没有最优值。</p><p>  Just recall what is necessary for a set of parameters to be at a optimum. All the gradients need to be zero, and the hessian needs to be positive semidefinite. In other words, you need to be surrounded by walls. In 4 dimensions, you can walk through walls. GPT3 has 175 billion parameters. In 175 billion dimensions, walls are so far beneath your notice that if you observe them at all it is like God looking down upon individual protons.</p><p>只需回想一下一组参数处于最佳状态所需的条件。所有的梯度都需要为零，而Hessian需要是半正定的。换句话说，你需要被围墙包围。在4维空间中，你可以穿墙而过。GPT3有1750亿个参数。在1750亿个维度中，墙壁远远低于你的注意，如果你完全观察它们，那就像是上帝俯视着单个质子。</p><p> If there’s any randomness at all in the loss landscape, which of course there is, it’s vanishingly unlikely that all of the millions or billions of directions the model has to choose from will be simultaneously uphill. With so many directions to choose from you will always have at least one direction to escape. It’s just completely implausible that any big model comes close to any optima at all. In fact it’s implausible that an optimum exists. Without explicit regularization that bounds the magnitude of the values, forces positive curvature, and hurts performance of the model, all real models diverge.</p><p>如果损失前景中有任何随机性(当然是随机的)，那么模型必须选择的数百万或数十亿个方向都不太可能同时上坡。有这么多方向可供选择，你总会有至少一个方向可逃。这是完全不可能的，任何一个大模型都能接近任何一个最优模型。事实上，最优的存在是令人难以置信的。如果没有明确的正则化来限制值的大小，强制正曲率，并损害模型的性能，所有真实的模型都会发散。</p><p>  Early stopping is better regularization than any hand picked a priori regularization, including implicit regularization like model size.</p><p>提前停止的正则化比任何手工挑选的先验正则化都要好，包括隐式正则化，如模型大小。</p><p> Half of an introductory statistics textbook is about how to deal with collinearity. But collinearity only matters if you care about attribution, which you don’t. Suppose you have two nearly collinear inputs. At the beginning of gradient descent, every correlated input’s coefficient is getting nearly the same gradients. Eventually they won’t, and eventually some of the difference between their gradients will be noise and thus overfitting. So when that happens you just stop!</p><p>统计学入门教科书中有一半是关于如何处理共线性的。但是共线性只在你关心归因的情况下才重要，而你并不关心。假设你有两个近乎共线的输入。在梯度下降开始时，每个相关输入的系数都得到几乎相同的梯度。最终它们不会，最终它们的梯度之间的一些差异将是噪声，从而过度拟合。所以，当这种情况发生时，你就停下来！</p><p> This means that the difference between having n parameters and n+1 parameters is something that is effectively controlled by early stopping. You might have a lot of effectively unused parameters, but again, you don’t care. Having a low dimensional model just biases the model, because if a low dimensional model worked, early stopping would choose that automatically due to the way gradients work. The validation set tells you when most of the gradients you’re getting are just noise, so you just ignore them and stop.</p><p>这意味着具有n个参数和n+1个参数之间的差异可以通过提前停止有效地控制。您可能有很多实际上未使用的参数，但是同样，您并不关心。拥有低维模型只会使模型产生偏差，因为如果低维模型起作用，由于渐变的工作方式，提前停止会自动选择。当您获得的大多数渐变都是噪波时，验证集会告诉您，因此您只需忽略它们并停止。</p><p>  When we start walking to it, we can’t ever get stuck along the way, because there are no local optima.</p><p>当我们开始向它走去的时候，我们永远不会被困在路上，因为没有局部最优。</p><p> Once we’ve stumbled upon a good set of parameters, we’ll know it and we can just stop.</p><p>一旦我们偶然发现了一组很好的参数，我们就会知道它，然后我们就可以停下来了。</p><p> That’s it. Once you believe all of these things, then it becomes intuitive that big models are always better even without more data, because all of these trends become more and more true the more dimensions you have.</p><p>就这样。一旦你相信了所有这些事情，那么你就会直觉地发现，即使没有更多的数据，大的模型也总是更好，因为所有这些趋势都会变得越来越真实，你拥有的维度越多。</p><p>  Stop talking about minima. Stop talking about how your optimization algorithm behaves around a minimum. Nobody ever trains their model remotely close to convergence. ADAM doesn’t even provably converge. All real models diverge! You are nowhere close to a minimum! Stop talking about minima already goddamnit! Why even think about minima?! Minima are a myth!</p><p>别再说最低限度了。不要再谈论您的优化算法是如何在最小值附近运行的。从来没有人把他们的模型训练到接近收敛的程度。亚当甚至不能被证明是收敛的。所有的真人模特都分道扬镳了！你离最低限度还差得远呢！别再说Minima了，他妈的！为什么要考虑最低限度呢？！最低限度是个神话！</p><p> Everybody proves their results for minima of a convex function, and then talks about how further research is necessary to prove results about non-convex functions. But this doesn’t make any sense. Deep models are close enough to convex for all the usual things to work just because the dimensionality makes local optima impossible. What really needs further research is how algorithms behave very far from minima.</p><p>每个人都证明了他们关于凸函数极小值的结果，然后谈论如何需要进一步的研究来证明关于非凸函数的结果。但这没有任何意义。深度模型足够接近于凸，所有常见的东西都可以工作，因为维数使得局部最优变得不可能。真正需要进一步研究的是算法的行为如何远离极小值。</p><p> An optimization algorithm is best thought of as a priority queue of things to learn, and the thing that’s important to prove is that your algorithm learns the good things first. When your model peaks on the validation set, it is starting to learn more bad things than good things. If you have an optimization algorithm that is better at learning the good things before the bad things, then it will achieve a lower loss. For me this explains a lot of otherwise confusing phenomena, like why  distillation works so well. Distillation causes the student model to learn more of the good things before it learns the bad things, because the teacher model knows more good things than bad things already. This is why it’s possible for a more powerful model to learn usefully from a weaker one.</p><p>优化算法最好被认为是要学习的事物的优先队列，需要证明的重要一点是，您的算法首先学习好的东西。当您的模型在验证集上达到顶峰时，它开始了解更多不好的东西而不是好的东西。如果你有一个优化算法，它更善于在坏事之前学习好的东西，那么它就会实现更低的损失。对我来说，这解释了很多其他令人困惑的现象，比如为什么蒸馏效果这么好。蒸馏使学生模型在学习坏东西之前先学习更多的好东西，因为教师模型已经知道更多的好东西了。这就是为什么一个更强大的模型可以从一个较弱的模型中有效地学习的原因。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://moultano.wordpress.com/2020/10/18/why-deep-learning-works-even-though-it-shouldnt/">https://moultano.wordpress.com/2020/10/18/why-deep-learning-works-even-though-it-shouldnt/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/深度学习/">#深度学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>