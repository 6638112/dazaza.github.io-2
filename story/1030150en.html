<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Julia GPU：Julia语言如何让程序员轻松使用GPU</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Julia GPU：Julia语言如何让程序员轻松使用GPU</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-21 03:23:19</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/26810144d219c4006831567257391f46.png"><img src="http://img2.diglog.com/img/2020/10/26810144d219c4006831567257391f46.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Join the Not a Monad Tutorial Telegram   group  or   channel  to talk about programming, computer science and papers. See you there!</p><p>加入Not a Monad Tutorial电报小组或频道，讨论编程、计算机科学和论文。那里见！</p><p>  If you are looking for good engineers send me an email to mail@fcarrone.com or you can also reach me via twitter at   @federicocarrone .</p><p>如果您正在寻找优秀的工程师，请给我发一封电子邮件到mail@fcarrone.com，或者您也可以通过Twitter联系我，电子邮件地址是@Federation iccarrone。</p><p>  We are living in a time where more and more data is being created every day as well as new techniques and complex algorithms that try to extract the most out of it. As such, CPU capabilities are approaching a bottleneck in their computing power. GPU computing opened its way into a new paradigm for high-performance and parallel computation a long time ago, but it was not until recently that it become massively used for data science. In this interview,  Tim Besard, one of the main contributors to the JuliaGPU project, digs into some of the details about GPU computing and the features that make Julia a language suited for such tasks, not only from a performance perspective but also from a user one.</p><p>我们生活在一个每天都在创建越来越多的数据的时代，也有新的技术和复杂的算法试图从这些数据中提取出最大的价值。因此，CPU能力正在接近其计算能力的瓶颈。GPU计算很早以前就开启了高性能和并行计算的新范式，但直到最近才开始大规模用于数据科学。在这次采访中，JuliaGPU项目的主要贡献者之一Tim Besard不仅从性能角度，而且从用户角度，深入探讨了有关GPU计算的一些细节，以及使Julia成为适合这类任务的语言的特性。</p><p>  Please tell us a bit about yourself. What is your background? what is your current position?</p><p>请给我们介绍一下你自己。你的背景是什么？你现在的职位是什么？</p><p> I’ve always been interested in systems programming, and after obtaining my CS degree I got the opportunity to start a PhD at Ghent University, Belgium, right when Julia was first released around 2012. The language seemed intriguing, and since I wanted to gain some experience with LLVM, I decided to port some image processing research code from MATLAB and C++ to Julia. The goal was to match performance of the C++ version, but some of its kernels were implemented in CUDA C… So obviously Julia needed a GPU back-end!</p><p>我一直对系统编程感兴趣，在获得CS学位后，我有机会开始在比利时根特大学攻读博士学位，就在2012年左右Julia第一次获释的时候。该语言似乎很有趣，因为我想获得一些LLVM的经验，所以我决定将一些图像处理研究代码从MATLAB和C++移植到Julia。我们的目标是与C++版本的性能相匹配，但它的一些内核是在CUDAC…中实现的。所以很明显，Julia需要一个GPU后端！</p><p> That was easier said than done, of course, and much of my PhD was about implementing that back-end and (re)structuring the existing Julia compiler to facilitate these additional back-ends. Nowadays I’m at Julia Computing, where I still work on everything GPU-related.</p><p>当然，这说起来容易做起来难，我的博士学位主要是关于实现后端和(重新)构造现有的Julia编译器以促进这些额外的后端。现在我在Julia Computing，在那里我仍然在做所有与GPU相关的事情。</p><p>  JuliaGPU is the name we use to group GPU-related resources in Julia: There’s a  GitHub organization where most packages are hosted, a  website to point the way for new users, we have  CI infrastructure for JuliaGPU projects, there’s a Slack channel and Discourse category, etc.</p><p>JuliaGPU是我们用来对Julia中的GPU相关资源进行分组的名称：有一个托管大多数软件包的GitHub组织，有一个为新用户指路的网站，我们有JuliaGPU项目的CI基础设施，有一个松弛频道和话语类别，等等。</p><p> The goal of all this is to make it easier to use GPUs for all kinds of users. Current technologies often impose significant barriers to entry: CUDA is fairly tricky to install, C and C++ are not familiar to many users, etc. With the software we develop as part of the JuliaGPU organization, we aim to make it easy to use GPUs, without hindering the ability to optimize or use low-level features that the hardware has to offer.</p><p>所有这些都是为了让各种用户更容易使用GPU。目前的技术往往设置了很大的进入壁垒：CUDA安装相当棘手，许多用户不熟悉C和C++，等等。通过我们作为JuliaGPU组织的一部分开发的软件，我们的目标是使GPU的使用变得容易，而不会阻碍优化或使用硬件必须提供的低级功能的能力。</p><p>  GPU computing means using the GPU, a device originally designed for graphics processing, to perform general-purpose computations. It has grown more important now that CPU performance is not improving as steadily as it used to. Instead, specialized devices like GPUs or FPGAs are increasingly used to improve the performance of certain computations. In the case of GPUs, the architecture is a great fit to perform highly-parallel applications. Machine learning networks are a good example of such parallel applications, and their popularity is one of the reasons GPUs have become so important.</p><p>GPU计算是指使用GPU(一种最初设计用于图形处理的设备)执行通用计算。由于CPU性能不再像过去那样稳步提高，这一点变得更加重要。取而代之的是，越来越多地使用诸如GPU或FPGA之类的专用设备来提高某些计算的性能。在使用GPU的情况下，该架构非常适合执行高度并行的应用程序。机器学习网络就是这种并行应用的一个很好的例子，它们的普及是GPU变得如此重要的原因之一。</p><p>  Julia’s main advantage is that the language was designed to be compiled. Even though the syntax is high-level, the generated machine code is compact and has great performance characteristics (for more details, see  this paper). This is crucial for GPU execution, where we are required to run native binaries and cannot easily (or efficiently) interpret code as is often required by other language’s semantics.</p><p>Julia的主要优点是该语言是为编译而设计的。尽管语法是高级的，但是生成的机器码是紧凑的，并且具有很好的性能特征(有关更多详细信息，请参阅本文)。这对于GPU执行是至关重要的，在GPU执行中，我们需要运行本机二进制文件，并且不能像其他语言的语义通常所要求的那样容易(或有效地)解释代码。</p><p> Because we’re able to directly compile Julia for GPUs, we can use almost all of the language’s features to build powerful abstractions. For example, you can define your own types, use those in GPU arrays, compose that with existing abstractions like lazy &#34;Transpose&#34; wrappers, access those on the GPU while benefiting from automatic bounds-checking (if needed), etc.</p><p>因为我们能够直接为GPU编译Julia，所以我们几乎可以使用该语言的所有功能来构建强大的抽象。例如，您可以定义自己的类型，在GPU数组中使用这些类型，将其与现有的抽象(如惰性转置包装)组合在一起，在GPU上访问这些类型，同时受益于自动边界检查(如果需要)，等等。</p><p> From a Python programmer perspective, how does CUDA.jl compare to PyCUDA? Are their functionalities equivalent?</p><p>从Python程序员的角度来看，CUDA.jl与PyCUDA相比如何？它们的功能是否相同？</p><p> PyCUDA gives the programmer access to the CUDA APIs, with high-level Python functions that are much easier to use. CUDA.jl provides the same, but in Julia. The `hello world` from PyCUDA’s home page looks almost identical in Julia:</p><p>PyCUDA为程序员提供了对CUDAAPI的访问，并提供了更易于使用的高级Python函数。Jl提供相同的功能，但在Julia中提供。PyCUDA主页上的“hello world”与Julia几乎一模一样：</p><p> using CUDA function multiply_them(dest, a, b)  i = threadIdx().x  dest[i] = a[i] * b[i]  return end a = CuArray(randn(Float32, 400)) b = CuArray(randn(Float32, 400)) dest = similar(a) @cuda threads=400 multiply_them(dest, a, b) println(dest-a.*b)</p><p>使用CUDA函数Multiply_Them(DEST，a，b)i=threadIdx().x DEST[i]=a[i]*b[i]return end a=CuArray(randn(Float32,400))b=CuArray(randn(Float32,400))DEST=相似(A)@Cuda线程=400 Multiply_Them(DEST，a，b)println(DEST-a.*b)。</p><p> There’s one very big difference: &#34;multiply_them&#34; here is a function written in Julia, whereas PyCUDA uses a kernel written in CUDA C. The reason is straightforward: Python is not simple to compile. Of course, projects like Numba prove that it is very much possible to do so, but in the end those are separate compilers that try to match the reference Python compilers as closely as possible. With CUDA.jl, we integrate with that reference compiler, so it’s much easier to guarantee consistent semantics and follow suit when the language changes (for more details, refer to  this paper).</p><p>有一个非常大的区别：&#34；Multiply_Them&#34；这里是用Julia编写的函数，而PyCUDA使用的是用CUDA C编写的内核。原因很简单：Python不容易编译。当然，像Numba这样的项目证明了这样做的可能性很大，但最终这些都是独立的编译器，它们试图尽可能地与参考Python编译器相匹配。使用CUDA.jl，我们可以与该引用编译器集成，因此更容易保证语义一致，并在语言更改时遵循(有关更多详细信息，请参阅本文)。</p><p>  Not at all. CUDA.jl targets different kinds of (GPU) programmers. If you are confident writing your own kernels, you can do so, while using all of the low-level features CUDA GPUs have to offer. But if you are new to the world of GPU programming, you can use high-level array operations that use existing kernels in CUDA.jl. For example, the above element-wise multiplication could just as well be written as:</p><p>不用谢。CUDA.jl面向不同类型的(GPU)程序员。如果您对编写自己的内核很有信心，那么您可以这样做，同时使用CUDA GPU必须提供的所有低级功能。但是，如果您是GPU编程的新手，您可以使用使用CUDA.jl中现有内核的高级数组操作。例如，上面的逐元素乘法可以写成：</p><p>  Is it necessary to know how to code in CUDA.jl to take full advantage of GPU computing in Julia?</p><p>是否需要知道如何在CUDA.jl中编码才能充分利用Julia中的GPU计算？</p><p> Not for most users. Julia has a powerful language of generic array operations (&#34;map&#34;, &#34;reduce&#34;, &#34;broadcast&#34;, &#34;accumulate&#34;, etc) which can be applied to all kinds of arrays, including GPU arrays. That means you can often re-use your codebase developed for the CPU with CUDA.jl ( this paper shows some powerful examples). Doing so often requires minimal changes: changing the array type, making sure you use array operations instead of for loops, etc.</p><p>对于大多数用户来说并非如此。Julia有一种强大的泛型数组操作语言(&#34；map&34；，&#34；Reduce&#34；，&#34；Broadcast&#34；，&#34；Columate&#34；等)，可以应用于所有类型的数组，包括GPU数组。这意味着您经常可以通过CUDA.jl重用为CPU开发的代码库(本文展示了一些强大的示例)。这样做通常只需要最少的更改：更改数组类型，确保使用数组操作而不是for循环，等等。</p><p> It’s possible you need to go beyond this style of programming, e.g., because your application doesn’t map cleanly onto array operations, to use specific GPU features, etc. In that case, some basic knowledge about CUDA and the GPU programming model is sufficient to write kernels in CUDA.jl.</p><p>您可能需要超越这种编程风格，例如，因为您的应用程序没有清晰地映射到数组操作，使用特定的GPU功能等。在这种情况下，一些关于CUDA和GPU编程模型的基本知识就足以用CUDA.jl编写内核。</p><p> How is the experience of coding a kernel in CUDA.jl in comparison to CUDA C and how transferable is the knowledge to one another?</p><p>与CUDAC相比，在CUDA.jl中编写内核的体验如何？这些知识彼此之间的传递性如何？</p><p> It’s very similar, and that’s by design: We try to keep the kernel abstractions in CUDA.jl close to their CUDA C counterparts such that the programming environment is familiar to existing GPU programmers. Of course, by using a high-level source language there’s many quality-of-life improvements. You can allocated shared memory, for example, statically and dynamically as in CUDA C, but instead of a raw pointers we use an N-dimensional array object you can easily index. An example from the  NVIDIA developer blog:</p><p>它非常相似，这是经过设计的：我们试图使CUDA.jl中的内核抽象接近它们的CUDAC对应物，以便现有的GPU程序员熟悉编程环境。当然，通过使用高级源语言可以提高生活质量。例如，您可以像在CUDAC中那样静态和动态地分配共享内存，但是我们使用的不是原始指针，而是一个可以轻松索引的N维数组对象。NVIDIA开发人员博客中的一个示例：</p><p> __global__ void staticReverse(int *d, int n) {  __shared__ int s[64];  int t = threadIdx.x;  int tr = n-t-1;  s[t] = d[t];  __syncthreads();  d[t] = s[tr]; }</p><p>__global__void staticReverse(int*d，int n){__Shared__int s[64]；int t=threadIdx.x；int tr=n-t-1；s[t]=d[t]；__syncthread()；d[t]=s[tr]；}。</p><p> The CUDA.jl equivalent of this kernel looks very familiar, but uses array objects instead of raw pointers:</p><p>此内核的CUDA.jl等效项看起来非常熟悉，但是使用数组对象而不是原始指针：</p><p> function staticReverse(d)  s = @cuStaticSharedMem(Int, 64)  t = threadIdx().x  tr = length(d)-t+1  s[t] = d[t]  sync_threads()  d[t] = s[tr]  return end</p><p>函数静态Reverse(D)s=@cuStaticSharedMem(Int，64)t=threadIdx().x tr=长度(D)-t+1 s[t]=d[t]sync_thread()d[t]=s[tr]返回端</p><p> Using array objects has many advantages, e.g. multi-dimensional is greatly simplified and we can just do &#34;d[i,j]&#34;. But it’s also safer, because these accesses are bounds checked:</p><p>使用数组对象有很多优点，例如，多维被极大地简化，我们可以只做#34；d[i，j]&#34；。但它也更安全，因为这些访问是经过边界检查的：</p><p> julia&gt; a = CuArray(1:64) 64-element CuArray{Int64,1}:  1  2  3  ⋮  62  63  64 julia&gt; @cuda threads=65 staticReverse(a) ERROR: a exception was thrown during kernel execution. Stacktrace:  [1] throw_boundserror at abstractarray.jl:541</p><p>Julia&gt；a=cuArray(1：64)64元素cuArray{Int64，1}：1 2 3⋮62 63 64 julia&gt；@cuda thread=65 staticReverse(A)错误：内核执行期间引发异常。堆栈跟踪：[1]在abstractarray.jl：541处抛出_rangserror。</p><p> Bounds checking isn’t free, of course, and once we’re certain our code is correct we can add an &#34;@inbounds&#34; annotation to our kernel and get the high-performance code we expect:</p><p>当然，边界检查不是免费的，一旦我们确定我们的代码是正确的，我们就可以向内核添加@inbound；注释，并获得我们期望的高性能代码：</p><p> julia&gt; @device_code_ptx @cuda threads=64 staticReverse(a) .visible .entry staticReverse(.param .align 8 .b8 d[16]) {  .reg .b32 %r&lt;2&gt;;  .reg .b64 %rd&lt;15&gt;;  .shared .align 32 .b8 s[512]; mov.b64 %rd1, d;  ld.param.u64 %rd2, [%rd1];  ld.param.u64 %rd3, [%rd1+8];  mov.u32 %r1, %tid.x;  cvt.u64.u32 %rd4, %r1;  mul.wide.u32 %rd5, %r1, 8;  add.s64 %rd6, %rd5, -8;  add.s64 %rd7, %rd3, %rd6;  ld.global.u64 %rd8, [%rd7+8];  mov.u64 %rd9, s;  add.s64 %rd10, %rd9, %rd6;  st.shared.u64 [%rd10+8], %rd8;  bar.sync 0;  sub.s64 %rd11, %rd2, %rd4;  shl.b64 %rd12, %rd11, 3;  add.s64 %rd13, %rd9, %rd12;  ld.shared.u64 %rd14, [%rd13+-8];  st.global.u64 [%rd7+8], %rd14;  ret; } julia&gt; a 64-element CuArray{Int64,1}:  64  63  62  ⋮  3  2  1</p><p>Julia&gt；@device_code_ptx@cuda线程=64静态反向(A).可见.entry静态反向(.param.align 8.b8 d[16]){.reg.b32%r&lt；2&gt；；.reg.b64%rd&lt；15&gt；；.Shared.align 32.b8 s[512]；mov.b64%rd1，d；ld.param.u64%rd2，[%rd1]；ld.param.u64%rd3，[%rd1+8]；Mov.u32%r1，%tid.x；cvt.u64.u32%rd4，%r1；mul.wide.u32%rd5，%r1，8；add.s64%rd6，%rd5，-8；add.s64%rd7，%rd3，%rd6；ld.global.u64%rd8，[%rd7+8]；mov.u64%rd9，s；add.s64%rd10，%rd9，%rd6；st.shared.u64[%rd10+8]，%rd8；bar.sync 0；S64%rd11，%rd2，%rd4；shl.b64%rd12，%rd11，3；add.s64%rd13，%rd9，%rd12；ld.shared.u64%rd14，[%rd13+-8]；st.global.u64[%rd7+8]，%rd14；ret；}julia&gt；a 64元素立方体数组{Int64，1}：64 63 62⋮3 2 1。</p><p> Tools like &#34;@device_code_ptx&#34; make it easy for an experienced developer to inspect generated code and ensure the compiler does what he wants.</p><p>像&#34；@DEVICE_CODE_PTX&#34；这样的工具使有经验的开发人员可以轻松地检查生成的代码，并确保编译器执行他想要的操作。</p><p> Why does having a compiler have such an impact in libraries like CUDA.jl? (How was the process of integrating it to the Julia compiler?)</p><p>为什么拥有编译器会对CUDA.jl这样的库产生如此大的影响？(将其集成到Julia编译器的过程如何？)。</p><p> Because we have a compiler at our disposal, we can rely on higher-order functions and other generic abstractions that specialize based on the arguments that users provide. That greatly simplifies our library, but also gives the user very powerful tools. As an example, we have carefully implemented a `mapreduce` function that uses shared memory, warp intrinsics, etc to perform a high-performance reduction. The implementation is generic though, and will automatically re-specialize (even at run time) based on the arguments to the function:</p><p>因为我们可以使用编译器，所以我们可以依赖高阶函数和其他基于用户提供的参数进行专门化的泛型抽象。这大大简化了我们的库，但也为用户提供了非常强大的工具。例如，我们仔细实现了一个使用共享内存、WARP内部函数等来执行高性能缩减的`mapduce‘函数。不过，该实现是泛型的，并且将根据函数的参数自动重新专门化(即使在运行时也是如此)：</p><p>  With this powerful `mapreduce` abstraction, implemented by a experienced GPU programmer, other developers can create derived abstractions without such experience. For example, let’s implement a `count` function that evaluates for how many items a predicate holds true:</p><p>有了这个由经验丰富的GPU程序员实现的强大的“地图缩减”抽象，其他开发人员就可以在没有这种经验的情况下创建派生的抽象。例如，让我们实现一个`count`函数，该函数计算一个谓词有多少项为真：</p><p> count(predicate, array) = mapreduce(predicate, +, array) julia&gt; a = CUDA.rand(Int8, 4) 4-element CuArray{Int8,1}:  51  3  70  100 julia&gt; count(iseven, a) 2</p><p>Count(谓词，数组)=MapReduce(谓词，+，数组)Julia&gt；a=CUDA.rand(Int8，4)4元素CuArray{Int8，1}：51 3 70 100 Julia&gt；count(iseven，a)2。</p><p> Even though our `mapreduce` implementation has not been specifically implemented for the `Int8` type or the `iseven` predicate, the Julia compiler automatically specializes the implementation, resulting in kernel optimized for this specific invocation.</p><p>尽管没有为`Int8`类型或`iseven`谓词专门实现我们的`mapduce`实现，但Julia编译器会自动专门化该实现，从而导致内核针对此特定调用进行优化。</p><p> What were the biggest challenges when developing packages for JuliaGPU, particularly writing a low level package such as CUDA.jl in a high level programming language such as Julia?</p><p>在为JuliaGPU开发软件包，特别是用Julia等高级编程语言编写CUDA.jl这样的低级软件包时，最大的挑战是什么？</p><p> Much of the initial work focused on developing tools that make it possible to write low-level code in Julia. For example, we developed the  LLVM.jl package that gives us access to the LLVM APIs. Recently, our focus has shifted towards generalizing this functionality so that other GPU back-ends, like  AMDGPU.jl or  oneAPI.jl can benefit from developments to CUDA.jl. Vendor-neutral array operations, for examples, are now implemented in  GPUArrays.jl whereas shared compiler functionality now lives in  GPUCompiler.jl. That should make it possible to work on several GPU back-ends, even though most of them are maintained by only a single developer.</p><p>最初的大部分工作都集中在开发工具上，这些工具使得用Julia编写低级代码成为可能。例如，我们开发了允许我们访问LLVMAPI的LLVM.jl包。最近，我们的重点已转向推广此功能，以便其他GPU后端，如AMDGPU.jl或oneAPI.jl可以从CUDA.jl的开发中受益。例如，供应商中立的数组操作现在在GPUArrays.jl中实现，而共享编译器功能现在在GPUCompiler.jl中实现。这应该可以在多个GPU后端上工作，即使大多数后端只由一个开发人员维护。</p><p> Regarding the  latest release announced in the JuliaGPU blog about multi-device programming, what are the difficulties that this new functionality solves? Is this relevant in the industry where big computational resources are needed?</p><p>关于JuliaGPU博客中宣布的关于多设备编程的最新版本，这个新功能解决了哪些困难？这与需要大量计算资源的行业相关吗？</p><p> In industry or large research labs, MPI is often used to distribute work across multiple nodes or GPUs. Julia’s MPI.jl supports that use case, and integrates with CUDA.jl where necessary. The multi-device functionality added to CUDA 1.3 additionally makes it possible to use multiple GPUs within a single process. It maps nicely on Julia’s task-based concurrency, and makes it easy to distribute work within a single node:</p><p>在工业或大型研究实验室中，MPI通常用于跨多个节点或GPU分配工作。Julia的MPI.jl支持该用例，并在必要时与CUDA.jl集成。CUDA 1.3新增的多设备功能还使得在单个进程中使用多个GPU成为可能。它很好地映射了Julia的基于任务的并发性，并使得在单个节点内分发工作变得容易：</p><p>   There aren’t any specific roadmaps, but one upcoming major feature is proper support for reduced-precision inputs, like 16-bits floating point. We already support Float16 arrays where CUBLAS or CUDNN does, but the next version of Julia will make it possible to write kernels that operate on these values.</p><p>没有任何具体的路线图，但即将到来的一个主要功能是适当支持降低精度的输入，如16位浮点。我们已经支持CUBLAS或CUDNN支持的Float16数组，但是Julia的下一个版本将使编写对这些值进行操作的内核成为可能。</p><p> Other than that, features come as they do :-) Be sure to subscribe to the  JuliaGPU blog where we publish a short post for every major release of Julia’s GPU back-ends.</p><p>除此之外，特性是原封不动的：-)一定要订阅JuliaGPU博客，在那里我们会为Julia的GPU后端的每个主要版本发布一篇简短的帖子。</p><p>   A happy member of The Erlang, Rust/ML and Lisp Evangelism Strikeforce. Network Protocol’s RFC fanatic. Big Data and Machine Learning</p><p>一个快乐的Erlang，Rust/ML和Lisp福音派Strikeforce的成员。网络协议的RFC狂热分子。大数据和机器学习。</p><p>  Writings, reviews and interviews about programming languages, operating systems, network protocols, artificial intelligence and machine learning</p><p>关于编程语言、操作系统、网络协议、人工智能和机器学习的写作、评论和采访。</p><p> A happy member of The Erlang, Rust/ML and Lisp Evangelism Strikeforce. Network Protocol’s RFC fanatic. Big Data and Machine Learning</p><p>一个快乐的Erlang，Rust/ML和Lisp福音派Strikeforce的成员。网络协议的RFC狂热分子。大数据和机器学习。</p><p> Writings, reviews and interviews about programming languages, operating systems, network protocols, artificial intelligence and machine learning</p><p>关于编程语言、操作系统、网络协议、人工智能和机器学习的写作、评论和采访。</p><p> Medium is an open platform where 170 million readers come to find insightful and dynamic thinking. Here, expert and undiscovered voices alike dive into the heart of any topic and bring new ideas to the surface.  Learn more</p><p>媒体是一个开放的平台，1.7亿读者来这里发现有洞察力和动态的思维。在这里，专家和未被发现的声音都会深入到任何话题的核心，并将新想法浮出水面。了解更多。</p><p> Follow the writers, publications, and topics that matter to you, and you’ll see them on your homepage and in your inbox.  Explore</p><p>关注对你来说重要的作家、出版物和话题，你会在你的主页和收件箱里看到它们。探索。</p><p> If you have a story to tell, knowledge to share, or a perspective to offer — welcome home. It’s easy and free to post your thinking on any topic.  Write on Medium</p><p>如果你有故事要讲，有知识要分享，或者有观点要提供-欢迎回家。在任何话题上发表你的想法都很容易，也很自由。在介质上写入</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://notamonadtutorial.com/julia-gpu-98a461d33e21">https://notamonadtutorial.com/julia-gpu-98a461d33e21</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/julia/">#julia</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/gpu/">#gpu</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1030130.html"><img src="http://img2.diglog.com/img/2020/10/thumb_99645582f7b9378abfbc70b633d16287.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030130.html">安装300,000次的广告拦截程序是恶意的，应该立即删除</a></div><span class="my_story_list_date">2020-10-21 2:4</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030121.html"><img src="http://img2.diglog.com/img/2020/10/thumb_1e79b4463c05a9d129833cea88dc15c2.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030121.html">Parallels推出Parallels Desktop for Chromebook Enterprise，每位用户售价69.99美元，可让Windows应用程序在Chromebook上运行</a></div><span class="my_story_list_date">2020-10-21 1:55</span></div><div class="col-sm"><div><a target="_blank" href="/story/1030074.html"><img src="http://img2.diglog.com/img/2020/10/thumb_a32a0d7dc57b293b5e7c3dcb10aa5521.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1030074.html">
随着千禧一代/Z世代使用定位应用来对抗Covid的封锁，Pickle应用程序对用户起到了推波助澜的作用</a></div><span class="my_story_list_date">2020-10-20 22:55</span></div><div class="col-sm"><div><a target="_blank" href="/story/1029857.html"><img src="http://img2.diglog.com/img/2020/10/thumb_fadbf58ab0b09da1bdb796fbf3d37f0a.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1029857.html">Linux内核5.9尚不支持NVIDIA驱动程序</a></div><span class="my_story_list_date">2020-10-20 1:42</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>