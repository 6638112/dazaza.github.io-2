<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>ScaleOut元数据文件系统已经存储了大量数据。 这些是什么？ Scaleout Metadata File Systems already store much of your data. What are they?</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Scaleout Metadata File Systems already store much of your data. What are they?<br/>ScaleOut元数据文件系统已经存储了大量数据。 这些是什么？ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-06 08:38:08</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/e94c4cbaea947bae14451a1696df6cfb.jpeg"><img src="http://img2.diglog.com/img/2021/6/e94c4cbaea947bae14451a1696df6cfb.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>TL;DR: A new class of hierarchical distributed file system with scaleout metadata has taken over at Google, Facebook, and Microsoft, and provides a single centralized file system that manages the data for an entire data center, scaling to Exabytes in size. The common architectural feature of these systems is scaleout metadata, so we call them scaleout metadata file systems.</p><p>TL; DR：在Google，Facebook和Microsoft拍摄了一个具有扩张元数据的新类别分布式文件系统，并提供了一个单一的集中文件系统，管理整个数据中心的数据，缩放到大小的exabytes。这些系统的常见架构特征是缩放元数据，因此我们调用它们扩张元数据文件系统。</p><p>    Hierarchical file systems typically provide well-defined behaviour (a POSIX API) for how a client can securely create, read, write, modify, delete, organize, and find files.</p><p>    分层文件系统通常提供定义的行为（POSIX API），了解客户端如何安全地创建，读取，写入，修改，删除，组织和查找文件。</p><p> The data in such   file systems is stored in files as blocks or extents. Distributed file systems spread and replicate these blocks/extents over many servers for improved performance and high availability. However, the data about what files, directories, blocks, and file system permissions are in the system have historically been stored in a single server called the metaserver or namenode. We call this data about the file system objects  metadata. In file systems like HDFS, the namenode stores its metadata in-memory to improve both latency and throughput in the number of metadata operations it can support per second. Example metadata operations are: create a directory, move or rename a file or directory, change file permissions or ownership.</p><p> 此类文件系统中的数据存储在文件中作为块或范围。分布式文件系统在许多服务器上传播并复制这些块/范围，以提高性能和高可用性。但是，系统中有关文件，目录，块和文件系统权限的数据历史上存储在一个名为MetaServer或NameNode的单个服务器中。我们调用关于文件系统对象元数据的此数据。在像HDF等文件系统中，NameNode将其元数据存储在内存中，以提高它可以支持每秒支持的元数据操作的延迟和吞吐量。示例元数据操作是：创建目录，移动或重命名文件或目录，更改文件权限或所有权。</p><p> As the size of data under management by distributed file systems increased, it was quickly discovered that metadata servers became a bottleneck. For example, HDFS could scale to, at a push, a Petabyte, but not handle more than 100K reads/sec and only a few thousand writes/sec.</p><p> 随着分布式文件系统管理的数据的大小增加，它很快发现元数据服务器成为瓶颈。例如，HDFS可以按推送，脚本，但不处理超过100K的读取/秒，只有几千条写入/秒。</p><p> It has long been desired to re-architect distributed file systems to shard their metadata across many servers to enable them to support (1) larger volumes of metadata and (2) more operations/second. But it is a  very hard problem.</p><p> 它长期以来希望重新建立分布式文件系统，以在许多服务器上分离它们的元数据，以使它们能够支持（1）更大的元数据和（2）更多操作/秒。但这是一个非常艰难的问题。</p><p> Now, we discuss the only four scaleout metadata file systems that are publicly known today: Google Colossus, Facebook Tectonic, Microsoft ADLSv2, and Logical Clocks’ HopsFS.</p><p> 现在，我们讨论了今天公开知名的唯一四个扩展元数据文件：谷歌巨蟹座，Facebook构造，Microsoft ADLSv2和逻辑时钟的啤酒花。</p><p>  Even though we first heard about Colossus’ architecture in  2009 and its name in  2012, Google has been surprisingly secretive about the lowest layer of their scalable storage and compute architecture. However, after the release of Tectonic (coincidence?) in early 2021, Google released  more details on Colossus in May 2021.</p><p>  尽管我们在2009年首次听到巨大的建筑和2012年的名称，但谷歌令人惊讶地暗中秘密地秘密地阐述了其可扩展存储和计算架构的最低层。然而，在2021年初发布构造（巧合？）后，谷歌在2021年5月发布了更多关于巨大的细节。 </p><p>   Colossus’ metadata storage service is BigTable, which does not support cross-shard transactions. We assume this means that Colossus lacks atomic rename, a hole that is filled for tabular data (at least) by Spanner, which supports cross-shard transactions.</p><p>Colossus的Metadata Storage Service是Bigtable，它不支持交叉碎片交易。我们假设这意味着巨大缺乏原子重命名，一个漏洞填充用于表格数据（至少）的扳手，它支持交叉碎片交易。</p><p> In Colossus, file system clients connect to curators to perform metadata operations, who, in turn, talk to BigTable. Custodians perform file system maintenance operations, and “D” services provide block storage services, where clients read/write blocks directly from/to “D” servers.</p><p> 在巨大的情况下，文件系统客户端连接到策展程序来执行元数据操作，谁又与Bigtable交谈。托管人执行文件系统维护操作，并“D”服务提供块存储服务，其中客户端直接从/到“D”服务器读取/写入块。</p><p>  Different clients of Colossus can store their data on different volumes (metadata shards). Atomic rename is possible within a volume, but not across volumes.</p><p>  巨大的不同客户端可以将他们的数据存储在不同的卷上（元数据碎片）。在卷内可​​能进行原子重命名，但不会横跨卷。</p><p>  Tectonic was first announced as a file system at  USENIX Fast 2021, and it unifies Facebook’s previous storage services (federated HDFS, Haystack, and others) to provide a data-center scale file system.</p><p>  Tectonic首次在Usenix Fast 2021宣布为文件系统，它统一Facebook之前的存储服务（联合HDFS，Haystack等）以提供数据中心缩放文件系统。</p><p>  Similar to Colossus, Tectonic stores its metadata in a key-value store, but in this case in ZippyDB. As ZippyDB lacks cross-partition transactions, cross-namespace file system operations are not supported. That is, you cannot atomically move a file from one volume (metadata shard) to another. Often, such operations are not needed, as all the data for a given service can fit in a single namespace, and there are no file system operations between different applications. There are separate stateless services to manage the name space, blocks, files, and file system maintenance operations.</p><p>  类似于巨蟹座，构造在键值存储中存储其元数据，但在这种情况下在Zippydb中。由于ZippydB缺少跨分区事务，不支持跨名称空间文件系统操作。也就是说，您不能从一个卷（元数据碎片）到另一个卷（元数据碎片）原子上的文件。通常，不需要这种操作，因为给定服务的所有数据都可以适合单个命名空间，并且在不同的应用程序之间没有文件系统操作。有单独的无状态服务来管理名称空间，块，文件和文件系统维护操作。</p><p>   Azure Data Lake Storage (ADLS) was  first announced at Sigmod 2017 and it supports Hadoop distributed file system (HDFS) and Cosmos APIs. It has since been redesigned as Azure Data Lake Gen 2 (ADLSv2) that provides multi-protocol support to the same data using the  Hadoop File System API, the  Azure Data Lake Storage API and the  Azure Blob storage API. Unlike Colossus and Tectonic, it is available for use as a service — but only on Azure.</p><p>   Azure Data Lake Storage（ADL）首次在Sigmod 2017宣布，它支持Hadoop分布式文件系统（HDFS）和Cosmos API。它已被重新设计为Azure数据湖Gen 2（ADLSv2），它使用Hadoop文件系统API，Azure Data Lake Storage API和Azure Blob Storage API为同一数据提供多协议支持。与巨型和构造不同，它可以作为服务使用 - 但仅限于Azure。</p><p>  The most recent information about ADLS’ architecture is the  original paper describing ADLS from 2017 — no architecture has been published yet for ADLSv2. However, ADLS used RSL-HK to store metadata and it has a key-value store (ring) with shards using state machine replication (Paxos) and with transactions across shards, al in an in-memory engine (“It implements a novel combination of Paxos and a new transactional in-memory block data management design.”).</p><p>  有关ADLS架构的最新信息是描述2017年的ADL的原始文件 - 尚未为ADLSv2发布架构。但是，ADL使用RSL-HK存储元数据，它具有使用状态机复制（PaxoS）的碎片碎片键值存储（环），并且在内存引擎中的碎片中的交易（“它实现了一种新颖的组合paxoS和新的交易内存块数据管理设计。“）。 </p><p>   HopsFS was first announced at  USENIX Fast 2017 and provides a HDFS API. HopsFS is a rewrite of HDFS and it supports multiple stateless namenode (metadata servers), where  the leader performs file system maintenance operations, and a pluggable metadata storage layer.</p><p>HOPSFS首次在Usenix Fix于2017年宣布，并提供了HDFS API。 HOPSFS是一种重写HDFS，它支持多个无状态NameNode（元数据服务器），其中Leader执行文件系统维护操作，以及可插拔元数据存储层。</p><p>  HopsFS provides a DAL API to support different metadata storage engines. Currently the default engine for HopsFS is RonDB (a fork of NDB Cluster, the storage engine for  MySQL Cluster), a scalable key-value store with SQL capabilities. RonDB can  scale to handle hundreds of millions of transactional reads per second and 10s of millions of transactional writes per second and it provides both a  native key-value API and  a SQL API via a MySQL Server. RonDB also provides a CDC (change-data-capture) API to allow us to  automatically replicate changes in metadata to Elasticsearch, providing a free-text search API to HopsFS’ metadata (including its extended metadata). Metadata can be queried using any of the 3 APIs: the native key-value API for RoNDB, the SQL API, or using free-text search in Elasticsearch.</p><p>  HOPSFS提供DAL API来支持不同的元数据存储引擎。目前，HOPSFS的默认引擎是RONDB（NDB集群的叉子，MySQL群集的存储引擎），具有SQL功能的可伸缩键值存储。 Rondb可以扩展到每秒处理数亿个交易读数，每秒10多个事务写入，并且它通过MySQL服务器提供本机键值API和SQL API。 Rondb还提供CDC（Change-Data-Capture）API，以允许我们自动将元数据的变化复制到Elasticsearch，为HoPSFS的元数据提供自由文本搜索API（包括其扩展元数据）。可以使用3个API中的任何一个来查询元数据：Rondb，SQL API的本机键值API，或在Elasticsearch中使用自由文本搜索。</p><p>  HopsFS scales the Namespace Layer with RonDB and Stateless Namenodes, while the block layer is cloud object storage.</p><p>  HOPSFS将命名空间层与RONDB和无状态Namenodes缩放，而块层是云对象存储。</p><p>  When sharding the state of the metadata server over many servers, you need to make decisions about how to do it. Google used its existing  BigTable key-value store to store Colossus’ metadata. Facebook, similarly, chose the  ZippyDB key-value store for Tectonic. Microsoft built their own Replicated State Library — Hekaton Ring Service (RSL-HK) to scale-out ADLS’ metadata. The RSL-HK ring architecture combines Paxos-based metadata with  Hekaton (in-memory engine from SQL Server). HopsFS used NDBCluster (now  RonDB) to scale out its metadata.</p><p>  在多个服务器上分离元数据服务器的状态时，您需要做出关于如何执行此操作的决定。谷歌使用现有的Bigtable键值商店来存储巨大的元数据。同样，Facebook撰写了构造的Zippydb键值存储。 Microsoft内置了自己的复制状态库 -  Hebakaton Ring Service（RSL-HK），以扩展ADLS的元数据。 RSL-HK环形架构将基于PaxoS的元数据与Hebkaton（来自SQL Server的内存引擎）相结合。 HOPSFS使用NDBCLUSTER（现在RONDB）扩展其元数据。</p><p> The capabilities of these underlying storage engines are reflected in the semantics provided by the higher level file systems. For example, Tectonic and (probably) Colossus do not support atomic move of files from any directory to any other directory. Their key-value stores do not support agreement protocols across shards (only within a shard). So, at the file system level, you introduce an abstraction like a file system volume (Tectonic calls them tenants), and users then know they can perform atomic rename/move within that volume, but not across volumes. Google solves this problem at a higher layer for structured data with Spanner by implementing two-phase commit transactions to ensure consistency across shards. In contrast, RSL-HK Ring by Microsoft and RonDB by Logical Clocks support cross-shard transactions that enable both ADLSv2 and HopsFS to support atomic rename/move between any two paths in the file system.</p><p> 这些底层存储引擎的功能反映在更高级文件系统提供的语义中。例如，构造和（可能）巨大不支持从任何目录到任何其他目录的文件的原子移动。它们的键值存储不支持碎片的协议协议（仅在碎片内）。因此，在文件系统级别，您将介绍一个像文件系统卷（构造调用它们租户）的抽象，然后用户知道它们可以在该卷内执行原子重命名/移动，但不会跨越卷。 Google通过实现两阶段提交事务，在具有扳手的结构化数据的较高层中解决此问题，以确保横跨分片的一致性。相比之下，Microsoft和Rondb的RSL-HK环通过逻辑时钟支持跨碎片事务，使得ADLSv2和HOPSF在文件系统中的任何两个路径之间都能支持原子重命名/移动。</p><p> To put this in database terms, the consistency models provided by the scaleout metadata file systems are tightly coupled to the capabilities provided by the underlying metadata store. If the store does not support cross-partition transactions — consistent operations across multiple shards, you will not get strongly consistent cross-partition file system operations. For example, if the metadata store is a key-value store, where each shard typically maintains strongly consistent key-value data using Paxos. But Paxos do not compose — you cannot run Paxos between two shards that themselves maintain consistency using Paxos. In contrast, RonDB supports 2-phase commit (2PC) across shards, enabling strongly consistent metadata operations both within shards and across shards.</p><p> 要将其放在数据库术语中，缩放元数据文件系统提供的一致性模型紧紧地耦合到基础元数据存储器提供的功能。如果商店不支持跨分区事务 - 跨多个碎片的一致操作，则不会获得强烈一致的交叉分区文件系统操作。例如，如果元数据存储器是键值存储，则每个碎片通常使用paxoS维护强烈一致的键值数据。但PaxoS不撰写 - 您无法在两个碎片之间运行PaxoS，它们本身使用paxoS保持一致性。相比之下，RONDB支持横跨分片的2阶段提交（2PC），在碎片和碎片中都能在碎片中实现强烈一致的元数据操作。</p><p> Once a scaleout metadata storage layer is in place, stateless services can be used to provide access control and implement background maintenance tasks like maintaining the durability and availability of data, disk space balancing, and repairing blocks.</p><p> 一旦大规模的元数据存储层到位，可以使用无状态服务来提供访问控制并实现类似于维护数据，磁盘空间平衡和修复块的耐用性和可用性的背景维护任务。 </p><p>  The journey from a stronger POSIX-like file system to a weaker object storage paradigm and back again has parallels in the journey that databases have made in recent years. Databases made the transition from strongly consistent single-host systems (relational databases) to highly available (HA), eventually consistent distributed systems (NoSQL systems) to handle the massive increases in data managed by databases. However,  NoSQL is just too hard for developers, and databases are returning to strongly consistent (but now scalable) NewSQL systems, with databases such as Spanner, CockroachDB, SingleSQL, and NDB Cluster.</p><p>从更强大的POSIX的文件系统到较弱的对象存储范式并再次回到近年来数据库的旅程中的平价。数据库使从强大一致的单主机系统（关系数据库）转换为高可用性（HA），最终一致的分布式系统（NoSQL系统）来处理数据库管理的数据中的大规模增加。但是，NoSQL对于开发人员来说太难了，数据库返回强烈一致（但现在可扩展）的NewsQL系统，数据库，如扳手，蟑螂，SingleSQL和NDB集群。</p><p> The scaleout metadata file systems, introduce here, show that distributed hierarchical file systems are completing a similar journey, going from strongly consistent POSIX-compliant file systems to object stores (with their weaker consistency models), and back to distributed hierarchical file systems that are have solved the scalability problem by redesigning the file system around a mutable, scaleout metadata service.</p><p> 缩放元数据文件系统介绍此处，显示分布式分层文件系统完成了类似的旅程，从强烈一致的POSIX  - 符合文件系统到对象存储（具有较弱的一致性模型），并返回分布式分层文件系统通过重新设计围绕可变的缩放元数据服务来解决文件系统来解决可伸缩性问题。</p><p>   Ismail Mahmoud, Salman Niazi, Gautier Berthou, Mikael Ronström, Seif Haridi, Jim Dowling.  HopsFS-S3: Extending Object Stores with POSIX-like Semantics and more. ACM/IFIP Middleware 2020.</p><p>   Ismail Mahmoud，Salman Niazi，Gautier Berthou，MikaelRonström，Seif Haridi，Jim Dowling。 HOPSFS-S3：使用POSIX的语义和更多扩展对象存储。 ACM / IFIP中间件2020。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://jim-dowling.medium.com/scaleout-metadata-file-systems-already-store-much-of-your-data-what-are-they-a377bd4ae42">https://jim-dowling.medium.com/scaleout-metadata-file-systems-already-store-much-of-your-data-what-are-they-a377bd4ae42</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/metadata/">#metadata</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>