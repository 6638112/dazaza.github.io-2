<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>关于学校的按需寻呼，他们不会告诉你什么？™</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">关于学校的按需寻呼，他们不会告诉你什么？™</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-17 10:06:53</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/9854fc37c69b2457fc46b68960629e34.jpg"><img src="http://img2.diglog.com/img/2020/10/9854fc37c69b2457fc46b68960629e34.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This post details my adventures with the Linux virtual memory subsystem, and my discovery of a creative way to taunt the OOM (out of memory) killer by accumulating memory in the kernel, rather than in userspace.</p><p>这篇文章详细介绍了我使用Linux虚拟内存子系统的经历，以及我发现了一种创造性的方法，通过在内核而不是在用户空间积累内存来嘲弄OOM(内存不足)杀手。</p><p>  A cute way to get killed by the OOM killer while appearing to consume very little memory (great for parties)</p><p>一个可爱的被OOM杀手杀死的方式，同时看起来占用很少的内存(非常适合聚会)。</p><p>   As usual, the story begins with me asking questions about implementation details. This time, about the Linux kernel’s demand paging implementation.</p><p>像往常一样，故事从我询问有关实现细节的问题开始。这一次，是关于Linux内核的请求分页实现。</p><p> In Operating Systems 101 we learn that operating systems are “lazy” when they allocate memory to processes. When you mmap() an anonymous page, the kernel slyly returns a pointer immediately. It then waits until you trigger a page fault by “touching” that memory before doing the real memory allocation work. This is called “demand paging”.</p><p>在操作系统101中，我们了解到操作系统在向进程分配内存时是“懒惰的”。当您mmap()匿名页面时，内核会立即狡猾地返回一个指针。然后，它会等待，直到您通过“触摸”该内存触发页面错误，然后再执行实际的内存分配工作。这就是所谓的“按需寻呼”。</p><p> This is efficient — if the memory is never touched, no physical memory is ever allocated. This also means you can allocate virtual memory in vast excess of what is physically available (“overcommit”), which can be useful.    1. You just can’t touch it all.</p><p>这是高效的-如果内存从未被触及，则不会分配任何物理内存。这也意味着您可以分配远远超过物理可用内存的虚拟内存(“过量使用”)，这可能很有用。1.你就是摸不到所有的东西。</p><p> Let’s dive deeper. Barring execution, “touching” memory means reading or writing. Writes to a new mmap’d region require the kernel to perform a full memory allocation. You need memory, you need it  now, and the kernel can’t push it off any longer.</p><p>让我们潜入更深的地方。除非执行，“触摸”记忆意味着读或写。写入新的mmap‘d区域需要内核执行完整的内存分配。您需要内存，现在就需要，内核不能再推它了。</p><p>  Unlike writes, reads to a new mmap’d region do  not trigger a memory allocation. The kernel continues to push off the allocation by exploiting how new anonymous mappings must be zero initialized. Instead of allocating memory, the kernel services the page fault using the “zero page”: a pre-allocated page of physical memory, completely filled with zeros. In theory this is “free” — a single physical frame can back all zero-initialized pages.</p><p>与写入不同，对新mmap区域的读取不会触发内存分配。内核通过利用必须将新的匿名映射初始化为零的方式来继续推送分配。内核不分配内存，而是使用“零页”来处理页错误：预分配的物理内存页，完全用零填充。从理论上讲，这是“免费的”--单个物理帧可以支持所有零初始化的页面。</p><p> The point?  Demand paging is nuanced — not all ways of accessing a new mapping require the kernel to allocate memory.</p><p>重点是什么？请求分页是细微差别的-并不是所有访问新映射的方式都需要内核分配内存。</p><p> Let’s see what this looks like in the  source. The core page fault handler,  handle_mm_fault is in  mm/memory.c. A few calls deep via  __handle_mm_fault and  handle_pte_fault, we hit this block:</p><p>让我们看看源代码中的这个是什么样子。核心页面错误处理程序HANDLE_MM_FAULT位于mm/ememy.c中。通过__HANDLE_MM_FAULT和HANDLE_PTE_FAULT进行了几个深度调用，我们遇到了这个块：</p><p> static vm_fault_t handle_pte_fault(struct vm_fault *vmf){	// ...	if (!vmf-&gt;pte) {		if (vma_is_anonymous(vmf-&gt;vma))			return do_anonymous_page(vmf);		else			// ...	}	// ...}</p><p>静态VM_FAULT_t HANDLE_PTE_FAULT(结构VM_FAULT*vmf){//...如果(！vmf-&gt；pte){if(vma_is_anous(vmf-&gt；vma)返回DO_ANONOWARY_PAGE(Vmf)；否则//...}//...}。</p><p>  static vm_fault_t do_anonymous_page(struct vm_fault *vmf){	// ...		if (pte_alloc(vma-&gt;vm_mm, vmf-&gt;pmd))		return VM_FAULT_OOM;	// ...	/* Use the zero-page for reads */	if (!(vmf-&gt;flags &amp; FAULT_FLAG_WRITE) &amp;&amp; // (1)			!mm_forbids_zeropage(vma-&gt;vm_mm)) {		entry = pte_mkspecial(pfn_pte(my_zero_pfn(vmf-&gt;address), // (2)						vma-&gt;vm_page_prot));		vmf-&gt;pte = pte_offset_map_lock(vma-&gt;vm_mm, vmf-&gt;pmd,				vmf-&gt;address, &amp;vmf-&gt;ptl);		// ...		goto setpte;	}	// ...setpte:	set_pte_at(vma-&gt;vm_mm, vmf-&gt;address, vmf-&gt;pte, entry); // (3)	// ...	return ret;	// ...}</p><p>静态VM_FAULT_t do_ANNOWARY_PAGE(struct VM_FAULT*vmf){//...IF(pte_alloc(vma-&gt；vm_mm，vmf-&gt；pmd))return VM_FAULT_OOM；//.../*使用零页读取*/IF(！(vmf-&gt；标志&amp；FAULT_FLAG_WRITE)&amp；&amp；//(1)！mm_forbids_zeropage(vma-&gt；Vm_mm)){Entry=pte_mkspecial(pfn_pte(my_zero_pfn(vmf-&gt；address)，//(2)vma-&gt；VM_PAGE_PROT)；VMF-&gt；PTE=PTE_OFFSET_MAP_LOCK(vma-&gt；VM_mm，vmf-&gt；pmd，vmf-&&gt;地址，&amp；vmf-&gt；ptl)；//...转到Setpte；}//...setpte：set_pte_at(vma-&gt；vm_mm，vmf-&gt；address，vmf-&gt；pte，entry)；//(3)//...返回ret；//...}。</p><p> Bingo. It checks whether a read caused the fault (1), then maps the virtual page to the zero page (2 and 3).</p><p>对啰。它检查读取是否导致错误(1)，然后将虚拟页映射到零页(2和3)。</p><p> Note that this all happens in the page fault handler, which is an implementation choice. The mmap  core logic does not touch the page tables at all, and only records the presence of the new mapping. It leaves the mapping’s page table entry non-present (present bit = 0) which will trigger a page fault on access.</p><p>请注意，这一切都发生在页面错误处理程序中，这是一种实现选择。Mmap核心逻辑根本不接触页表，并且只记录新映射的存在。它使映射的页表条目不存在(当前位=0)，这将在访问时触发页面错误。</p><p> Alternatively, mmap could proactively allocate the page table entry, and initialize it to the zero page. This would avoid a page fault on the first read, but at the cost of initializing (potentially many) page table entries up front. Given that it’s most efficient to be maximally lazy, the current implementation is best.</p><p>或者，mmap可以主动分配页表条目，并将其初始化为零页。这将避免第一次读取时出现页面错误，但代价是提前初始化(可能会有许多)页表条目。考虑到最大程度的懒惰是最有效的，那么当前的实现是最好的。</p><p>  This led me to another question. Since reads from anonymous mappings are “free”, in addition to allocating excessive virtual memory, can’t you actually touch all of it? As long as that “touch” is a read?</p><p>这就引出了另一个问题。既然来自匿名映射的读取是“自由的”，那么除了分配过多的虚拟内存之外，难道您不能实际接触到所有的虚拟内存吗？只要那个“触摸”是一种阅读？</p><p> Time for an experiment. Here’s some code that allocates 100 GB of linear memory, and tries to read from the first byte of each page. It allocates 512 MB at a time because you can’t directly ask mmap for 100 GB :).    2 My test system was a x64 Ubuntu 20.04 VPS.</p><p>实验时间到了。下面是一些代码，它分配100 GB的线性内存，并尝试从每页的第一个字节读取。它一次分配512MB，因为您不能直接向mmap请求100 GB：)。2我的测试系统是x64 Ubuntu20.04VPS。</p><p> #include &lt;sys/mman.h&gt;#include &lt;iostream&gt;const size_t MB = 1024 * 1024;const size_t GB = MB * 1024;int main() { size_t alloc_size = 512 * MB; size_t total_alloc = 100 * GB; size_t num_allocs = total_alloc / alloc_size; // std::cout &lt;&lt; &#34;alloc_size (MB)&#34; &lt;&lt; alloc_size / (1024*1024)&lt;&lt; &#34;\n&#34;; // std::cout &lt;&lt; &#34;total_alloc &#34; &lt;&lt; total_alloc &lt;&lt; &#34;\n&#34;; // std::cout &lt;&lt; &#34;num_allocs &#34; &lt;&lt; num_allocs &lt;&lt; &#34;\n&#34;; std::cout &lt;&lt; &#34;Allocating mem...\n&#34;; char* base = nullptr; // Allocate a ton of memory for (size_t i = 0; i &lt; num_allocs; i++) { // Unsound alert - assuming allocations are contiguous and grow down. base = (char*)mmap(NULL, alloc_size, PROT_READ, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0); if (base == MAP_FAILED) { perror(NULL); throw std::runtime_error(&#34;Fail&#34;); } std::cout &lt;&lt; (void*)base &lt;&lt; &#34; &#34; &lt;&lt; i &lt;&lt; &#34;\n&#34;; } std::cout &lt;&lt; &#34;Allocated Virtual Mem (GB): &#34; &lt;&lt; total_alloc / GB &lt;&lt; &#34;\n&#34;; std::cout &lt;&lt; &#34;Base Addr: &#34; &lt;&lt; (void*)base &lt;&lt; &#34;\n&#34;; std::cout &lt;&lt; &#34;Press enter to start reading.\n&#34;; getchar(); std::cout &lt;&lt; &#34;Reading each page...\n&#34;; // Read the first byte of each page for (size_t i = 0; i &lt; total_alloc; i += 0x1000) { auto x = base[i]; } std::cout &lt;&lt; &#34;Done!\n&#34;; getchar();}</p><p>#include&lt；sys/mman.h&gt；#include&lt；iostream&gt；const size_t MB=1024*1024；const size_t GB=MB*1024；int main(){size_t alloc_size=512*MB；size_t total_alloc=100*GB；size_t num_allocs=total_alloc/alloc_size；//std：：cout&lt；&lt；&#34；alloc_size(MB)&#34；&lt；&lt；Alloc_size/(1024*1024)&lt；&lt；&#34；\n&#34；；//std：：cout&lt；&lt；&#34；Total_alloc&lt；&#34；\n&#34；；//std：：cout&lt；&lt；&#34；num_allocs&#34；&lt；&lt；num_allocs&lt；&lt；&34；\n&#34；；std：：cout&lt；&lt；&#34；分配内存...\n&#34；；char*base=nullptr；//为(size_t i=0；i&lt；num_allocs；i++){//错误警报-假设分配是连续的并逐渐减少。Base=(char*)mmap(NULL，ALLOC_SIZE，PROT_READ，MAP_PRIVATE|MAP_ANOUNTY，-1，0)；IF(BASE==MAP_FAILED){perror(NULL)；抛出std：：Runtime_Error(&#34；FAIL&#34；)；}std：：cout&lt；&lt；(void*)base&lt；&#34；&lt；&lt；i&lt；&lt；&#34；\n&#34；；}std：：cout&lt；&lt；&#34；已分配虚拟内存(GB)：&#34；&lt；&lt；Total_alloc/GB&lt；&lt；&#34；\n&#34；；std：：cout&lt；&lt；&#34；基本地址：&#34；&lt；&lt；(void*)基本&lt；&lt；&#34；\n&#34；；标准：：cout&lt；&lt；&#34；按Enter键开始读取。\n&#34；；getchar()；std：：cout&lt；&lt；&#34；读取每页的第一个字节...\n&#34；；//读取每页的第一个字节(size_t i=0；i&lt；total_alloc；i+=0x1000){auto x=bas[i]；}std：：cout&lt；&lt；&#34；完成！\n&#34；；getchar()；}。</p><p>  $ ./demoAllocating mem...0x7f3f6d300000 10x7f3f4d300000 20x7f3f2d300000 3...0x7f26cd300000 1980x7f26ad300000 1990x7f268d300000 200Allocated Virtual Mem (GB): 100Base Addr: 0x7f268d300000Press enter to start reading.</p><p>$./demo正在分配内存...0x7f3f6d300000 10x7f3f4d300000 20x7f3f2d300000 3...0x7f26cd300000 1980x7f26ad300000 1990x7f268d300000 200已分配的虚拟内存(GB)：100BASE Addr：0x7f268d300000按Enter开始阅读。</p><p> It successfully allocated 100 GB of linear virtual memory in 512 MB chunks. We can confirm this with  pmap, which shows a 100GB region of anonymous virtual memory at the base address printed.</p><p>它成功地以512MB块为单位分配了100 GB的线性虚拟内存。我们可以使用PMAP确认这一点，它在打印的基址显示一个100 GB的匿名虚拟内存区域。</p><p> $ pmap `pidof demo`485209: ./demo00005600e1d0c000 4K r---- demo00005600e1d0d000 4K r-x-- demo00005600e1d0e000 4K r---- demo00005600e1d0f000 4K r---- demo00005600e1d10000 4K rw--- demo00005600e2a47000 132K rw--- [ anon ]00007f268d300000 104857600K r---- [ anon ] &lt;&lt;&lt;&lt; 100 GB region00007f3f8d300000 16K rw--- [ anon ]00007f3f8d304000 60K r---- libm-2.31.so00007f3f8d313000 668K r-x-- libm-2.31.so...</p><p>$pmap`pidof demo`485209：./demo00005600e1d0c000 4K r-demo00005600e1d0d000 4K r-demo00005600e1d0e000 4K r-demo00005600e1d0f000 4K r-demo0000005600e1d10000 4K rw--demo00003f5600e2a47000 132K rw--[anon]00007f268d300000 1048600K r--[anon]&lt；100 GB region7f3f8d000 16K RW--[on0000007f3f8d304000 60K r-libm。</p><p>   htop confirms that 100 GB of virtual memory is allocated (VIRT column), but a much more reasonable 1540 KB of resident memory (RES) is actually occupying RAM. Note the MINFLT column — this is the number of “minor” page faults that have occurred. A minor page fault is one that does not require loading from disk. We will be triggering lots of these and should expect to see this number grow dramatically.</p><p>HTOP确认分配了100 GB的虚拟内存(VIRT专栏)，但是更合理的154KB驻留内存(RES)实际上正在占用RAM。请注意MINFLT列-这是已发生的“次要”页面错误数。次要页面错误是指不需要从磁盘加载的错误。我们将触发大量这样的事件，应该会看到这个数字戏剧性地增长。</p><p>  0x7f26cd300000 1980x7f26ad300000 1990x7f268d300000 200Allocated Virtual Mem (GB): 100Base Addr: 0x7f268d300000Press enter to start reading.Reading each page...Done!</p><p>0x7f26cd300000 1980x7f26ad300000 1990x7f268d300000 200分配的虚拟内存(GB)：100BASE地址：0x7f268d300000按Enter开始读取，读取每页...完成！</p><p> The process hits the “Done!” print. This means it successfully touched every page of the 100 GB allocation!</p><p>这个过程点击“完成！”打印出来。这意味着它成功地触及了100 GB分配的每一页！</p><p>  htop confirms that many minor faults have occurred. We would expect it to cause exactly 26214400 faults (100 GB / 4 KB), and indeed, 26214552 – 26214400 = 152, the number we started with. Intriguingly, the resident memory appears to have also increased, which should not have happened. See the  Appendix A for discussion of this.</p><p>HTOP确认发生了许多小故障。我们预计它会导致26214400个错误(100G/4KB)，实际上，26214552-26214400=152%，这是我们开始时的数字。有趣的是，常驻内存似乎也增加了，这是不应该发生的。有关这方面的讨论，请参阅附录A。</p><p>   So the theory is confirmed! You can apparently “Allocate your memory, and touch it too” (as long as that touch is a read).    3</p><p>所以这个理论被证实了！显然，您可以“分配内存，也可以触摸它”(只要该触摸是读操作)。3个。</p><p> If your application, for some reason, benefits from the ability to have a 100 GB array of zeros then this is perfect for you. What about the rest of us?</p><p>如果您的应用程序出于某种原因受益于拥有100 GB的零数组，那么这非常适合您。那我们其他人呢？</p><p> A closer-to-real-life application is a  sparse array. A sparse array is a (typically very large) array, whose elements are mostly zero. By exploiting demand paging, you can implement a memory efficient sparse array, where the majority of the array is backed by the zero page (or not even mapped). You get the fast indexing benefits of an array while avoiding the memory overhead.</p><p>更接近现实的应用程序是稀疏数组。稀疏数组是一个(通常非常大的)数组，其元素大多为零。通过利用按需分页，您可以实现内存效率高的稀疏数组，其中大部分数组由零页支持(甚至不映射)。在避免内存开销的同时，您可以获得数组的快速索引优势。</p><p>  I have a confession. Remember when I said not all ways of accessing memory require the kernel to allocate memory? Yeah, that’s a lie.</p><p>我有件事要坦白。还记得我说过不是所有的内存访问方式都需要内核分配内存吗？是啊，那是谎话。</p><p> Even though a read can be serviced by the shared zero page, that doesn’t mean no memory is allocated. Even the process of mapping a zero page requires allocating memory. And here’s where we get into the nitty gritty.</p><p>尽管共享零页可以服务于读取，但这并不意味着没有分配内存。即使映射零页的过程也需要分配内存。这就是我们进入细节的地方。</p><p>  The overhead comes from the virtual memory infrastructure itself — the page tables. Page tables are data structures that power the virtual memory subsystem. Like regular data structures, they occupy memory, only their overhead is easy to overlook, since it’s hidden from userspace.</p><p>开销来自虚拟内存基础设施本身-页表。页表是为虚拟内存子系统提供动力的数据结构。与常规数据结构一样，它们占用内存，只是它们的开销很容易被忽略，因为它对用户空间是隐藏的。</p><p>   They’re a tree data structure that is 4 levels deep, with each node (table) being an array of 512 8-byte entries. Together these tables offer an efficient way to represent a mapping between every virtual page in the address space and a physical frame.</p><p>它们是4层深的树数据结构，每个节点(表)是512个8字节条目的数组。这些表一起提供了一种有效的方式来表示地址空间中的每个虚拟页和物理帧之间的映射。</p><p> Here’s where the overhead comes from. Each page touched requires 1 Page Table Entry (PTE) to be allocated. However, PTEs aren’t allocated individually. They’re allocated in blocks of 512 called Page Tables. Each Page Table requires 1 Page Directory Entry to be allocated. But Page Directory Entries aren’t allocated individually either, they’re allocated in blocks of 512 called Page Directories. This propagates all the way up to the top level of the tree: the PML4 Table (“Page Map Level 4” Table). There is only one PML4 Table and it’s pointed to by the CPU’s CR3 register.</p><p>这就是开销的来源。所接触的每个页面都需要分配1个页表条目(PTE)。但是，PTE不是单独分配的。它们被分配在称为页表的512个块中。每个页表需要分配1个页目录条目。但是页面目录条目也不是单独分配的，它们是以512个称为页面目录的块来分配的。这会一直传播到树的顶层：PML4表(“Page Map Level 4”表)。只有一个PML4表，它由CPU的CR3寄存器指向。</p><p> Note that while “Page Table” refers to a specific type of table, these tables are all colloquially referred to as “page tables”.</p><p>请注意，虽然“页表”指的是特定类型的表，但这些表都通俗地称为“页表”。</p><p> Since page table entries are 8 bytes, and all page tables contain 512 entries, it follows that page tables are 4096 bytes.  This doesn’t seem like much, but to map even one page, you need one of each table. That’s already 16 KB of overhead. If you’re mapping gigabytes of virtual address space, this overhead will add up.</p><p>由于页表条目为8字节，且所有页表包含512个条目，因此得出页表为4096字节。这看起来不是很多，但是要映射一个页面，每个表都需要一个。这已经是16KB的开销了。如果您要映射千兆字节的虚拟地址空间，则此开销将会增加。</p><p>  This leads us to the final question(s). Specifically, how much can page table overhead add up to? Is it possible for page tables to occupy some non-negligible portion of memory?  Is it possible to  exhaust memory, purely from page tables? How much virtual memory would we need to map to do so?</p><p>这就引出了最后一个问题。具体地说，页表开销加起来可以达到多少？页表是否可能占用一些不可忽略的内存部分？是否有可能完全从页表耗尽内存？为此，我们需要映射多少虚拟内存？</p><p> Here’s pseudocode to calculate the page table overhead from a virtual memory allocation.</p><p>下面是计算虚拟内存分配的页表开销的伪代码。</p><p> Input: virtual_pages_allocatedpage_table_entries = virtual_pages_allocatedpage_tables = CEILING(page_table_entries, 512) / 512page_tables_bytes = page_tables * 4096page_dirs = CEILING(page_tables, 512) / 512page_dirs_bytes = page_dirs * 4096pdp_tables = CEILING(page_dirs, 512) / 512pdp_tables_bytes = pdp_tables * 4096pml4_table = 1pml4_table_bytes = 1 * 4096Output: total_overhead = page_tables_bytes + page_dirs_bytes + \ pdp_tables_bytes + pml4_table_bytes</p><p>输入：Virtual_Pages_allocatedpage_table_Entries=virtual_page_allocatedpage_tables=ceiming(page_table_entry，512)/512page_tables_bytes=page_tables*4096page_dirs=ceiming(page_tables，512)/512page_dirs_bytes=page_dirs*4096pdp_tables=ceiming(page_dirs，512)/512pdp_tables_bytes=PDP_Tables*4096pml4_table=1pml4_table_bytes=1*4096输出：Total_Overload=page_tables_bytes+page_dirs_bytes+\pdp_tables_bytes=pml4_table_bytes=1*4096输出：Total_Overload=page_tables_bytes+page_dirs_bytes+\pdp_tables_bytes=pml4_table_bytes=1*4096输出：Total_Overload=page_tables_bytes+page_dirs_bytes+\pdp_tables_bytes。</p><p> Remember that tables are allocated as a whole. Even if a table is partially filled, it still occupies the full 4 KB. This is why the calculation needs to round up to the nearest multiple of 512 (expressed as the CEILING function from Excel).</p><p>请记住，表是作为整体分配的。即使表被部分填满，它仍然占据全部4KB。这就是为什么计算需要向上舍入到最接近的512的倍数(在Excel中表示为上限函数)的原因。</p><p> Using this method, we calculate that a  512 GB allocation of virtual memory requires slightly over 1 GB of page tables!</p><p>使用此方法，我们计算出分配512 GB的虚拟内存需要略高于1 GB的页表！</p><p> VM Alloc GB 512Bytes Alloc 549755813888Pages Alloc 134217728PTEs 134217728Page Tables 2621445Page Tables Bytes 1073741824Page Dirs 512Page Dirs Bytes 2097152PDP Tables 1PDP Tables Bytes 4096PML4 Tables 1PML4 Tables Bytes 4096Total Overhead Bytes 1075847168Total Overhead GB 1.001960754</p><p>虚拟机分配GB 512字节分配549755813888页分配134217728PTE 134217728页表2621445页表字节1073741824页目录512页目录字节2097152PDP表1PDP表字节4096PML4表1PML4表字节4096总开销字节1075847168总开销GB 1.001960754</p><p>  My machine only has 1 GB of memory, so this should be more than enough to exhaust memory and ideally, trigger the OOM killer.</p><p>我的机器只有1 GB的内存，所以这应该足以耗尽内存，理想情况下，还会触发OOM杀手。</p><p> Time for another experiment! I changed the code above to allocate 512 GB instead of 100 GB, and reran it.</p><p>又到了做另一个实验的时候了！我将上面的代码更改为分配512 GB而不是100 GB，并重新运行它。</p><p>  ...0x7eca126ba000 10220x7ec9f26ba000 10230x7ec9d26ba000 1024Allocated Virtual Mem (GB): 512Base Addr: 0x7ec9d26ba000Press enter to start reading.Reading each page...fish: &#39;./demo2&#39; terminated by signal SIGKILL (Forced quit)</p><p>...0x7eca126ba000 10220x7ec9f26ba000 10230x7ec9d26ba000 10224分配的虚拟内存(GB)：512Base Addr：0x7ec9d26ba000按Enter开始读取。正在读取每页...fish：&#39；./demo2&#39；由信号SIGKILL(强制退出)终止。</p><p> It worked! After a few minutes, memory was exhausted, and the OOM killer killed the process.</p><p>啊，真灵!。几分钟后，内存耗尽，OOM杀手终止了该进程。</p><p>   Resident memory remains extremely low despite the OOM score being extremely high. This shows that page table overhead does not affect resident memory.</p><p>尽管OOM分数非常高，但驻留内存仍然非常低。这表明页表开销不会影响驻留内存。</p><p>  The bottom section shows the  VmPTE field of  /proc/*/status, confirming that close to 800MB of memory is used by page tables (close to physical memory limit).</p><p>底部显示/proc/*/status的VmPTE字段，确认页表使用了接近800MB的内存(接近物理内存限制)。</p><p> The pictures are nice, but it’s much more exciting to see it happen live.</p><p>照片很漂亮，但看到现场直播更令人兴奋。</p><p>  Note that the code performs all the mmaps first, then touches the memory. This is intentional. An alternate approach where it touches the mapping immediately after mmap’ing won’t work because mmap will eventually fail. This will prevent us from allocating more memory and pushing the system to its limit.</p><p>请注意，代码首先执行所有mmap，然后触及内存。这是故意的。在mmap‘ing之后立即接触映射的另一种方法不会起作用，因为mmap最终会失败。这将防止我们分配更多内存并将系统推向极限。</p><p> When we mmap everything up front and then touch the pages, we take advantage of overcommit, which will allow mmaps to succeed long after physical memory limits would have been bypassed. We then exploit the fact that a valid memory access has no way to “fail” —  page faults must be handled.</p><p>当我们预先mmap所有内容，然后触摸页面时，我们利用了过度提交，这将允许mmap在绕过物理内存限制之后很长一段时间才能成功。然后，我们利用这样一个事实，即有效的内存访问不可能“失败”-必须处理页面错误。</p><p>  With a sufficiently quiet system (and some luck), it’s possible to get this stack trace in the OOM killer’s logging:</p><p>有了足够安静的系统(加上一些运气)，就有可能在OOM杀手的日志中获得堆栈跟踪：</p><p>  This is the kernel stack trace of the allocation that triggered the OOM kill. In the middle of the trace, we find familiar functions:  handle_mm_fault, and  do_anonymous_page. This is the page fault handler for anonymous pages, which we saw earlier.</p><p>这是触发OOM终止的分配的内核堆栈跟踪。在跟踪的中间，我们发现了熟悉的函数：handlemmfault和do匿名page。这是我们前面看到的匿名页面的页面错误处理程序。</p><p> This trace shows that the final allocation that triggered the OOM killer was our own attempt at mapping another zero page —  an operation that’s “free” in theory, but can get you OOM killed in practice.</p><p>此跟踪显示，触发OOM杀手的最终分配是我们自己尝试映射另一个零页-这一操作在理论上是“免费的”，但在实践中可能会导致您的OOM死亡。</p><p>  If you look closely enough at demand paging you’ll find nuance. Writes obviously trigger allocations, but reads can be efficiently serviced by the shared zero page for “free”.</p><p>如果您足够仔细地查看请求分页，您会发现细微差别。写操作显然会触发分配，但是读操作可以由共享的零页面有效地提供“空闲”服务。</p><p> This enables applications to allocate vast amounts of virtual address space  and access all of it!.. as long as they only want to read (zeros). This can be applied practically with sparse data structures, whose elements are mostly zero.</p><p>这使应用程序能够分配大量的虚拟地址空间并访问所有这些空间！只要他们只想读取(零)。这实际上可以应用于元素大多为零的稀疏数据结构。</p><p> That said, the eager virtual-memory-savvy programmer should be warned — while invisible to userspace, “free” zero page mappings are not free in practice, and can incur substantial memory overhead from the virtual memory infrastructure itself.</p><p>这就是说，应该警告那些热衷于虚拟内存的程序员-虽然用户空间看不到“自由”的零页映射，但实际上“自由”的零页映射并不是免费的，而且可能会从虚拟内存基础设施本身产生大量的内存开销。</p><p> Page tables are just one of several sources of system memory overhead (per-thread kernel stacks being another example). Usually application developers can safely ignore these, but I hope this article offers a glimpse into the world of systems developers whose responsibility it is to provide that safety.</p><p>页表只是系统内存开销的几个来源之一(每个线程的内核堆栈是另一个例子)。通常，应用程序开发人员可以安全地忽略这些问题，但我希望这篇文章能让我们一窥负责提供这种安全性的系统开发人员的世界。</p><p> Did you learn something from this post? I’d love to hear what it was — tweet me  @offlinemark!</p><p>你从这篇帖子里学到什么了吗？我很想听听是什么--发推特给我@offlinemark！</p><p> I also have a mailing list if you want to know when I write new posts:</p><p>如果你想知道我什么时候写新帖子，我还有一个邮件列表：</p><p>  Thanks to Jann Horn for help understanding the resident memory spike. Thanks to Jake Miller, Roderic Morris, James Larisch, and William Woodruff for reviewing earlier drafts of this post.</p><p>感谢Jann Horn帮助理解常驻记忆峰值。感谢杰克·米勒、罗德里克·莫里斯、詹姆斯·拉里希和威廉·伍德拉夫审阅了这篇文章的早期草稿。</p><p>  Summary: if you need to measure resident memory usage with high accuracy, do not use htop (or  /proc/*/{statm,status}). Use  /proc/*/smaps or  /proc/*/smaps_rollup instead.</p><p>摘要：如果您需要高精度地测量驻留内存使用情况，请不要使用HTOP(或/proc/*/{statm，status})。请改用/proc/*/smaps或/proc/*/smaps_roll up。</p><p> In these experiments, a mysterious 1.5 MB spike in resident memory appears while triggering reads of all the pages. This should not be the case — as described above, faulting zero pages only allocates page tables, which don’t count toward resident memory. The resident memory reported immediately before performing the reads should stay constant for the remainder of execution.</p><p>在这些实验中，当触发对所有页面的读取时，驻留内存中会出现一个神秘的1.5MB峰值。情况不应该是这样-如上所述，缺省为零页只会分配页表，而不会计入驻留内存。在执行读取之前立即报告的驻留内存应该在剩余的执行过程中保持不变。</p><p> I was mystified, and convinced that I found a kernel bug. I confirmed that there is nothing happening in userspace that would cause this — the page faulting loop has a single memory access instruction in its body. Single stepping confirms that this instruction alone triggers the spike. I also confirmed that the source of htop’s data,  /proc/*/statm, shows this spike also, so it’s not a htop bug.</p><p>我很困惑，并确信我发现了一个内核错误。我确认用户空间中没有发生任何会导致这种情况的事情-页面出错循环的主体中只有一条内存访问指令。单步执行确认此指令单独触发尖峰。我还确认了HTOP的数据源/proc/*/statm也显示了这个峰值，所以这不是HTOP错误。</p><p> To my disappointment, it turns out that this isn’t a bug. (Thanks to Jann Horn for showing me this.)</p><p>令我失望的是，事实证明这不是一个错误。(感谢Jann Horn给我看了这个。)。</p><p> Different files in  /proc offer varying levels of accuracy. This is not documented in the man page, but is mentioned in the kernel’s internal  documentation and this  stack overflow post. htop receives memory statistics information from  /proc/*/statm which intentionally offers a lower accuracy measurement, in exchange for better multithreaded performance. To avoid lock contention, the kernel records memory usage in  per-thread caches, and then  synchronizes those with the  process-wide cache after every  64 page faults.</p><p>/proc中的不同文件提供不同级别的准确性。这没有记录在手册页中，但是在内核的内部文档和这个堆栈溢出帖子中提到了。HTOP从/proc/*/statm接收内存统计信息，该信息故意提供较低的精度度量，以换取更好的多线程性能。为了避免锁争用，内核在每个线程缓存中记录内存使用情况，然后在每次64页错误后将这些内存使用与进程范围的缓存同步。</p><p> So, the “spike” I was observing is not actually a spike — it’s the synchronization threshold being hit, which updates the process-wide information in  /proc/*/statm. The resident memory is actually always at 3 MB, it’s just that the reporting layer takes some time to get updated because my program happens to stop on a  getchar() in the middle of the 64 fault cycle.</p><p>因此，我观察到的“峰值”实际上并不是峰值-它是达到同步阈值，它更新/proc/*/statm中的进程范围信息。驻留内存实际上总是3MB，只是报告层需要一些时间才能更新，因为我的程序恰好在64故障周期中间的getchar()上停止。</p><p> If you want high accuracy memory information, use  /proc/*/smaps and  /proc/*/smaps_rollup. At the expense of performance, these files offer higher accuracy because their implementations walk internal data structures, rather than using internal caches.</p><p>如果您需要高精度的内存信息，请使用/proc/*/smaps和/proc/*/smaps_roll up。以牺牲性能为代价，这些文件提供了更高的准确性，因为它们的实现遍历内部数据结构，而不是使用内部缓存。</p><p>  A small mystery remains: if the statistics sync every 64 page faults, how can 1.5 MB of error accumulate? Syncing every 64 faults suggests that the maximum error would be 4 KB * 63 faults = 252 KB of error.</p><p>还有一个小谜团：如果统计数据每64页错误同步一次，怎么会累积1.5MB的错误呢？每64个故障同步一次表明最大错误为4KB*63个故障=252KB错误。</p><p> Both Jann and I are stumped by this. If you can help explain this, please  contact me.  LKML discussion here.</p><p>詹恩和我都被这事难住了。如果您能帮助解释这一点，请与我联系。LKML讨论在这里。</p><p>   This is lazy, dirty research code :) It is not sound or portable, and only just works because Linux happens to place the mmaps contiguously, growing down in memory.</p><p>这是懒惰、肮脏的研究代码：)它既不健全，也不便携，只是因为Linux碰巧将mmap放在连续的位置，在内存中不断向下增长，所以它才能正常工作。</p><p> Or if you prefer, the famous Henry Ford  quote: “You can touch as much memory as you want, as long as you are only reading”.</p><p>或者，如果你愿意，可以引用亨利·福特(Henry Ford)的名言：“只要你只是在阅读，你想触摸多少记忆就能触摸到多少。”</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://offlinemark.com/2020/10/14/demand-paging/">https://offlinemark.com/2020/10/14/demand-paging/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/不会/">#不会</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/demand/">#demand</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/内存/">#内存</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>