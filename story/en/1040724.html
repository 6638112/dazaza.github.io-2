<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>计算144维上的欧几里得距离 Computing Euclidean Distance on 144 Dimensions</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Computing Euclidean Distance on 144 Dimensions<br/>计算144维上的欧几里得距离 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-19 23:35:41</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/99020d6365d719ff4c48abb328d568a1.png"><img src="http://img2.diglog.com/img/2020/12/99020d6365d719ff4c48abb328d568a1.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Late last year I read a blog post about  our CSAM image scanning tool. I remember thinking: this is so cool! Image processing is always hard, and deploying a real image identification system at Cloudflare is no small achievement!</p><p>去年下半年，我读了一篇有关CSAM图像扫描工具的博客文章。我记得当时在想：这太酷了！图像处理总是很困难，在Cloudflare上部署真实的图像识别系统可不是一件容易的事！</p><p> Some time later, I was chatting with Kornel: &#34;We have all the pieces in the image processing pipeline, but we are struggling with the performance of one component.&#34; Scaling to Cloudflare needs ain&#39;t easy!</p><p> 一段时间后，我和Kornel聊天：＆＃34;我们在图像处理管道中拥有所有组件，但是我们在为一个组件的性能而苦苦挣扎。扩展到Cloudflare并非易事！</p><p> The problem was in the speed of the matching algorithm itself. Let me elaborate. As  John explained in his blog post, the image matching algorithm creates a fuzzy hash from a processed image. The hash is exactly 144 bytes long. For example, it might look like this:</p><p> 问题在于匹配算法本身的速度。让我详细说明。如John在其博客文章中所述，图像匹配算法会根据处理后的图像创建模糊哈希。哈希正好是144个字节长。例如，它可能看起来像这样：</p><p>  The hash is designed to be used in a fuzzy matching algorithm that can find &#34;nearby&#34;, related images. The specific algorithm is well defined, but making it fast is left to the programmer — and at Cloudflare we need the matching to be done super fast. We want to match thousands of hashes per second, of images passing through our network, against a database of millions of known images. To make this work, we need to seriously optimize the matching algorithm.</p><p>  哈希设计用于模糊匹配算法中，该算法可以找到附近的相关图像。特定的算法定义明确，但是要使其快速完成则由程序员负责–在Cloudflare，我们需要以超快的速度完成匹配。我们希望将每秒数以千计的散列通过我们的网络传递的图像与数百万个已知图像的数据库进行匹配。为此，我们需要认真优化匹配算法。</p><p>  The first algorithm that comes to mind has  O(K*N) complexity: for each query, go through every hash in the database. In naive implementation, this creates a lot of work. But how much work exactly?</p><p>  我想到的第一个算法具有O（K * N）复杂度：对于每个查询，都要遍历数据库中的每个哈希。在幼稚的实现中，这会产生很多工作。但是到底有多少工作？</p><p>  Given a query hash, the fuzzy match is the &#34;closest&#34; hash in a database. This requires us to define a distance. We treat each hash as a vector containing 144 numbers, identifying a point in a 144-dimensional space. Given two such points, we can calculate the distance using the standard Euclidean formula.</p><p>  给定查询散列，模糊匹配为＆＃34; closest＆＃34;。在数据库中哈希。这要求我们定义一个距离。我们将每个哈希视为包含144个数字的向量，以标识144维空间中的一个点。给定两个这样的点，我们可以使用标准的欧几里得公式来计算距离。</p><p> For our particular problem, though, we are interested in the &#34;closest&#34; match in a database only if the distance is lower than some predefined threshold. Otherwise, when the distance is large,  we can assume the images aren&#39;t similar. This is the expected result — most of our queries will not have a related image in the database.</p><p> 不过，对于我们的特定问题，我们对＆＃34; closest＆＃34;感兴趣。仅当距离小于某个预定义阈值时才匹配数据库。否则，当距离较大时，我们可以假定图像不相似。这是预期的结果-我们的大多数查询在数据库中都没有相关的图像。 </p><p>   To calculate the distance between two 144-byte hashes, we take each byte, calculate the delta, square it, sum it to an accumulator, do a square root, and ta-dah! We have the distance!</p><p>要计算两个144字节散列之间的距离，我们取每个字节，计算增量，将其平方，然后将其求和到一个累加器中，求平方根，然后ta-dah！我们有距离！</p><p>   This function returns the squared distance. We avoid computing the actual distance to save us from running the square root function - it&#39;s slow. Inside the code, for performance and simplicity, we&#39;ll mostly operate on the squared value. We don&#39;t need the actual distance value, we just need to find the vector with the smallest one. In our case it doesn&#39;t matter if we&#39;ll compare distances or squared distances!</p><p>   此函数返回距离的平方。我们避免计算实际距离，以免我们无法运行平方根函数-它很慢。在代码内部，为了提高性能和简化操作，我们主要对平方值进行操作。我们不需要实际的距离值，只需要找到最小的向量即可。对于我们来说，比较距离还是平方距离并不重要！</p><p> As you can see, fuzzy matching is basically a standard problem of finding the closest point in a multi-dimensional space. Surely this has been solved in the past — but let&#39;s not jump ahead.</p><p> 如您所见，模糊匹配基本上是在多维空间中找到最接近点的标准问题。当然，过去已经解决了这个问题-但我们不要跳过。</p><p> While this code might be simple, we expect it to be rather slow. Finding the smallest hash distance in a database of, say, 1M entries, would require going over all records, and would need at least:</p><p> 虽然这段代码可能很简单，但我们希望它会很慢。要在数据库中找到最小的哈希距离（例如1M条目），将需要遍历所有记录，并且至少需要：</p><p>  And more. This alone adds up to 432 million operations! How does it look in practice? To illustrate this blog post  we prepared a full test suite. The large database of known hashes can be  well emulated by random data. The query hashes can&#39;t be random and must be slightly more sophisticated, otherwise the exercise wouldn&#39;t be that interesting. We generated the test smartly by byte-swaps of the actual data from the database — this allows us to precisely control the distance between test hashes and database hashes.  Take a look at the scripts for details. Here&#39;s our first run of  the first, naive, algorithm:</p><p>  和更多。仅此一项，就增加了4.32亿次操作！实际情况如何？为了说明此博客文章，我们准备了完整的测试套件。大型的已知哈希数据库可以通过随机数据很好地模拟。查询散列不能是随机的，并且必须稍微复杂一些，否则，练习就不会那么有趣了。我们通过字节交换数据库中的实际数据来智能地生成测试-这使我们能够精确地控制测试哈希与数据库哈希之间的距离。查看脚本以了解详细信息。这是我们首次运行的第一个幼稚算法：</p><p> $ make naive&lt; test-vector.txt ./mmdist-naive &gt; test-vector.tmpTotal: 85261.833ms, 1536 items, avg 55.509ms per query, 18.015 qps</p><p> $天真＆lt; test-vector.txt ./mmdist-naive＆gt; test-vector.tmp总计：85261.833ms，1536项，平均每次查询55.509ms，18.015 qps</p><p> We matched 1,536 test hashes against a database of 1 million random vectors in 85 seconds. It took 55ms of CPU time on average to find the closest neighbour. This is rather slow for our needs.</p><p> 我们在85秒内将1536个测试哈希与100万个随机向量的数据库进行了匹配。找到最接近的邻居平均花了55ms CPU时间。这对于我们的需求而言相当慢。 </p><p>  An obvious improvement is to use more complex SIMD instructions. SIMD is a way to instruct the CPU to process multiple data points using one instruction. This is a perfect strategy when dealing with vector problems — as is the case for our task.</p><p>一个明显的改进是使用了更复杂的SIMD指令。 SIMD是一种指示CPU使用一条指令处理多个数据点的方法。当处理向量问题时，这是一个完美的策略-正如我们的任务一样。</p><p> We settled on using AVX2, with 256 bit vectors. We did this for a simple reason — newer AVX versions are not supported by our AMD CPUs. Additionally, in the past, we were  not thrilled by the AVX-512 frequency scaling.</p><p> 我们决定使用具有256位向量的AVX2。我们这样做的原因很简单-AMD CPU不支持较新的AVX版本。此外，过去，我们对AVX-512的频率缩放并不感到兴奋。</p><p> Using AVX2 is easier said than done. There is no single instruction to count Euclidean distance between two uint8 vectors! The fastest way of counting the full distance of two 144-byte vectors  with AVX2 we could find is authored by  Vlad:</p><p> 说使用AVX2容易做起来难。没有任何一条指令可以计算两个uint8向量之间的欧几里得距离！由Vlad编写的用AVX2计算两个144字节向量的完整距离的最快方法是由Vlad编写的：</p><p>  It’s actually simpler than it looks: load 16 bytes, convert vector from uint8 to int16, subtract the vector, store intermediate sums as int32, repeat. At the end, we need to do complex 4 instructions to extract the partial sums into the final sum.  This AVX2 code improves the performance around 3x:</p><p>  它实际上比看起来简单：加载16个字节，将向量从uint8转换为int16，减去向量，将中间和存储为int32，然后重复。最后，我们需要执行4条复杂的指令以将部分和提取为最终和。此AVX2代码将性能提高了3倍左右：</p><p>  We measured 17ms per item, which is still below our expectations. Unfortunately, we can&#39;t push it much further without major changes. The problem is that this code is limited by memory bandwidth. The measurements come from my Intel i7-5557U CPU, which has the max theoretical memory bandwidth of just 25GB/s. The database of 1 million entries takes 137MiB, so it takes at least 5ms to feed the database to my CPU. With this naive algorithm we won&#39;t be able to go below that.</p><p>  我们测量的每个项目为17毫秒，仍然低于我们的预期。不幸的是，如果没有重大更改，我们将无法进一步推进。问题在于此代码受内存带宽限制。测量来自我的Intel i7-5557U CPU，其最大理论内存带宽仅为25GB / s。 100万个条目的数据库需要137MiB，因此至少需要5毫秒才能将数据库提供给我的CPU。使用这种幼稚的算法，我们将无法做到这一点。</p><p>  Since the naive brute force approach failed, we tried using more sophisticated algorithms. My colleague  Kornel Lesiński implemented a super cool  Vantage Point algorithm. After a few ups and downs, optimizations and rewrites, we gave up. Our problem turned out to be unusually hard for this kind of algorithm.</p><p>  由于朴素的暴力破解方法失败，因此我们尝试使用更复杂的算法。我的同事KornelLesiński实现了超酷的Vantage Point算法。经过几次跌宕起伏，优化和重写之后，我们放弃了。对于这种算法，我们的问题变得异常困难。</p><p> We observed  &#34;the curse of dimensionality&#34;. Space partitioning algorithms don&#39;t work well in problems with large dimensionality — and in our case, we have an enormous number of 144 dimensions. K-D trees are doomed. Locality-sensitive hashing is also doomed. It&#39;s a bizarre situation in which the space is unimaginably vast, but everything is close together. The volume of the space is a 347-digit-long number, but the maximum distance between points is just 3060 - sqrt(255*255*144).</p><p> 我们观察到了“维度的诅咒”。空间划分算法在处理大维问题时效果不佳-在我们的案例中，我们有144个维。 K-D树注定要失败。局域性哈希也注定要失败。这是一个奇怪的情况，其中的空间是难以想象的巨大，但是所有东西都紧密地结合在一起。该空间的大小为347位数字长，但点之间的最大距离仅为3060-sqrt（255 * 255 * 144）。 </p><p> Space partitioning algorithms are fast, because they gradually narrow the search space as they get closer to finding the closest point. But in our case, the common query is never close to any point in the set, so the search space can’t be narrowed to a meaningful degree.</p><p>空间划分算法之所以快速，是因为随着它们越来越靠近寻找最近的点，它们逐渐缩小了搜索空间。但是在我们的情况下，通用查询永远不会接近集合中的任何一点，因此搜索空间无法缩小到有意义的程度。</p><p> A VP-tree was a promising candidate, because it operates only on distances, subdividing space into near and far partitions, like a binary tree. When it has a close match, it can be very fast, and doesn&#39;t need to visit more than  O(log(N)) nodes. For non-matches, its speed drops dramatically. The algorithm ends up visiting nearly half of the nodes in the tree. Everything is close together in 144 dimensions! Even though the algorithm avoided visiting more than half of the nodes in the tree, the cost of visiting remaining nodes was higher, so the search ended up being slower overall.</p><p> VP树是一个很有前途的候选者，因为它仅在一定距离上运行，将空间细分为近和远的分区，就像二叉树一样。当它具有紧密匹配时，它可以非常快，并且不需要访问超过O（log（N））个节点。对于不匹配的游戏，其速度会急剧下降。该算法最终访问树中几乎一半的节点。一切都在144个维度上紧密结合在一起！即使该算法避免访问树中超过一半的节点，但访问其余节点的成本较高，因此搜索最终总体上变慢了。</p><p>  This experience got us thinking. Since space partitioning algorithms can&#39;t narrow down the search, and still need to go over a very large number of items, maybe we should focus on going over all the hashes, extremely quickly. We must be smarter about memory bandwidth though — it was the limiting factor in the naive brute force approach before.</p><p>  这次经历使我们开始思考。由于空间划分算法无法缩小搜索范围，并且仍然需要遍历大量项目，因此也许我们应该专注于非常快速地遍历所有哈希。但是，我们必须对内存带宽更加机敏-以前它是天真的暴力方法的限制因素。</p><p>   The breakthrough came from the realization that we don&#39;t need to count the full distance between hashes. Instead, we can compute only a subset of dimensions, say 32 out of the total of 144. If this distance is already large, then there is no need to compute the full one! Computing more points is not going to reduce the Euclidean distance.</p><p>   突破来自认识到我们不需要计算散列之间的完整距离。相反，我们只能计算尺寸的一个子集，例如144个尺寸中的32个。如果该距离已经很大，则无需计算整个尺寸！计算更多的点不会减少欧几里得距离。</p><p>   2. Go over all the 1 million 32-byte short hashes from the database. They must be densely packed in the memory to allow the CPU to perform good prefetching and avoid reading data we won&#39;t need.</p><p>   2.遍历数据库中所有的100万个32字节短哈希。它们必须密集地包装在内存中，以使CPU能够执行良好的预取，并避免读取我们不需要的数据。</p><p> 3. If the distance of the 32-byte short hash is greater or equal a best score so far, move on</p><p> 3.如果到目前为止32字节短哈希的距离大于或等于最佳分数，请继续</p><p>  Even though this algorithm needs to do less arithmetic and memory work, it&#39;s not faster than the previous naive one. See  make short-avx2. The problem is: we still need to compute a full distance for hashes that are promising, and there are quite a lot of them. Computing the full distance for promising hashes adds enough work, both in ALU and memory latency, to offset the gains of this algorithm.</p><p>  即使此算法需要执行较少的算术和内存工作，也不会比以前的幼稚算法快。请参见make short-avx2。问题是：我们仍然需要为有前途的哈希计算完整距离，并且其中有很多。计算有希望的哈希的完整距离会增加ALU和内存延迟方面的工作量，以抵消该算法的收益。 </p><p> There is one detail of our particular application of the image matching problem that will help us a lot moving forward. As we described earlier, the problem is less about finding the closest neighbour and more about proving that the neighbour with a reasonable distance doesn&#39;t exist. Remember — in practice, we don&#39;t expect to find many matches! We expect almost every image we feed into the algorithm to be unrelated to image hashes stored in the database.</p><p>我们对图像匹配问题的特殊应用有一个细节，它将帮助我们取得更大的进步。如我们先前所述，问题不在于寻找最接近的邻居，而在于证明不存在具有合理距离的邻居。请记住-实际上，我们不希望找到很多比赛！我们期望我们输入到算法中的几乎所有图像都与存储在数据库中的图像哈希无关。</p><p> It&#39;s sufficient for our algorithm to prove that no neighbour exists within a predefined distance threshold. Let&#39;s assume we are not interested in hashes more distant than, say, 220, which squared is 48,400.  This makes our short-distance algorithm variation work much better:</p><p> 对于我们的算法而言，足以证明在预定的距离阈值内不存在邻居。假设我们对比220距离更远的哈希值不感兴趣，例如220,400的平方。这使我们的短距离算法变体效果更好：</p><p>    At some point, John noted that the threshold allows additional optimization. We can order the hashes by their distance from some origin point. Given a query hash which has origin distance of A, we can inspect only hashes which are distant between |A-threshold| and |A+threshold| from the origin. This is pretty much how each level of Vantage Point Tree works, just simplified. This optimization — ordering items in the database by their distance from origin point — is relatively simple and can help save us a bit of work.</p><p>    在某个时候，John指出该阈值允许进行其他优化。我们可以通过哈希值到某个原点的距离对其进行排序。给定查询散列的起始距离为A，我们只能检查| A-threshold |之间的散列和| A +阈值|从起源。这就是Vantage点树的每个级别的工作原理，只是进行了简化。这种优化-按距原点的距离对数据库中的项目进行排序-比较简单，可以帮助我们节省一些工作。</p><p> While great on paper, this method doesn&#39;t introduce much gain in practice, as the vectors are not grouped in clusters — they are pretty much random! For the threshold values we are interested in, the origin distance algorithm variation gives us ~20% speed boost, which is okay but not breathtaking. This change might bring more benefits if we ever decide to reduce the threshold value, so it might be worth doing for production implementation. However, it doesn&#39;t work well with query batching.</p><p> 尽管在纸面上很不错，但是这种方法在实践中并没有带来太大的收益，因为矢量没有按聚类分组-它们几乎是随机的！对于我们感兴趣的阈值，原点距离算法的变化使我们的速度提高了约20％，这虽然可以，但令人叹为观止。如果我们决定降低阈值，此更改可能会带来更多好处，因此对于生产实施可能值得这样做。但是，它不适用于查询批处理。</p><p>  But we&#39;re not done with AVX optimizations! The usual problem with AVX is that the instructions don&#39;t normally fit a specific problem. Some serious mind twisting is required to adapt the right instruction to the problem, or to reverse the problem so that a specific instruction can be used. AVX2 doesn&#39;t have useful &#34;horizontal&#34; uint16 subtract, multiply and add operations. For example,  _mm_hadd_epi16 exists, but it&#39;s slow and cumbersome.</p><p>  但是我们还没有完成AVX优化！ AVX的常见问题是说明通常不适合特定问题。为了使正确的指令适应问题或使问题逆转，需要认真思考，以便可以使用特定的指令。 AVX2没有有用的＆＃34;水平＆＃34; uint16减，乘和加运算。例如，存在_mm_hadd_epi16，但是它又慢又麻烦。</p><p> Instead, we can twist the problem to make use of fast available uint16 operands. For example we can use:</p><p> 相反，我们可以解决问题以利用快速可用的uint16操作数。例如，我们可以使用：</p><p>   The saturated  add is great and saves us conversion to uint32. It just adds a small limitation: the threshold passed to the program (i.e., the max squared distance) must fit into uint16. However, this is fine for us.</p><p>   饱和添加非常有用，可以节省我们转换为uint32的时间。它只是增加了一个小限制：传递给程序的阈值（即最大平方距离）必须适合uint16。但是，这对我们很好。 </p><p> To effectively use these instructions we need to transpose the data in the database. Instead of storing hashes in rows, we can store them in columns:</p><p>为了有效地使用这些指令，我们需要转置数据库中的数据。除了将哈希存储在行中之外，我们还可以将它们存储在列中：</p><p>        Now we can load 16 first bytes of hashes using one memory operation. In the next step, we can subtract the first byte of the querying hash using a single instruction, and so on. The algorithm stays exactly the same as defined above; we just make the data easier to load and easier to process for AVX.</p><p>        现在，我们可以使用一个内存操作加载16个哈希的第一个字节。在下一步中，我们可以使用一条指令减去查询哈希的第一个字节，依此类推。该算法与上面定义的完全相同；我们只是使数据更易于加载和更易于AVX处理。</p><p>   With the well-tuned batch size and short distance size parameters we can  see the performance of this algorithm:</p><p>   通过调整好的批次大小和短距离大小参数，我们可以看到此算法的性能：</p><p>  Whoa! This is pretty awesome. We started from 55ms per query, and we finished with just 0.73ms. There are further micro-optimizations possible, like memory prefetching or using huge pages to reduce page faults, but they have diminishing returns at this point.</p><p>  哇！太棒了我们从每个查询55毫秒开始，仅以0.73毫秒结束。还有可能进行进一步的微优化，例如内存预取或使用大页面来减少页面错误，但此时它们的收益递减。</p><p>  If you are interested in architectural tuning such as this, take a look at  the new performance book by Denis Bakhvalov. It discusses roofline model analysis, which is pretty much what we did here.</p><p>  如果您对这样的架构调整感兴趣，请阅读Denis Bakhvalov撰写的新性能手册。它讨论了车顶线模型分析，这几乎就是我们在这里所做的。</p><p> Do take a look at our code and tell us if we missed some optimization!</p><p> 请看一下我们的代码，并告诉我们是否错过了一些优化！</p><p>  What an optimization journey! We jumped between memory and ALU bottlenecked code. We discussed more sophisticated algorithms, but in the end, a brute force algorithm — although tuned — gave us the best results.</p><p>  多么优化的旅程！我们在内存和ALU瓶颈代码之间跳转。我们讨论了更复杂的算法，但最后，尽管进行了调整，但蛮力算法给了我们最好的结果。 </p><p> To get even better numbers, I experimented with Nvidia GPU using CUDA. The CUDA intrinsics like  vabsdiff4 and  dp4a fit the problem perfectly. The V100 gave us some amazing numbers, but I wasn&#39;t fully satisfied with it. Considering how many AMD Ryzen cores with AVX2 we can get for the cost of a single server-grade GPU, we leaned towards general purpose computing for this particular problem.</p><p>为了获得更好的数字，我使用CUDA对Nvidia GPU进行了实验。像vabsdiff4和dp4a这样的CUDA内在函数完全适合该问题。 V100给了我们一些惊人的数字，但我并不完全满意。考虑到我们可以用一个服务器级GPU的成本获得多少个带有AVX2的AMD Ryzen内核，我们针对这个特殊问题倾向于通用计算。</p><p> This is a great example of the type of complexities we deal with every day. Making even the best technologies work “at Cloudflare scale” requires thinking outside the box. Sometimes we rewrite the solution dozens of times before we find the optimal one. And sometimes we settle on a brute-force algorithm, just very very optimized.</p><p> 这是我们每天处理的复杂类型的一个很好的例子。要使最佳技术“在Cloudflare规模上”发挥作用，就需要跳出思路。有时，在找到最佳解决方案之前，我们重写了数十次解决方案。有时我们会采用蛮力算法，只是非常非常优化。</p><p> The computation of hashes and image matching are challenging problems that require running very CPU intensive operations.. The CPU we have available on the edge is scarce and workloads like this are incredibly expensive. Even with the optimization work talked about in this blog post, running the CSAM scanner at scale is a challenge and has required a huge engineering effort. And we’re not done! We need to solve more hard problems before we&#39;re satisfied. If you want to help, consider  applying!</p><p> 散列的计算和图像匹配是具有挑战性的问题，需要运行非常占用CPU的操作。我们在边缘上可用的CPU稀缺，这样的工作量非常昂贵。即使在本博客文章中讨论了优化工作，大规模运行CSAM扫描器也是一个挑战，并且需要大量的工程工作。我们还没有完成！在我们满意之前，我们需要解决更多的难题。如果您想提供帮助，请考虑申请！</p><p>  Performance  Optimization  Speed</p><p>  性能优化速度 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.cloudflare.com/computing-euclidean-distance-on-144-dimensions/">https://blog.cloudflare.com/computing-euclidean-distance-on-144-dimensions/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/欧几里得/">#欧几里得</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/euclidean/">#euclidean</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/需要/">#需要</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>