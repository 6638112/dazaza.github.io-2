<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>PySpark样式指南</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">PySpark样式指南</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-21 16:07:17</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/d46f0f248f8718c165edc5bde56db42a.png"><img src="http://img2.diglog.com/img/2020/10/d46f0f248f8718c165edc5bde56db42a.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>PySpark is a wrapper language that allows users to interface with an Apache Spark backend to quickly process data. Spark can operate on massive datasets across a distributed network of servers, providing major performance and reliability benefits when utilized correctly. It presents challenges, even for experienced Python developers, as the PySpark syntax draws on the JVM heritage of Spark and therefore implements code patterns that may be unfamiliar.</p><p>PySpark是一种包装器语言，允许用户与Apache Spark后端交互以快速处理数据。Spark可以在分布式服务器网络中的海量数据集上运行，如果使用得当，可提供重大的性能和可靠性优势。它带来了挑战，即使对于经验丰富的Python开发人员也是如此，因为PySpark语法利用了Spark的JVM传统，因此实现了可能不熟悉的代码模式。</p><p> This opinionated guide to PySpark code style presents common situations we&#39;ve encountered and the associated best practices based on the most frequent recurring topics across PySpark repos.</p><p>这本关于PySpark代码风格的固执己见的指南介绍了我们遇到的常见情况，以及基于PySpark Repos中最频繁出现的主题的相关最佳实践。</p><p> Beyond PySpark specifics, the general practices of clean code are important in PySpark repositories- the Google  PyGuide is a strong starting point for learning more about these practices.</p><p>除了PySpark的细节之外，干净代码的一般实践在PySpark存储库中也很重要-Google PyGuide是了解更多这些实践的有力起点。</p><p>   The preferred option is more complicated, longer, and polluted - and correct. While typically it is best to avoid using F.col() altogether, there are certain cases where using it, or the alternative explicit selection, is unavoidable. There is, however, good reason to prefer the second example over the first one.</p><p>首选选项更复杂、更长、更污染-而且是正确的。虽然通常最好避免完全使用F.ol()，但在某些情况下，使用它或另一种显式选择是不可避免的。然而，我们有充分的理由更喜欢第二个例子而不是第一个例子。</p><p> When using explicit columns as in the first case, both the dataframe name and schema are explicitly bound to the dataframe variable. This means that if  df1 is deleted or renamed, the reference  df1.colA will break.</p><p>与第一种情况一样使用显式列时，DataFrame名称和架构都显式绑定到dataframe变量。这意味着如果df1被删除或重命名，引用df1.colA将中断。</p><p> By contrast,  F.col(&#39;colA&#39;) will always reference a column designated  &#39;colA&#39; in the dataframe being operated on, named  df, in this case. It does not require keeping track of other dataframes&#39; states at all, so the code becomes more local and less prone to “spooky interaction at a distance,” which is often challenging to debug.</p><p>相比之下，F.ol(&#39；Cola&39；)将始终引用正在操作的数据帧中指定的列，在本例中名为df。它根本不需要跟踪其他数据帧状态，因此代码变得更加本地化，不太容易出现“远距离的诡异交互”，这通常是调试上的挑战。</p><p>  If the column name has a space or other unsupported character that requires access by the bracketoperator then  df1[&#39;colA&#39;] is just as difficult to write as  F.col(&#39;colA&#39;);</p><p>如果列名包含需要括号操作符访问的空格或其他不受支持的字符，则df1[&#39；Cola&39；]与F.ol(&#39；Cola&39；)；一样难以写入。</p><p> Assigning an abstract expression like  F.col(&#39;prod_status&#39;) == &#39;Delivered&#39; to a variable makes it reusablefor multiple dataframes, while  df.prod_status == &#39;Delivered&#39; is always bound to df</p><p>将像F.ol(&#39；PROD_STATUS&#39；)==&#39；Delivered&#39；这样的抽象表达式赋给一个变量，使其可用于多个数据帧，而df.prod_status==&#39；Delivered&#39；总是绑定到DF</p><p> Fortunately, a convoluted expression with  F.col() is usually not required.  Prior to Spark 3.0, this was necessary for some functions, like  F.upper(), but since then the API has become much more uniform.</p><p>幸运的是，通常不需要使用F.ol()复杂的表达式。在Spark3.0之前，这对于一些函数来说是必需的，比如F.Upper()，但是从那以后API变得更加统一了。</p><p>  In some contexts there may be access to columns from more than one dataframe, and there may be an overlap in names. A common example is in matching expressions like  df.join(df2, on=(df.key == df2.key), how=&#39;left&#39;). In such cases it is fine to reference columns by their dataframe directly. You can also disambiguate joins using dataframe aliases (see more in the  Joins section in this guide).</p><p>在某些上下文中，可能可以访问多个数据帧中的列，并且名称可能会重叠。常见的示例是df.join(df2，on=(df.key==df2.key)，HOW=&#39；LEFT&#39；)这样的匹配表达式。在这种情况下，可以通过数据帧直接引用列。您还可以使用DataFrame别名消除连接的歧义(请参阅本指南的连接一节中的更多内容)。</p><p>  Logical operations, which often reside inside  .filter() or  F.when(), need to be readable. We apply the same rule as with chaining functions, keeping logic expressions inside the same code block to  three (3) expressions at most. If they grow longer, it is often a sign that the code can be simplified or extracted out. Extracting out complex logical operations into variables makes the code easier to read and reason about, which also reduces bugs.</p><p>逻辑操作通常驻留在.filter()或F.When()中，需要具有可读性。我们应用与链接函数相同的规则，将同一代码块内的逻辑表达式最多保留为三(3)个表达式。如果它们变长了，通常是代码可以简化或提取出来的迹象。将复杂的逻辑运算提取到变量中使代码更易于阅读和推理，这也减少了错误。</p><p> # bad F. when( ( F. col( &#39;prod_status&#39;)  ==  &#39;Delivered&#39;)  | ((( F. datediff( &#39;deliveryDate_actual&#39;,  &#39;current_date&#39;)  &lt;  0)  &amp; (( F. col( &#39;currentRegistration&#39;)  !=  &#39;&#39;)  | (( F. datediff( &#39;deliveryDate_actual&#39;,  &#39;current_date&#39;)  &lt;  0)  &amp; (( F. col( &#39;originalOperator&#39;)  !=  &#39;&#39;)  | ( F. col( &#39;currentOperator&#39;)  !=  &#39;&#39;)))))),  &#39;In Service&#39;)</p><p>#Bad F.When((F.ol(&#39；Prod_Status&#39；)==&#39；Delivered&#39；)|(F.datediff(&#39；deliveryDate_Actual&39；，&#39；Current_Date&#39；)&lt；0)&amp；((F.ol(&#39；CurrentRegistry&39；)！=&#39；&#39；)|((F.datediff(&#39；DeliveryDate_Actual&#39；，&#39；Current_Date&#39；)&lt；0)&amp；((F.ol(&#39；OriginalOperator&#39；)！=&#39；&#39；)|(F.ol(&#39；currentOperator&#39；)！=&#39；&#39；)，&#39；正在使用&#39；)。</p><p> The code above can be simplified in different ways. To start, focus on grouping the logic steps in a few named variables. PySpark requires that expressions are wrapped with parentheses. This, mixed with actual parenthesis to group logical operations, can hurt readability. For example the code above has a redundant  (F.datediff(df.deliveryDate_actual, df.current_date) &lt; 0) that the original author didn&#39;t notice because it&#39;s very hard to spot.</p><p>上面的代码可以用不同的方式简化。首先，将重点放在将逻辑步骤分组到几个命名变量中。PySpark要求表达式用括号括起来。这与用于分组逻辑操作的实际括号混合在一起，可能会影响可读性。例如，上面的代码有一个原始作者没有注意到的冗余(F.datediff(df.deliveryDate_Actual，df.current_date)&lt；0)，因为它很难发现。</p><p> # better has_operator  = (( F. col( &#39;originalOperator&#39;)  !=  &#39;&#39;)  | ( F. col( &#39;currentOperator&#39;)  !=  &#39;&#39;)) delivery_date_passed  = ( F. datediff( &#39;deliveryDate_actual&#39;,  &#39;current_date&#39;)  &lt;  0) has_registration  = ( F. col( &#39;currentRegistration&#39;). rlike( &#39;.+&#39;)) is_delivered  = ( F. col( &#39;prod_status&#39;)  ==  &#39;Delivered&#39;) F. when( is_delivered  | ( delivery_date_passed  &amp; ( has_registration  |  has_operator)),  &#39;In Service&#39;)</p><p>#较好的HAS_OPERATOR=((F.ol(&#39；OriginalOperator&#39；)！=&#39；&#39；)|(F.ol(&#39；currentOperator&#39；)！=&#39；&#39；)Delivery_Date_Passed=(F.datediff(&#39；deliveryDate_Actual&#39；，&#39；Current_Date&#39；)&lt；0)HAS_REGISTION=(F.ol(&#39；CurrentRegistry&#39；)。)。RLIKE(&#39；.+&#39；)is_delivered=(F.ol(&#39；Prod_Status&#39；)==&#39；Delivered&#39；)F.When(IS_Delivery|(Delivery_Date_Passed&amp；(HAS_REGISTION|HAS_OPERATOR))，&#39；服务中&#39；)。</p><p> The above example drops the redundant expression and is easier to read. We can improve it further by reducing the number of operations.</p><p>上面的示例去掉了冗余表达式，更易于阅读。我们可以通过减少手术次数来进一步改善它。</p><p> # good has_operator  = (( F. col( &#39;originalOperator&#39;)  !=  &#39;&#39;)  | ( F. col( &#39;currentOperator&#39;)  !=  &#39;&#39;)) delivery_date_passed  = ( F. datediff( &#39;deliveryDate_actual&#39;,  &#39;current_date&#39;)  &lt;  0) has_registration  = ( F. col( &#39;currentRegistration&#39;). rlike( &#39;.+&#39;)) is_delivered  = ( F. col( &#39;prod_status&#39;)  ==  &#39;Delivered&#39;) is_active  = ( has_registration  |  has_operator) F. when( is_delivered  | ( delivery_date_passed  &amp;  is_active),  &#39;In Service&#39;)</p><p>#Good Has_Operator=((F.ol(&#39；OriginalOperator&#39；)！=&#39；&#39；)|(F.ol(&#39；currentOperator&#39；)！=&#39；&#39；)Delivery_Date_Passed=(F.datediff(&#39；deliveryDate_Actual&#39；，&#39；Current_Date&#39；)&lt；0)HAS_REGISTION=(F.ol(&#39；currentRegister&#39；)；)。RLIKE(&#39；.+&#39；)IS_DEVERED=(F.ol(&#39；PROD_STATUS&#39；)==&#39；Delivered&#39；)IS_ACTIVE=(HAS_REGISTION|HAS_OPERATOR)F.When(IS_Delivery|(Delivery_Date_Passed&amp；is_active)，&#39；在服务中&#39；)</p><p> Note how the  F.when expression is now succinct and readable and the desired behavior is clear to anyone reviewing this code. The reader only needs to visit the individual expressions if they suspect there is an error. It also makes each chunk of logic easy to test if you have unit tests in your code, and want to abstract them as functions.</p><p>注意F.When表达式现在是如何简洁和可读的，并且所需的行为对于任何审阅这段代码的人来说都是显而易见的。如果读者怀疑有错误，则只需查看各个表达式。如果您的代码中有单元测试，并且希望将它们抽象为函数，它还可以使每个逻辑块易于测试。</p><p> There is still some duplication of code in the final example: how to remove that duplication is an exercise for the reader.</p><p>在最后一个示例中仍然有一些代码重复：如何消除重复是读者的练习。</p><p>  Doing a select at the beginning of a PySpark transform, or before returning, is considered good practice. This  select statement specifies the contract with both the reader and the code about the expected dataframe schema for inputs and outputs. Any select should be seen as a cleaning operation that is preparing the dataframe for consumption by the next step in the transform.</p><p>在PySpark转换开始时或在返回之前执行SELECT被认为是很好的做法。此SELECT语句指定与读取器的约定和有关输入和输出的预期数据帧模式的代码。任何选择都应被视为正在准备数据帧以供转换的下一步使用的清理操作。</p><p> Keep select statements as simple as possible. Due to common SQL idioms, allow only  one function from  spark.sql.function to be used per selected column, plus an optional  .alias() to give it a meaningful name. Keep in mind that this should be used sparingly. If there are more than  three such uses in the same select, refactor it into a separate function like  clean_&lt;dataframe name&gt;() to encapsulate the operation.</p><p>使SELECT语句尽可能简单。由于常见的SQL习惯用法，对于每个选定的列，只允许使用spk.sql.function中的一个函数，外加一个可选的.alias()以赋予它一个有意义的名称。请记住，这应该谨慎使用。如果在同一SELECT中有三个以上这样的用法，则将其重构为一个单独的函数，如CLEAN_&lt；dataframe name&gt；()来封装操作。</p><p> Expressions involving more than one dataframe, or conditional operations like  .when() are discouraged to be used in a select, unless required for performance reasons.</p><p>除非出于性能原因需要，否则不鼓励在SELECT中使用涉及多个数据帧的表达式或条件操作(如.When())。</p><p>    The  select() statement redefines the schema of a dataframe, so it naturally supports the inclusion or exclusion of columns, old and new, as well as the redefinition of pre-existing ones. By centralising all such operations in a single statement, it becomes much easier to identify the final schema, which makes code more readable. It also makes code more concise.</p><p>SELECT()语句重新定义数据帧的模式，因此它自然支持包含或排除旧列和新列，以及重新定义预先存在的列。通过将所有此类操作集中在一条语句中，识别最终模式变得容易得多，从而使代码更具可读性。它还使代码更加简洁。</p><p>      # bad df. select( (( F. coalesce( F. unix_timestamp( &#39;closed_at&#39;),  F. unix_timestamp())  -  F. unix_timestamp( &#39;created_at&#39;))  /  86400). alias( &#39;days_open&#39;)) # good df. withColumn(  &#39;days_open&#39;, ( F. coalesce( F. unix_timestamp( &#39;closed_at&#39;),  F. unix_timestamp())  -  F. unix_timestamp( &#39;created_at&#39;))  /  86400)</p><p>#错误的df。SELECT((F.coalesce(F.unix_Timestamp(&#39；CLOSED_AT&#39；，F.unix_Timestamp())-F.unix_Timestamp(&#39；Created_at&#39；))/86400)。别名(&#39；Days_OPEN&#39；))#Good DF。With Column(&#39；Days_OPEN&#39；，(F.coalesce(F.UNIX_TIMESTAMP(&#39；CLOSED_AT&#39；)，F.UNIX_TIMESTAMP())-F.Unix_TIMESTAMP(&#39；CREATED_AT&#39；))/86400)。</p><p> Avoid including columns in the select statement if they are going to remain unused and choose instead an explicit set of columns - this is a preferred alternative to using  .drop() since it guarantees that schema mutations won&#39;t cause unexpected columns to bloat your dataframe. However, dropping columns isn&#39;t inherintly discouraged in all cases; for instance- it is commonly appropriate to drop columns after joins since it is common for joins to introduce redundant columns.</p><p>如果SELECT语句中的列将保持未使用状态，请避免在其中包含这些列，而是选择一组显式的列-这是使用.drop()的首选替代方法，因为它可以保证模式突变不会导致意外的列膨胀数据帧。然而，并不是所有情况下都天生不鼓励删除列；例如，通常在联接之后删除列是合适的，因为联接通常会引入冗余列。</p><p> Finally, instead of adding new columns via the select statement, using  .withColumn() is recommended instead for single columns. When adding or manipulating tens or hundreds of columns, use a single  .select() for performance reasons.</p><p>最后，不建议通过SELECT语句添加新列，而是建议对单个列使用.with Column()。在添加或操作数十或数百列时，出于性能原因，请使用单个.select()。</p><p>  If you need to add an empty column to satisfy a schema, always use  F.lit(None) for populating that column. Never use an empty string or some other string signalling an empty value (such as  NA).</p><p>如果需要添加空列以满足模式，请始终使用F.litt(无)填充该列。切勿使用空字符串或表示空值的其他字符串(如NA)。</p><p> Beyond being semantically correct, one practical reason for using  F.lit(None) is preserving the ability to use utilities like  isNull, instead of having to verify empty strings, nulls, and  &#39;NA&#39;, etc.</p><p>除了语义上正确之外，使用F.litt(None)的一个实际原因是保留使用isNull等实用工具的能力，而不必验证空字符串、NULL和&#39；na&#39；na等。</p><p>   While comments can provide useful insight into code, it is often more valuable to refactor the code to improve its readability. The code should be readable by itself. If you are using comments to explain the logic step by step, you should refactor it.</p><p>虽然注释可以提供对代码的有用见解，但重构代码以提高其可读性通常更有价值。代码本身应该是可读的。如果您使用注释一步一步地解释逻辑，则应该对其进行重构。</p><p> # bad # Cast the timestamp columns cols  = [ &#39;start_date&#39;,  &#39;delivery_date&#39;] for  c  in  cols:  df  =  df. withColumn( c,  F. from_unixtime( F. col( c)  /  1000). cast( TimestampType()))</p><p>#BAD#为COLS：df=df中的c转换时间戳列COLS=[&#39；start_date&#39；，&#39；delivery_date&#39；]。With Column(c，f.from_unixtime(f.ol(C)/1000))。CAST(TimestampType()。</p><p> In the example above, we can see that those columns are getting cast to Timestamp. The comment doesn&#39;t add much value. Moreover, a more verbose comment might still be unhelpful if it onlyprovides information that already exists in the code. For example:</p><p>在上面的示例中，我们可以看到这些列被强制转换为时间戳。这条评论并没有增加多少价值。此外，如果更冗长的注释只提供代码中已经存在的信息，那么它可能仍然没有帮助。例如：</p><p> # bad # Go through each column, divide by 1000 because millis and cast to timestamp cols  = [ &#39;start_date&#39;,  &#39;delivery_date&#39;] for  c  in  cols:  df  =  df. withColumn( c,  F. from_unixtime( F. col( c)  /  1000). cast( TimestampType()))</p><p>#BAD#遍历每一列，除以1000，因为MILIS并转换为COLS中的时间戳COLS=[&#39；START_DATE&#39；，&#39；Delivery_Date&#39；]：df=df。With Column(c，f.from_unixtime(f.ol(C)/1000))。CAST(TimestampType()。</p><p> Instead of leaving comments that only describe the logic you wrote, aim to leave comments that give context, that explain the &#34; why&#34; of decisions you made when writing the code. This is particularly important for PySpark, since the reader can understand your code, but often doesn&#39;t have context on the data that feeds into your PySpark transform. Small pieces of logic might have involved hours of digging through data to understand the correct behavior, in which case comments explaining the rationale are especially valuable.</p><p>与其留下只描述您编写的逻辑的注释，不如留下给出上下文的注释，解释您在编写代码时做出的决定的原因。这对于PySpark来说尤其重要，因为读者可以理解您的代码，但是通常没有关于馈送到PySpark转换的数据的上下文。小的逻辑片段可能需要数小时的数据挖掘才能理解正确的行为，在这种情况下，解释其基本原理的注释尤其有价值。</p><p> # good # The consumer of this dataset expects a timestamp instead of a date, and we need # to adjust the time by 1000 because the original datasource is storing these as millis # even though the documentation says it&#39;s actually a date. cols  = [ &#39;start_date&#39;,  &#39;delivery_date&#39;] for  c  in  cols:  df  =  df. withColumn( c,  F. from_unixtime( F. col( c)  /  1000). cast( TimestampType()))</p><p>#Good#此数据集的使用者需要的是时间戳而不是日期，我们需要#将时间调整1000，因为原始数据源将这些数据存储为Millis#，尽管文档中说它实际上是一个日期。COLS=[&#39；START_DATE&#39；，&#39；Delivery_Date&#39；]COLS中的c：df=df。With Column(c，f.from_unixtime(f.ol(C)/1000))。CAST(TimestampType()。</p><p>  It is highly recommended to avoid UDFs in all situations, as they are dramatically less performant than native PySpark. In most situations, logic that seems to necessitate a UDF can be refactored to use only native PySpark functions.</p><p>强烈建议在所有情况下避免使用UDF，因为它们的性能明显不如原生PySpark。在大多数情况下，似乎需要UDF的逻辑可以重构为只使用本机PySpark函数。</p><p>  Be careful with joins! If you perform a left join, and the right side has multiple matches for a key, that row will be duplicated as many times as there are matches. This is called a &#34;join explosion&#34; and can dramatically bloat the output of your transforms job. Always double check your assumptions to see that the key you are joining on is unique, unless you are expecting the multiplication.</p><p>使用连接时要小心！如果您执行左联接，并且右侧对一个键有多个匹配项，则该行将与匹配项一样多次重复。这被称为加入爆炸，可以极大地增加变换作业的输出。一定要仔细检查您的假设，以确保您要连接的密钥是唯一的，除非您希望进行乘法运算。</p><p> Bad joins are the source of many tricky-to-debug issues. There are some things that help like specifying the  how explicitly, even if you are using the default value  (inner):</p><p>错误的联接是许多难以调试的问题的根源。即使您使用的是默认值(内部)，也有一些内容可以帮助您明确指定方式：</p><p> # bad flights  =  flights. join( aircraft,  &#39;aircraft_id&#39;) # also bad flights  =  flights. join( aircraft,  &#39;aircraft_id&#39;,  &#39;inner&#39;) # good flights  =  flights. join( aircraft,  &#39;aircraft_id&#39;,  how = &#39;inner&#39;)</p><p>#不良航班=航班。加入(飞机，飞机)#也是不好的航班=航班。加入(飞机，#39；飞机_id&#39；，&#39；内部#)#好的飞行=飞行。加入(Aircraft，&#39；Aircraft_id&#39；，HOW=&#39；INTERNAL&#39；)。</p><p> Avoid  right joins. If you are about to use a  right join, switch the order of your dataframes and use a  left join instead. It is more intuitive since the dataframe you are doing the operation on is the one that you are centering your join around.</p><p>避免右连接。如果您要使用右联接，请切换数据帧的顺序，改为使用左联接。这更加直观，因为您正在对其执行操作的数据帧就是您以连接为中心的数据帧。</p><p>  Avoid renaming all columns to avoid collisions. Instead, give an alias to thewhole dataframe, and use that alias to select which columns you want in the end.</p><p>避免重命名所有列以避免冲突。取而代之的是，为整个数据帧指定一个别名，并使用该别名来选择在末尾需要哪些列。</p><p> # bad columns  = [ &#39;start_time&#39;,  &#39;end_time&#39;,  &#39;idle_time&#39;,  &#39;total_time&#39;] for  col  in  columns:  flights  =  flights. withColumnRenamed( col,  &#39;flights_&#39;  +  col)  parking  =  parking. withColumnRenamed( col,  &#39;parking_&#39;  +  col) flights  =  flights. join( parking,  on = &#39;flight_code&#39;,  how = &#39;left&#39;) flights  =  flights. select(  F. col( &#39;flights_start_time&#39;). alias( &#39;flight_start_time&#39;),  F. col( &#39;flights_end_time&#39;). alias( &#39;flight_end_time&#39;),  F. col( &#39;parking_total_time&#39;). alias( &#39;client_parking_total_time&#39;)) # good flights  =  flights. alias( &#39;flights&#39;) parking  =  parking. alias( &#39;parking&#39;) flights  =  flights. join( parking,  on = &#39;flight_code&#39;,  how = &#39;left&#39;) flights  =  flights. select(  F. col( &#39;flights.start_time&#39;). alias( &#39;flight_start_time&#39;),  F. col( &#39;flights.end_time&#39;). alias( &#39;flight_end_time&#39;),  F. col( &#39;parking.total_time&#39;). alias( &#39;client_parking_total_time&#39;))</p><p>列：航班=航班的#BAD列=[&#39；START_TIME&#39；，&#39；END_TIME&#39；，&#39；IDLE_TIME&#39；，&#39；TOTAL_TIME&#39；]。With ColumnRename(COL，&#39；FISTING_+COL)停车=停车。WITH COLUN RENAMED(COL，&#39；PARKING_##+COL)航班=航班。加入(Parking，on=&#39；Flight_code&#39；，How=##39；Left&39；)航班=航班。选择(F.ol(&#39；FLASS_START_TIME&#39；)。别名(&#39；FIRST_START_TIME&#39；)，F.COL(&#39；FIRTS_END_TIME&#39；)。别名(&#39；FIRST_END_TIME&#39；)，F.COL(&#39；PARKING_TOTAL_TIME&#39；)。别名(&#39；CLIENT_PARKING_TOTAL_TIME&#39；))#好的航班=航班。别名(航班)停车=停车。别名(停车)航班=航班。加入(Parking，on=&#39；Flight_code&#39；，How=##39；Left&39；)航班=航班。选择(F.ol(&#39；flights.start_time&#39；))。别名(&#39；FIRST_START_TIME&#39；)，F.COL(&#39；flights.end_Time&#39；)。别名(&#39；FIRST_END_TIME&#39；)，F.COL(&#39；PARKIN.TOTAL_TIME&#39；)。别名(&#39；CLIENT_PARKING_TOTAL_TIME&#39；)</p><p>  It&#39;s probably best to drop overlapping columns  prior to joining if you don&#39;t need both;</p><p>如果两个列都不需要，最好在连接之前删除重叠的列；</p><p> In case you do need both, it might be best to rename one of them prior to joining;</p><p>如果您确实需要这两个名称，最好在加入之前将它们中的一个重命名；</p><p> You should always resolve ambiguous columns before outputting a dataset. After the transform is finished running you can no longer distinguish them.</p><p>在输出数据集之前，应始终解析不明确的列。运行完转换后，您将无法再区分它们。</p><p> As a last word about joins, don&#39;t use  .dropDuplicates() or  .distinct() as a crutch. If unexpected duplicate rows are observed, there&#39;s almost always an underlying reason for why those duplicate rows appear. Adding  .dropDuplicates() only masks this problem and adds overhead to the runtime.</p><p>关于联接的最后一句话是，不要使用.dropDuplates()或.Distant()作为拐杖。如果观察到意外的重复行，那么出现这些重复行几乎总是有一个根本原因。添加.dropDuplates()只会屏蔽此问题，并增加运行时的开销。</p><p>  Chaining expressions is a contentious topic, however, since this is an opinionated guide, we are opting to recommend some limits on the usage of chaining. See the conclusion of this section for a discussion of the rationale behind this recommendation.</p><p>链接表达式是一个有争议的话题，但是，由于这是一个固执己见的指南，我们选择建议对链接的使用进行一些限制。有关此建议背后的基本原理的讨论，请参阅本节的结论。</p><p> Avoid chaining of expressions into multi-line expressions with different types, particularly if they have different behaviours or contexts. For example- mixing column creation or joining with selecting and filtering.</p><p>避免将表达式链接到不同类型的多行表达式中，特别是当它们具有不同的行为或上下文时。例如，混合柱创建或加入选择和过滤。</p><p> # bad df  = (  df . select( &#39;a&#39;,  &#39;b&#39;,  &#39;c&#39;,  &#39;key&#39;) . filter( F. col( &#39;a&#39;)  ==  &#39;truthiness&#39;) . withColumn( &#39;boverc&#39;,  F. col( &#39;b&#39;)  /  F. col( &#39;c&#39;)) . join( df2,  &#39;key&#39;,  how = &#39;inner&#39;) . join( df3,  &#39;key&#39;,  how = &#39;left&#39;) . drop( &#39;c&#39;)) # better (seperating into steps) # first: we select and trim down the data that we need # second: we create the columns that we need to have # third: joining with other dataframes df  = (  df . select( &#39;a&#39;,  &#39;b&#39;,  &#39;c&#39;,  &#39;key&#39;) . filter( F. col( &#39;a&#39;)  ==  &#39;truthiness&#39;)) df  =  df. withColumn( &#39;boverc&#39;,  F. col( &#39;b&#39;)  /  F. col( &#39;c&#39;)) df  = (  df . join( df2,  &#39;key&#39;,  how = &#39;inner&#39;) . join( df3,  &#39;key&#39;,  how = &#39;left&#39;) . drop( &#39;c&#39;))</p><p>#BAD df=(df.。选择(&#39；a&#39；，&#39；b&#39；，&#39；c&#39；，&#39；键&#39；)。过滤器(F.ol(&#39；a&#39；)==&#39；真实性&#39；)。With Column(&#39；boverc&39；，F.ol(&#39；b&39；)/F.coln(&#39；c&#39；))。加入(df2，&#39；键&#39；，HOW=#39；内部&#39；)。加入(DF3，&#39；KEY&#39；，HOW=&#39；LEFT&#39；)。DROP(&#39；c&#39；)#Better(分步骤)#第一：我们选择并裁剪我们需要的数据#第二：我们创建需要的列#第三：连接其他数据帧df=(Df)。选择(&#39；a&#39；，&#39；b&#39；，&#39；c&#39；，&#39；键&#39；)。过滤器(F.ol(&#39；a&#39；)==&#39；真实性&#39；)df=df。使用Column(&#39；boverc&39；，F.ol(&#39；b&39；)/F.coln(&#39；c&#39；))df=(df。加入(df2，&#39；键&#39；，HOW=#39；内部&#39；)。加入(DF3，&#39；KEY&#39；，HOW=&#39；LEFT&#39；)。丢弃(&#39；c&#39；)。</p><p> Having each group of expressions isolated into its own logical code block improves legibility and makes it easier to find relevant logic.For example, a reader of the code below will probably jump to where they see dataframes being assigned  df = df....</p><p>将每组表达式隔离到其自己的逻辑代码块中提高了易读性，并且更容易找到相关逻辑。例如，下面代码的读者可能会跳到他们看到数据帧被分配df=df的位置。</p><p> # bad df  = (  df . select( &#39;foo&#39;,  &#39;bar&#39;,  &#39;foobar&#39;,  &#39;abc&#39;) . filter( F. col( &#39;abc&#39;)  ==  123) . join( another_table,  &#39;some_field&#39;)) # better df  = (  df . select( &#39;foo&#39;,  &#39;bar&#39;,  &#39;foobar&#39;,  &#39;abc&#39;) . filter( F. col( &#39;abc&#39;)  ==  123)) df  =  df. join( another_table,  &#39;some_field&#39;,  how = &#39;inner&#39;)</p><p>#BAD df=(df.。选择(&#39；foo&#39；，&#39；bar&#39；，&#39；foobar&#39；，&#39；abc&#39；)。过滤器(F.ol(&#39；abc&#39；)==123)。JOIN(ANTHER_TABLE，&#39；SOME_FIELD&#39；)#Better df=(df.。选择(&#39；foo&#39；，&#39；bar&#39；，&#39；foobar&#39；，&#39；abc&#39；)。过滤器(F.ol(&#39；ABC&#39；)==123)df=df。JOIN(ANTHER_TABLE，&#39；SOME_FIELD&#39；，HOW=&#39；INTERNAL&#39；)。</p><p> There are legitimate reasons to chain expressions together. These commonly represent atomic logic steps, and are acceptable. Apply a rule with a maximum of number chained expressions in the same block to keep the code readable.We recommend chains of no longer than 5 statements.</p><p>将表达式链接在一起是有正当理由的。这些通常表示原子逻辑步骤，并且是可以接受的。在同一块中应用最多包含数字链接表达式的规则，以保持代码的可读性。我们建议使用不超过5条语句链。</p><p> If you find you are making longer chains, or having trouble because of the size of your variables, consider extracting the logic into a separate function:</p><p>如果您发现您正在制造更长的链条，或者因为变量的大小而遇到麻烦，请考虑将逻辑提取到一个单独的函数中：</p><p> # bad customers_with_shipping_address  = (  customers_with_shipping_address . select( &#39;a&#39;,  &#39;b&#39;,  &#39;c&#39;,  &#39;key&#39;) . filter( F. col( &#39;a&#39;)  ==  &#39;truthiness&#39;) . withColumn( &#39;boverc&#39;,  F. col( &#39;b&#39;)  /  F. col( &#39;c&#39;)) . join( df2,  &#39;key&#39;,  how = &#39;inner&#39;)) # also bad customers_with_shipping_address  =  customers_with_shipping_address. select( &#39;a&#39;,  &#39;b&#39;,  &#39;c&#39;,  &#39;key&#39;) customers_with_shipping_address  =  customers_with_shipping_address. filter( F. col( &#39;a&#39;)  ==  &#39;truthiness&#39;) customers_with_shipping_address  =  customers_with_shipping_address. withColumn( &#39;boverc&#39;,  F. col( &#39;b&#39;)  /  F. col( &#39;c&#39;)) customers_with_shipping_address  =  customers_with_shipping_address. join( df2,  &#39;key&#39;,  how = &#39;inner&#39;) # better def  join_customers_with_shipping_address( customers,  df_to_join):  customers  = (  customers . select( &#39;a&#39;,  &#39;b&#39;,  &#39;c&#39;,  &#39;key&#39;) . filter( F. col( &#39;a&#39;)  ==  &#39;truthiness&#39;) )  customers  =  customers. withColumn( &#39;boverc&#39;,  F. col( &#39;b&#39;)  /  F. col( &#39;c&#39;))  customers  =  customers. join( df_to_join,  &#39;key&#39;,  how = &#39;inner&#39;)  return  customers</p><p>#Bad Customers_with_Shipping_Address=(Customers_with_Shipping_Address。选择(&#39；a&#39；，&#39；b&#39；，&#39；c&#39；，&#39；键&#39；)。过滤器(F.ol(&#39；a&#39；)==&#39；真实性&#39；)。With Column(&#39；boverc&39；，F.ol(&#39；b&39；)/F.coln(&#39；c&#39；))。Join(df2，&#39；key&#39；，How=&#39；Internal&#39；))#Also Bad Customers_with_Shipping_Address=Customers_with_Shipping_Address。选择(&#39；a&#39；，&#39；b&#39；，&#39；c&#39；，&#39；key&#39；)Customers_With_Shipping_Address=Customers_With_Shipping_Address。筛选器(F.ol(&#39；a&#39；)==&#39；true&39；)Customers_With_Shipping_Address=Customers_With_Shipping_Address。With Column(&#39；boverc&39；，F.ol(&#39；b&#39；)/F.ol(&#39；c&#39；))Customers_With_Shipping_Address=Customers_With_Shipping_Address。Join(df2，&#39；key&#39；，HOW=&#39；Internal&#39；)#Better def Join_Customers_with_Shipping_Address(Customers，df_to_Join)：Customers=(Customers.。选择(&#39；a&#39；，&#39；b&#39；，&#39；c&#39；，&#39；键&#39；)。筛选器(F.ol(&#39；a&#39；)==&#39；真实性&#39；))Customers=Customers。With Column(&#39；boverc&39；，F.ol(&#39；b&39；)/F.ol(&#39；c&39；))Customers=Customers。JOIN(DF_TO_JOIN，&#39；KEY&#39；，HOW=&#39；INTERNAL&39；)退货客户。</p><p> Chains of more than 3 statement are prime candidates to factor into separate, well-named functions since they are already encapsulated, isolated blocks of logic.</p><p>3条以上语句链是分解成单独的、命名良好的函数的主要候选，因为它们已经封装了隔离的逻辑块。</p><p>  Differentiation between PySpark code and SQL code. Chaining is something that goes against most, if not all, other Python styling. You don’t chain in Python, you assign.</p><p>PySpark代码和SQL代码的区别。链接是与大多数(如果不是全部)其他Python样式背道而驰的。您不是在Python中链接，而是赋值。</p><p> Discourage the creation of large single code blocks. These would often make more sense extracted as a named function.</p><p>不鼓励创建大型单代码块。这些通常作为命名函数提取会更有意义。</p><p> It doesn’t need to be all or nothing, but a maximum of five lines of chaining balances practicality with legibility.</p><p>它不需要全部或不需要，但最多五行链接就可以在实用性和易读性之间取得平衡。</p><p> If you are using an IDE, it makes it easier to use automatic extractions or do code movements (i.e:  cmd + shift + up in pycharm)</p><p>如果您使用的是IDE，则可以更轻松地使用自动提取或执行代码移动(即：pycharm中的cmd+Shift+Up)。</p><p>  The reason you can chain expressions is because PySpark was developed from Spark, which comes from JVM languages. This meant some design patterns were transported, specifically chainability. However, Python doesn&#39;t support multiline expressions gracefully and the only alternatives are to either provide explicit line breaks, or wrap the expression in parentheses. You only need to provide explicit line breaks if the chain happens at the root node. For example:</p><p>可以链接表达式的原因是因为PySpark是从Spark开发的，Spark来自JVM语言。这意味着传输了一些设计模式，特别是链式模式。但是，Python不能优雅地支持多行表达式，唯一的选择是提供显式换行符，或者将表达式括在圆括号中。如果链发生在根节点，则只需要提供显式换行符。例如：</p><p> # needs `\` df  =  df. filter( F. col( &#39;event&#39;)  ==  &#39;executing&#39;)\ . filter( F. col( &#39;has_tests&#39;)  ==  True)\ . drop( &#39;has_tests&#39;) # chain not in root node so it doesn&#39;t need the `\` df  =  df. withColumn( &#39;safety&#39;,  F. when( F. col( &#39;has_tests&#39;)  ==  True,  &#39;is safe&#39;) . when( F. col( &#39;has_executed&#39;)  ==  True,  &#39;no tests but runs&#39;) . otherwise( &#39;not safe&#39;))</p><p>#需要`\`df=df。筛选器(F.ol(&#39；event&#39；)==&#39；正在执行&#39；)\。过滤器(F.ol(&#39；HAS_TESTS&#39；)==True)\。删除(&#39；has_tests&#39；)#链不在根节点中，因此它不需要`\`df=df。With Column(&#39；安全&#39；，F.When(F.ol(&#39；has_test&39；)==True，&#39；is safe&#39；)。当(F.ol(&#39；Has_Executed&#39；)==True，&#39；没有测试但运行&#39；)。否则(&#39；不安全&#39；)。</p><p> To keep things consistent, please wrap the entire expression into a single parenthesis block, and avoid using  \:</p><p>为保持一致，请将整个表达式放入单个括号块中，并避免使用\：</p><p> # bad df  =  df. filter( F. col( &#39;event&#39;)  ==  &#39;executing&#39;)\ . filter( F. col( &#39;has_tests&#39;)  ==  True)\ . drop( &#39;has_tests&#39;) # good df  = (  df . filter( F. col( &#39;event&#39;)  ==  &#39;executing&#39;) . filter( F. col( &#39;has_tests&#39;)  ==  True) . drop( &#39;has_tests&#39;))</p><p>#BAD DF=DF。筛选器(F.ol(&#39；event&#39；)==&#39；正在执行&#39；)\。过滤器(F.ol(&#39；HAS_TESTS&#39；)==True)\。Drop(&#39；has_tests&#39；)#Good df=(df.。筛选器(F.ol(&#39；event&#39；)==&#39；正在执行&#39；)。筛选器(F.ol(&#39；HAS_TESTS&#39；)==True)。丢弃(&#39；HAS_TESTS&#39；)。</p><p>  Be wary of functions that grow too large. As a general rule, a fileshould not be over 250 lines, and a function should not be over 70 lines.</p><p>要小心函数变得太大。一般来说，一个文件不应该超过250行，一个函数不应该超过70行。</p><p> Try to keep your code in logical blocks. For example, if you havemultiple lines referencing the same things, try to keep themtogether. Separating them reduces context and readability.</p><p>尽量将您的代码保存在逻辑块中。例如，如果你有多行引用相同的东西，试着把它们放在一起。将它们分开会降低上下文和可读性。</p><p> Test your code! If you  can run the local tests, do so and makesure that your new code is c</p><p>测试您的代码！如果可以运行本地测试，请这样做，并确保您的新代码是c++。</p><p>......</p><p>.</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/palantir/pyspark-style-guide">https://github.com/palantir/pyspark-style-guide</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/样式/">#样式</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/style/">#style</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/代码/">#代码</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>