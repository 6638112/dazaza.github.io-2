<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>(浅？)。强化学习</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">(浅？)。强化学习</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-18 07:11:18</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/8901794480b4f9b070c96b46b7180981.jpeg"><img src="http://img2.diglog.com/img/2020/10/8901794480b4f9b070c96b46b7180981.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Recent feats such as AlphaGo’s victory over the world’s best Go player have brought reinforcement learning (RL) to the spotlight. However, what is RL and how does it achieve such remarkable results?</p><p>最近的壮举，比如AlphaGo战胜了世界上最好的围棋选手，把强化学习(RL)带到了聚光灯下。然而，什么是RL，它是如何取得如此显著的效果的呢？</p><p> In this first article, we will explore the Monte Carlo Control Method (not the deep kind) which, despite being elegantly simple, is the basis upon which some of the most advanced RL is built.</p><p>在第一篇文章中，我们将探讨蒙特卡罗控制方法(不是深度控制方法)，尽管该方法非常简单，但它是构建一些最先进的RL的基础。</p><p>  RL problems consist of (at least) 2 entities: The  agent and the  environment, as shown in the figure below. The environment gives the agent a  state (also called an  observation). The agent then chooses an  action based on the provided state and applies it to the environment. The environment then replies to the action by giving the agent a  reward (a score for the action).</p><p>RL问题包括(至少)两个实体：代理和环境，如下图所示。环境给予代理一种状态(也称为观察)。然后，代理根据提供的状态选择操作，并将其应用于环境。然后，环境通过给代理奖励(该操作的分数)来回应该操作。</p><p>  For example, consider a kid (the agent) playing a game (the environment) for the first time. The kid starts by seeing the game screen containing all its elements (the state) and decides on an action to take. To which the game scores him (the reward) and the process reprises until the game ends (we consider environments with a clear termination  episodic). After enough repetitions, the kid will start to understand how his actions influence the environment and (assuming he is a competitive child) choose the actions that maximizes his score.</p><p>例如，考虑一个孩子(代理)第一次玩游戏(环境)。孩子首先看到包含所有元素(状态)的游戏屏幕，然后决定要采取的行动。游戏给他打分(奖励)，过程重复到游戏结束(我们认为环境有明确的终止插曲)。经过足够的重复之后，孩子将开始理解他的行为如何影响环境，并(假设他是一个好胜心强的孩子)选择能使他的分数最大化的行为。</p><p> An RL agent, attempts to do exactly the same. However, unlike a human child, computers don’t yet possess innate intelligence. So how does the computer learn what the best actions should be? For the remainder of the text we will distill this problem and re-derive an algorithm that provides a solution.</p><p>RL代理试图执行完全相同的操作。然而，与人类儿童不同的是，计算机还不具备天生的智能。那么，计算机如何学习最好的动作应该是什么呢？对于文本的其余部分，我们将提取这个问题，并重新推导出提供解决方案的算法。</p><p>  Monte Carlo Learning is a subfield of RL that is well suited for solving finite (limited number of states) Markov Decision Processes (MDPs). An MDP is just a class of problems for which knowledge of the current state provides sufficient information to decide on an optimal action to take. Chess is an example of a problem that can be framed as an MDP. The board state at any point in time contains all the information required to determine the next action.</p><p>蒙特卡罗学习是RL的一个子域，非常适合于求解有限(有限状态数)的马尔可夫决策过程(MDP)。MDP只是一类问题，对于这些问题，当前状态的知识提供了足够的信息来决定要采取的最佳行动。国际象棋就是一个可以被框定为MDP的问题的例子。任何时间点的板状态都包含确定下一步操作所需的所有信息。</p><p> Let’s consider the problem depicted below. We have a household floor cleaning robot (the agent) that has a battery that can be either LOW or HIGH in charge. The robot can be either CLEANING or DOCKED. Thus, the robot’s environment consists of 4 possible states: {CLEANING, HIGH}, {CLEANING, LOW}, {DOCKED, HIGH}, {DOCKED, LOW}. The robot controller always chooses between 3 actions: CLEAN, DOCK or WAIT, and the environment provides the agent with a reward for its action. However, since our battery sensor is not precise and our robot may get lost, our transitions are stochastic (probabilistic in nature).</p><p>让我们考虑一下下面描述的问题。我们有一个家用地板清洁机器人(清洁剂)，它的电池可以是低电量的，也可以是高电量的。机器人可以是清洁的，也可以是对接的。因此，机器人的环境由4种可能的状态组成：{清洁，高}，{清洁，低}，{停靠，高}，{停靠，低}。机器人控制器总是在3个动作中进行选择：清洁、停靠或等待，环境会为代理的动作提供奖励。然而，由于我们的电池传感器不精确，我们的机器人可能会迷路，所以我们的转换是随机的(本质上是概率的)。</p><p>  For instance, if the robot is currently in the state of CLEANING with HIGH battery and chooses to continue cleaning it has an 80% chance of receiving a reward of 1 and continuing on the same state. At this point, it becomes important to introduce some notation. We express a probability (from 0 to 1) of a new state  s′ and reward  r given a previous state  s and action  a as:  p(s′,r | s,a). Therefore, we could write our opening sentence as:  p({cleaning,high},1 | {cleaning,high},clean)=0.8.</p><p>例如，如果机器人当前处于电池电量较高的清洁状态，并选择继续清洁，则有80%的机会获得1的奖励并在相同状态下继续清洁。在这一点上，引入一些符号变得很重要。我们将给定先前状态s和动作a的新状态s‘的概率(从0到1)表示为：P(s’，r|s，a)。因此，我们可以将开场白写成：P({CLEVISH，HIGH}，1|{CLEVISH，HIGH}，CLEAN)=0.8。</p><p> By inspection of the expected rewards from each path, it is possible to guess that an optimal solution to this environment is likely to be in the form of the blue paths shown below.</p><p>通过检查来自每条路径的预期回报，可以猜测该环境的最佳解决方案可能是下面所示的蓝色路径的形式。</p><p>  However, the agent has no information about the environment’s probabilities prior to interacting with the environment. So how can we train an agent to operate on this environment? In other words: How do we find a  policy (represented as  π) that the agent can follow (that maps a state to an action) such that it maximizes total reward?</p><p>但是，在与环境交互之前，代理没有关于环境概率的信息。那么，我们如何才能培训一名座席在这种环境下操作呢？换句话说：我们如何找到代理可以遵循的策略(表示为π)(将状态映射到操作)，从而最大化总回报？</p><p>  What if we start with an agent that simply chooses actions at random? Below is an example of the results from such an agent running over an episode consisting of 50 steps (the first 10 of which are shown below):</p><p>如果我们从一个简单地随机选择操作的代理开始会怎么样？下面是这样一个代理在包含50个步骤(其中前10个步骤如下所示)的插曲中运行的结果示例：</p><p> S0: Docked, High A0: Wait R1: 0 S1: Docked, High A1: Wait R2: 0 S2: Docked, High A2: Wait R3: 0 S4: Docked, High A4: Wait R5: 0 S5: Docked, High A5: Clean R6: 1 S6: Cleaning, Low A6: Clean R7: -3 S7: Docked, Low A7: Clean R8: -3 S8: Docked, Low A8: Clean R9: -3 S9: Docked, Low ...</p><p>S0：停靠，高A0：等待R1：0 S1：停靠，高A1：等待R2：0 S2：停靠，高A2：等待R3：0 S4：停靠，高A4：等待R5：0 S5：停靠，高A5：清洁R6：1 S6：清洁，低A6：清洁R7：-3 S7：停靠，低A7：清洁R8：-3 S8：停靠，低A8：清洁R9：-3 S9：停靠，低...。</p><p> How can we make use of the information we have just collected? To start, we can represent this data using a table. Where the rows will correspond to the states and the columns to actions. The entries would be the weighted-average expected return from taking the given action while in the given state under the current operating policy. With some notation:   Q(s,a​)← Q(s​,a​)+ α( G​− Q(s​,a​)). Where α is our weighting factor for the average and is often called the  learning rate since a large alpha means the network learns faster as recent observations have a greater impact. The table below is termed a  Q-table.</p><p>我们怎样才能利用我们刚刚收集到的信息呢？首先，我们可以使用表格表示此数据。其中行将对应于状态，列将对应于动作。这些条目将是在当前操作策略下处于给定状态时采取给定行动的加权平均预期回报。用某种符号表示：Q(s，a​)←Q(s​，a​)+α(G​−Q(s​，a​)。其中，α是我们对平均值的加权因子，通常被称为学习率，因为阿尔法越大意味着网络学习的速度越快，因为最近的观察结果有更大的影响。下面的表格称为Q表。</p><p>  If we repeat this process for enough episodes we would expect our table to tend towards the actual returns. However, for sufficiently large problems this random exploration can take too long to converge. What if we use the information already in the table to focus our exploration? Instead of always picking random actions we pick the actions that would maximize our return (termed  greedy actions) based on the current values in the table. Under this proposed method, when running episode 2 if the agent were in the cleaning-high state it would always opt to dock. We know from the suggested solution in the previous section that this is likely not an optimal solution. It suggests, that this new approach may be susceptible to local minimas. What if we start with random exploration but over time (as more data is collected and our Q-table tends towards better approximations of the expected returns) we become more greedy?</p><p>如果我们重复这个过程足够多的剧集，我们预计我们的表格会趋向于实际的回报。然而，对于足够大的问题，这种随机探索可能需要太长时间才能收敛。如果我们使用表中已有的信息来集中我们的探索，会怎么样？我们并不总是选择随机的操作，而是根据表中的当前值选择可以最大化回报的操作(称为贪婪操作)。在这种建议的方法下，当运行第2集时，如果代理处于清理高状态，它将始终选择停靠。我们从上一节建议的解决方案中了解到，这可能不是最佳解决方案。这表明，这种新的方法可能容易受到局部极小值的影响。如果我们从随机探索开始，但随着时间的推移(随着收集的数据越来越多，我们的Q表趋向于更好地逼近预期收益)，我们会变得更加贪婪吗？</p><p> This final idea is the concept behind the  𝜀-greedy algorithm where we pick the action in the table that maximizes our expected return with  1 — 𝜀 + 𝜀/n and the values that don’t maximize our return with  𝜀/n probability. We want to explore more in the beginning (when our Q-table is unlikely to be a good representation of the rewards) and narrow our exploration as our approximations improves with a larger number of episodes. A method to achieve this is to have an 𝜀 that decays (becomes smaller) with increasing number of episodes but is limited by some bound so that our algorithm is never fully greedy such as  𝜀 ← min(𝜀 * 𝜀_decay^num_episodes, 𝜀_min_bound). This notion is often referred to as  greedy in the limit with infinite exploration  (GLIE). Using this algorithm and running for 1000 episodes we end up with the Q table below.</p><p>最后一个想法是𝜀-贪婪算法背后的概念，我们在表中选择以1-𝜀+𝜀/n最大化预期回报的操作，以及不以𝜀/n概率最大化回报的值。我们希望在开始时探索更多(当我们的Q表不太可能很好地表示回报时)，并随着我们的近似随着剧集数量的增加而改进，缩小我们的探索范围。实现这一点的一种方法是使𝜀随着剧集数量的增加而衰减(变得更小)，但受到某些界限的限制，以便我们的算法永远不会完全贪婪，例如𝜀←MIN(𝜀*𝜀_Decen^num_epsides，𝜀_MIN_Bound)。这个概念通常被称为无限探索的贪婪(Glie)。使用这个算法，运行1000集，我们最终得到了下面的Q表。</p><p>  Observe how choosing greedy actions based on this table is our optimal (blue) policy as shown earlier in the MDP figure.</p><p>观察基于此表选择贪婪操作如何成为我们的最佳(蓝色)策略，如前面的MDP图所示。</p><p>  We have derived a simple reinforcement learning algorithm based on Monte Carlo Control. However, what if the problem we are attempting to solve is not episodic but instead a continuing task (one that has no clear end)? In the next article we will explore Temporal-Difference (TD) Methods that are useful for this class of problems.</p><p>我们推导了一种简单的基于蒙特卡罗控制的强化学习算法。然而，如果我们试图解决的问题不是间歇性的，而是一个持续性的任务(一个没有明确结束的任务)，该怎么办？在下一篇文章中，我们将探索对这类问题有用的时差(TD)方法。</p><p>  Udacity’s course in deep reinforcement learning provides a solid introduction to reinforcement learning.</p><p>Udacity的深度强化学习课程为强化学习提供了坚实的介绍。</p><p> The scripts used to generate the Q table in the example can be found  here.</p><p>可以在这里找到示例中用于生成Q表的脚本。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/swlh/shallow-reinforcement-learning-3e8b59ff66c7">https://medium.com/swlh/shallow-reinforcement-learning-3e8b59ff66c7</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/强化/">#强化</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/状态/">#状态</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>