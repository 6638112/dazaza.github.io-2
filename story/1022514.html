<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>福楼拜与弗劳：法国伯特</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">福楼拜与弗劳：法国伯特</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-09-07 14:53:25</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/9/95b6834f1164521eace740a008bbcac8.jpeg"><img src="http://img2.diglog.com/img/2020/9/95b6834f1164521eace740a008bbcac8.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>福楼拜是一个法语伯特，在一个非常大的、不同种类的法语语料库上接受训练。不同大小的模型使用新的CNRS(法国国家科学研究中心)Jean Zay超级计算机进行训练。这个存储库共享一切：预先训练好的模型(基础和大型)、数据、使用模型的代码和训练它们的代码(如果需要)。</p><p>与福楼拜一起出现的还有FIFE：一种针对法国NLP系统的评估设置，类似于流行的胶水基准。目标是在未来实现进一步的可重复实验，并分享法语的模式和进展。</p><p>福楼拜是一个法语伯特，在一个非常大的、不同种类的法语语料库上接受训练。不同大小的模型使用新的CNRS(法国国家科学研究中心)Jean Zay超级计算机进行训练。我们已经发布了以下模型大小的预先训练的权重。</p><p>注意：Flaubert-Small-Case是经过部分训练的，因此不能保证性能。请考虑仅将其用于调试目的。</p><p>在本节中，我们描述了两种从预先训练的福楼拜模型中获得句子嵌入的方法：通过拥抱Face的Transformer库或通过Facebook的XLM库。在不久的将来，我们将把福楼拜整合到Facebook&Fairseq中。</p><p>从变压器导入焊炬导入FlaubertModel，FlaubertTokenizer#从[&#39；Flaubert/Flaubert_Small_Case&#39；，&#39；Flaubert/Flaubert_BASE_CASED&#39；，#&#39；Flaubert/Flaubert_BASE_CASED&#39；，&#39；]model name=&#39；Flaubert/Flaubert_Large_Case&#39；中选择。FROM_PRESTED(MODENAME，OUTPUT_LOADING_INFO=True)Flaubert_tokenizer=FlaubertTokenizer。From_preTraded(model name，do_lowercase=false)#do_lowercase=false，如果使用大小写模型，则为True，如果使用无大小写模型语句=&#34；Le chat mange une pomme.&#34；Token_IDS=Torch。张量([Flaubert_tokenizer.。Encode(句子)])last_layer=Flaubert(Token_IDs)[0]Print(last_layer.。Shape)#torch.Size([1，8,768])-&gt；(批次大小x令牌数x嵌入维度)#bert[cls]令牌对应最后一层的第一个隐藏状态cls_embedding=last_layer[：，0，：]。</p><p>可从此处下载经过预先培训的福楼拜模型。每个压缩文件夹包括3个文件：</p><p>注意：以下示例仅适用于本回购中提供的修改后的XLM，不适用于原始XLM。代码取自本教程。</p><p>Import sys import torch import fast BPE#将Flaubert root添加到系统路径(相应更改)Flaubert_root=&#39；/home/user/flaubert&#39；sys。小径。从XLM追加(FLOUBERT_ROOT)。模特。Embedder从XLM导入SentenceEmbedder。数据。字典导入PAD_WORD#模型文件的路径MODEL_PATH=&#39；/home/user/flaubert_base_cased/flaubert_base_cased_xlm.pth&#39；CODES_PATH=&#39；/home/user/flaubert_base_cased/codes&#39；VOVAB_PATH=&#39；/home/user/flaubert_base_cased/vocab&#39；do_lowercase=FALSE#如果您使用unase Flaubert BPE=fast BPE，则将其更改为True。快速BPE(CODES_PATH，VORAB_PATH)语句=&#34；勒聊天管理工具&#34；如果do_lowercase：语句=语句。LOWER()#应用BPE语句=BPE。应用([句子])句子=[(&#39；&lt；/s&gt；%s&lt；/s&gt；%发送。条带())。Split())For Sent in句子]Print(语句)#Create Batch bs=len(句子)slen=max([len(Sent)for Sent in句子])#重新加载预先训练好的模型嵌入器=SentenceEmbedder。重新加载(Model_Path)嵌入器。Eval()dico=嵌入器。Dico#准备输入以建模word_ids=torch。长张量(Slen，bs)。填充_(DICO。范围内i的索引(PAD_WORD))(len(句子))：SEND=TORCH。朗张量([DICO.。句子中w的索引(W)[i])word_ids[：LEN(SENT)，i]=SENT LENGTH=TORCH。东南张量([len(Sent)for Sent in Packages])#get语句嵌入(对应于BERT[CLS]内标识)cls_embedding=Embedder。Get_embedding(x=WORD_ID，LENGTH=LENGTHS)打印(cls_embedding。Size())#获取所有令牌的整个输出张量#请注意，cls_embedding=tensor[0]tensor=embedder。Get_embedding(x=word_id，length=length，all_tokens=True)print(张量。大小()。</p><p>您应该克隆此repo，然后将WikiExtractor、fast BPE和Mosse标记化器安装在Tools下：</p><p>Git clone https://github.com/getalp/Flaubert.git cd flaubert#安装工具包cd toolsgit克隆https://github.com/attardi/wikiextractor.gitgit克隆https://github.com/moses-smt/mosesdecoder.gitgit克隆https://github.com/glample/fastBPE.git cd fast BPEG++-std=c++11-pthread-O3 fast BPE/main.cc-ifast BPE-o fast。</p><p>在本节中，我们将介绍为训练福楼拜准备数据的管道。这是基于Facebook XLM的库。具体步骤如下：</p><p>在训练场上学习BPE。然后将学习的BPE代码应用于训练、验证和测试集。</p><p>在下面的代码中，将$DATA_DIR和$CORPUS_NAME分别替换为本地目录的路径以保存下载的数据，以及脚本中指定的选项中要下载的语料库的名称。</p><p>第一个命令将原始数据下载到$DATA_DIR/RAW/fr_Gutenberg，第二个命令对其进行处理并保存到$DATA_DIR/Proceded/fr_Gutenberg。</p><p>运行以下命令将清理后的语料库划分为训练集、验证集和测试集。您可以在脚本中修改训练/验证/测试比率。</p><p>输出文件为：fr.train、fr.valid、fr.test，它们保存在与原始文件相同的目录下。</p><p>运行以下命令以学习训练集上的BPE代码，并将BPE代码应用于训练、验证和测试集。然后将数据二值化，并为训练做好准备。</p><p>其中$DATA_DIR是保存上述3个文件fr.train、fr.valid、fr.test的目录的路径。$BPE_SIZE是BPE词汇表大小的个数，例如：30表示30k，50表示50k等，输出文件对应保存在$DATA_DIR/BPE/30k或$DATA_DIR/BPE/50k中。</p><p>我们用于预培训的代码库Flaubert在很大程度上是基于XLM repo，并进行了一些修改。你可以用他们的代码来训练福楼拜，它会工作得很好。</p><p>Python Train.py\--EXP_NAME Flaubert_BASE_CASED\--DUMP_PATH$DUMP_PATH\--DATA_PATH$DATA_PATH\--amp 1\-lgs&#39；fr&#39；\-clm_step&#39；&#39；\--mlm_step&#39；fr&#39；\--emb_dim768\--n_layer 12\--n_head 12\-Dropout0.1\-注意_Dropout0.1\--GLU_ACTIVATION TRUE\--BATCH_SIZE 16\--bptt512\--优化器#34；ADAM_INVERSE_SQRT，LR=0.0006，WORKUP_UPDATES=24000，Beta1=0.9，Beta2=0.98Wight_Decay=0.01Eps=0.000001&#34；\--EPOCH_SIZE 300000\--MAX_EPOCH 100000\--VALIDATION_METRICS_VALID_fr_mlm_ppl\--STOPING_Criteria_VALID_fr_mlm_ppl，20\--fp16 TRUE\--累加梯度16\--WORD_MASK_KEEP_RAND&#39；0.8，0.1，0.1&#39；\--WORD_PRED&#39；0.15&#39；</p><p>其中$DUMP_PATH是要保存预训练模型的路径，$DATA_PATH是二进制数据集的路径，例如$DATA_DIR/BPE/50k。</p><p>要在一台机器上的多个GPU上运行实验，可以使用以下命令(Train.py后面的参数与上面相同)。</p><p>Export NGPU=4 export CUDA_Visible_Devices=0，1，2，3，4#如果您只使用计算机中的一些GPU epython-m torch.Distributed.Launch--nproc_per_node=$NGPU train.py</p><p>要使用Slurm作为资源管理器在集群中的多个节点、多个GPU上运行实验，可以在使用#SBATCH请求资源后使用以下命令启动培训(Training.py后的参数与上面加--master_port参数相同)。</p><p>要将XLM预先训练的模型转换为拥抱面孔的变形金刚，您可以使用以下命令。</p><p>其中$inputdir是XLM预训练模型目录的路径，$outputdir是要保存拥抱脸的Transformer模型的输出目录的路径。</p><p>FASH(法语理解评估)是评估法语NLP系统的通用基准。请参考本页了解关于此基准测试的微调福楼拜的示例。</p><p>如果您在您的科学出版物中使用福楼拜或FASH基准，或者如果您觉得此库中的资源有用，请引用以下论文之一：</p><p>@InProceases{le2020flaubert，作者={Le，Hang and Vial，Lo\&#34；{i}c和Frej，Jibril和Segonne，Vincent和Coavoux，Maximin和Lecouteux，Benjamin和Allazen，Alexandre和Crabb\&#39；{e}，Beno{i}t和Besacier，Laurent and Schwab，Didier}，title={Flaubert：无人监督的法语语言模型预训}，Booktitle={第12届语言资源和评估会议论文集}，月={5}，年份={2 0 0}，地址={法国马赛}，出版商={欧洲语言资源协会}，页面={2 4 79-2 490}，URL={https://www.aclweb.org/anthology/2020.lrec-1.302}}。</p><p>@inProcese{le2020flaubert，title={flaubert：des mod{\`e}les de langue context tualis}s pr{e}s pr{e}-entra n n s Pour le Fran{\c{c}}ais}，作者={Le，Hang and Vial，Lo{\i；c；和Frej，Jibril和Segonne}}，作者={Le，Hang and Vial，Lo{\i；c和Frej，Jibril和Segonne}，作者={Le，Hang and Vial，Lo{\i；c；和Frej，Jibril和Segonne}s Pour le le Fran{\c}c。书名：{actes de la 6e conf{{}e；es d&#39；es d&#39；e；e；t}tods sur la la Parole(JEP，31e；31e；Schwab，Didier})，传播自然语言(TALN)的自动传播(TALN)的书名={actes de la 6e conf，didier}，书名={actes de la 6e conf{{#；e；e}endence conconcontete Journ=#39；es d&#39；e；引证，东经22E；条件)。第2卷：自然语言传播自动化}，页数={268--278}，年份={2020}，组织={阿塔拉}}</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://github.com/getalp/Flaubert">https://github.com/getalp/Flaubert</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/弗劳/">#弗劳</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/flue/">#flue</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/flaubert/">#flaubert</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>