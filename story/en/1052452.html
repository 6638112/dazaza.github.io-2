<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>性能比较：在Python，Go，C ++，C，AWK，Forth，Rust中计算单词 Performance comparison: counting words in Python, Go, C++, C, Awk, Forth, Rust</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Performance comparison: counting words in Python, Go, C++, C, Awk, Forth, Rust<br/>性能比较：在Python，Go，C ++，C，AWK，Forth，Rust中计算单词 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-15 19:27:19</div><div class="page_narrow text-break page_content"><p>Summary: I describe a simple interview problem (counting frequencies of unique words), solve it in various languages, and compare performance across them. For each language, I’ve included a simple, idiomatic solution as well as a more optimized approach via profiling.</p><p>摘要：我描述了一个简单的面试问题（计算唯一单词的频率），以各种语言解决它，并将性能进行比较。对于每种语言，我已经包括一个简单，惯用的解决方案以及通过分析更优化的方法。</p><p> I’ve conducted many coding interviews over the past few years, and one of the questions I like to ask is this:</p><p> 我在过去几年中进行了许多编码访谈，以及我想问的问题之一是：</p><p> Write a program to count the frequencies of unique words from standard input, then print them out with their frequencies, ordered most frequent first. For example, given this input:</p><p> 编写一个程序以计算标准输入的唯一单词的频率，然后用频率打印出来，首先订购最常见的。例如，给定此输入：</p><p> I think this is a good interview question because it’s somewhat harder to solve than  FizzBuzz, but it doesn’t suffer from the “invert a binary tree on this whiteboard” issue. It’s the kind of thing a programmer might have to write a script for in real life, and it shows whether they understand file I/O, hash tables (maps), and how to use their language’s sort function. There’s a little bit of trickiness in the sorting part, because most hash tables aren’t ordered, and if they are, it’s by key or insertion order and not by value.</p><p> 我认为这是一个良好的面试问题，因为它比FizzBu​​zz解决了，但它不会遭受“在这个白板上倒置二叉树”的问题。这是程序员在现实生活中编写脚本的那种东西，它显示了他们是否了解文件I / O，哈希表（地图）以及如何使用他们的语言的排序功能。排序部分中有一点棘手，因为大多数哈希表都没有订购，如果它们是，它是通过键或插入顺序而不是值。</p><p> After the candidate has a basic solution, you can push it in all sorts of different directions: what about capitalization? punctuation? how does it order two words with the same frequency? what’s the performance bottleneck likely to be? how does it fare in terms of big-O? what’s the memory usage? roughly how long would your program take to process a 1GB file? would your solution still work for 1TB? and so on. Or you can take it in a “software engineering” direction and talk about error handling, testability, turning it into a hardened command line utility, etc.</p><p> 候选人有一个基本解决方案后，您可以在各种不同方向上推动它：大写内容如何？标点？它是如何用相同频率的两个单词？什么是性能瓶颈可能是什么？在大o方面如何票价？内存用法是什么？大致您的程序需要多长时间来处理1GB文件？您的解决方案是否仍然适用于1TB？等等。或者您可以在“软件工程”方向上，讨论错误处理，可测试性，将其转换为硬化的命令行实用程序等。</p><p> A basic solution reads the file line-by-line, converts to lowercase, splits each line into words, and counts the frequencies in a hash table. When that’s done, it converts the hash table to a list of word-count pairs, sorts by count (largest first), and prints them out.</p><p> 一个基本解决方案读取文件行逐行，转换为小写，将每一行拆分为单词，并计算哈希表中的频率。完成后，它将哈希表转换为单词计数对列表，按计数排序（最大的第一个），并将它们打印出来。</p><p> In Python, one obvious solution using a plain  dict might look like this (imports elided):</p><p> 在Python中，使用普通地区的一个明显的解决方案可能看起来像这样（引入所示）： </p><p> counts  =  {} for  line  in  sys . stdin :  words  =  line . lower (). split ()  for  word  in  words :  counts [ word ]  =  counts . get ( word ,  0 )  +  1 pairs  =  sorted ( counts . items (),  key = lambda  kv :  kv [ 1 ],  reverse = True ) for  word ,  count  in  pairs :  print ( word ,  count )</p><p>counts = {}在sys中的行。 STDIN：单词=行。降低 （）。单词的拆分（）单词：counts [word] = counts。获取（Word，0）+ 1对=排序（计数。项目（），key = lambda kv：kv [1]，反向= true）对于word，成对计数：print（word，count）</p><p> If the candidate was a Pythonista, they might use   collections.defaultdict or even   collections.Counter – see below for code using the latter. In that case I’d ask them how it worked under the hood, or how they might solve it with a plain dictionary.</p><p> 如果候选人是Pythonista，它们可能会使用CollectionS.defaultdict ventics.counter.counter  - 请参阅下面的使用后者代码。在这种情况下，我会问他们如何在引擎盖下工作，或者它们如何用普通词典解决它。</p><p> Incidentally, this problem  set the scene for a wizard duel between two computer scientists several decades ago. In 1986, Jon Bentley asked Donald Knuth to show off “literate programming” with a solution to this problem, and he came up with an exquisite, ten-page Knuthian masterpiece. Then Doug McIlroy (the inventor of Unix pipelines) replied with a one-liner  Unix shell version using  tr,  sort, and  uniq.</p><p> 顺便提一下，这个问题将几十年前为两位计算机科学家之间的巫师决斗设定了场景。 1986年，Jon Bentley要求Donald Knuth炫耀“识字编程”并解决这个问题的解决方案，他提出了一个精致的十页的持剑杰作。然后Doug McIlroy（Unix管道的发明者）使用TR，SORT和UNIQ使用单行UNIX Shell版本回复。</p><p>   In any case, I’ve been playing with this problem for a while now, and I wanted to see what the program would look like in various languages, and how fast they would run, both with a simple idiomatic solution and with a more optimized version. I’m including large snippets of code in the article, but full source for each version is in my  benhoyt/countwords repository. Or you can cheat and jump straight to the  performance results.</p><p>   在任何情况下，我现在一直在玩这个问题，我想看看这个程序在各种语言中看起来像什么，以及他们将如何运行，两者都有一个简单的惯用解决方案，并且更优化版本。我在文章中包括大型代码片段，但每个版本的完整来源都在我的benhoyt / countwords存储库中。或者您可以欺骗并直接跳转到性能结果。</p><p>  Each program must read from standard input and print the frequencies of unique, space-separated words, in order from most frequent to least frequent. To keep our solutions simple and consistent, here are the (self-imposed) constraints I’m working against:</p><p>  每个程序必须从标准输入中读取并打印唯一的空间分开的单词的频率，从最常见的是最小频繁的顺序。为了使我们的解决方案简单且保持一致，这里是我努力的（自我强加）的限制：</p><p> Case: the program must normalize words to lowercase, so “The the THE” should appear as “the 3” in the output.</p><p> 案例：程序必须将单词正常化为小写，因此“”“该”应显示为输出中的“3”。</p><p>  Words: anything separated by whitespace – ignore punctuation. This does make the program less useful, but I don’t want this to become a tokenization battle.</p><p>  单词：用空格分隔的东西 - 忽略标点符号。这确实使程序不太有用，但我不希望这成为象征化战斗。 </p><p>  ASCII: it’s okay to only support ASCII for the whitespace handling and lowercase operation. Most of the optimized variants do this.</p><p>ASCII：只需支持空白处理和小写操作即可支持ASCII即可。大多数优化的变体都这样做。</p><p>  Ordering: if the frequency of two words is the same, their order in the output doesn’t matter. I use a  normalization script to ensure the output is correct.</p><p>  订购：如果两个单词的频率相同，则其在输出中的顺序无关紧要。我使用归一化脚本来确保输出正确。</p><p>  Threading: it should run in a single thread on a single machine (though I often discuss concurrency in my interviews).</p><p>  线程：它应该在单个机器上的单个线程中运行（尽管我经常在访谈中讨论并发）。</p><p>  Memory: don’t read whole file into memory. Buffering it line-by-line is okay, or in chunks with a maximum buffer size of 64KB. That said, it’s okay to keep the whole word-count map in memory (we’re assuming the input is text in a real language, not full of randomized unique words).</p><p>  内存：不要将整个文件读入内存。缓冲IT线路是可以的，或者用最大缓冲区大小为64KB的块。也就是说，可以在内存中保持整个单词映射地图（我们假设输入是实际语言的文本，而不是完整的随机唯一词）。</p><p>  Text: assume that the input file is text, with “reasonable” length lines shorter than the buffer size.</p><p>  文字：假设输入文件是文本，具有比缓冲区大小短的“合理”长度线。</p><p>  Safe: even for the optimized variants, try not to use unsafe language features, and don’t drop down to assembly.</p><p>  安全：即使对于优化的变体，也不要使用不安全的语言功能，并不会下拉到装配。</p><p>  Hashing: don’t roll our own hash table (with the exception of the optimized C version).</p><p>  哈希：不要滚动我们自己的哈希表（除了优化的C版本外）。 </p><p> Our  test input file will be the text of the King James Bible, concatenated ten times. I sourced this  from Gutenberg.org, replaced smart quotes with the ASCII quote character, and used  cat to multiply it by ten to get the 43MB reference input file.</p><p>我们的测试输入文件将是国王詹姆斯圣经的文本，连续十次连接。我从Gutenberg.org寻求这一点，用ASCII报价字符替换智能报价，并使用CAT将其乘以十个以获取43MB参考输入文件。</p><p> So let’s get coding! The solutions below are in the order I solved them.</p><p> 让我们来编码！下面的解决方案是我解决了它们的顺序。</p><p>  An idiomatic Python version would probably use  collections.Counter. Python’s  collections library is really nice – thanks Raymond Hettinger! It’s about as simple as you can get:</p><p>  惯用的python版本可能会使用collections.counter。 Python的集合图书馆真的很好 - 谢谢雷蒙德Hettinger！这与你可以得到的简单：</p><p>  counts  =  collections . Counter () for  line  in  sys . stdin :  words  =  line . lower (). split ()  counts . update ( words ) for  word ,  count  in  counts . most_common ():  print ( word ,  count )</p><p>  counts =集合。在sys中的柜台（）。 STDIN：单词=行。降低 （）。拆分（）计数。单词的更新（单词），计数计数。 most_common（）：打印（Word，Count）</p><p> This is Unicode-aware and is probably what I’d write in “real life”. It’s actually quite efficient, because all the low-level stuff is really done in C: reading the file, converting to lowercase and splitting on whitespace, updating the counter, and the sorting that   Counter.most_common does.</p><p> 这是Unicode感知，可能是我在“现实生活”中写的。它实际上非常有效，因为所有低级的东西都是在c：读取文件中的所有低级的东西，转换为小写和拆分在空格上，更新计数器，以及计数器的排序。</p><p> But let’s try to optimize! Python comes with a  profiling module called  cProfile. It’s easy to use – simply run your program using  python3 -m cProfile. I’ve commented out the final  print call to avoid the profiling output mixing with the program’s output – it’s fairly negligible anyway. Here’s the output ( -s tottime sorts by total time in each function):</p><p> 但让我们试着优化！ Python附带一个名为cprofile的分析模块。它很容易使用 - 只需使用Python3 -m cprofile运行您的程序。我评论了最终的打印呼吁，以避免与程序的产出进行分析输出混合 - 无论如何，它相当忽略不计。以下是输出（-s tottime在每个函数中的总时间排序）：</p><p> $ python3 -m cProfile -s tottime simple.py &lt;kjvbible_x10.txt 6997799 function calls (6997787 primitive calls) in 3.872 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 998170 1.361 0.000 1.361 0.000 {built-in method _collections._count_elements} 1 0.911 0.911 3.872 3.872 simple.py:1(&lt;module&gt;) 998170 0.415 0.000 0.415 0.000 {method &#39;split&#39; of &#39;str&#39; objects} 998171 0.405 0.000 2.388 0.000 __init__.py:608(update) 998170 0.270 0.000 0.622 0.000 {built-in method builtins.isinstance} 998170 0.182 0.000 0.351 0.000 abc.py:96(__instancecheck__) 998170 0.170 0.000 0.170 0.000 {built-in method _abc._abc_instancecheck} 998170 0.134 0.000 0.134 0.000 {method &#39;lower&#39; of &#39;str&#39; objects} 5290 0.009 0.000 0.018 0.000 codecs.py:319(decode) 5290 0.009 0.000 0.009 0.000 {built-in method _codecs.utf_8_decode} 1 0.007 0.007 0.007 0.007 {built-in method builtins.sorted} 7/1 0.000 0.000 0.000 0.000 {built-in method _abc._abc_subclasscheck} 1 0.000 0.000 0.007 0.007 __init__.py:559(most_common) 1 0.000 0.000 0.000 0.000 __init__.py:540(__init__) 1 0.000 0.000 3.872 3.872 {built-in method builtins.exec} 7/1 0.000 0.000 0.000 0.000 abc.py:100(__subclasscheck__) 7 0.000 0.000 0.000 0.000 _collections_abc.py:392(__subclasshook__) 1 0.000 0.000 0.000 0.000 {method &#39;disable&#39; of &#39;_lsprof.Profiler&#39; objects} 1 0.000 0.000 0.000 0.000 {method &#39;items&#39; of &#39;dict&#39; objects}</p><p> $ python3-m cprofile -s tottime simple.py＆lt; kjvbible_x10.txt 6997799函数调用（6997787原始呼叫）在3.872秒排序：内部时间ncalls tottime percall percall percall filename：lineno（功能）998170 1.361 0.000 1.361 0.000 {构建-in方法_collections._count_elements} 1 0.911 0.911 3.872 3.872 simple.py：1（&lt ;module& gt;）998170 0.415 0.415 0.415 0.415 0.000 {方法＆＃39;拆分＆＃39; ＆＃39; str＆＃39;对象} 998171 0.405 0.000 2.388 0.000 __Init__.py：608（update）998170 0.270 0.622 0.000 {内置方法构成。998170 0.182 0.30 0.10 abc.py：96 (_xinstancecheck__）998170 0.170 0.170 0.000 {内置在方法_abc._abc_instanceCheck} 998170 0.134 0.10134 0.134 0.000 {方法＆＃39;较低的＆＃39; ＆＃39; str＆＃39; Objects} 5290 0.009 0.0.018 0.018 0.018 0.010 0.000 0.009 0.009 0.009 0.000 {内置方法_codecs.utf_8_decode} 1 0.007 0.007 0.007 {内置方法构成。7/1 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 {内置方法_abc._abc_subclasscheck} 1 0.000 0.007 0.007 __init__.py：559（最多_common）1 0.000 0.000 0.000 0.000 __init__.py：540 (_mit__）1 0.000 0.000 3.872 3.872 {内置方法构建insize.exec} 7 / 1 0.000 0.000 0.000 0.000 abc.py：100 (_Subclasscheck__）7 0.000 0.000 0.000 _Collections_abc.py：392 (_Subclasshook__）1 0.000 0.000 0.000 {方法＆＃39;禁用＆＃39; ＆＃39; _lsprof.profiler＆＃39;对象} 1 0.000 0.000 0.000 0.000 {方法＆＃39;物品＆＃39; ＆＃39; dict＆＃39;对象} </p><p>  998,170 is the number of lines in the input, and because we’re reading line-by-line, we’re calling functions and executing the Python loop that many times.</p><p>998,170是输入中的行数，因为我们正在阅读逐行，我们正在调用多次的函数并执行Python循环。</p><p>  The large amount of time spent in  simple.py itself shows how (relatively) slow it is to execute Python bytecode – the main loop is pure Python, again executed 998,170 times.</p><p>  在Simple.py中花费的大量时间都显示了如何（相对）慢它是执行Python字节码的速度 - 主循环是纯Python，再次执行998,170次。</p><p>    Counter.update calls  isinstance, which adds up. I thought about calling the C function  _count_elements directly, but that’s an implementation detail and I decided it fell into the “unsafe” category.</p><p>    counter.update呼叫isinstance，它增加了。我想直接调用C函数_count_elements，但这是一个实现细节，我决定它落入“不安全”的类别。</p><p> The main thing we need to do is reduce the number of times around the main Python loop, and hence reduce the number of calls to all those functions. So let’s read it in 64KB chunks:</p><p> 我们需要做的主要内容是减少主Python循环周围的次数，因此减少对所有这些功能的调用次数。所以让我们在64kb块中阅读它：</p><p>  counts  =  collections . Counter () remaining  =  &#39;&#39; while  True :  chunk  =  sys . stdin . read ( 64 * 1024 )  if  not  chunk :  break  chunk  =  remaining  +  chunk  last_lf  =  chunk . rfind ( &#39; \n &#39; )  # process to last LF character  if  last_lf  ==  - 1 :  remaining  =  &#39;&#39;  else :  remaining  =  chunk [ last_lf + 1 :]  chunk  =  chunk [: last_lf ]  counts . update ( chunk . lower (). split ()) for  word ,  count  in  counts . most_common ():  print ( word ,  count )</p><p>  counts =集合。计数器（）剩余=＆＃39;＆＃39;虽然真实：块= sys。 stdin。读取（64 * 1024）如果没有块：打破chunk =剩余+ chunk last_lf = chunk。 rfind（＆＃39; \ n＆＃39;）＃进程到最后lf字符如果last_lf ==  -  1：剩余=＆＃39;＆＃39;否则：剩余=块[last_lf + 1：] chunk = chunk [：last_lf] counts。更新（块。leow（）。拆分（））用于单词，计数计数。 most_common（）：打印（Word，Count）</p><p> Instead of our main loop processing 42 characters at a time (the average line length), we’re processing 65,536 at a time (less the partial line at the end). We’re still reading and processing the same number of bytes, but we’re now doing most of it in C rather than in the Python loop. Many of the optimized solutions use this basic approach – process things in bigger chunks.</p><p> 而不是我们的主循环处理一次42个字符（平均线长度），我们一次处理65,536（最后的部分线）。我们还在读取和处理相同数量的字节，但我们现在在C中做大部分，而不是在Python循环中。许多优化的解决方案使用这种基本方法 - 在更大的块中处理事物。</p><p> The profiling output looks much better now. The  _count_elements and  str.split functions are still taking most of the time, but they’re only being called 662 times instead 998170 (on roughly 64KB at a time rather than 42 bytes):</p><p> 剖析输出现在看起来好多了。 _count_elements和str.split函数仍在大部分时间，但它们仅被调用662次而不是998170（在大约64kb的时间而不是42字节）： </p><p> $ python3 -m cProfile -s tottime optimized.py &lt;kjvbible_x10.txt 7980 function calls (7968 primitive calls) in 1.280 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 662 0.870 0.001 0.870 0.001 {built-in method _collections._count_elements} 662 0.278 0.000 0.278 0.000 {method &#39;split&#39; of &#39;str&#39; objects} 1 0.080 0.080 1.280 1.280 optimized.py:1(&lt;module&gt;) 662 0.028 0.000 0.028 0.000 {method &#39;lower&#39; of &#39;str&#39; objects} 663 0.010 0.000 0.016 0.000 {method &#39;read&#39; of &#39;_io.TextIOWrapper&#39; objects} 1 0.007 0.007 0.007 0.007 {built-in method builtins.sorted} 664 0.004 0.000 0.004 0.000 {built-in method _codecs.utf_8_decode} 663 0.001 0.000 0.872 0.001 __init__.py:608(update) 664 0.001 0.000 0.005 0.000 codecs.py:319(decode) 662 0.001 0.000 0.001 0.000 {built-in method builtins.isinstance} 662 0.000 0.000 0.001 0.000 {built-in method _abc._abc_instancecheck} 662 0.000 0.000 0.000 0.000 {method &#39;rfind&#39; of &#39;str&#39; objects} 664 0.000 0.000 0.000 0.000 codecs.py:331(getstate) 662 0.000 0.000 0.001 0.000 abc.py:96(__instancecheck__) 7/1 0.000 0.000 0.000 0.000 {built-in method _abc._abc_subclasscheck} 1 0.000 0.000 0.007 0.007 __init__.py:559(most_common) 1 0.000 0.000 0.000 0.000 __init__.py:540(__init__) 7/1 0.000 0.000 0.000 0.000 abc.py:100(__subclasscheck__) 1 0.000 0.000 1.280 1.280 {built-in method builtins.exec} 7 0.000 0.000 0.000 0.000 _collections_abc.py:392(__subclasshook__) 1 0.000 0.000 0.000 0.000 {method &#39;disable&#39; of &#39;_lsprof.Profiler&#39; objects} 1 0.000 0.000 0.000 0.000 {method &#39;items&#39; of &#39;dict&#39; objects}</p><p>$ python3-m cprofile -s tottime优化.py＆lt; kjvbible_x10.txt 7980函数调用（7968原始呼叫）在1.280秒排序：内部时间ncalls tottime percall cumpere percall文件名：lineno（功能）662 0.870 0.001 0.870 0.001 {构建-in方法_collections._count_elements} 662 0.278 0.000 0.278 0.000 {方法＆＃39;拆分＆＃39; ＆＃39; str＆＃39;物体} 1 0.080 0.080 1.280 1.280 1.280优化.TY： ＆＃39; str＆＃39;对象} 663 0.010 0.016 0.000 0.016 0.000 {方法＆＃39;阅读＆＃39; ＆＃39; _io.textiowrapper＆＃39;对象} 1 0.007 0.007 0.007 0.007 {内置方法构建.SORTED} 664 0.004 0.000 0.004 0.000 {内置方法_CodeCS.UTF_8_DECODE} 663 0.001 0.000 0.872 0.001 0.000 0.872 0.001 __Init_.py：608（Hudate）664 0.001 0.005 0.000编解码器。 PY：319（解码）662 0.001 0.001 0.000 {内置方法构建.Isinstance} 662 0.000 0.001 0.000 0.001 0.000 0.001 0.000【内置方法_abc._abc_instanceCheck} 662 0.000 0.000 0.000 {方法＆＃39; rfind＆＃39; ＆＃39; str＆＃39; Objects} 664 0.000 0.000 0.000 0.000 CODECS.PY：331（GETTSTATE）662 0.000 0.001 0.000 ABC.YCECHECK__）7/1 0.000 0.000 0.000 0.000 {内置方法_abc._abc_subclasscheck} 1 0.000 0.000 0.007 0.007 __init__ .py：559（most_common）1 0.000 0.000 0.000 0.000 __init__.py：540 (_mit__）7/1 0.000 0.000 0.000 0.000 abc.py：100 (_Subclasscheck__）1 0.000 0.000 1.280 1.280 {内置方法构建.Exec} 7 0.000 0.000 0.000 0.000 _Collections_abc.py：392 (_Mubclasshook__）1 0.000 0.000 0.000 {方法＆＃39;禁用＆＃39; ＆＃39; _lsprof.profiler＆＃39;对象} 1 0.000 0.000 0.000 0.000 {方法＆＃39;物品＆＃39; ＆＃39; dict＆＃39;对象}</p><p> I also found that with the Python solution, reading and processing  bytes vs  str doesn’t make a noticeable difference (the  utf_8_decode is relatively far down the list). In addition, any buffer size above about 2KB is not much slower than 64KB – I’ve noticed many systems have a default buffer size of 4KB, which seems very reasonable in light of that.</p><p> 我还发现，通过Python解决方案，读取和处理字节VS STR不会产生明显的区别（UTF_8_decode相对较远）。此外，大约2kb的任何缓冲大小比64kb都不多得多 - 我注意到许多系统具有4kb的默认缓冲区大小，这似乎是非常合理的。</p><p> I tried various other ways to improve performance, but this was about the best I could manage with standard Python. (I tried running it using the  PyPy optimizing compiler, but it’s significantly slower for some reason.) Trying to optimize at the byte level just makes no sense in Python (or leads to a 10-100x slowdown) – any per-character processing has to be done in C. Let me know if you find a better approach.</p><p> 我尝试了各种其他方式来提高性能，但这是我可以使用标准Python管理的最佳状态。 （我尝试使用Pypy优化编译器运行它，但由于某种原因，它显着慢。在C中完成。如果您发现更好的方法，请告诉我。</p><p>  A simple, idiomatic Go version would probably use   bufio.Scanner with   ScanWords as the split function. Go doesn’t have anything like Python’s  collection.Counter, but it’s easy to use a  map[string]int for counting, and a slice of word-count pairs for the sort operation:</p><p>  一个简单的惯用的GO版本可能会使用Bufio.scanner与scanwords作为拆分功能。 Go没有像Python的Collection.Counter这样的东西，但它很容易使用Map [String] Int进行计数，以及用于排序操作的一片单词计数对：</p><p>  func  main ()  {  scanner  :=  bufio . NewScanner ( os . Stdin )  scanner . Split ( bufio . ScanWords )  counts  :=  make ( map [ string ] int )  for  scanner . Scan ()  {  word  :=  strings . ToLower ( scanner . Text ())  counts [ word ] ++  }  if  err  :=  scanner . Err ();  err  !=  nil  {  fmt . Fprintln ( os . Stderr ,  err )  os . Exit ( 1 )  }  var  ordered  [] Count  for  word ,  count  :=  range  counts  {  ordered  =  append ( ordered ,  Count { word ,  count })  }  sort . Slice ( ordered ,  func ( i ,  j  int )  bool  {  return  ordered [ i ] . Count  &gt;  ordered [ j ] . Count  })  for  _ ,  count  :=  range  ordered  {  fmt . Println ( string ( count . Word ),  count . Count )  } } type  Count  struct  {  Word  string  Count  int }</p><p>  func main（）{scanner：= bufio。 newscanner（操作系统。stdin）扫描仪。拆分（bufio。scanwords）counts：= make（map [string] int）扫描仪。扫描（）{word：= strings。 tolower（扫描仪。text（））计数[word] ++}如果错误：=扫描仪。呃 （）; err！= nil {fmt。 fprintln（操作系统。stderr，err）操作系统。退出（1）}} var有序[]计数Word，count：=范围计数{已排序=附加（有序，计数{word，count}）}排序。切片（订购，Func（i，J int）bool {返回订购[i]。count＆gt;命令[j]。count}）for _，count：=排序范围{fmt。 println（string（count.word），count。count）}}}键入count struct {word字符串count int}</p><p> The simple Go version is significantly faster than the simple Python version, but only a little bit faster than the optimized Python version (and almost double the number of lines of code – there’s definitely more boilerplate and low-level concerns).</p><p> 简单的GO版本明显比简单的Python版本更快，但只能比优化的Python版本快一点（并且几乎双倍代码行数 - 肯定更多的样板和低级问题）。</p><p> To use Go’s profiler, you have to add a few lines of code to the start of your program:</p><p> 要使用Go的Profiler，您必须将几行代码添加到程序的开始： </p><p> f ,  err  :=  os . Create ( &#34;cpuprofile&#34; ) if  err  !=  nil  {  fmt . Fprintf ( os . Stderr ,  &#34;could not create CPU profile: %v \n &#34; ,  err )  os . Exit ( 1 ) } if  err  :=  pprof . StartCPUProfile ( f );  err  !=  nil  {  fmt . Fprintf ( os . Stderr ,  &#34;could not start CPU profile: %v \n &#34; ,  err )  os . Exit ( 1 ) } defer  pprof . StopCPUProfile ()</p><p>f，err：=操作系统。创建（＆＃34; cpuprofile＆＃34;）如果err！= nil {fmt。 fprintf（操作系统。stderr，＆＃34;无法创建CPU配置文件：％v \ n＆＃34;，err）操作系统。退出（1）}如果错误：= pprof。 startcpuprofile（f）; err！= nil {fmt。 fprintf（操作系统。stderr，＃34;无法启动CPU配置文件：％v \ n＆＃34;，err）操作系统。退出（1）}推迟pprof。 stopcpuprofile（）</p><p> Once you’ve run the program, you can view the CPU profile using this command (click to view the image full size):</p><p> 运行程序后，您可以使用此命令查看CPU配置文件（单击以查看图片全大小）：</p><p>   The results are interesting, though not unexpected – the operations in the per-word hot loop take all the time. A good chunk of the time is spent in the scanner, and another chunk is spent allocating strings to insert into the map, so let’s try to optimize both of those parts.</p><p>   结果很有趣，但没有意外 - 每次单词热门循环中的操作一直在。扫描仪中花了一个好的时间，另一个块在分配字符串中即可插入地图，因此让我们尝试优化这些部分。</p><p> To improve scanning, we’ll essentially make a cut-down version of  bufio.Scanner and  ScanWords (and do an ACIII to-lower operation in place). To reduce the allocations, we’ll use a  map[string]*int instead of  map[string]int so we only have to allocate once per unique word, instead of for every increment (Martin Möhrmann gave me this tip on the Gophers Slack #performance channel).</p><p> 为了提高扫描，我们将基本上制作Bufio.Scanner和Scanword的剪切版本（并在适当的操作中进行ACIII到更低的操作）。要减少分配，我们将使用地图[字符串] * int而不是映射[字符串] int所以我们只需每一个单词分配一次，而不是每一个增量（Martinmöhrmann给了我这个提示的Gophers Slack #performance通道）。</p><p> Note that it took me a few iterations and profiling passes to get to this result. One in-between step was to still use  bufio.Scanner but with a custom split function,  scanWordsASCII. However, it’s a bit faster, and not much harder, to avoid  bufio.Scanner altogether. Another thing I tried was a  custom hash table, but I decided that was out of scope for the Go version, and it’s not much faster than the  map[string]*int in any case.</p><p> 请注意，我花了几个迭代和分析通过来实现这一结果。一个在一起的步骤是仍然使用bufio.scanner，但是使用自定义拆分功能，扫描唱片函数。但是，它有点快，而且更难，避免bufio.scanner完全。我尝试的另一件事是一个自定义哈希表，但我决定没有出于Go版本的范围，而不是在任何情况下比地图[字符串] * int更快。</p><p>  func  main ()  {  offset  :=  0  buf  :=  make ([] byte ,  64 * 1024 )  counts  :=  make ( map [ string ] * int )  for  {  // Read input in 64KB blocks till EOF.  n ,  err  :=  os . Stdin . Read ( buf [ offset : ])  if  err  !=  nil  &amp;&amp;  err  !=  io . EOF  {  fmt . Fprintln ( os . Stderr ,  err )  os . Exit ( 1 )  }  if  n  ==  0  {  break  }  // Offset remaining from last time plus number of bytes read.  chunk  :=  buf [ : offset + n ]  // Find last end-of-line character in block read.  lastLF  :=  bytes . LastIndexByte ( chunk ,  &#39;\n&#39; )  toProcess  :=  chunk  if  lastLF  !=  - 1  {  toProcess  =  chunk [ : lastLF ]  }  // Loop through toProcess slice and count words.  start  :=  - 1  // start -1 means in whitespace run  for  i ,  c  :=  range  toProcess  {  // Convert to ASCII lowercase in place as we go.  if  c  &gt;=  &#39;A&#39;  &amp;&amp;  c  &lt;=  &#39;Z&#39;  {  c  =  c  +  ( &#39;a&#39;  -  &#39;A&#39; )  toProcess [ i ]  =  c  }  if  start  &gt;=  0  {  // In a word, look for end of word (whitespace).  if  c  &lt;=  &#39; &#39;  {  // Count this word!  increment ( counts ,  toProcess [ start : i ])  start  =  - 1  }  }  else  {  // In whitespace, look for start of word (non-space).  if  c  &gt;  &#39; &#39;  {  start  =  i  }  }  }  // Count last word, if any.  if  start  &gt;=  0  &amp;&amp;  start  &lt;  len ( toProcess )  {  increment ( counts ,  toProcess [ start : ])  }  // Copy remaining bytes (incomplete line) to start of buffer.  if  lastLF  !=  - 1  {  remaining  :=  chunk [ lastLF + 1 : ]  copy ( buf ,  remaining )  offset  =  len ( remaining )  }  else  {  offset  =  0  }  }  var  ordered  [] Count  for  word ,  count  :=  range  counts  {  ordered  =  append ( ordered ,  Count { word ,  * count })  }  sort . Slice ( ordered ,  func ( i ,  j  int )  bool  {  return  ordered [ i ] . Count  &gt;  ordered [ j ] . Count  })  for  _ ,  count  :=  range  ordered  {  fmt . Println ( string ( count . Word ),  count . Count )  } } func  increment ( counts  map [ string ] * int ,  word  [] byte )  {  if  p ,  ok  :=  counts [ string ( word )];  ok  {  // Word already in map, increment existing int via pointer.  * p ++  return  }  // Word not in map, insert new int.  n  :=  1  counts [ string ( word )]  =  &amp; n }</p><p>  func main（）{offset：= 0 buf：= make（[]字节，64 * 1024）计数：= make（map [string] * int）for {//读取输入的64kb块块直到eof。 n，err：=操作系统。 stdin。读取（Buf [offset：]）如果Err！= nil＆amp;＆amp;呃！= io。 ef {fmt。 fprintln（操作系统。stderr，err）操作系统。退出（1）}如果n == 0 {break} // expset剩余的剩余时间加上读取的字节数。 chunk：= buf [：offset + n] //在块读取中找到最后的行末端字符。 lastlf：= bytes。 LastIndexbyte（块，＆＃39; \ n＆＃39;）toprocess：= chunk如果lastlf！=  -  1 {toprocess = chunk [：lastlf]} //循环通过toprocess切片和计数单词。启动：=  -  1 // start -1在空格中运行i，c：=范围toprocess {//在我们走的时候转换为ASCII小写。如果c＆gt; =＆＃39; a＆＃39; ＆amp;＆amp; C＆lt;＆＃39; z＆＃39; {c = c +（＆＃39; a＆＃39;  - ＆＃39; a＆＃39;）toprocess [i] = c}如果start＆gt; = 0 {//在一个字中，查找单词的结尾（空白）。如果c <=＆＃39; ＆＃39; {//计算这个词！递增（计数，Toprocess [开始：i]）start =  -  1}} else {//在空格中，查找单词（非空间）的开始。如果c＆gt; ＆＃39; ＆＃39; {start = i}}} //计算最后一个单词，如果有的话。如果开始＆gt; = 0＆amp;＆amp;开始＆lt; len（toprocess）{递增（计数，toprocess [start：]）} //复制剩余的字节（不完整的线）到缓冲区的开始。如果lastlf！=  -  1 {剩余：= chunk [lastlf + 1：]复制（buf，剩余）offset = len（剩余）} els {offset = 0}} var有序[]计数，count：=范围计数{已订购=附加（有序，计数{word，* count}）}排序。切片（订购，Func（i，J int）bool {返回订购[i]。count＆gt;命令[j]。count}）for _，count：=排序范围{fmt。 println（字符串（计数字），计数。计数）}}}}}}}}} func增量（counts map [string] * int，word [] byte）{如果p，确定：= counts [string（word）]; OK {// Word已在Map中，递增现有int通过指针。 * p ++返回} // word不在地图中，插入新int。 n：= 1 counts [string（word）] =＆amp; n}</p><p> The profiling results are now very flat – almost everything’s in the main loop or the map access:</p><p> 分析结果现在非常平坦 - 几乎所有内容都在主循环或地图访问中： </p><p>  It was a fun exercise, and Go gives you a fair bit of low-level control (and you could go quite a lot further – memory mapped I/O, a custom hash table, etc). However, programmer time is valuable, and the optimized version above is not something I’d want to test or maintain. It’s tricky code, and there is lots of potential for off-by-one errors (I’d be surprised if there isn’t some bug already). In practice I’d probably stick with a  bufio.Scanner with  ScanWords,  bytes.ToLower, and the  map[string]*int trick.</p><p>这是一个有趣的练习，Go给你一个公平的低级控制（你可以进一步进一步 - 内存映射I / O，自定义哈希表等）。但是，程序员时间是有价值的，上面的优化版本不是我想要测试或维护的东西。这是棘手的代码，并且有很多潜力的偏离一误错误（如果没有一些错误，我会感到惊讶）。在实践中，我可能会用一个bufio.scanner用scanwords，bytes.tolower和地图[字符串] * int技巧。</p><p>  C++ has come a long way since I last used it seriously: lots of goodies in C++11, and then more in C++14, 17, and 20. Features, features everywhere! It’s definitely a lot terser than old-school C++, though the error messages are still a mess. Here’s the simple version I came up with (with some  help from Code Review Stack Exchange to make it a bit more idiomatic):</p><p>  自从我上次使用它，C ++很长的路：C ++ 11中的许​​多好东西，然后在C ++ 14,17和20中更多。功能，到处都是！它绝对是旧学校C ++的宗旨，但错误消息仍然是一团糟。这是我想出的简单版本（通过代码评论堆栈交换的一些帮助，使其有点惯用）：</p><p>  int  main ()  {  std :: string  word ;  std :: unordered_map &lt; std :: string ,  int &gt;  counts ;  while  ( std :: cin  &gt;&gt;  word )  {  std :: transform ( word . begin (),  word . end (),  word . begin (),  []( unsigned  char  c ){  return  std :: tolower ( c );  });  ++ counts [ word ];  }  if  ( std :: cin . bad ())  {  std :: cerr  &lt;&lt;  &#34;error reading stdin \n &#34; ;  return  1 ;  }  std :: vector &lt; std :: pair &lt; std :: string ,  int &gt;&gt;  ordered ( counts . begin (),  counts . end ());  std :: sort ( ordered . begin (),  ordered . end (),  []( auto  const &amp;  a ,  auto  const &amp;  b )  {  return  a . second &gt; b . second ;  });  for  ( auto  const &amp;  count  :  ordered )  {  std :: cout  &lt;&lt;  count . first  &lt;&lt;  &#34; &#34;  &lt;&lt;  count . second  &lt;&lt;  &#34; \n &#34; ;  } }</p><p>  int main（）{std :: string word; std :: unordered_map＆lt; std :: string，int＆gt;计数;而（std :: cin＆gt; word）{std :: transform（word。begine begine（），word。结束（），word。begen（），[]（无符号char c）{return std :: tolower（ C ）;  }）; ++计数[Word]; }如果（std :: cin。bad（））{std :: cerr lt;＆lt; ＆＃34;读取stdin \ n＆＃34错误; ;返回1; } std :: vector＆lt; std ::对＆lt; std :: string，int＆gt;＆gt;订购（计数。begin（），counts。结束（））; std :: sort（命令。begin（），订购。结束（），[]（auto const＆amp; a，auto const＆amp; b）{返回a。第二＆gt; b。第二;}; for（auto const＆amp; count：downered）{std :: cout＆lt;＆lt;数数 。首先＆lt; ＆＃34; ＆＃34; ＆lt;＆lt;数数 。第二＆lt; ＆＃34; \ n＆＃34; ; }}</p><p> When optimizing this, the first thing to do is compile with optimizations enabled ( g++ -O2). I kind of like the fact that with Go you don’t have to worry about this – optimizations are always on.</p><p> 优化此时，首先要做的是使用已启用的优化（G ++ -O2）编译。我有点像这样的事实，你不必担心这个 - 优化始终打开。</p><p> I noticed that I/O was comparatively slow. It turns out there is a magic incantation you can recite at the start of your program to disable synchronizing with the C stdio functions after each I/O operation. This line makes it run almost twice as fast:</p><p> 我注意到I / O相对较慢。事实证明，在每个I / O操作后，您可以在程序开始时拒绝与C stdio函数同步的魔法咒语。这条线使它速度几乎运行了两倍：</p><p>   GCC can generate a profiling report for use with  gprof. Here’s what a few lines of it looks like – I kid you not:</p><p>   GCC可以生成与GPROF一起使用的分析报告。这是几行看起来像 - 我孩子不是：</p><p> index % time self children called name 13 frame_dummy [1][1] 100.0 0.01 0.00 0+13 frame_dummy [1] 13 frame_dummy [1]----------------------------------------------- 0.00 0.00 32187/32187 std::vector&lt;std::pair\&lt;std:</p><p> 索引％的时间自我称为名称13 Frame_Dummy [1] [1] 100.0 0.01 0.00 0 + 13 Frame_Dummy [1] 13 Frame_Dummy [1] ------------------ ------------------------- 0.00 0.00 0.00 0 0.00 0.00 32187/32187 STD :: Vector＆lt; std :: pair \＆lt; std： </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://benhoyt.com/writings/count-words/">https://benhoyt.com/writings/count-words/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/性能/">#性能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/python/">#python</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/rust/">#rust</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/c++/">#c++</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/awk/">#awk</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012 - 2021 diglog.com </div></div></body></html>