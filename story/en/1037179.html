<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>通用机器人的软件和硬件Software and Hardware for General Robots</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Software and Hardware for General Robots<br/>通用机器人的软件和硬件</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-30 04:45:14</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/087b1c9776d44b0b50a0b09fd5aeacde.jpeg"><img src="http://img2.diglog.com/img/2020/11/087b1c9776d44b0b50a0b09fd5aeacde.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Disclaimer, these are just my opinions and not necessarily those of my employer or robotics colleagues.</p><p>免责声明，这些只是我的观点，不一定是我的雇主或机器人同事的观点。</p><p> Moravec&#39;s Paradox describes the observation that our AI systems can solve &#34;adult-level cognitive&#34; tasks like  chess-playing or passing  text-based intelligence tests fairly easily, while accomplishing basic sensorimotor skills like crawling around or grasping objects - things one-year old children can do - are very difficult.</p><p> Moravec的悖论描述了一种观察，即我们的AI系统可以相当轻松地解决象棋或通过基于文本的智力测验等“成人级认知”任务，同时完成基本的感觉运动技能，例如爬行或抓紧物体（一岁的孩子）可以做到-很难。</p><p> Anyone who has tried to build a robot to do anything  will realize that Moravec&#39;s Paradox is not a paradox at all, but rather a direct corollary of our physical reality being so  irredeemably complex and  constantly demanding . Modern humans traverse  millions of square kilometers in their lifetime, a labyrinth full of dangers and opportunities. If we had to consciously process and deliberate all the survival-critical sensory inputs and motor decisions, like we do moves in a game of chess, we would have probably been selected out of the gene pool by Darwinian evolution. Evolution has optimized our biology to perform sensorimotor skills in a split second and make it feel easy.</p><p> 任何试图构建机器人来做任何事情的人都会意识到，莫拉韦克的悖论根本不是悖论，而是对我们物理现实的直接推论，是如此不可思议的复杂和不断要求。现代人类一生要穿越数百万平方公里的迷宫，充满了危险和机遇。如果我们必须象棋牌游戏中那样有意识地处理和考虑所有对生存至关重要的感觉输入和运动决定，我们可能会被达尔文进化论从基因库中选出。进化优化了我们的生物学，可以在一秒钟内执行感觉运动技能，并使感觉变得轻松。</p><p> Another way to appreciate this complexity is to adjust your daily life to a major motor disability, like losing fingers or trying to get around San Francisco without legs.</p><p> 欣赏这种复杂性的另一种方法是将您的日常生活调整为严重的运动障碍，例如失去手指或试图无腿走动旧金山。</p><p>  The difficulty of sensorimotor problems is especially apparent to people who work in robotics and get their hands dirty with the messiness of &#34;the real world&#34;. What are the consequences of an irredeemably complex reality on how we build software abstractions for controlling robots?</p><p>  感觉运动问题的困难对于从事机器人工作并因“现实世界”的混乱而肮脏的人尤其明显。不可思议的复杂现实对我们如何构建用于控制机器人的软件抽象有什么后果？</p><p> One of my pet peeves is when people who do not have sufficient respect for Moravec&#39;s Paradox propose a programming model where high-level robotic tasks (&#34;make me dinner&#34;) can be divided into sequential or parallel computations with clearly defined logical boundaries: wash rice, de-frost meat, get the plates, set the table, etc. These sub-tasks can be in turn broken down further. When a task cannot be decomposed further because there are too many edge cases for conventional software to handle (&#34;does the image contain a cat?&#34;), we can attempt to invoke a Machine Learning model as &#34;magic software&#34; for that capability.</p><p> 我的烦恼之一是，当那些对Moravec悖论没有足够尊重的人提出一种编程模型时，可以将高级机器人任务（“让我吃饭”）划分为具有明确定义的逻辑边界的顺序或并行计算：洗米，解冻肉，拿盘子，摆桌子等。这些子任务又可以进一步细分。当由于常规软件无法处理的极端情况而无法进一步分解任务时（“图像是否包含猫？”），我们可以尝试将机器学习模型称为该功能的“魔术软件”。</p><p> This way of thinking - symbolic logic that calls upon ML code - arises from engineers who are used to clinging to the tidiness of  Software 1.0 abstractions and  programming tutorials that use cooking analogies.</p><p> 这种思维方式-调用ML代码的符号逻辑-是由习惯于坚持使用烹饪类比的Software 1.0抽象和编程教程的工程师带来的。</p><p> Do you have any idea how much intelligence goes into a task like &#34;fetching me a snack&#34;, at the very lowest levels of motor skill? Allow me to illustrate. I recorded a short video of me opening a package of dates and annotated it with all the motor sub-tasks I performed in the process.</p><p>您是否知道在最低水平的运动技能下，像“给我吃零食”这样的任务需要多少智力？请允许我举例说明。我录制了一段简短的视频，内容是我打开了一个日期包，并用在此过程中执行的所有马达子任务对其进行了注释。</p><p>     In the span of 36 seconds, I counted about 14 motor and cognitive skills. They happened so quickly that I didn&#39;t consciously notice them until I went back and analyzed the video, frame by frame.</p><p>     在36秒的时间里，我数了大约14项运动和认知技能。它们发生得如此之快，以至于直到我回去逐帧分析视频时，我才有意识地注意到它们。</p><p>   Leverage past experience opening this sort of package to understand material properties and how much force to apply.</p><p>   利用过去打开此类包装的经验来了解材料特性以及施加的力。</p><p> As a roboticist, it&#39;s humbling to watch  videos of animals making decisions so quickly and then watch our own robots struggle to do the simplest things. We even have to  speed up the robot video 4x-8x to prevent the human watcher from getting bored!</p><p> 作为一名机器人专家，它非常着急地观看动物决策的视频，然后观看我们自己的机器人努力做最简单的事情。我们甚至必须加快4x-8x机器人视频的速度，以防止观看者感到无聊！</p><p>  With this video in mind, let&#39;s consider where we currently are in the state of robotic manipulation. In the last decade or so, multiple research labs have used deep learning to develop robotic systems that can perform any-object robotic grasping from vision. Grasping is an important problem because in order to manipulate objects, one must usually first grasp them. It took the Google Robotics and X teams  2-3 years to develop our own system, QT-Opt. This was a huge research achievement because it was a general method that worked on pretty much any object and, in principle, could be used to learn other tasks.</p><p>  考虑到该视频，让我们考虑一下当前处于机器人操纵状态的位置。在过去十年左右的时间里，多个研究实验室使用深度学习来开发可以从视觉上执行任何对象的机器人抓取的机器人系统。抓取是一个重要的问题，因为要操纵对象，通常必须首先抓住它们。 Google Robotics和X团队花了2-3年的时间来开发我们自己的系统QT-Opt。这是一项巨大的研究成果，因为它是一种适用于几乎所有对象的通用方法，原则上可以用来学习其他任务。</p><p> Some people think that this capability to pick up objects can be wrapped in a simple programmatic API and then used to bootstrap us to human-level manipulation. After all, hard problems are just composed of simpler problems, right?</p><p> 有人认为，这种拾取对象的功能可以包装在简单的编程API中，然后用于引导我们进行人工操作。毕竟，难题只是简单的问题，对吗？</p><p> I don&#39;t think it&#39;s quite so simple. The high-level API call &#34; pick_up_object()&#34; implies a clear semantic boundary between when the robot grasping begins and when it ends. If you re-watch the above video above, how many times do I perform a grasp? It&#39;s not clear to me at all where you would slot those function calls. Here is  a survey if you are interested in participating in a poll of &#34;how many grasps do you see in this video&#34;, whose results I will update in this blog post.</p><p> 我认为这不是那么简单。高级API调用“ pick_up_object（）”表示在机器人抓取开始与结束之间的明确语义边界。如果您重新观看上面的视频，我应该进行几次掌握？我根本不清楚您将这些函数调用放在什么位置。如果您有兴趣参与一项关于“您在此视频中看到了多少掌握情况”的民意测验，这是一项调查。我将在此博客文章中更新其结果。</p><p>  If we need to solve 13 separate manipulation skills just to open a package of dates, and each one of these capabilities take 2-3 years to build, then we are a long, long way from making robots that match the capabilities of humans. Never mind that there isn&#39;t a clear strategy for how to integrate all these behaviors together into a single algorithmic routine. Believe me, I wish reality was simple enough that complex robotic manipulation could be done mostly in Software 1.0. However, as we move beyond pick-and-place towards dexterous and complex tasks, I think we will need to completely rethink how we integrate different capabilities in robotics.</p><p>如果我们只需要打开一组日期就需要解决13种单独的操纵技能，而这些功能中的每一个都需要2-3年的时间才能建成，那么距离制造与人类能力相匹配的机器人还有很长的路要走。没关系，对于将所有这些行为整合到单个算法例程中，没有明确的策略。相信我，我希望现实足够简单，以至于大多数复杂的机器人操作都可以在Software 1.0中完成。但是，随着我们从拾放技术转向灵活而复杂的任务，我认为我们将需要完全重新考虑如何在机器人技术中集成不同的功能。</p><p>  As you might note from the video, the meaning of a &#34;grasp&#34; is somewhat blurry. Biological intelligence was not specifically evolved for grasping - rather, hands and their behaviors emerged from a few core drives:  regulate internal and external conditions, find snacks, replicate.</p><p>  您可能会从视频中注意到，“抓取”的含义有些模糊。生物智能并不是专门为抓握而进化的-手及其行为是从几个核心驱动因素中产生的：调节内部和外部条件，寻找零食，复制。</p><p>  None of this is to say that our current robot platforms and the Software 1.0 programming models are useless for robotics research or applications. A general purpose function  pick_up_object() can still be combined with &#34;Software 1.0 code&#34; into a reliable system worth billions of dollars in value to Amazon warehouses and other logistics fulfillment centers. General pick-and-place for any object in any unstructured environment remains an unsolved, valuable, and  hard research problem.</p><p>  这并不是说我们当前的机器人平台和Software 1.0编程模型对机器人研究或应用没有用。通用函数pick_up_object（）仍可以与“ Software 1.0代码”组合成一个可靠的系统，对亚马逊仓库和其他物流履行中心价值数十亿美元。在任何非结构化环境中，任何对象的通用取放仍然是一个尚未解决，有价值且艰巨的研究问题。</p><p>     Willow Garage was one of the pioneers in home robots, showing that a teleoperated PR2 robot could be used to tidy up a room (note that two arms are needed here for more precise placement of pillows). These are made up of many pick-and-place operations.</p><p>     Willow Garage是家用机器人的先驱者之一，表明可以使用遥控PR2机器人整理房间（请注意，这里需要两个手臂才能更精确地放置枕头）。这些由许多拾取和放置操作组成。</p><p>     This video was made in 2008. That was 11 years ago! It&#39;s sobering to think of how much time has passed and how little the needle has seemingly moved. Reality is hard.</p><p>     该视频制作于2008年。那是11年前！想到已经过去了多少时间，针头似乎没动多少，真是令人发狂。现实很难。</p><p>  The  Stretch is a simple telescoping arm attached to a vertical gantry. It can do things like pick up objects, wipe planar surfaces, and open drawers.</p><p>  Stretch是连接到垂直机架的简单伸缩臂。它可以做诸如捡起物体，擦拭平面表面和打开抽屉等操作。</p><p>  However, futurist beware! A common source of hype for people who don&#39;t think enough about physical reality is to watch demos of robots doing useful things in one home, and then conclude that the same robots are ready to do those tasks in  any home.</p><p>  但是，未来主义者要当心！对于那些对物理现实没有足够了解的人的普遍炒作来源是观看机器人在一个家庭中做有用的事情的演示，然后得出结论，同样的机器人已经准备好在任何一个家庭中完成这些任务。</p><p>  The Stretch video shows the  robot pulling open a dryer door (left-swinging) and retrieving clothes from it. The video is a bit deceptive - I think the camera  physically cannot see the interior of the dryer, so even though a human can teleoperate the robot to do the task, it would run into serious difficulty when ensuring that the dryer has been completely emptied.</p><p>Stretch视频显示了机器人拉开烘干机门（向左旋转）并从中取出衣服。该视频有点欺骗性-我认为相机实际上看不到烘干机的内部，因此即使人类可以遥控机器人执行任务，在确保完全清空烘干机时也会遇到严重的困难。</p><p>  Here is a picture of my own dryer, which features a dryer with a  right-swinging door close to a wall. I&#39;m not sure if the Stretch actually can fit in this tight space, but the PR2 definitely would not be able to open this door without the base getting in the way.</p><p>  这是我自己的干衣机的照片，该干衣机的干衣机带有靠近墙壁的右旋门。我不确定Stretch是否真的可以适合这个狭小的空间，但是PR2肯定会在没有底座的情况下无法打开这扇门。</p><p>   Reality&#39;s edge cases are often swept under the rug when making robot demo videos, which usually show the robot operating in an optimal environment that the robot is well-suited for. But the full range of tasks humans do in the home is  vast.  Neither the PR2 nor the Stretch can crouch under a table to pick up lint off the floor, change a lightbulb while standing on a chair, fix caulking in a bathroom, open mail with a letter opener, move dishes from the dishwasher to the high cabinets, break down cardboard boxes for the recycle bin, go outside and retrieve the mail.</p><p>   制作机器人演示视频时，现实的边缘案例经常被人扫到地毯下，这通常显示出机器人在机器人非常适合的最佳环境中运行。但是，人类在家中要完成的所有任务都很广泛。 PR2和Stretch都不能蹲在桌子底下捡起地板上的绒毛，站在椅子上时要更换灯泡，在浴室里固定缝隙，用开信刀打开邮件，将餐具从洗碗机移到高柜子，分解回收箱的纸板箱，到外面去取回邮件。</p><p>  And of course, they can&#39;t even open a Ziploc package of dates. If you think  that was complex, here is a first-person video of me chopping strawberries, washing utensils, and decorating a cheesecake. This was recorded with a GoPro strapped to my head. Watch each time my fingers twitch - each one is a separate manipulation task!</p><p>  当然，他们甚至不能打开Ziploc日期包。如果您认为那很复杂，请看以下第一人称视频：我切碎草莓，洗碗器和装饰起司蛋糕。这是用绑在我头上的GoPro录制的。每次手指抽动时都要注意-每个操作都是单独的操作任务！</p><p> We often talk about a future where robots do our cooking for us, but I don&#39;t think it&#39;s possible with any hardware on the market today. The only viable hardware for a robot meant to do  any task in human spaces is an adult-sized humanoid, with two-arms, two-legs, and five fingers on each hand.</p><p> 我们经常谈论机器人为我们做饭的未来，但我认为当今市场上的任何硬件都不可能实现。打算在人类空间中执行任何任务的机器人唯一可行的硬件是成年大小的类人动物，每只手有两只胳膊，两条腿和五个手指。</p><p>  Just like I discussed about Software 1.0 in robotics, there is still an enormous space of robot morphologies that can still provide value to research and commercial applications. That doesn&#39;t change the fact that any alternative hardware can&#39;t do all the things a humanoid can in a human-centric space. Agility Robotics is one of the companies  that gets it on the hardware design front. People who build physical robots use their hands a lot - could you imagine the robot you are building assembling a copy of itself?</p><p>  就像我讨论过机器人技术中的Software 1.0一样，仍然存在巨大的机器人形态空间，仍然可以为研究和商业应用提供价值。但这并没有改变任何替代硬件都无法在以人为中心的空间中完成类人动物可以做的所有事情的事实。 Agility Robotics是在硬件设计方面获得它的公司之一。建造物理机器人的人经常动用双手-您能想象正在建造的机器人会组装自己的副本吗？</p><p>     A compromise is to co-design the environment with the robot to avoid infeasible tasks like above. This can simplify both the hardware and software problems. Common examples I hear  incessantly go like this:</p><p>     一种折衷方案是与机器人共同设计环境，以避免上述不可行的任务。这样可以简化硬件和软件问题。我经常听到的常见示例如下：</p><p> Washing machines are better than a bimanual robot washing dishes in the sink, and a dryer is a more efficient machine than a human hanging out clothes to air-dry.</p><p>洗衣机比双手机器人在水槽中洗碗要好，而且干衣机比人类晾衣服晾干的效率更高。</p><p> In the home robot setting, we could design special dryer machine doors that the robot can open easily, or have custom end-effectors (tools) for each task instead of a five-fingered hand. We could go as far as to to have the doors be motorized and open themselves with a remote API call, so the robot doesn&#39;t even need to open the dryer on its own.</p><p> 在家用机器人的环境中，我们可以设计特殊的烘干机门，使机器人可以轻松打开，或者为每项任务配备定制的末端执行器（工具），而不是五只手指。我们甚至可以通过遥控API调用使门自动开门，从而使机器人甚至不需要自己打开烘干机。</p><p>  At the far end of this axis, why even bother with building a robot? We could re-imagine the design of homes themselves to be a single  ASRS system that brings you whatever you need from any location in the house like a  Dumbwaiter (except it would work horizontally and vertically). This would dispenses with the need to have a robot walking around in your home.</p><p>  在这个轴的尽头，为什么还要费心去建造机器人呢？我们可以将房屋设计重新想象成一个单一的ASRS系统，该系统可以从房屋的任何位置（如Dumbwaiter）为您带来所需的一切（除了可以水平和垂直地工作）。这样就无需在家里四处走动的机器人了。</p><p>  This pragmatic line of thinking is fine for commercial applications, but as a human being and a scientist, it feels a bit like a concession of defeat that we cannot make robots do tasks the way humans do. Let&#39;s not forget the Science Fiction dreams that inspired so many of us down this career path - it is not about doing the tasks better, it is about doing everything humans can. A human can wash dishes and dry clothes by hand, so a truly general-purpose robot should be able too. For many people, this endeavor is as close as we can get to the Biblical  Act of Creation, in which God creates Man, body and mind, in His own image.</p><p>  这种务实的思维方式适合商业应用，但是作为人类和科学家，感觉有点像失败的让步，即我们无法让机器人像人类一样完成任务。让我们不要忘记科幻小说的梦想，这激发了我们许多人在这一职业道路上的梦想–它不是在更好地完成任务，而是在人类能做的一切。人可以用手洗碗和洗衣服，因此真正的通用机器人也应该可以。对于许多人来说，这一努力与我们接近《圣经创造法》的努力一样。在《圣经创世法》中，上帝以他自己的形象创造了人，身体和心灵。</p><p> Yes, we&#39;ve built airplanes to fly people around. Airplanes are wonderful flying machines. But to build a bird, which can do a million things  and fly? That, in my mind, is the true spirit of general purpose robotics.</p><p> 是的，我们已经建造了可以使人飞来飞去的飞机。飞机是很棒的飞行器。但是要造一只能做一百万个东西并能飞的鸟呢？在我看来，这就是通用机器人技术的真正精神。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/机器人/">#机器人</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/硬件/">#硬件</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/hardware/">#hardware</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>