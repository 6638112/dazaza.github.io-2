<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>用于NLP的微调变压器 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">用于NLP的微调变压器 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-21 23:03:26</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/f1fb5f5de744f8e6aa9e74825c969084.png"><img src="http://img2.diglog.com/img/2021/6/f1fb5f5de744f8e6aa9e74825c969084.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>在本教程中，我们＆＃39; LL展示了如何微调两种不同的变压器模型，BERT和Distilbert，两个不同的NLP问题：情绪分析和重复的问题检测。</p><p> 您可以在我们的COLAB笔记本中看到一个完整的工作示例，您可以使用HuggingFace上的培训模型。让＆＃39;跳入！</p><p>  由于首先在关注中开发和发布的是，您需要纸张变压器已完全重新定义自然语言处理领域（NLP）在众多任务上设置最先进的问题，例如问题应答，语言生成和命名 - 实体识别。在这里，我们赢得了＆＃39; t＆＃39;对于一个变压器是什么，而是如何申请和训练他们来帮助实现一些任务。在概念上致力于变形金刚的主要事项是他们真的擅长处理顺序数据（文本，语音等），它们充当编码器 - 解码器框架，其中数据被映射到编码器之前的某些代表空间然后通过解码器映射到输出，并且它们非常良好地扩展到并行处理硬件（GPU）。</p><p> 自然语言处理领域的变压器已经接受大量文本数据培训，其允许他们非常了解语言的语法和语义。例如，在BookScorpus培训了通过生成预培训提高语言理解的原始GPT模型，超过7,000多本身。同样地，着名的BERT模型在纸伯特释放：对语言理解的深双向变压器的预训练是在书架和英语维基百科的培训。对于对潜入变压器神经网络架构的读者，原始纸张和所示的变压器是两大资源。</p><p> 变形金刚背后的主要福利，以及我们将在整个博客中看一下，这是一旦预先训练的变压器可以快速微调众多下游任务，并且经常从框中执行得很好。这主要是由于变形金刚已经了解了语言，这允许培训专注于学习如何做问题应答，语言生成，命名实体识别，或者某人为其模型的思想进行任何其他目标。</p><p>    第一个任务模型将受过培训，是情感分析。情绪分析是NLP领域的长期基准，其目标是能够检测到某些文本是否为正，负面或之间的某个地方。这有许多用例，例如检测产品是否以顾客评论或负面方式以正面或负面方式查看或候选者基于推文具有高或低批准等级。我们将用于培训的数据集是培训情绪分析模型是包含11,855个电影评论句子的斯坦福情绪树木银行V2（SST2）数据集。此任务和数据集是普通语言理解评估（胶水）基准的一部分，这是用于培训，评估和分析自然语言理解系统的资源集合。</p><p> 以下是来自此数据集的一些示例，其中数字更接近0表示负面情绪，并且更接近1表示正： </p><p>岩石注定要成为21世纪＆＃39;新的＆＃34;柯南＆＃34;并且他将甚至大于阿诺德施瓦辛格，让克拉夫·瓦姆梅或史蒂文·塞哥尔的飞溅甚至大。</p><p> 华丽精心制作的延续＆＃34;戒指的主＆＃34; Trilogy是如此庞大的是一栏的单词无法充分描述共同作家/董事彼得杰克逊＆＃39; S扩大了J.R.R.的展望。 Tolkien＆＃39;中土。</p><p> 一开始雷鸣，纯粹的精心般的节奏很少，距离;他们的短缺弥补了其他可敬的行动的效力。</p><p>  第二个任务模型将被培训，以进行重复的问题检测。同样，此任务还具有各种用例，例如从Quora平台中删除类似的问题，以限制用户之间的混淆。我们将使用的数据集进行重复的问题检测模型是Quora问题对数据集。此任务/数据集也是胶水基准的一部分。</p><p> 来自此数据集的许多示例，其中0表示未重复和1表示重复项：</p><p>   两个不同的基于变压器的架构将接受上述任务/数据集的培训。预先接受的型号将从HuggingFace变压器恢复，其中包含60多种不同的网络类型。 HuggingFace Model集线器也是一个很好的资源，在各种各样的任务上包含超过10,000种不同的预训练变压器。</p><p>   我们将培训的第一个架构是在Distilbert开放的ZERT的蒸馏和发布的第一个架构，伯特蒸馏出来：较小，更快，更便宜，更轻。该变压器比伯特小40％，同时保留了97％的语言理解能力，也更快地为60％。我们将为SST2和QQP数据集培训此架构。 </p><p>我们将培训的第二个架构是BERT在BERT发布的：用于语言理解的深双向变压器的预训练。这是第一个通过在释放时为11个不同的NLP任务设置新的最先进的新技术，这是一个真正在NLP域中展示了这种模型类型的权力。</p><p>   在这个背景下，让＆＃39;现在看看代码和火车/微调这些型号！在这里，我们使用Pytorch深度学习框架，并且仅包括SST2数据集的代码。要运行此代码，您可以随时查看我们的COLAB笔记本，可以轻松编辑以容纳QQP数据集。</p><p>  首先让＆＃39; s为SST2创建我们的Pytorch DataSet类。此类定义了以下三个重要功能，以下目的：</p><p>  #libraries从orch.utils.data导入数据集#pytorch dataset类class sst_dataset（dataset）：#name：__init__ #purpose：init函数加载dataSet #inputs：数据集 - ＆gt; DataSet #Outputs：none def __init __（self，dataSet）：self .dataset = dataSet return #name：__len__ #purpose：获取数据集的长度#inputs：none #outputs：length  -  lengts-＆gt; DataSet DEF的长度__len __（self）：返回len（self .dataset）#name：__getitem__ #purpose：从数据集#inputs获取一个随机文本段及其标签：IDX  - ＆gt;随机文本段的索引加载#ourputs：text  - ＆gt;文本段＃标签 - ＆gt;情绪分数def __getItem __（self，Idx）：text = self .dataset [idx] [＆＃39;句子＆＃39;]标签=火炬。ractch .zeros（2）标签[round（self .dataset [idx] [＆＃39 ;标签＆＃39;]）] = 1返回文本，标签</p><p>  下一个Let＆＃39; S创建了一对夫妻辅助功能来做像GET GPU，将数据转移到它等等。神经网络，尤其是变压器基于变压器，几乎总是在加速器硬件等加速器硬件上训练，所以发送至关重要如果它有用于处理的模型和数据＆＃39;可用。这允许显着的训练加速，因为可以使用并行处理能力。</p><p> #name：get_gpu #purpose：检查gpu设备是否是可用的#input：none #output：gpu  - ＆gt; GPU设备如果适用，如果不是def get_gpu（）：#check如果是avaliable，如果是这样的话，如果是的话，如果是这样，如果torch .cuda .is_available（）：打印（＆＃34;使用gpu＆＃34;）gpu =火炬.Device（＆＃34; cuda＆＃34;）else：print（＆＃34;没有gpu设备可用！使用cpu＆＃34;）return gpu #name：transfer_device #purpose：将模型/数据转移到GPU Devie如果是#Inputs：GPU  - ＆gt; GPU设备如果适用，如果不是＃data  - ＆gt;数据传输#output：data  - ＆gt;已转移的数据（如果适用DEF Transfer_Device（GPU），数据）：if（gpu！= none）：data = data .ta（gpu）返回数据#name：count_correct #purpose：计算批次中正确的模型预测数量#inputs：预测 - ＆gt;模型预测＃目标 - ＆gt;目标标签#OUTPUTS：正确 - ＆gt;正确模型预测Def Count_correct（预测，目标）：#Create变量存储正确的预测数量以及批量预测的索引= 0索引= 0 #Loop跨批处理中的所有预测并计算数量正确（index＆len（预测））：#convert预测和目标列出预测=列表（预测[index]）target = list（targets [index]）#get最大值指示真值值预测和目标预测_INDEX =预测.index（max（预测））target_index = target .index（max（target））＃如果最大索引是相同的，如果（prediction_index == target_index）：正确+ = 1索引+ = 1返回正确</p><p>  现在我们将定义损失函数......因为我们正在训练一个分类器来预测句子是否具有正面或负面情绪，或者如果两个问题是重复的，我们将使用二进制交叉熵损失功能。这种损失背后的数学是： </p><p>这里是真正的标签（0或1），而P（y）是我们的模型预测。通过最小化这种价值我们的网络学会了解更准确的预测。</p><p> #name：binary_cross_entropy #purpose：定义二进制交叉熵丢失函数#inputs：预测 - ＆gt;模型预测＃目标 - ＆gt;目标标签#OUTPUTS：损失 - ＆gt;损失值def binary_cross_entropy（预测，目标）：损失=  - （目标*火炬.log（预测）+（1  - 目标）*火炬.log（1  - 预测））损失=火炬.mean（损失）回报损失</p><p>  接下来让我们将核心培训/评估逻辑写入微调和测试我们的模型，其中包含3个主要功能：</p><p>  TRATE_MODEL功能首先在验证集上评估预先训练的模型，并在发生任何培训之前计算性能。然后，此功能在培训训练集上培训模型并在验证集中评估其性能时循环循环。 epoch基本上是某些数据集中所有数据的循环。</p><p> 列车功能通过为时代培训模型来运行。请注意，在进行任何培训之前，我们的模型被投入到培训模式，指示Pytorch，需要为参数更新存储梯度。然后通过迭代Pytorch DataLoader来循环时代中的所有批次。然后，每批处理都会通过销售器，允许这些令牌将其发送到模型以进行情感评分预测。在De Facto Pytorch训练循环设置之后，计算损耗值，优化器被归零，损耗导出梯度，并且通过采用优化器步骤更新模型。</p><p> 除了最终优化器归零之外，评估函数具有类似的设置，除了最终优化器归零，梯度导出和优化程序步骤，因为模型不应在验证集上培训。这两个函数之间的其他差异是，在此，我们的模型设置为评估模式，其允许更快推论，因为渐变Don＆＃39; t需要存储。</p><p> 建立在列车和评估功能中是对Count_Correct的调用，从而计算每个批量的正确情绪评分预测的数量，允许在整个数据集中派生最终精度分数。另请注意，SoftMax在模型上调用＆＃39; s输出映射分数到概率。 </p><p>Warning: Can only detect less than 5000 characters</p><p>在本博客中，我们学习了如何在下游任务上进行微调变换器，特别是情感分析和重复的问题检测。 通过微调预先训练的变压器，可以节省大量时间  ...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.assemblyai.com/blog/fine-tuning-transformers-for-nlp">https://www.assemblyai.com/blog/fine-tuning-transformers-for-nlp</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/nlp/">#nlp</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/tuning/">#tuning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>