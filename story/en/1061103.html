<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>我们如何从MongoDB从MongoDB移动到Postgres，没有停机时间，将成本降低30％？ How We Moved from MongoDB to Postgres Without Downtime and Cut Our Costs by 30%?</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">How We Moved from MongoDB to Postgres Without Downtime and Cut Our Costs by 30%?<br/>我们如何从MongoDB从MongoDB移动到Postgres，没有停机时间，将成本降低30％？ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-07 03:22:11</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/5/f1049e188a0a485b906f1c555f93c6e3.jpg"><img src="http://img2.diglog.com/img/2021/5/f1049e188a0a485b906f1c555f93c6e3.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>‍Voucherify was born in 2015 as a weekend hackathon project run by our small-scale software house,  rspective. Initially, it was backed up by a  MongoDB database. Truth be told, this choice was random – it was the most common database we used in our projects. We already had some experience with it so Mongo was a pretty natural component at that stage. However, as Voucherify&#39;s scale grew, we’ve added a second database –  PostgreSQL – that seemed to be more suitable for the upcoming features. Then for some time we kept part of our data in Mongo, and the other part in Postgres, until the day we decided to move it all to Postgres.</p><p>凭证在2015年出生于2015年作为一个周末Hackathon项目，由我们的小型软件房屋运行，RSPlective。最初，它由MongoDB数据库备份。真相被告知，这种选择是随机的 - 它是我们在我们项目中使用的最常见的数据库。我们已经有了一些经验，所以Mongo在那个阶段是一个非常自然的组成部分。但是，如凭证，我们添加了第二个数据库 -  PostgreSQL  - 这似乎更适合即将到来的功能。然后我们一段时间我们保留了Mongo的一部分数据，并在Postgres的另一部分，直到我们决定将其全部移动到Postgres。</p><p> When we started out, we already had around  five years of data collected, spread around multiple database instances, located on three continents, each dedicated to a different Voucherify cluster. Millions of voucher codes that could be updated anytime. Around a terabyte of constantly changing data. And to make matters worse, a lot of code had to be prepared for the upcoming breaking change. If presented on a timeline, we spent  three months rewriting and testing new code and next three months migrating all the data.</p><p> 当我们开始时，我们已经收集了大约五年的数据，围绕三大洲的多个数据库实例传播，每个数据库实例都专用于不同的凭证群集。数百万个可以随时更新的优惠券代码。围绕一个不断变化的数据。并使事情变得更糟，因此必须为即将到来的突破变化做好大量的代码。如果在时间表上呈现，我们花了三个月重写和测试新代码，并在接下来的三个月迁移所有数据。</p><p> Why then go through all this trouble? We had two valid reasons to do that.</p><p> 那么为什么要经历所有这些麻烦？我们有两种有效的理由要这样做。</p><p> First of all, as you can easily imagine, maintaining two different database types creates a  cascade effect of doubled codebase, paradigms and concepts you have to keep in mind while adding new features. It was also the source of problems with the initial setup followed by issues popping up randomly (usually, on Friday afternoons). If one of them looks redundant, then all these issues sum up and cause tension and frustration in the engineering team.</p><p> 首先，您可以轻松地想象，维护两个不同的数据库类型，创造了一倍的CodeBase，范式和概念的级联效果，在添加新功能时必须记住。它也是初始设置的问题的源头，后跟随机弹出的问题（通常是星期五下午）。如果其中一个看起来冗余，那么所有这些问题总结并导致工程团队的紧张和挫折。</p><p> Secondly, Compose – a SaaS platform serving MongoDB that we were using, was  very expensive in comparison to alternatives. It became a significant percentage of our monthly expenses. Additionally, we were not satisfied with the quality of the support we got. Sometimes response delays could be as long as several days. In some cases, the only offered solution was to restart the database, with no good explanation of why the weird stuff happened in the first place or whether they plan to fix that in the future.</p><p> 其次，撰写 - 用于我们使用的MongoDB的SaaS平台，与替代方案相比，非常昂贵。它成为我们每月费用的显着比例。此外，我们对我们得到的支持的质量并不满足。有时，响应延迟可能会长达几天。在某些情况下，唯一提供的解决方案是重新启动数据库，没有良好的解释为什么奇怪的事情在第一位置发生或他们是否计划在将来解决这个问题。</p><p> To succeed with the migration and maintain platform stability when the traffic was high, we split it into a couple of tasks – each corresponding to a different entity. Most of them were easy migrations of relatively small chunks of data that was updated rarely. Each of these tasks has its own story, but this article is going to tell the story of the last task. It was about two core entities – vouchers and campaigns – that serve as primary objects in Voucherify API. As you can imagine, these were kept in Mongo for the longest. You could say that  the core of our system was built around the database that had to be replaced.</p><p> 为了成功，当流量高时，我们将其拆分成几个任务 - 每个任务 - 每个任务对应于不同的实体。他们中的大多数都很轻松地迁移相对较小的数据块已经更新的数据。这些任务中的每一个都有自己的故事，但本文将讲述最后一项任务的故事。它是关于两个核心实体 - 凭证和广告系列 - 它用作凭证API中的主要对象。正如你想象的那样，这些是最长的Mongo。您可以说，我们的系统的核心是在必须被替换的数据库周围构建的。</p><p> We used  AWS’s Database Migration Service to help us with migration. The primary motivation was to reduce the time of preparing the setup for the migration tool, by relying on a SaaS solution tested by hundreds of developers already.</p><p> 我们使用AWS的数据库迁移服务来帮助我们迁移。主要动机是减少准备迁移工具的设置的时间，依赖于已经由数百种开发人员测试的SaaS解决方案。 </p><p> We decided to create new temporary tables for each Mongo collection, and somehow safely merge them with production tables in the next steps. The PostgreSQL database has a nice feature that helped us, called table inheritance. It gave us a possibility to join two tables together in a hierarchical order to obtain a parent table having multiple independent child tables.</p><p>我们决定为每个Mongo系列创建新的临时表，以某种方式安全地将它们与下一步中的生产表合并。 PostgreSQL数据库具有一个很好的功能，帮助我们称为表继承。它使我们有可能以分层顺序将两个表连接在一起，以获取具有多个独立子表的父表。</p><p>    First, we had to find out which database we should choose. This migration was different from the previous ones, so we were still struggling with the question if Postgres is the best choice. After dropping a few options, we took a closer look at DocumentDB and PostgreSQL from the AWS environment. DocumentDB is a non-relational database released in January 2019 used for storing JSON documents. Its mission is to be compatible with the MongoDB API to some extent and it is fully shifted to the AWS team, which translates to scalability and availability at a  reasonable price.</p><p>    首先，我们不得不找出我们应该选择的数据库。这种迁移与以前的迁移不同，所以如果Postgres是最佳选择，我们仍然在努力解决问题。丢弃几个选项后，我们仔细研究了AWS环境的DocumentDB和PostgreSQL。 DocumentDB是用于存储JSON文档的2019年1月发布的非关系数据库。其使命将在某种程度上与MongoDB API兼容，它完全转移到AWS团队，以合理的价格转化为可扩展性和可用性。</p><p> Postgres had the obvious advantage of unifying the underlying technologies. As a result, buying DocumentDB would be an additional expense when compared to reusing an existing database. By design, DocumentDB does not support all MongoDB commands either, even with its greater scalability or availability. However, the supported features should be good enough for the majority of cases.</p><p> Postgres具有统一潜在技术的明显优势。因此，与重用现有数据库相比，购买DocumentDB将是额外的费用。通过设计，DocumentDB也不支持所有MongoDB命令，即使具有更大的可扩展性或可用性。然而，支持的特征对于大多数情况来说应该足够好。</p><p> Therefore we could expect no code changes for the migration only if we didn’t play around with exotic queries too much. Our estimation was that we had around 5-10 places in the code that use some not-supported Mongo features. Doing some additional (usually small) migration of schema, like additional flags to flatten the state, were usually the solution to such problems. In the next step, such flags would let us simplify more sophisticated queries. However, such changes require adding many small adjustments across the whole application in the first place.</p><p> 因此，只有我们没有太大的疑问，我们只能预期迁移的代码更改。我们的估计是我们在使用一些不支持的Mongo功能的代码中拥有大约5-10个位置。做一些额外的（通常小）模式的迁移，如额外的旗帜来平整状态，通常是这些问题的解决方案。在下一步中，这些标志将让我们简化更复杂的查询。但是，这种变化需要首先在整个应用中添加许多小调整。</p><p> After finally gathering all cost predictions, it became clear that if we manage to reuse current database instances, maybe enhancing them slightly, we are still better off in comparison to other options. Therefore, after all, the choice was not that hard and when the decision was made the only question left was how we are going to migrate that huge amount of data.</p><p> 最后收集所有成本预测后，如果我们设法重用当前数据库实例，则可以轻松增强它们，与其他选项相比，我们仍然更好。因此，毕竟，选择并不是那么艰难，当决定做出了唯一的问题是我们将如何迁移大量数据。</p><p> To answer this question we did not dive into vast research about available migration tools. What we did instead was to check out the possibilities of AWS’s Database Migration Service, and see how far we can go with that idea. We wanted to reduce the time of preparing the setup for the migration tool, by relying on a SaaS solution tested by hundreds of developers already. ‍</p><p> 为了回答这个问题，我们没有潜入关于可用迁移工具的巨大研究。我们所做的就是检查AWS数据库迁移服务的可能性，看看我们可以使用这种想法。我们希望减少准备迁移工具的设置的时间，依靠数百名开发人员测试的SaaS解决方案。</p><p>  DMS is a great tool which can be used to migrate data between different types of databases – including Mongo to Postgres transfer. After turning it on, you get an underlying  EC2 instance with ready-to-use software for migrating your data. To be able to keep your data in sync, even during long-running migrations, you can launch it in  Multi-AZ mode, which takes just one click. With DMS, you also get a decent migration monitoring process out-of-the-box. Database Migration Service provides several abstractions, out of which three seem to be essential. So endpoints, replication instances, and database migration tasks.</p><p>  DMS是一个很好的工具，可用于在不同类型的数据库之间迁移数据 - 包括Mongo到Postgres Transfer。打开它后，您将获得带有用于迁移数据的即用的软件的底层EC2实例。为了能够保持数据同步，即使在长期运行迁移期间，您也可以以多Az模式推出它，只需单击一下即可。使用DMS，您还可以在框中获得体面的迁移监控过程。数据库迁移服务提供了多种抽象，其中三个似乎是必不可少的。所以端点，复制实例和数据库迁移任务。 </p><p>  Starting from the last, a task is a description of a particular copy-paste job executed once and/or as an ongoing replication. It holds filtering and transformation rules together with some additional options.</p><p>从上次开始，任务是执行一次和/或作为正在进行的复制的特定副本粘贴作业的描述。它将过滤和转换规则与一些其他选项保持过滤。</p><p> Replication instances are self-explanatory, but to be clear, these are lists of EC2 instances that we can choose to be used in a particular migration task. After AWS admin creates an instance with desired RAM and CPU resources, the payment counter is triggered. Yet, all in all, the overall cost of using DMS in our case was marginable. But, if you really want to know, then yeah... we spent maybe like 200$ in total for this tool. But compared to the predicted savings it was like a drop in the ocean.</p><p> 复制实例是不言自明的，但要清除，这些是我们可以选择用于特定迁移任务的EC2实例列表。 AWS管理员创建具有所需RAM和CPU资源的实例后，将触发付款计数器。然而，总而言之，我们案件中使用DMS的总成本是Margnable。但是，如果你真的想知道，那么是的......我们花了这个工具的总共花了200美元。但与预测的节省相比，它就像在海洋中的一滴水。</p><p> Last but not least, an endpoint is a description of how to connect to a particular database. Besides some obvious parameters, it includes the intent of direction it will be used, meaning if that’s a source or rather a target in the migration process. Once you have a replication instance running you can use it to test the connection for defined endpoints. When you create a DMS replication instance in the same VPC where your source/target endpoint is, then you are one step ahead because you don’t have to make your database public on the internet for the migration time. All in all this made DMS pretty easy to use in our case.</p><p> 最后但并非最不重要的是，端点是如何连接到特定数据库的描述。除了一些明显的参数之外，它还包括将使用的方向的意图，这意味着如果这是迁移过程中的源代码或者是目标。运行Replication实例后，可以使用它来测试定义的端点的连接。当您在源/目标端点的同一VPC中创建DMS Replication实例，那么您将前进一步，因为您不必在Internet上将您的数据库公开进行迁移时间。所有这些都在我们的情况下使DMS非常易于使用。</p><p>  First of all, its GUI isn’t perfect. There are two modes – graphical and json – where the first does not support all the features of the json mode. So going through the documentation is necessary to understand all of the possible filters and transformations. However, after we managed to set up the migration process using the json mode, it became way better to use DMS that way. The trick was to generate, using simple bash scripts, big JSONs with precise task descriptions that we simply pasted to the DMS website’s text area as a whole.</p><p>  首先，它的GUI并不完美。有两种模式 - 图形和JSON  - 首先不支持JSON模式的所有功能。所以通过文档是必要的，了解所有可能的过滤器和转换。但是，在我们设法使用JSON模式设置迁移过程之后，它变得更好地使用DMS。诀窍是使用简单的Bash脚本，大JSONs生成，具有精确的任务描述，我们简单地粘贴到DMS网站的文本区域整体。</p><p> Another thing that we didn’t like about DMS was that when a new migration job was created the option to drop the destination table in the target database at the beginning was preselected by default. For sure there are cases when such behaviour is desirable,  but why is the most dangerous option a default one? We planned to migrate the data project by project in a very controllable way, so we envisioned hundreds of migration tasks to be created. Having this constant risk of deleting all the production data by a simple mistake pushed us towards a safer, but much harder to achieve variant of the migration process. Our solution was to migrate the data in two steps – first to temporary tables that could be wiped by the DMS accidentally, and then to the destination production tables.</p><p> 我们不喜欢DMS的另一件事是，当创建一个新的迁移作业时，默认情况下会选中在开始时删除目标数据库中的目标表。确保有人需要这种行为，但为什么最危险的选项是默认的？我们计划以非常可控的方式按项目迁移数据项目，因此我们设想要创建数百个迁移任务。通过这种持续的风险，简单的错误将所有生产数据删除所有的生产数据推动我们走向更安全，但更难实现迁移过程的变体。我们的解决方案是以两个步骤迁移数据 - 首先到临时表可以意外地由DMS擦除，然后到目的地生产表。</p><p> Before we were sure that DMS was an acceptable tool for this job, we still had to overcome many technical obstacles. Initially we planned to do two rounds of the migration, one for campaigns and second for vouchers. We imagined that each round would have only two steps for each Project (workspace in Voucherify) – run DMS with ongoing replication and switch a boolean flag in the project&#39;s config. As you can guess, it became much more complex. Let’s dive briskly into the story.</p><p> 在我们确定DMS是这项工作的可接受的工具之前，我们还必须克服许多技术障碍。最初，我们计划做两轮迁移，一个是一个用于竞选和第二次职业券。我们想象每轮每个项目都只有两个步骤为每个项目（工作空间） - 运行DMS，正在进行的复制，并在项目中切换布尔标志＆＃39; s配置。正如您所猜的，它变得更加复杂。让我们轻快进入故事。</p><p>   First, we needed to clarify precisely what was the setup for each project before the migration. From the DMS task’s point of view, each project is basically a list of campaigns, and a number of vouchers that are standalone or belong to a campaign. Doable? Sure, but if you multiply that by hundreds of projects it becomes clear that you cannot type all of these settings by hand.</p><p>   首先，我们需要精确澄清迁移前每个项目的设置是什么。从DMS任务的角度来看，每个项目基本上都是一个广告系列列表，以及一系列独立或属于广告系列的凭证。可行的吗？当然，但如果您将数百个项目乘以，则清楚地清楚地无法用手键入所有这些设置。 </p><p> Before such big tasks,  it’s always good to clear the code from some leftover logic to simplify the migration a little bit. In our case, this was for instance checking if all Campaign and Voucher fields are still in usage. Dropping old code is the easiest part of the process that can shorten its overall time. Unfortunately, we didn’t have any work to do here.</p><p>在如此重要的任务之前，清除一些剩余的逻辑中的代码总是很好，以简化迁移一点点。在我们的情况下，这是例如检查所有广告系列和凭证字段是否仍在使用。丢弃旧代码是过程中最简单的部分，可以缩短其整体时间。不幸的是，我们没有任何工作要做。</p><p> This allowed us to have pretty straightforward core migration scripts, which was definitely one of the goals. To save time in further steps, we made some assumptions while describing the valid model of each entity type. As you probably know, MongoDB is a document database, which means that it is schemaless. If you use it, like we did, to store data that has a schema, then you not only shift the responsibility for keeping data in good state to the application level, but you still have to expect that some part of your data will be dirty when migration comes. And yes, as we’ll see later on, data in Mongo can be dirty in many ways (or elastic, depending on the point of view). With these assumptions in mind, we were performing appropriate sanity checks before the migration of each project to find the polluted entries. This allowed us to fix corrupted entries quickly in isolation and perform the core migration in peace. Short disclaimer – a part of these assumptions could have been checked much faster in the destination database, therefore in our case it made sense to have two phases of sanity checks – initial one in Mongo and the second in Postgres. ‍</p><p> 这使我们可以拥有非常简单的核心迁移脚本，这绝对是目标之一。为了进一步节省时间，我们在描述每个实体类型的有效模型时作出了一些假设。正如您可能所知，MongoDB是一个文档数据库，这意味着它是艺术模式。如果使用它，就像我们这样做一样，要存储具有架构的数据，那么您不仅要将其保持良好状态的责任转移到应用程序级别，但您仍然需要预期数据的某些部分将脏迁移到来时。是的，正如我们稍后会看到的那样，Mongo中的数据可以在许多方面（或弹性，具体取决于观点）。考虑到这些假设，我们在迁移每个项目的迁移之前进行适当的理智检查，以找到污染的条目。这允许我们以孤立地快速修复损坏的条目，并按住核心迁移。短缺免责声明 - 目的地数据库中可能会检查这些假设的一部分，因此在我们的情况下，有两个阶段的理智检查是有意义的 - 在蒙古和第二部分中的初始阶段。</p><p>  We performed these sanity checks while data was still in temporary tables, to which DMS copied the data. More details will come, but I can mention already that we were creating short-term destination tables for each MongoDB collection, and when data was loaded we merged these temp child tables one-by-one into their parents.</p><p>  我们在数据仍处于临时表中，我们执行了这些理智检查，DMS复制了数据。更详细信息将来，但我可以提到我们为每个MongoDB集合创建短期目的地表，并且当加载数据时，我们将这些临时子表逐一合并到父母中。</p><p> To have a better insight into what type of checks we did, let’s explore the first piece of code that we used to find if entities are free from from two simple types of errors:</p><p> 要更好地了解我们所做的何种类型的检查，让我们探索我们常常从两种简单类型的错误中找到实体的第一部代码：</p><p> {{CODE}} db[&#34;vouchers-TENANT-PROJECT&#34;].count({$or: [{ type: { $exists: false } }, { deleted_at: { $exists: true } }]})</p><p> {{码}} db [＆＃34;凭证 - 租户 - 项目＆＃34;]。count（{$或：[{type：{$ alivings：false}}，{deleted_at：{$ sipers：true}}] }）</p><p> +db[&#34;campaigns-TENANT-PROJECT&#34;].count({$or: [{ campaign_type: { $exists: false } }, { deleted_at: { $exists: true } }]})+</p><p> + DB [＆＃34;竞选 - 租户 - 项目＆＃34;]。计数（{$或：[{campaign_type：{$ alivings：false}}，{deleted_at：{$ sipers：true}}}）}}）}</p><p>  The first thing we wanted to find was if one of the old migrations of voucher and campaign types is fully done. We decided to have these types with a NON NULL constraint in the final SQL form, so all entries must have been equipped with some value before the migration. It was also possible to check and fix this issue also in the final database, but it was easier to cut the problem in the roots and forget about it once for all.</p><p>  我们想找到的第一件事是，如果是完全完成凭证和广告系列类型的旧迁移之一。我们决定使用最终SQL表单中具有非空约束的这些类型，因此所有条目必须在迁移之前配备某些值。还可以在最终数据库中检查和修复此问题，但更容易削减根源中的问题并为所有人忘记它。 </p><p> The second thing we checked was if there exist any entries with an old way of naming the variable keeping the deletion time – deleted_at. Currently we had an alternative camel-case naming deletedAt, and it occurred later that for the sake of a simple migration script it’s better to clean the old data first. So if the total returned from the above query was non-zero, we listed all wrong entries and either fixed or removed them in justified cases.</p><p>我们选中的第二件事是如果存在具有旧方法的任何条目，可让变量保留删除时间 -  deleted_at。目前我们有一个替代的骆驼案命名删除了，它稍后发生，因为为了简单的迁移脚本，最好先清理旧数据。因此，如果从上面查询返回的总数是非零，我们列出了所有错误的条目，并在证明的情况下修复或删除它们。</p><p> We performed the second round of sanity checks right after the data landed in the Postgres database. Let’s jump a few steps ahead and quickly describe what was checked in this phase. Firstly, we searched for entries corrupted internally using the following query:</p><p> 在Postgres数据库中降落的数据后，我们执行第二轮Sanity检查。让我们跳过几步，并迅速描述在此阶段检查的内容。首先，我们使用以下查询搜索内部损坏的条目：</p><p>           You can see a part of the checks that were performed on the table holding the vouchers. This is the most interesting piece, so the rest of this query is cut here. The same kind of query was performed on the campaign&#39;s data right after that. Whenever some interesting findings came up we fixed them in Mongo, so that the effect of this action was preserved if the migration had to be aborted for some reason.</p><p>           您可以看到持有凭证的表上执行的检查部分。这是最有趣的作品，所以此次查询的其余部分都在此处切割。在此之后，在广告系列和＃39;数据上进行了相同类型的查询。每当一些有趣的发现来，我们将它们固定在蒙古，因此如果迁移必须因某种原因中止，则保留了这种行动的效​​果。</p><p>  Let’s explore the details of the piece of code above. The first line will be hard to grasp at the moment yet. We will get back to it when describing the core migration script. The next two lines compared the ‘discount’ field with SQL-like ‘NULL’ and json-like ‘null’ values. These two types of NULLs were the first issue we faced. Both the old and the new code covered such a possibility nicely in the GET responses, but we wanted to be sure that data was fully complete after it was pushed through the migration script. SQL’s NULL value was expected for the missing data, however json null was a bit surprising. We didn’t have a code that could set a null value at that point, so maybe we had a code like that some time ago or these null values were a result of some old manual actions. Anyway, reading and parsing corrupted data like that worked well, however a jsonb concatenation ‘||’ applied on a null stored inside a PostgreSQL field resulted in a nasty error. So it’s good to be cautious about this little quirk.</p><p>  让我们探索上面的代码的细节。第一行仍然很难掌握。在描述核心迁移脚本时，我们将回复它。接下来的两行比较了“折扣”字段，具有SQL样的“NULL”和JSON  - 类似的“NULL”值。这两种类型的空白是我们面临的第一个问题。旧的和新代码都暗示了在获取响应中的可能性，但我们希望确保在通过迁移脚本推动后完全完成数据。缺少数据预期SQL的空值，但JSON NULL有点令人惊讶。我们没有一个代码可以在该点设置空值，所以也许我们在前段时间的代码或这些空值是一些旧手动操作的结果。无论如何，阅读和解析损坏的数据，如此运行良好，但是在存储在PostgreSQL字段中的NULL上应用的JSONB连接'||'导致令人讨厌的错误。所以对这个小怪癖对谨慎谨慎态度很好。</p><p> The next line regarding the count of the publications shows how we checked if any integer-like fields were not stored in Mongo as a string value in a float-like form. For instance instead of expected 1, we got “1.0000.” Maybe we wouldn’t even notice that if we didn’t have plenty of (CAST .. AS INT) SQL transformation already in the code. That combination caused unexpected errors after migrating a couple of old test projects, so you can call it a second quirk we found. There were very few cases like that so we fixed all of them manually. These two problems – “1.0000” and json-like null values – were addressed by us later on in the migration script, but we left all these checks here to double check if all is fine.</p><p> 关于出版物的计数的下一行显示了我们如何检查任何像性相同的字段是否未以浮动形式作为字符串值存储在Mongo中。例如，而不是预期的1，我们得到了“1.0000”。也许我们甚至不会注意到，如果我们没有大量的（演员...为INT）SQL转换已经在代码中。这种组合在迁移了几个旧的测试项目后引起了意外错误，因此您可以称之为我们发现的第二次怪癖。很少有这样的案例，所以我们手动修复了所有这些。这两个问题 - “1.0000”和JSON的空值 - 在迁移脚本中稍后通过我们解决，但我们在此处留下了所有这些检查，请仔细检查一切是否正常。</p><p> The last check “(jsonb_array_length(publish-&gt;&#39;entries&#39;) &gt; 0 AND publish::text LIKE &#39;%$date%&#39;)” was meant to find vouchers with publication entries stored in an invalid way. At that point, our code was already ignoring the data stored there, except one query that checked the total count of these entries. Therefore, we decided to migrate vouchers with all these publication entries, and fix this issue later on after we had all the data in one database. But in the first place we had to fix a nasty bug that was captured after the first manual tests. In rare cases dates were stored in MongoDB as ISODates, which are transferred by the DMS as json objects with one $date field containing a numeric timestamp value. Even though we didn’t really use this data anymore, our ORM system was still parsing it and obviously failed to read dates in this format. Same as before, there were not many cases like that, so manual fixes were most effective.</p><p> 最后一张检查（jsonb_array_length（发布 - ＆gt;＆＃39;条目＆＃39;）＆gt; 0和publish :: text＆＃39;％$ date％＆＃39;）“本来是为了找到出版物的优惠券输入以无效方式存储。此时，我们的代码已忽略存储在那里的数据，除了检查这些条目的总计数的一个查询。因此，我们决定使用所有这些出版物条目迁移凭证，并在我们在一个数据库中的所有数据稍后修复此问题。但首先，我们必须修复第一次手动测试后捕获的令人讨厌的错误。在极少数情况下，日期将在MongoDB中存储为isoDate，该isodate将被DMS转移为JSON对象，其中包含数值时间戳值的一个$日期字段。即使我们不再真正使用此数据了，我们的ORM系统仍在解析它，显然无法以此格式读取日期。与以前一样，没有许多情况下，因此手动修复最有效。</p><p> Besides checking internal vouchers and campaigns data, we also checked all kinds of relations at this step. That wasn’t possible back when vouchers and campaigns were in one database and the other part in another. That’s because a script for that would run for a very long time and most probably would give us a lot of false positives. So, we verified the vouchers count inside each campaign. This check was possible before the migration using a pure MongoDB script, but it was much easier to write it in the SQL version. Also SQL guaranteed that counting vouchers was not affected by parallel operations, like adding or removing vouchers, so comparing it with campaign’s ‘vouchers_count’ always gave reliable results. We also inspected the total of redemptions and publications stored in vouchers data. That kind of check was doable only after all data was inside one database, and even then it took a significant amount of time. ‍</p><p> 除了检查内部凭证和竞选数据，我们还在这一步检查了各种关系。当凭证和广告系列在一个数据库中以及另一个数据库中的其他部分时，这是不可能的。这是因为这是一个很长一段时间的脚本，大多数可能会给我们很多误报。因此，我们核实了每个广告系列内的凭证计数。在使用纯MongoDB脚本迁移之前，此检查是可能的，但在SQL版本中将其写入更容易。 SQL也保证计数凭证不受并行操作的影响，例如添加或删除凭证，因此将其与广告系列的“凭证_Count”进行比较，总是给出可靠的结果。我们还检查了存储在凭证数据中的赎回和出版物的总数。只有在所有数据内部在一个数据库内完成，那种检查只是可行的，即使那么花费大量的时间。 </p><p>  For each project, there were two Mongo collections to be migrated. We decided that each collection will be transferred to a temporary table to limit the risk that one of the DMS jobs will drop a target table full of active campaigns or vouchers. While preparing for the migration, the choice whether to migrate to final or temporary tables was crucial. We went for the option that seemed the safest, if for instance we make a mistake while setting up the DMS task. This was unlikely to happen, but we wanted to use the safest path and see if we can stand the burden that will be put on us.</p><p>对于每个项目，有两个蒙古收藏率迁移。我们决定将每个集合转移到临时表，以限制其中一个DMS作业将丢弃充满活跃的活动或优惠券的目标表的风险。在准备迁移时，选择是否迁移到最终或临时表至关重要。我们去了似乎最安全的选项，如果例如我们在设置DMS任务时犯了错误。这不太可能发生，但我们想使用最安全的路径，看看我们是否能够忍受将掌握我们的负担。</p><p> Additionally, as you will see later on we needed extra fields to finalize the migration, and having them in the final table would add a risk of returning this data through the API, or storing it in the system events data. We would need to prepare the code for such risk, but we still could have missed something. Therefore having child tables extending the model gave us absolute guarantee that this won’t happen. Soon you will see that some of the problems that we faced wouldn’t exist if we chose to migrate the data directly to the production tables. It’s hard to say how the other path would end, yet this article will present at least one side of the coin. From the time perspective, I can safely say that the road we settled on was the right one as it brought us the best performance and the expected results.</p><p> 此外，正如您稍后会看到我们需要额外的字段以完成迁移，并且在最终表中将它们添加到通过API返回此数据的风险，或将其存储在系统事件数据中。我们需要为这些风险准备代码，但我们仍然可能错过了一些东西。因此，延长模型的子表给我们绝对保证了这不会发生这种情况。很快您就会看到我们选择的一些问题，如果我们选择将数据直接迁移到生产表。很难说其他路径如何结束，但本文将呈现硬币的一侧。从时间的角度来看，我可以安全地说，我们解决的道路是正确的，因为它给我们带来了最佳表现和预期的结果。</p><p>  We decided to create new temporary tables for each Mongo collection, and somehow safely merge them with production tables in the next steps. The PostgreSQL database has a nice feature that helped us here called table inheritance. It allowed us to join two tables together in a hierarchical order to obtain a parent table having multiple independent child tables. Each child table stores data and indexes separately, but when you read from the parent table you get the results aggregated from all engaged tables as if there was only one logical table. The obvious condition to join two tables in such inheritance relation is that the child tables must possess all the columns of a parent table, yet they can have more in the same sense as inheritance works in any programming language. These columns cannot be accessed while reading data through the parent table, but they can be useful in many ways. In our migration process we used these additional columns to store MongoDB raw data and  _id of each object.</p><p>  我们决定为每个Mongo系列创建新的临时表，以某种方式安全地将它们与下一步中的生产表合并。 PostgreSQL数据库有一个很好的功能，帮助我们在这里称为表继承。它允许我们以分层顺序将两个表连接在一起，以获取具有多个独立子表的父表。每个子表单独存储数据和索引，但是当您从父表读取时，您将从所有接合表中聚合的结果，就像只有一个逻辑表一样。在此类继承关系中加入两个表的明显条件是子表必须拥有父表的所有列，但它们可以在与任何编程语言中的继承工作相同的感觉中具有更多。在通过父表读取数据时，无法访问这些列，但它们可以在许多方面都很有用。在我们的迁移过程中，我们使用这些附加列来存储每个对象的MongoDB原始数据和_ID。</p><p> Let me stress once again that each child table holds a separate data with separate indexes. It’s especially important for unique indexes, because it’s possible to obtain the se</p><p> 让我再次压力每个子表每个子表都包含一个单独的索引的单独数据。对于独特的索引来说尤为重要，因为可以获得SE</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.voucherify.io/blog/how-we-moved-from-mongodb-to-postgres-without-downtime-and-cut-our-costs-by-30">https://www.voucherify.io/blog/how-we-moved-from-mongodb-to-postgres-without-downtime-and-cut-our-costs-by-30</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/mongodb/">#mongodb</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/成本/">#成本</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/迁移/">#迁移</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>