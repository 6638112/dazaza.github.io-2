<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习Pns老式Atari游戏 Machine Learning Pwns Old-School Atari Games</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Machine Learning Pwns Old-School Atari Games<br/>机器学习Pns老式Atari游戏 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-28 23:51:48</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/fded007c0f6e1f914b1fcc7da5c8c17c.jpg"><img src="http://img2.diglog.com/img/2021/2/fded007c0f6e1f914b1fcc7da5c8c17c.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>You can call it the “revenge of the computer scientist.” An algorithm that made headlines for mastering the notoriously difficult Atari 2600 game Montezuma’s Revenge can now beat more games, achieving near perfect scores, and help robots explore real-world environments. Pakinam Amer reports.</p><p>您可以称其为“计算机科学家的报复”。现在，一种算法已经成为掌握极难的Atari 2600游戏《蒙特祖玛的复仇》的头条新闻，现在可以击败更多游戏，获得近乎完美的得分，并帮助机器人探索现实环境。 Pakinam Amer报告。</p><p> Whether you’re a pro gamer or you dip your toes in that world every once in a while, chances are you got stuck while playing a video game once or was even gloriously defeated by one. I know I have. Maybe, in your frustration, you kicked the console a little. Maybe you took it out on the controllers or —if you’re an 80s kid like me —made the joystick pay.</p><p> 无论您是职业游戏玩家还是不时地在这个世界上dip脚趾，都有可能您曾经在玩电子游戏时被卡住，甚至被一次光荣地击败了。我知道我有也许是在您沮丧的情况下，您将控制台稍微踢了一下。也许您是从控制器上拿出来的，或者-如果您是像我这样的80年代的孩子-则是支付操纵杆费用。</p><p>  Now a group of computer scientists from Uber AI are taking revenge for all of us who’ve been in this situation before. Using a family of simple algorithms, tagged “Go-Explore,” they went back and beat some of the most notoriously difficult Atari games whose chunky blocks of pixels and eight-bit tunes had once challenged, taunted and even enraged us. [Adrien Ecoffet et al.,  First return, then explore]</p><p>  现在，一群来自Uber AI的计算机科学家正在为我们所有人一直报仇。他们使用一系列标记为“ Go-Explore”的简单算法，回过头来击败了一些最臭名昭著的难度很大的Atari游戏，这些游戏的块状像素和八位音调曾经挑战，嘲弄甚至激怒了我们。 [Adrien Ecoffet等，先返回，然后探索]</p><p>    But what does revisiting those games from the 80s and 90s accomplish, besides fulfilling a childhood fantasy?</p><p>    但是，重新实现80年代和90年代的那些游戏，除了实现童年时代的幻想外，还能实现什么？</p><p>  According to the scientists, who published their work in  Nature, experimenting with solving video games that require complex, hard exploration gives way to better learning algorithms. They become more intelligent and perform better under real-world scenarios.</p><p>  根据在《自然》杂志上发表论文的科学家的说法，尝试解决需要复杂而艰苦探索的视频游戏，可以让人们更好地学习算法。它们变得更加智能，并在实际情况下表现更好。</p><p>  “One of the nice things of Go-Explore is that it’s not just limited to video games, but that you can also apply it to practical applications like robotics.”</p><p>  “ Go-Explore的优点之一是它不仅限于视频游戏，而且还可以将其应用于机器人等实际应用中。”</p><p>  That was  Joost Huizinga, one of the principal researchers at  Uber AI. Joost developed Go-Explore with  Adrien Ecoffet and other scientists.</p><p>  那就是Uost AI的主要研究人员之一Joost Huizinga。 Joost与Adrien Ecoffet和其他科学家一起开发了Go-Explore。 </p><p>  So how does it actually work? Let’s start with the basics. When AI processes images of the world in the form of pixels, it does not know which changes should count and which should be ignored. For instance, a slight change in the pattern of the clouds in the sky in a game environment is probably unimportant when exploring said game. But finding a missing key certainly is .  But to the AI, both involve changing a few pixels in that world.</p><p>那么它实际上是如何工作的呢？让我们从基础开始。当AI以像素形式处理世界图像时，它不知道哪些变化应该计算，哪些变化应该忽略。例如，在探索所述游戏时，在游戏环境中天空中云的图案的微小变化可能并不重要。但是找到丢失的钥匙肯定是。但是对于AI来说，两者都涉及改变那个世界中的一些像素。</p><p>  This is where deep reinforcement learning comes in. It’s an area of machine learning that helps an agent analyze an environment to decide what matters and which actions count through feedback signals in the form of extrinsic and intrinsic rewards.</p><p>  这就是深度强化学习的用武之地。这是机器学习的一个领域，它可以帮助代理分析环境，以通过外部和内在奖励的形式通过反馈信号来决定重要的事情和哪些行动。</p><p>  “This is something that animals, basically, constantly do. You can imagine, if you touch a hot stove, you immediately get strong negative feedback like ‘Hey, this is something you shouldn’t do in the future.’ If you eat a bar of chocolates, assuming you like chocolates, you immediately get a positive feedback signal like ‘Hey, maybe I should seek out chocolate more in the future.’ The same is true for machine learning. These are problems where the agent has to take some actions, and then maybe it wins a game.”</p><p>  “这基本上是动物不断做的事情。您可以想象，如果您触摸热炉，会立即收到强烈的负面反馈，例如“嘿，这是您将来不应该做的事情。”如果您吃了一块巧克力，假设您喜欢巧克力，您会立即得到积极的反馈信号，例如“嘿，也许将来我应该更多地寻找巧克力。”机器学习也是如此。这些都是代理商必须采取一些行动，然后才可能赢得一场比赛的问题。”</p><p>  Creating an algorithm that can navigate rooms with traps, obstacles to jump over, rewards to collect and pitfalls to avoid, means that you have to create an artificial intelligence that is curious and that can explore an environment in a smart way. This helps it decide what brings it closer to a goal or how to collect hard-to-get treasures.</p><p>  创建一种算法，可以用陷阱，越过障碍物，收集奖励和避免陷阱来导航房间，这意味着您必须创建一个好奇的人工智能，并且可以以一种智能的方式探索环境。这有助于它决定什么使它更接近目标或如何收集难以获得的宝藏。</p><p>  Reinforcement learning is great for that but it isn’t perfect in every situation.</p><p>  强化学习对此非常有用，但并非在每种情况下都是完美的。</p><p>  “In practice, reinforcement learning works very well, if you have very rich feedback—if you can tell, ‘Hey, this move is good, that move is bad, this move is good, that move is bad.’”</p><p>  “在实践中，强化学习非常有效，如果您有非常丰富的反馈意见-如果您能说，‘嘿，这一举动是好的，那一步是不好的，这一步是好的，那一步是坏的。”</p><p>  In Atari games like Montezuma’s Revenge, the game environment offers little feedback, and its rewards can intentionally lead to dead ends. Randomly exploring the space just doesn’t cut it.</p><p>  在诸如蒙特祖玛的复仇之类的Atari游戏中，游戏环境提供的反馈很少，其奖励可能会故意导致死胡同。随意探索太空并不会削减它。 </p><p>    “You could imagine, and this is especially true in video games like Montezuma’s Revenge, that sometimes you have to take a lot of very specific actions—you have to dodge hazards, jump over enemies—you can imagine that random actions like,‘Hey, maybe I should jump here,’ in this new place, is just going to lead to a ‘Game Over’ because that was a bad place to jump—especially if you’re already fairly deep into the game. So let’s say you want to explore level two: if you start taking random actions in level one and just randomly dying, you’re not going to make progress on exploring level two.”</p><p>“您可以想象，在蒙特祖玛的复仇之类的视频游戏中尤其如此，有时您必须采取许多非常具体的动作-您必须躲避危险，跳过敌人-您可以想象随机动作，例如，'嘿，也许我应该跳到这里，”在这个新地方，这将导致“游戏结束”，因为那是一个不好的地方，尤其是如果您已经相当了解游戏的话。因此，假设您想探索第二级：如果您开始在第一级中随机采取行动而只是随机死亡，那么您在第二级探索中将不会取得进展。”</p><p>  You can’t rely on “intrinsic motivation” alone, which, in the context of artificial intelligence, typically comes from exploring new or unusual situations.</p><p>  您不能仅仅依靠“内在动机”，就人工智能而言，它通常来自于探索新的或不寻常的情况。</p><p>  “Let’s say you have a robot, and it can go left into the house and right into the house. Let’s say at first it goes left, it explores left, meaning that it gets this intrinsic reward for a while. It doesn’t quite finish exploring left, and at some point, the episode ends, and it starts anew in the starting room. This time it goes right. It goes fairly far into the room on the right; it doesn&#39;t quite explore it. And then it goes back to the starting room. Now the problem is because it has gone both left and right, and basically it’s already seen the start, it no longer gets as much intrinsic motivation from going there.”</p><p>  “比方说，您有一个机器人，它可以左走进屋子，然后右走进屋子。假设一开始它向左走，向左探索，这意味着它会暂时获得这种内在奖励。它并没有完全向左探索，并且在某个时刻结束了情节，并在起居室重新开始。这次是正确的。它进入右边的房间相当远。它并没有完全探索它。然后回到起始房间。现在的问题是，它已经左右走了，而且基本上已经有了起步，因此不再从那里获得太多内在动力。”</p><p>    Detaching from a place that was previously visited after collecting a reward doesn’t work in difficult games, because you might leave out important clues.</p><p>    在获得奖励之后，从先前曾去过的地方撤离在困难的游戏中是行不通的，因为您可能会遗漏重要的线索。</p><p>  Go-Explore goes around this by  not rewarding some actions, such as going somewhere new. Instead it encourages “sufficient exploration” of a space, with no or little hints, by enabling its agent to explicitly “remember” promising places or states in a game.</p><p>  Go-Explore通过不奖励某些动作来解决此问题，例如去新的地方。相反，它使代理能够明确地``记住''游戏中有希望的位置或状态，从而鼓励对空间进行``足够的探索''而没有或几乎没有提示。</p><p>    Once the agent keeps a record of that state, it can then reload it and intentionally explore—what Adrien and Joost call, the “first return, then explore” principle.</p><p>    一旦代理保存了该状态的记录，便可以重新加载该状态并有意探索-Adrien和Joost所说的“先返回再探索”原则。</p><p>  According to Adrien, leaning on another form of learning called imitation learning, where agents can mimic human tasks, their AI can go a long way, especially in the field of robotics.</p><p>  根据阿德里安（Adrien）的看法，他依靠另一种学习形式，即模仿学习，即代理可以模仿人工任务，他们的AI可以走很长一段路，特别是在机器人技术领域。 </p><p>  “You have a difference between the world that you can train in and the real world. So one example would be if you’re doing robotics: You know, in robotics, it’s possible to have simulations of your robotics environments. But then, of course, you want your robot to run in the real world, right? And so what can you do, then? If you’re in a situation like that, of course, the simulation is not exactly the same as the environment, so just having something that works in simulation is not necessarily sufficient. We show that in our work. What we’re doing is that we’re using existing algorithms that are called ‘imitation learning.’ And what it is is it just takes an existing solution to a problem and just makes sure that you can reliably use that solution even when, you know, there are slight variations in your environment, including, you know, it being the real world rather than a simulation.”</p><p>“您可以训练的世界与现实世界之间存在差异。因此，举一个例子，如果您正在使用机器人技术：您知道，在机器人技术中，可以对您的机器人环境进行仿真。但是，当然，您想让机器人在现实世界中运行，对吗？那你该怎么办呢？当然，如果您处在这种情况下，则模拟与环境并不完全相同，因此仅具有可以在模拟中起作用的功能并不一定足够。我们在工作中证明了这一点。我们正在做的是我们正在使用被称为``模仿学习''的现有算法。它是什么只是将现有的解决方案用于问题，并确保即使在您遇到问题时也可以可靠地使用该解决方案。知道，您的环境会有细微的变化，包括，它是真实世界而不是模拟。”</p><p>  Adrien and Joost say their model’s strength lies in its simplicity. It can be adapted and expanded easily into real-life applications such as language learning or drug design.</p><p>  Adrien和Joost说，他们模型的优势在于其简单性。它可以适应并轻松扩展到实际应用中，例如语言学习或药物设计。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.scientificamerican.com/podcast/episode/gamer-machine-learning-vanquishes-old-school-atari-games/">https://www.scientificamerican.com/podcast/episode/gamer-machine-learning-vanquishes-old-school-atari-games/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>