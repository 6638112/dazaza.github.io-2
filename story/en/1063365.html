<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>分布式云为每个人构建 Distributed Cloud Builds for Everyone</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Distributed Cloud Builds for Everyone<br/>分布式云为每个人构建 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-04 04:14:46</div><div class="page_narrow text-break page_content"><p>CPU cycles are cheaper than they have ever been, and cloud computing has never been more ubiquitous. All the major cloud providers offer generous free tiers, and services like GitHub Actions offer free compute resources to open-source repositories. So why do so many developers still build software on their laptops?</p><p>CPU周期比以往任何时候都便宜，云计算从未如此无处不在。所有主要的云提供商都提供宽大的免费层，以及GitHub操作等服务为开源存储库提供免费计算资源。那么为什么这么多开发人员仍然在笔记本电脑上建立软件？</p><p> Despite the embarrassment of riches of cheap or even free cloud compute, most projects I know of, and most developers, still do most of their software development — building and running code — directly on their local machines. Cloud builds — be they distributed, or just running on a single giant instance larger than the machine in front of you — are relatively common inside large, sophisticated organizations, but rare for individual developers, open-source project, or smaller operations.</p><p> 尽管有丰富的廉价甚至是免费云计算的尴尬，但我所知道的大多数项目以及大多数开发人员仍然可以直接在其本地机器上进行大多数软件开发 - 建设和运行代码。云构建 - 他们分发，或者只是在大于你前面的机器的单个巨型实例上运行 - 在大型，复杂的组织内部相对普遍，但为单个开发人员，开源项目或较小的操作罕见。</p><p> Recently, I blogged about  building LLVM in 90 seconds using AWS Lambda and my Llama project. Today I want to share the larger vision behind Llama’s design, and why I believe Llama’s approach could finally make distributed cloud builds ubiquitously accessible, even to smaller open-source projects and individual hobbyist developers.</p><p> 最近，我在使用AWS Lambda和我的骆驼项目中在90秒内扮演LLVM。今天我想分享Llama设计背后的更大愿景，为什么我认为Llama的方法最终可以使分布式云建立普遍访问，甚至更小的开源项目和个人爱好者开发人员。</p><p> I believe that Llama’s design — enabled in large part by AWS Lambda — can remove the most significant barriers to entry to using cloud compilation. With more maturity and implementation work, I think it has the potential to make cloud builds almost the default for a wide range of developers, and I’m incredibly excited about that future.</p><p> 我相信骆驼的设计 - 由AWS Lambda大部分启用 - 可以消除最重要的障碍，以使用云编译。通过更新的成熟和实施工作，我认为它有可能使云构建几乎是各种开发人员的默认值，而且我对此未来非常兴奋。</p><p> Let’s take a look at the features and design decisions that make this possible.</p><p> 让我们来看看这样做的功能和设计决策。</p><p>  Llama is cheap to use, and costs virtually nothing when you’re not actively running a build.</p><p>  骆驼使用便宜，并且在您没有积极运行建筑时，几乎没有成本。</p><p> My LLVM build cost around $0.40. If you were using ec2 directly, forty cents buys you about 10 minutes of on-demand compute on a  96-core c5 instance. I haven’t benchmarked, I estimate that that instance can likely build LLVM in about 3 minutes. On paper that makes Llama up to 3x more expensive … but only if you manage to start and stop the ec2 instance  exactly when you need it. If you were to leave up that instance while idle even for an hour, it would be incredibly hard to ever catch up to Llama. Llama, on the other hand, automatically scales down when you’re not doing a build, and Lambda’s impressive cold-start performance means it’s always there when you need it.</p><p> 我的LLVM Build成本约为0.40美元。如果您使用EC2直接使用EC2，请在96核心C5实例上为您提供大约10分钟的按需计算。我没有基准测试，我估计该实例可能会在大约3分钟内构建LLVM。在纸上使Llama能够更加昂贵3次...但只有当您设法启动和停止EC2实例时，只需在您需要时才。如果你要留下那个例子，而闲置甚至一个小时，那么赶到骆驼才能难以置信。另一方面，当您没有进行构建时，骆马会自动缩放，而Lambda令人印象深刻的冷启动性能意味着当您需要时始终存在。 </p><p> Furthermore, Lambda transparently scales to intermediate levels of utilization. If you do an incremental rebuild that only builds, say, 10% of the source files, your cost with Llama will be about 10% of the overall build, with no additional tuning or configuration.</p><p>此外，Lambda透明地缩放到中间利用水平。如果您只有仅构建的增量重建，例如源文件的10％，则Llama的成本将是整体构建的10％，没有额外的调整或配置。</p><p> So Llama is not only price-competitive with ec2’s best case, but is much simpler for developers to manage, and removes the risk of leaving an instance running and being stuck with a $3000 bill (the rough monthly cost of that c5 instance).</p><p> 因此，骆马不仅与EC2最佳案例有竞争力，而且开发人员更简单地管理，并消除离开实例的风险并用3000美元的账单（C5实例的粗糙的月费）陷入困境。</p><p> Llama always costs (nearly) nothing when you aren’t running a build. If you only do one build a month, you’ll still only pay a few cents a month. If you keep a suspended c5 instance around, by comparison, you’ll probably use an EBS disk, which costs 8¢/GB-mo. That cost can add up on its own if you keep large build trees around, but more importantly it creates cognitive overhead for infrequent users: when is it worth keeping your instance around in case you need to do more builds, and when is it worth turning it off to save a few bucks, at the cost of higher startup time and configuration cost the next time you want it?</p><p> 当你没有运行构建时，骆马总是成本（几乎）没有任何东西。如果你只做一个月，你仍然只支付每月几美分。如果通过比较，将保留暂停的C5实例，您可能会使用EBS磁盘，费用为8¢/ gb-mo。如果您围绕大型构建树，则可以自行加起来，但更重要的是，它更重要地为不常见的用户创造了认知开销：何时值得保留您的实例，以防您需要做更多的构建，并且何时值得转换它可以节省一些雄鹿，以更高的启动时间和配置成本下次想要它的成本</p><p>  If you’re using a cloud instance to build your software, somehow you have to get your code onto it. The two most obvious choices are:</p><p>  如果您正在使用云实例来构建软件，请以某种方式您必须将代码达到它。两个最明显的选择是：</p><p> Keep your source checkout remote, and develop using  ssh or something like  VS Code’s Remote Development</p><p> 保持源结账远程，并使用SSH或类似VS代码的远程开发的内容开发</p><p> Keep your checkout locally, and use  rsync or similar to copy source into the cloud, and artifacts back to your laptop.</p><p> 在本地保持结账，并使用rsync或类似于将源复制到云中，并将工件返回到笔记本电脑。</p><p> Either option imposes a change to your workflow, requiring you to think about the local and remote machines separately, and split your work, to at least some extent, between two machines. And if you do the first option, you additionally have to pay for the dev machine while you’re just doing development or reading code, even when you’re not doing any builds.</p><p> 任何一种选项都会对您的工作流程施加了更改，要求您单独思考本地和远程计算机，并将工作分配到两种机器之间至少一定程度。如果您执行第一个选项，即使您没有执行任何构建，您也必须在执行开发或阅读代码时支付DEV机器。 </p><p> By contrast, Llama leverages S3 and file-level caching to move individual files and artifacts between your workstation and Lambda transparently and on-demand. You can continue developing locally and running  make in exactly the same way you would without Llama. Everything works exactly the same, except your builds are faster!</p><p>相比之下，LLAMA利用S3和文件级缓存，以透明地和按需移动工作站和Lambda之间的各个文件和伪影。您可以在本地开发，并以完全相同的方式运行，您可以在没有骆驼的情况下完全相同。一切正常工作完全相同，除了你的构建更快！</p><p> Llama’s approach, to be explicit, does come with some downsides. It is somewhat slower than systems which require fewer network round-trips, and it adds complexity to the implementation. It also makes builds dependent on your workstation’s network speed; builds from a slow wireless connection may end up bottlenecked on bandwidth instead of compute. Nonetheless, I strongly believe in this choice, because I think there’s enormous power in presenting a completely transparent user experience, and asking for as small a behavior change from the user as possible.</p><p> 骆驼的方法是明确的，有一些缺点。它比需要较少的网络往返更少的系统慢，它会增加对实现的复杂性。它还使构建取决于您的工作站的网络速度;从慢速无线连接构建可能最终在带宽而不是计算上的瓶颈。尽管如此，我强烈地相信这一选择，因为我认为呈现完全透明的用户体验，并要求尽可能小的行为改变了巨大的力量。</p><p>  Some build systems — most notably  Google’s Bazel — natively support transparent remote build execution. However, accessing this feature requires you to rewrite your project’s entire build in Bazel (in addition to the cost and complexity of running or finding a remote build executor). This is a fairly sizeable task, since Bazel is quite strict and very opinionated. And, even if you succeed, few of your users will already have Bazel installed, and its heavyweight dependencies will scare off many potential users and contributors.</p><p>  一些构建系统 - 最常见的谷歌的Bazel  - 本身支持透明的远程构建执行。但是，访问此功能要求您在Bazel中重写您的项目整个构建（除了运行或查找远程构建执行程序的成本和复杂性）。这是一个相当相当大的任务，因为Bazel非常严格，非常自以为是。而且，即使您取得成功，您的少数用户也将安装Bazel，其重量级依赖项将危害许多潜在的用户和贡献者。</p><p> Llama uses the classic  distcc paradigm of providing a drop-in replacement ( llamacc) for the compiler (  gcc or  clang), and relying on the build system’s own parallelism  1. This approach lets it support virtually every C and C++ build system out there, out of the box, with minimal fuss.</p><p> Llama使用Classic DistCC范例来为编译器（GCC或Clang）提供替换（LLAMACC），并依赖于构建系统自己的并行性1.此方法允许它在那里几乎所有C和C ++构建系统，开箱即用，大惊小怪。</p><p> It might be more performant to specialize for, say,  cmake or  ninja — and it may still make sense to do so in the future — but here as well, I have optimized in Llama’s design for broad adoptability and low barriers to entry for new projects and new developers.</p><p> 专门化，比如cmake或ninja可能更加表现 - 在未来可能仍然有意义 - 但在这里，我也在骆驼的设计中优化了广泛的采用和新项目的低障碍和新的开发人员。</p><p>  I’ve tried to design Llama to require as little configuration as possible; the ultimate goal is that, with one or two commands, you can set up Llama with a cloud compiler environment mirroring your local desktop, and use that image across any and all of your projects.</p><p>  我试图设计Llama需要尽可能少的配置;最终目标是，使用一个或两个命令，您可以使用镜像本地桌面的云编译环境设置Llama，并在任何和所有项目中使用该图像。</p><p> This feature is enabled in large part by AWS and by the technologies of infrastructure-as-code, which lets me define a single  CloudFormation template that sets up all of Llama’s major dependencies in one fell swoop. I’ve also provided tooling to help build compiler toolchains and manage system headers between local nodes and the cloud environment, but that’s another place where Llama could be even more polished. AWS Lambda’s recent Docker support — announced while I was first building Llama — helps a lot here, letting Llama developers and users use standardized and widely-understood Docker images and Dockerfiles to build compiler images.</p><p> AWS和基础架构的技术在很大程度上通过AWS和基础架构的技术启用了此功能，这让我定义了一个单个CloudFormation模板，该模板在一次下降时设置所有Llama的主要依赖项。我还提供了工具，以帮助构建编译器工具链和管理本地节点和云环境之间的系统标题，但这是Llama可能更加抛光的另一个地方。 AWS Lambda最近的Docker支持 - 在我第一次建造Llama的时候宣布 - 在这里帮助，让Llama开发人员和用户使用标准化和广泛理解的Docker图像和Dockerfiles来构建编译器图像。 </p><p>  For decades now, technologists have talked about the vision of  “utility computing” — computing as a metered, on-demand service, in much the same way as we think of power or running water as a utility.</p><p>几十年来，技术人员谈到了“效用计算” - 计算为计量，按需服务的愿景，与我们认为电力或运行水作为实用性的方式。</p><p> The modern cloud era has been hailed by many as the realization of this vision, in which we rent computing an hour-at-a-time (and more recently,  a second-at-a-time), and virtually no one owns their own hardware at scale any more.</p><p> 现代云时代被许多人作为实现这一愿景的认识，其中我们租用时间（最近，最近，第二次），几乎没有人拥有他们的在比例下拥有自己的硬件。</p><p> However, cloud environments remain fairly involved to configure and to operate, and tend to demand a lot of expertise of their users. For me, the vision of utility computing can never be complete until the act of running computation — using the full power and scale of the cloud, at least as far as you’re willing to pay — is about as easy and worry-free as turning on the faucet in your home for fresh water.</p><p> 但是，云环境仍然相当涉及配置和运营，并倾向于要求用户的大量专业知识。对我来说，在运行计算的行为 - 使用云的全部功率和比例之前，效用计算的愿景永远不会完整，至少就你愿意付钱 - 大约是简单而无忧无虑的打开你家的龙头进行淡水。</p><p> Llama is my attempt to work towards this vision for — at the least — the specific problem of software builds. Thanks to Amazon Lambda, and the general advancement of cloud technologies, I believe we are finally within reach of concretely achieving this vision, and I’m hopeful we can make it so.</p><p> 骆驼是我努力实现这一愿景的 - 至少 - 软件构建的具体问题。谢谢亚马逊兰德，以及云技术的一般进步，我相信我们终于达到了具体实现这一愿景，我希望我们能够做到这一点。</p><p> That said, to be clear, Llama doesn’t yet realize this vision entirely. It’s a young project, with few users yet other than me, and lots of sharp edges and rough corners. However, I think results like my LLVM build performance — where it outperforms some of the largest machines money can buy — serve as strong evidence that the approach is feasible and can succeed if we push on it.</p><p> 也就是说，要清楚，骆马还没有完全实现这一愿景。这是一个年轻的项目，少数用户还有我，而且很多锋利的边缘和粗糙的角落。但是，我认为结果如我的LLVM构建性能 - 在那里优于一些最大的机器可以买到的东西可以买到这一方法，即这种方法是可行的，如果我们推动它就可以成功。</p><p>  I’ve talked about software builds here because that’s a domain I’m deeply familiar with, and because that’s the one I’ve chosen to tackle with  llamacc. However, I think the vision here can go well beyond building software. I imagine a world where  most compute-intensive tasks that are performed interactively are seamlessly outsourced to the cloud on-demand, while preserving a user experience  as though all of your data remained available locally.</p><p>  我已经谈到了这里的软件在这里构建，因为这是一个域，我对我很熟悉，而且因为这是我选择用LALLLEACC解决的那个。但是，我认为这里的愿景可以超越构建软件。我想到了一个世界间交互方式执行的大多数计算密集型任务，无缝地将云点燃，同时保留用户体验，好像所有数据都在本地可用。</p><p> As one other concrete example of feasibility, prior to the  gg paper, the same team demonstrated  ExCamera, which used Lambda in a similar fashion to perform video editing and transcoding. Between that project,  gg, and Llama, I feel confident that we’ve demonstrated that vision has broad feasibility; now we just need to finish building it!</p><p> 作为可行性的另一个具体示例，在GG纸之前，同一团队展示了Excamera，它以类似的方式使用Lambda来执行视频编辑和转码。在该项目之间，GG和Llama之间，我觉得我们已经证明了愿景具有广泛的可行性;现在我们只需要完成建造它！ </p><p> Especially in today’s multicore world, any build system worth talking about has some means to run builds using multiple cores, which is regularly exercised by developers who build on top of it.  ↩︎</p><p>特别是在当今的多夜世界中，任何值得谈论的构建系统都有一些方法可以使用多个核心运行构建，这些内容经常由构建在其顶部的开发人员进行。 ↩︎ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.nelhage.com/post/distributed-builds-for-everyone/">https://blog.nelhage.com/post/distributed-builds-for-everyone/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/分布式/">#分布式</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cloud/">#cloud</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/构建/">#构建</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>