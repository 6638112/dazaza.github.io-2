<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>人工神经网络最终为大脑学习提供了线索 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">人工神经网络最终为大脑学习提供了线索 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-21 07:51:00</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/e2c55484d9e80631288d70c54a7b46a8.jpg"><img src="http://img2.diglog.com/img/2021/2/e2c55484d9e80631288d70c54a7b46a8.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>在2007年，深层神经网络背后的一些主要思想家在著名的人工智能年度会议的间隙组织了一次非正式的“卫星”会议。会议拒绝了他们举行正式讲习班的要求；深度神经网络距离接管人工智能还需要几年的时间。盗版会议的最终发言人是多伦多大学的杰弗里·欣顿（Geoffrey Hinton），他是认知心理学家和计算机科学家，负责深网领域的一些最大突破。他打趣道：“所以，大约一年前，我回到家吃晚饭，然后我说：'我想我终于弄清了大脑的工作原理，'而我15岁的女儿说，'哦，爸爸，不再。”</p><p> 观众笑了。 Hinton继续说道：“所以，这就是它的工作原理。”随之而来的是更多的笑声。</p><p> 欣顿的笑话掩盖了一个严肃的追求：使用AI来了解大脑。如今，深网统治AI的部分原因是一种称为反向传播（backpropagation）或反向传播（backprop）的算法。该算法使深层网络能够从数据中学习，使它们能够对图像进行分类，识别语音，翻译语言，了解自动驾驶汽车的路况并完成许多其他任务。</p><p> 但是真正的大脑极不可能依靠相同的算法。蒙特利尔大学的计算机科学家，魁北克人工智能研究所的科学主任Yoshua Bengio表示，这不仅是“大脑能够比最先进的AI系统进行概括和更好，更快地学习，”以及2007年研讨会的组织者之一。由于多种原因，反向传播与大脑的解剖结构和生理学不兼容，尤其是在皮质中。</p><p>  Bengio和许多受Hinton启发的人一直在思考更合理的生物学学习机制，这些机制至少可以与反向传播的成功相提并论。其中三个-反馈对齐，平衡传播和预测编码-已显示出特别的希望。一些研究人员还将某些类型的皮层神经元的属性和过程（例如注意力）纳入其模型。所有这些努力使我们更加了解大脑中可能起作用的算法。</p><p> “大脑是一个巨大的谜。人们普遍认为，如果我们能够解开其中的某些原则，那么对人工智能可能会有所帮助。” “但是它本身也具有价值。”</p><p>  几十年来，神经科学家关于大脑学习方式的理论主要是由1949年由加拿大心理学家唐纳德·赫布（Donald Hebb）引入的规则指导的，该规则通常被解释为“神经元一起发射，连接在一起”。也就是说，相邻神经元的活动越相关，它们之间的突触连接越强。对该原则进行了一些修改，成功地解释了某些有限类型的学习和视觉分类任务。 </p><p>但是，对于必须从错误中学习的大型神经元网络，它的效果要差得多。网络内部的神经元没有直接针对性的方法来了解发现的错误，更新自身并减少错误。斯坦福大学的计算神经科学家和计算机科学家丹尼尔·亚明斯（Daniel Yamins）说：“ Hebbian规则是使用错误信息的一种非常狭窄，特殊且不太敏感的方法。”</p><p> 然而，这是神经科学家拥有的最好的学习规则，甚至在它支配神经科学之前，它就启发了1950年代后期第一个人工神经网络的发展。这些网络中的每个人工神经元都接收多个输入并产生输出，就像它的生物学对应物一样。神经元将每个输入乘以所谓的“突触”权重（一个数字表示分配给该输入的重要性），然后对加权的输入求和。这是神经元的输出。到1960年代，很明显，可以将此类神经元组织成具有输入层和输出层的网络，并且可以训练人工神经网络来解决特定类别的简单问题。在训练期间，神经网络为其神经元确定最佳权重，以消除或最小化错误。</p><p>  但是，即使在1960年代，很明显，解决更复杂的问题也需要在输入和输出层之间夹一层或多层“隐藏”的神经元。没有人知道如何有效地训练带有隐藏层的人工神经网络-直到1986年，当欣顿，已故的大卫·鲁梅尔哈特（David Rumelhart）和罗纳德·威廉姆斯（Ronald Williams）（现为东北大学）出版了反向传播算法。</p><p> 该算法分为两个阶段。在“转发”阶段，当为网络提供输入时，它将推断出输出，这可能是错误的。第二个“后退”阶段更新突触权重，使输出更符合目标值。</p><p> 要理解此过程，可以考虑一个“损失函数”，该函数将所推断出的输出与所需输出之间的差异描述为丘陵和山谷的景观。当网络根据给定的一组突触权重进行推断时，它最终会出现在损失格局中的某个位置。要学习，它需要沿着坡度或坡度朝着某个山谷移动，在该山谷中，损耗要尽可能地小。反向传播是一种更新突触权重以降低该梯度的方法。</p><p> 本质上，该算法的后退阶段计算每个神经元的突触权重对误差的贡献程度，然后更新这些权重以改善网络的性能。该计算从输出层到输入层顺序地向后进行，因此命名为反向传播。一遍又一遍地进行输入和期望输出的设置，最终您将获得整个神经网络可接受的一组权重。</p><p>  反向传播的发明立即引起了一些神经科学家的强烈抗议，他们说这不可能在真正的大脑中起作用。最著名的反对者是弗朗西斯·克里克（Francis Crick），他是获得诺贝尔奖的DNA结构的共同发现者，后来成为神经科学家。克里克（Crick）在1989年写道：“就学习过程而言，大脑不太可能实际使用反向传播。” </p><p>反向传播被认为是生物学上令人难以置信的，原因有几个。首先，虽然计算机可以很容易地在两个阶段中实现该算法，但对于生物神经网络而言，这样做并非易事。第二个是计算神经科学家所说的权重传递问题：反向传播算法会复制或“传递”有关推理中所有突触权重的信息，并更新这些权重以提高准确性。但是在生物网络中，神经元只能看到其他神经元的输出，而看不到形成该输出的突触权重或内部过程。从神经元的角度来看，“知道自己的突触权重是可以的，” Yamins说。 “不能让您知道其他一些神经元的突触权重。”</p><p>  任何生物学上合理的学习规则也必须遵守以下限制：神经元只能访问邻近神经元的信息；反向传播可能需要来自更多远程神经元的信息。因此，“如果对这封信采取反向支持，大脑似乎无法进行计算，”本吉奥说。</p><p> 但是，欣顿和其他一些人立即接受了研究生物学上可行的反向传播变异的挑战。宾夕法尼亚大学的计算神经科学家Konrad Kording说：“第一篇争论大脑进行反向传播的论文与反向传播的历史差不多。”在过去的十年左右的时间里，由于人工神经网络的成功带领他们在人工智能研究中占主导地位，因此为寻找反向传播器寻找生物学等效物的努力也在不断加强。</p><p>  例如，举重问题最奇怪的解决方案之一，由伦敦Google DeepMind的Timothy Lillicrap和他的同事在2016年提供。他们的算法不是依赖于向前传递记录的权重矩阵，而是使用用随机值初始化的矩阵用于后向传递。赋值后，这些值就永远不会改变，因此无需为每次后退传递权重。</p><p> 几乎每个人都惊讶，该网络了解到。由于用于推理的前向权重随每个后向传递而更新，因此网络仍会通过不同的路径降低损耗函数的梯度。正向权重将自己与随机选择的向后权重缓慢对齐，以最终产生正确的答案，从而将算法命名为：反馈对齐。</p><p> Yamins说：“事实证明，这实际上并没有您想象的那么糟糕，”至少在一些简单的问题上。对于大规模问题和具有更多隐藏层的更深层网络，反馈对齐不如反向传播那么好：由于前向权重的更新每次传递的准确性都低于真实反向传播信息的准确性，因此需要花费很多时间更多数据来训练网络。</p><p>  研究人员还探索了在保持经典的Hebbian学习要求（神经元仅对它们的本地邻居做出反应）的同时匹配反向传播器性能的方法。反向传播可以认为是一组神经元进行推理，而另一组神经元进行计算以更新突触权重。 Hinton的想法是研究每个神经元都进行两组计算的算法。本吉奥说：“基本上，这就是杰夫在2007年的讲话。” </p><p>Bengio团队在Hinton的工作基础上，于2017年提出了一条学习规则，该规则要求神经网络具有经常性的联系（也就是说，如果神经元A激活神经元B，则神经元B依次激活神经元A）。如果给这样的网络一些输入，它将设置网络回响，因为每个神经元都会对其直接邻居的推拉做出响应。</p><p> 最终，网络达到一种状态，在这种状态下，神经元与输入端以及彼此之间处于平衡状态，并产生输出，这可能是错误的。该算法然后将输出神经元推向所需结果。这设置了另一个信号，该信号通过网络向后传播，从而引起了类似的动态变化。网络找到了新的平衡点。</p><p> Bengio说：“数学的优点在于，如果您比较这两种配置，则在进行微调之前和之后，您将获得找到梯度所需的所有信息。”训练网络只需要简单地在大量标记数据上反复重复此“平衡传播”过程。</p><p>  神经元只能通过对局部环境做出反应才能学习的约束条件也在新的大脑感知理论中得到表达。爱丁堡大学（University of Edinburgh）博士生，苏塞克斯大学（Sussex University）客座研究员Beren Millidge及其同事一直在将这种新的感知观点称为预测编码，并与反向传播的要求相协调。 Millidge说：“预测编码，如果以某种方式进行设置，将为您提供生物学上合理的学习规则。”</p><p> 预测编码假定大脑不断对感觉输入的原因进行预测。该过程涉及神经处理的分层层。为了产生一定的输出，每一层都必须预测下一层的神经活动。如果最高层希望看到一张脸，则可以预测下面这一层的活动可以证明这种看法是正确的。下一层对下一层的预期做出类似的预测，依此类推。最低层可以预测实际的感觉输入，例如，落在视网膜上的光子。通过这种方式，预测从较高的层流到较低的层。</p><p>  但是，错误可能会在层次结构的每个级别上发生：层对预期输入的预测与实际输入之间的差异。最底层根据收到的感官信息调整其突触权重，以最大程度地减少错误。这种调整会导致新近更新的最低层与上面的层之间出现错误，因此高层必须重新调整其突触权重以最大程度地减少其预测误差。这些错误信号会向上波动。网络来回移动，直到每一层都将其预测误差降至最低。</p><p> Millidge表明，通过适当的设置，预测编码网络可以收敛于与反向传播几乎相同的学习梯度。他说：“您可以非常，非常，非常接近反向传播器的梯度。” </p><p>但是，对于传统的反向传播算法在深度神经网络中进行的每一次反向传播，预测编码网络都必须迭代多次。这在生物学上是否合理，取决于它在真实大脑中可能需要花费多长时间。至关重要的是，在外部世界的输入改变之前，网络必须收敛于解决方案。</p><p> “不能像‘我有一只老虎在向我跳跃，让我在大脑中来回往复进行100次迭代，” Millidge说。他说，尽管如此，如果某些误差是可以接受的，则预测编码可以迅速得出普遍有用的答案。</p><p>  一些科学家承担了根据单个神经元的已知特性来构建类似反向传播模型的艰巨任务。标准神经元具有树突，该树突从其他神经元的轴突收集信息。树突将信号传输到神经元的细胞体，信号在那里被整合。这可能会或可能不会导致尖峰或动作电位在神经元的轴突上突触后突触神经元的树突。</p><p> 但是，并非所有神经元都具有这种结构。特别是，锥体神经元（皮质中神经元类型最丰富的类型）截然不同。金字塔形神经元具有树状结构，具有两组不同的树突。树干伸直并分支成顶端的树突。根向下并分支成基底树突。</p><p>  由Kording于2001年独立开发的模型，以及最近由McGill大学的Blake Richards和魁北克人工智能研究所及其同事开发的模型表明，锥体神经元可以通过同时进行正向和反向计算来形成深度学习网络的基本单元。 。关键在于分离进入神经元的信号，以进行向前的推理和向后的错误，这可以在模型中分别由基础和顶端树突处理。这两个信号的信息都可以编码为神经元沿着其轴突向下发送的电活动峰值。</p><p> 理查兹说：在理查兹团队的最新工作中，“我们已经达到了这样的程度：可以证明，使用相当逼真的神经元模拟，您可以训练锥体神经元网络来完成各种任务。” “然后使用这些模型的稍微抽象的版本，我们可以获得金字塔神经元网络，以学习人们在机器学习中执行的困难任务。”</p><p>  对于使用反向传播的深层网络的一个隐含要求是“教师”的存在：可以计算神经元网络所产生的误差的东西。但是“大脑中没有老师告诉运动皮层中的每个神经元，‘您应该被打开，您应该被关闭，”阿姆斯特丹荷兰神经科学研究所的彼得·罗尔夫塞玛说。 </p><p>Roelfsema认为大脑对问题的解决方案正在关注中。在1990年代后期，他和他的同事们发现，当猴子将视线固定在一个物体上时，代表该物体在皮层中的神经元变得更加活跃。猴子集中注意力的行为会产生负责神经元的反馈信号。 “这是一个高度选择性的反馈信号，” Roelfsema说。 “这不是错误信号。这只是对所有这些神经元说的：您将要为[行为]负责。”</p><p> Roelfsema的见解是，当与某些其他神经科学发现中揭示的过程相结合时，这种反馈信号可以实现类似反向道具的学习。例如，剑桥大学的沃尔夫拉姆·舒尔茨（Wolfram Schultz）等人的研究表明，当动物做出比预期效果更好的动作时，大脑的多巴胺系统就会被激活。 Roelfsema说：“神经调节剂淹没了整个大脑。”多巴胺水平起着整体增强信号的作用。</p><p> 从理论上讲，注意反馈信号只能通过更新突触权重来启动负责响应整体增强信号的神经元。他和他的同事已经使用这种想法来构建一个深度神经网络并研究其数学特性。事实证明，您会得到错误的反向传播。您得到的方程基本相同，”他说。 “但是现在它在生物学上变得合理了。”</p><p> 该小组在12月的神经信息处理系统在线会议上介绍了这项工作。 “我们可以训练深层网络，” Roelfsema说。 “它只比反向传播慢两到三倍。”他说：“因此，它超越了所有其他在生物学上似乎可行的算法。”</p><p>  然而，关于活着的大脑使用这些合理机制的具体经验证据仍然难以捉摸。 “我认为我们仍然缺少一些东西，”本吉奥说。 “以我的经验，这可能是一件小事，可能会与现有方法中的一种发生一些曲折，这确实会有所作为。”</p><p> 同时，Yamins和他在斯坦福大学的同事对如何确定所提议的学习规则中的哪一项是正确的提出了建议。通过分析1,056个实现不同学习模型的人工神经网络，他们发现可以根据一段时间后神经元子集的活动来识别控制网络的学习规则类型。此类信息可能会从猴子的大脑记录下来。 Yamins表示：“事实证明，如果您具有正确的可观测量集合，则可能会提出一个相当简单的方案，使您能够确定学习规则。”</p><p> 有了这样的进步，计算神经科学家就悄悄地乐观了。科尔丁说：“大脑进行反向传播的方式有很多。” “进化真是太棒了。反向传播很有用。我认为这种进化可以使我们到达那里。” </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/">https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/neural/">#neural</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/神经元/">#神经元</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1046408.html"><img src="http://img2.diglog.com/img/2021/1/thumb_317f246d11fd5d36288c9d819adaf92d.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1046408.html">麻省理工学院的研究人员开发了一种新的“液态”神经网络，该网络更能适应新信息 </a></div><span class="my_story_list_date">2021-1-28 23:46</span></div><div class="col-sm"><div><a target="_blank" href="/story/1046027.html"><img src="http://img2.diglog.com/img/2021/1/thumb_cf8c135f0462753e65a7d5bf04d5322e.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1046027.html">为神经网络通用理论构建的基础（2019） </a></div><span class="my_story_list_date">2021-1-24 3:38</span></div><div class="col-sm"><div><a target="_blank" href="/story/1044723.html"><img src="http://img2.diglog.com/img/2021/1/thumb_8a999583a5d03c50c33d2464bc592ddb.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1044723.html">在水手游戏上训练的类人神经网络国际象棋引擎 </a></div><span class="my_story_list_date">2021-1-17 22:6</span></div><div class="col-sm"><div><a target="_blank" href="/story/1043403.html"><img src="http://img2.diglog.com/img/2021/1/thumb_9391322688e1b978cbf8e2bd9254eb7c.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1043403.html">OpenAI多模式研究 </a></div><span class="my_story_list_date">2021-1-6 21:18</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>