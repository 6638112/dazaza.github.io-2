<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>一个EPYC逃生：案例研究KVM突破 An EPYC escape: Case-study of a KVM breakout</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">An EPYC escape: Case-study of a KVM breakout<br/>一个EPYC逃生：案例研究KVM突破 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-30 01:39:17</div><div class="page_narrow text-break page_content"><p>KVM (for Kernel-based Virtual Machine) is the de-facto standard hypervisor for Linux-based cloud environments. Outside of Azure, almost all large-scale cloud and hosting providers are running on top of KVM, turning it into one of the fundamental security boundaries in the cloud.</p><p>KVM（基于内核的虚拟机）是基于Linux的云环境的De-Facto标准虚拟机管理程序。在Azure之外，几乎所有大型云和托管提供商都在KVM之上运行，将其转化为云中的一个基本安全边界之一。</p><p>    In this blog post I describe a   vulnerability  in KVM’s AMD-specific code and discuss how this bug can be turned into a full virtual machine escape.  To the best of my knowledge, this is the first public writeup of a KVM guest-to-host breakout that does not rely on bugs in user space components such as QEMU. The discussed bug was assigned CVE-2021-29657, affects kernel versions  v5.10-rc1 to v5.12-rc6  and was patched at the end of   March  2021 .  As the bug only became exploitable in v5.10 and was discovered roughly 5 months later, most real world deployments of KVM should not be affected. I still think the issue is an interesting case study in the work required to build a stable guest-to-host escape against KVM and hope that this writeup can strengthen the case that hypervisor compromises are not only theoretical issues.</p><p>    在本博客文章中，我描述了特定于kVM的AMD代码中的漏洞，并讨论了如何将此错误变成完整的虚拟机转义。据我所知，这是第一个公开写作KVM访客到主机突破，不依赖于QEMU等用户空间组件中的错误。讨论的错误被分配了CVE-2021-29657，影响了内核版本V5.10-RC1至V5.12-RC6，并在2021年3月底被修补。由于该错误仅在V5.10中被解释，并且在大约5个月后发现，大多数真实的KVM部署不应受到影响。我仍然认为这个问题是一个有趣的案例研究，可以在为KVM建立稳定的客人到主管逃生的工作中，希望这个写作能够加强虚拟机管理程序妥协不仅是理论问题的情况。</p><p>    I start with a short overview of KVM’s architecture, before diving into the bug and its exploitation.</p><p>    在潜入错误和剥削之前，我开始简短概述KVM的体系结构。</p><p>     KVM is a Linux based open source hypervisor supporting hardware accelerated virtualization on x86, ARM, PowerPC and S/390. In contrast to the other big open source hypervisor Xen, KVM is deeply integrated with the Linux Kernel and builds on its scheduling, memory management and hardware integrations to provide efficient virtualization.</p><p>     KVM是一种基于Linux的开源管理程序，支持X86，ARM，PowerPC和S / 390上的硬件加速虚拟化。与其他大开源管理程序Xen相比，KVM与Linux内核深度集成，并在其调度，内存管理和硬件集成上构建，以提供有效的虚拟化。</p><p>    KVM is implemented as one or more kernel modules (kvm.ko plus kvm-intel.ko or kvm-amd.ko on x86) that expose a low-level IOCTL-based   API  to user space processes over the /dev/kvm device. Using this API, a user space process (often called VMM for Virtual Machine Manager) can create new VMs, assign vCPUs and memory, and intercept memory or IO accesses to provide access to  emulate d or virtualization-aware hardware devices.   QEMU  has been the standard user space choice for KVM-based virtualization for a long time, but in the last few years alternatives like   LKVM ,   crosvm  or   Firecracker  have started to become popular.</p><p>    KVM实现为一个或多个内核模块（kvm.ko plus kvm-intel.ko或kvm-amd.ko上的x86），将低级IOCTL的API公开到/ dev / kvm设备上的用户空间进程。使用此API，用户空间进程（通常称为VMM for Virtual Machine Manager）可以创建新的VM，分配VCPU和内存，以及拦截内存或IO访问，以提供对模拟D或虚拟化感知硬件设备的访问。 QEMU一直是基于KVM的虚拟化的标准用户空间选择很长时间，但在过去的几年中，LKVM，CROSVM或鞭炮等替代方案已经开始变得流行。</p><p>    While KVM’s reliance on a separate user space component might seem complicated at first, it has a very nice benefit: Each VM running on a KVM host has a 1:1 mapping to a Linux process, making it managable using standard Linux tools.</p><p>    虽然KVM对单独的用户空间组件的依赖可能似乎是一个非常好的好处：KVM主机上运行的每个VM都有1：1映射到Linux过程，使其使用标准的Linux工具可管理。</p><p>    This means for example, that a guest&#39;s memory can be inspected by dumping the allocated memory of its user space process or that resource limits for CPU time and memory can be applied easily. Additionally, KVM can offload most work related to device emulation to the userspace component. Outside of a couple of performance-sensitive devices related to interrupt handling, all of the complex low-level code for providing virtual disk, network or GPU access can be implemented in userspace.</p><p>    例如，这意味着嘉宾＆＃39; s存储器可以通过转储其用户空间过程的分配内存或可以轻松地应用CPU时间和内存的资源限制来检查。此外，KVM可以将大多数与设备仿真相关的工作卸载到Userspace组件。在与中断处理相关的几个性能敏感设备之外，可以在用户佩纳中实现所有复杂的低级代码，用于提供虚拟磁盘，网络或GPU访问权限。 </p><p>    When looking at public writeups of KVM-related vulnerabilities and exploits it becomes clear that this design was a wise decision. The large majority of disclosed vulnerabilities and all publicly available exploits affect QEMU and its support for emulated/paravirtualized devices.</p><p>在查看与KVM相关的公众撰写相关的漏洞并利用它变得清楚，这种设计是一个明智的决定。大多数披露的漏洞和所有公共利用影响QEMU及其对模拟/半虚拟化设备的支持。</p><p>    Even though KVM’s kernel attack surface is significantly smaller than the one exposed by a default QEMU configuration or similar user space VMMs, a KVM vulnerability has advantages that make it very valuable for an attacker:</p><p>    尽管KVM的内核攻击表面明显小于由默认的QEMU配置或类似的用户空间VMMS暴露的攻击，但KVM漏洞具有使其对攻击者非常有价值的优势：</p><p> Whereas user space VMMs can be sandboxed to reduce the impact of a VM breakout, no such option is available for KVM itself. Once an attacker is able to achieve code execution (or similarly powerful primitives like write access to page tables) in the context of the host kernel, the system is fully compromised.</p><p> 虽然用户空间VMMS可以是Sandboxed以减少VM Breakout的影响，但没有此类选项可用于KVM本身。一旦攻击者能够在主机内核的上下文中实现代码执行（或类似强大的基语），系统完全泄露。</p><p> Due to the somewhat poor security history of QEMU, new user space VMMs like crosvm or Firecracker are written in Rust, a memory safe language. Of course, there can still be non-memory safety vulnerabilities or problems due to incorrect or buggy usage of the KVM APIs, but using Rust effectively prevents the large majority of bugs that were discovered in C-based user space VMMs in the past.</p><p> 由于QEMU的安全历史稍差，新的用户空间VMMS，如CROSVM或Firecracker都以Rust，一种内存安全语言编写。当然，由于KVM API的错误或错误使用，仍然存在非记忆安全漏洞或问题，但使用RECT有效地防止了过去在基于C的用户空间VMMS中发现的大多数错误。</p><p> Finally, a pure KVM exploit can work against targets that use proprietary or heavily modified user space VMMs. While the big cloud providers do not go into much detail about their virtualization stacks publicly, it is safe to assume that they do not depend on an unmodified QEMU version for their production workloads. In contrast, KVM’s smaller code base makes heavy modifications unlikely (and KVM’s contributor list points at a strong tendency to upstream such modifications when they exist).</p><p> 最后，纯KVM利用可以针对使用专有或重型用户空间VMM的目标。虽然大云提供商公开的虚拟化堆栈没有详细描述，但可以安全地假设它们不依赖于未修改的QEMU版本，以获得其生产工作负载。相比之下，KVM的较小的代码基础使得重大修改不太可能（并且KVM的贡献者列表点在存在时，在上游的上游修改时）。</p><p>      With these advantages in mind, I decided to spend some time hunting for a KVM vulnerability that could be turned into a guest-to-host escape. In the past, I had   some success  with finding vulnerabilities in KVM’s support for nested virtualization on Intel CPUs so reviewing the same functionality for AMD seemed like a good starting point. This is even more true,  because the recent increase of AMD’s market share in the server segment means that KVM’s AMD implementation is suddenly becoming a more interesting target than it was in the last years.</p><p>      考虑到这些优势，我决定花一些时间为kvm漏洞进行狩猎，这可能变成客人到主机逃生。在过去，我取得了一些成功，在英特尔CPU上找到了KVM支持嵌套虚拟化的漏洞，因此回顾了AMD的相同功能似乎是一个很好的起点。这更为真实，因为近期AMD的市场份额的增加意味着KVM的AMD实施突然成为一个比过去几年更有趣的目标。</p><p>    Nested virtualization, the ability for a VM (called L1) to spawn nested guests (L2), was also a niche feature for a long time. However, due to hardware improvements that reduce its overhead and increasing customer demand it’s becoming more widely available. For example, Microsoft is heavily pushing for   Virtualization-based Security  as part of newer Windows versions, requiring nested virtualization to support cloud-hosted Windows installations. KVM enables support for nested virtualization on both AMD  and  Intel by default, so if an administrator or the user space VMM does not explicitly disable it, it’s part of the attack surface for a malicious or compromised VM.</p><p>    嵌套虚拟化，VM（名为L1）的能力将嵌套的访客（L2）（L2），也是很长一段时间的利基特征。但是，由于硬件改进，减少了其开销和越来越多的客户需求，它变得更加广泛。例如，Microsoft严重推动基于虚拟化的安全性，作为较新的Windows版本的一部分，需要嵌套虚拟化来支持云托管的Windows安装。 KVM默认支持AMD和Intel上的嵌套虚拟化，因此如果管理员或用户空间VMM未明确禁用它，则它是恶意或受损VM的攻击曲面的一部分。 </p><p>    AMD’s virtualization extension is called SVM (for Secure Virtual Machine) and in order to support nested virtualization, the host hypervisor needs to intercept all SVM instructions that are executed by its guests, emulate their behavior and keep its state in sync with the underlying hardware. As you might imagine, implementing this correctly is quite difficult with a large potential for complex logic flaws, making it a perfect target for manual code review.</p><p>AMD的虚拟化扩展名为SVM（对于安全虚拟机），为了支持嵌套虚拟化，主机管理程序需要拦截其访客执行的所有SVM指令，模拟其行为并将其与底层硬件同步保持状态。正如您可能想象的那样，对复杂逻辑缺陷的巨大潜力来实现这一正确的困难，使其成为手动代码审查的完美目标。</p><p>     Before diving into the KVM codebase and the bug I discovered, I want to quickly introduce how AMD SVM works to make the rest of the post easier to understand. (For a thorough documentation see   AMD64 Architecture Programmer’s Manual, Volume 2: System Programming Chapter 15 .) SVM adds support for 6 new instructions to x86-64 if SVM support is enabled by setting the SVME bit in the EFER MSR. The most interesting of these instructions is  VMRUN , which (as its name suggests) is responsible for running a guest VM.  VMRUN  takes an implicit parameter via the RAX register pointing to the page-aligned physical address of a data structure called “virtual machine control block” (VMCB), which describes the state and configuration of the VM.</p><p>     在进入KVM CodeBase和我发现的错误之前，我想快速介绍AMD SVM如何使遗迹的其余部分更容易理解。 （对于彻底的文档，请参阅AMD64架构程序员手册，第2卷：系统编程第15章。）SVM如果通过在EFER MSR中设置SVME位，请支持6个新指令到X86-64。这些指令最有趣的是VMRUN（因为它的名字表明）负责运行Guest VM。 VMRUN通过rax寄存器指向名为“虚拟机控制块”（vmcb）的数据结构的页面对齐的物理地址，该rax寄存器采用隐式参数，该数据结构（vmcb）描述了VM的状态和配置。</p><p>    The VMCB is split into two parts: First, the State Save area, which stores the values of all guest registers, including segment and control registers. Second, the Control area which describes the configuration of the VM. The Control area describes the virtualization features enabled for a VM,  sets which VM actions are intercepted to trigger a VM exit  and  stores some fundamental configuration values such as the page table address used for   nested paging .</p><p>    VMCB分为两部分：首先，状态保存区域存储所有访客寄存器的值，包括段和控制寄存器。其次，描述VM配置的控制区域。控制区域描述为VM启用的虚拟化特征，设置截取的VM动作以触发VM退出，并存储一些基本配置值，例如用于嵌套分页的页面表地址。</p><p>    If the VMCB is correctly prepared (and we are not already running in a VM), VMRUN will first save the host state in a memory region called the host save area, whose address is configured by writing a physical address to the VM_HSAVE_PA MSR. Once the host state is saved, the CPU switches to the VM context and VMRUN only returns once a VM exit is triggered for one reason or another.</p><p>    如果VMCB正确准备（并且我们尚未在VM中运行），则VMRUN将首先将主机状态保存在名为HOSS SAVE区域的内存区域中，通过将物理地址写入VM_HSAVE_PA MSR来配置其地址。保存主机状态后，将CPU切换到VM上下文，只有在一个原因或另一个原因触发VM Exit后，vmrun仅返回。</p><p>    An interesting aspect of SVM is that a lot of the state recovery after a VM exit has to be done by the hypervisor. Once a VM exit occurs, only RIP, RSP and RAX are restored to the previous host values and all other general purpose registers still contain the guest values. In addition, a full context switch requires manual execution of the VMSAVE/VMLOAD instructions which save/load additional system registers (FS, SS, LDTR, STAR, LSTAR …) from memory.</p><p>    SVM的一个有趣方面是VM Exit后的大量状态恢复必须由虚拟机管理程序完成。发生VM退出后，仅恢复RIP，RSP和RAX，恢复到先前的主机值，并且所有其他通用寄存器仍包含访客值。此外，完整的上下文交换机需要手动执行VMSAVE / VMLOAD指令，该指令保存/加载额外的系统寄存器（FS，SS，LDTR，Star，LSTAR ...）。</p><p>    For nested virtualization to work, KVM intercepts execution of the VMRUN instruction and creates its own VMCB based on the VMCB the L1 guest prepared (called vmcb12 in KVM terminology). Of course, KVM can’t trust the guest provided vmcb12 and needs to carefully validate all fields that end up in the real VMCB that gets passed to the hardware (known as vmcb02).</p><p>    对于嵌套虚拟化工作，KVM拦截VMRUN指令的执行，并根据VMCB准备的L1访客（称为kVM术语中的VMCB12）创建自己的VMCB。当然，KVM不能相信客户提供vmcb12，并且需要仔细验证最终以来传递给硬件的真实VMCB的所有字段（称为vmcb02）。</p><p>    Most of the KVM’s code for nested virtualization on AMD is implemented in   arch/x86/kvm/svm/nested.c  and the code that intercepts VMRUN instructions of nested guests is implemented in  nested_svm_vmrun :</p><p>    AMD上的大多数KVM用于嵌套虚拟化的代码在ARCH / X86 / KVM / SVM /嵌套中实现.C，拦截嵌套访客的代码在Nested_svm_vmrun中实现： </p><p>           * Save the old vmcb, so we don&#39;t need to pick what we save, but can</p><p>*保存旧的VMCB，所以我们不需要选择我们的保存，但可以</p><p>      The function first fetches the value of RAX out of the currently active vmcb ( svm-&gt;vcmb ) in  1  (numbers are marked in the code samples) . For guests using nested paging (which is the only relevant configuration nowadays) RAX contains a guest physical address (GPA), which needs to be translated into a host physical address (HPA) first.  kvm_vcpu_map  ( 2 ) takes care of this translation and maps the underlying page to a host virtual address (HVA) that can be directly accessed by KVM.</p><p>      该功能首先在1（编码样本中标记）中的当前活动的VMCB（SVM-＆gt; VCMB）中的rax的值。对于使用嵌套分页的访客（现在是现在唯一的相关配置）Rax包含一个客户物理地址（GPA），它需要首先将其转换为主机物理地址（HPA）。 KVM_VCPU_MAP（2）负责此转换，并将底层页面映射到可通过KVM直接访问的主机虚拟地址（HVA）。</p><p>    Once the VMCB is mapped,  nested_vmcb_checks  is called for some basic validation in  3 . Afterwards, the L1 guest context which is stored in  svm-&gt;vmcb  is copied into the host save area  svm-&gt;nested.hsave  before KVM enters the nested guest context by calling  enter_svm_guest_mode  ( 4 ) .</p><p>    vmcb映射后，将indested_vmcb_checks调用3中的某些基本验证。之后，将存储在SVM-＆gt中的L1 Guest上下文。VMCB被复制到主机保存区域SVM-＆gt;嵌套。在KVM之前通过调用Enter_svm_guest_mode（4）进入嵌套的guatern never之前。</p><p>         Looking at  enter_svm_guest_mode  we can see that KVM copies the vmcb12 control area directly into svm-&gt;nested.ctl and does not perform any further checks on the copied value.</p><p>         查看Enter_svm_guest_Mode，我们可以看到KVM将VMCB12控制区域直接复制到SVM-＆GT;嵌套.DCL，并且不会对复制值进行任何进一步检查。</p><p>  Readers familiar with double fetch or Time-of-Check-to-Time-of-Use vulnerabilities might already see a potential issue here: The call to  nested_vmcb_checks  at the beginning of  nested_svm_vmrun  performs all of its checks on a copy of the VMCB that is stored in guest memory. This means that a guest with multiple CPU cores can modify fields in the VMCB after they are verified in  nested_vmcb_checks , but before they are copied to svm-&gt;nested.ctl in  load_nested_vmcb_control .</p><p>  熟悉双重获取或使用时间使用时间漏洞的读者可能已经看到了一个潜在的问题：indested_svm_vmrun开头的呼叫对vmcb的副本执行所有检查的indapt_vmcb_check存储在访客内存中。这意味着具有多个CPU内核的访客可以在indested_vmcb_checks验证后修改VMCB中的字段，但在它们被复制到SVM-＆gt之前;嵌套。在load_nested_vmcb_control中嵌套。</p><p>    Let’s look at  nested_vmcb_checks  to see what kind of checks we can bypass with this approach:</p><p>    让我们来看看Nested_vmcb_checks，看看我们可以用这种方法绕过什么样的检查：</p><p>         At first glance this looks pretty harmless.  control-&gt;asid  isn’t used anywhere and the last check is only relevant for systems where nested paging isn’t supported. However, the first check turns out to be very interesting.</p><p>         乍一看，这看起来很无害。 Control-＆gt; ASID未使用任何位置，最后一次检查仅对不支持嵌套分页的系统相关。但是，第一个检查结果是非常有趣的。 </p><p>    For reasons unknown to me, SVM VMCBs contain a bit that enables or disables interception of the VMRUN instruction when executed inside a guest. Clearing this bit isn’t actually supported by hardware and results in an immediate VMEXIT, so the check in  nested_vmcb_check_controls  simply replicates this behavior.  When we race and bypass the check by repeatedly flipping the value of the INTERCEPT_VMRUN bit, we can end up in a situation where svm-&gt;nested.ctl contains a 0 in place of the INTERCEPT_VMRUN bit. To understand the impact we first need to see how nested vmexit’s are handled in KVM:</p><p>出于ME未知的原因，SVM VMCB包含一位位，其在客座内部执行时启用或禁用VMRUN指令的拦截。清除该位实际上并非实际支持硬件并导致立即vmexit，因此indested_vmcb_check_controls中的校验只会复制此行为。当我们通过反复翻转拦截_VmRun位的值来竞争和绕过检查时，我们最终可以在SVM-＆gt;嵌套.ctl中包含0代替拦截_Vmrun位的情况。要了解我们首次需要了解嵌套vmexit的影响，请参见kvm：</p><p>    The main SVM exit handler is the function  handle_exit  in   arch/x86/kvm/svm.c , which is called whenever a VMexit occurs. When KVM is running a nested guest, it first has to check if the exit should be handled by itself or the L1 hypervisor. To do this it calls the function  nested_svm_exit_handled  ( 5 ) whi ch will return  NESTED_EXIT_DONE if the vmexit will be handled by the L1 hypervisor and no further processing by the L0 hypervisor is needed :</p><p>    主SVM退出处理程序是Arch / x86 / kvm / svm.c中的函数handle_exit，只要发生vmexit就会调用。当KVM正在运行嵌套的访客时，首先必须检查退出是否应由自身或L1虚拟机管理程序处理。为此，它调用unteed_svm_exit_handled（5）whi ch将返回nested_exit_done，如果vmexit将由L1虚拟机管理程序处理，并且不需要通过L0虚拟机管理程序进行进一步处理：</p><p>           nested_svm_exit_handled  first calls  nested_svm_intercept (6)  to see if the exit should be handled. When we trigger an exit by executing VMRUN in a L2 guest, the default case is executed ( 7 )  to see if the INTERCEPT_VMRUN bit in svm-&gt;nested.ctl is set.  Normally, this should always be the case and the function returns NESTED_EXIT_DONE to trigger a nested VM exit from L2 to L1 and to let the L1 hypervisor handle the exit ( 8 ). (This way KVM supports infinite nesting of hypervisors).</p><p>           indested_svm_exit_handled首次调用nested_svm_intercept（6）以查看是否应该处理退出。当我们通过在L2 Guest中执行VMRUN触发EXIT时，执行默认情况（7）以查看SVM-＆gt中的拦截器_Vmrun位是否设置。通常，这应该始终是这种情况，函数返回Nested_exit_done以触发从L2到L1的嵌套VM退出，并让L1虚拟机管理程序处理出口（8）。 （以这种方式KVM支持无限嵌套的虚拟机管理程序）。</p><p>    However, if the L1 guest exploited the race condition described above svm-&gt;nested.ctl won’t have the INTERCEPT_VMRUN bit set and the VM exit will be handled by KVM itself. This results in a second call to  nested_svm_vmrun  while still running inside the L2 guest context.  nested_svm_vmrun  isn’t written to handle this situation and will blindly overwrite the L1 context stored in  svm-&gt;nested.hsave  with data from the currently active  svm-&gt;vmcb  which contains data for the L2 guest:</p><p>    但是，如果L1客户利用上述SVM-＆gt所描述的竞争条件;嵌套.ctl不会有拦截器，并且VM Exit将由KVM本身处理。这导致第二次调用indest_svm_vmrun，同时仍在L2 Guest上下文中运行。 Nested_svm_vmrun未写入以处理这种情况，并盲目地覆盖存储在SVM-＆gt中的L1上下文;嵌套.HSAVE使用当前活动的SVM-＆GT的数据; VMCB包含L2 Guest的数据：</p><p>     * Save the old vmcb, so we don&#39;t need to pick what we save, but can</p><p>     *保存旧的VMCB，所以我们不需要选择我们的保存，但可以</p><p>    This becomes a security issue due to the way Model Specific Register (MSR) intercepts are handled for nested guests:</p><p>    这成为一个安全问题，因为型号特定寄存器（MSR）拦截被嵌套的访客处理：</p><p>  SVM uses a permission bitmap to control which MSRs can be accessed by a VM. The bitmap is a 8KB data structure with two bits per MSR, one of which controls read access and the other write access. A 1 bit in this position means the access is intercepted and triggers a vm exit, a 0 bit means the VM has direct access to the MSR. The HPA address of the bitmap is stored in the VMCB control area and for normal L1 KVM guests, the pages are allocated and pinned into memory as soon as a vCPU is created.</p><p>  SVM使用权限位图来控制VM可以访问哪些MSR。位图是一个8KB数据结构，每个MSR两个位，其中一个控制读取访问和另一个写访问。在该位置中的1位意味着接入被拦截并触发VM出口，0比特表示VM具有直接访问MSR。位图的HPA地址存储在VMCB控制区域中，对于正常的L1 KVM Guest，一旦创建VCPU，页面就会分配并固定到内存中。 </p><p>    For a nested guest, the MSR permission bitmap is stored in  svm-&gt;nested.msrpm  and its physical address is copied into the active VMCB (in  svm-&gt;vmcb-&gt;control.msrpm_base_pa ) while the nested guest is running. Using the described double invocation of  nested_svm_vmrun , a malicious guest can copy this value into the  svm-&gt;nested.hsave  VMCB when  copy_vmcb_control_area  is executed. This is interesting because the KVM’s hsave area normally only contains data from the L1 guest context so  svm-&gt;nested.hsave.msrpm_base_pa  would normally point to the pinned vCPU-specific MSR bitmap pages.</p><p>对于嵌套的访客，MSR权限位图存储在SVM-＆gt中;嵌套.msrpm及其物理地址被复制到活动的VMCB中（在SVM-＆gt; vmcb-＆gt; control.msrpm_base_pa）中，而嵌套guest虚拟机运行。使用所描述的双重调用inested_svm_vmrun，恶意客户机可以将此值复制到svm-＆gt;执行Copy_vmcb_Control_area时，将此值复制到SVM-＆gt;嵌套.hsave vmcb。这很有趣，因为KVM的HSAVE区域通常仅包含L1 Guest上下文所以SVM-＆gt;嵌套.hsave.msrpm_base_pa通常指向Pinned VCPU的MSR位图页面。</p><p>      Since commit “  2fcf4876: KVM: nSVM: implement on demand allocation of the nested state ” from last October, svm-&gt;nested.msrpm is dynamically allocated and freed when a guest changes the SVME bit of the MSR_EFER register:</p><p>      自提交“2FCF4876：KVM：NSVM：从去年10月开始嵌套状态的需求分配”，SVM-＆GT;当客户更改MSR_EFER寄存器的SVME位时，动态分配并释放嵌套.MSRPM：</p><p>       For the “disable SVME” case, KVM will first call  svm_leave_nested  to forcibly leave potential</p><p>       对于“禁用SVME”案例，KVM将首先调用svm_leave_，以强制留下潜力</p><p>  nested guests and then free the  svm-&gt;nested  data structures (including the backing pages for the MSR permission bitmap) in  svm_free_nested . As  svm_leave_nested  believes that  svm-&gt;nested.hsave  contains the saved context of the L1 guest, it simply copies its control area to the real VMCB:</p><p>  嵌套客人然后释放SVM-＆gt;嵌套数据结构（包括MSR权限位图的备份页面）在SVM_FREE_已被中。正如SVM_LEAVE_NESTED认为svm-＆gt;嵌套.hsave包含l1 guest虚拟机的保存上下文，它只是将其控件区域复制到真实的vmcb：</p><p>           svm-&gt;nested-&gt;msrpm . Once  svm_free_nested  is finished and KVM passes control back to the guest, the CPU will use the freed pages for its MSR permission checks. This gives a guest unrestricted access to host MSRs if the pages are reused and partially overwritten with zeros.</p><p>           svm-＆gt;嵌套 - ＆gt; msrpm。一旦完成了SVM_FREE_已完成并且KVM通过控制返回给客户端，CPU将使用释放页面进行其MSR权限检查。如果页面重复使用并将其部分覆盖为零，则这为客户提供了对主机MSR的访问权限。</p><p>    To summarize, a malicious guest can gain access to host MSRs using the following approach:</p><p>    为了总结，恶意客人可以使用以下方法获得主机MSRS：</p><p> Repeatedly try to launch a L2 guest using the VMRUN instruction while flipping the INTERCEPT_VMRUN bit on a second CPU core.</p><p> 反复尝试使用VMRUN指令启动L2访客，同时在第二个CPU核心上翻转Intercept_vmrun位。 </p><p> If VMRUN succeeds, try to launch a “L3” guest using another invocation of VMRUN. If this fails, we have lost the race in step 2 and must try again. If VMRUN succeeds we have successfully overwritten  svm-&gt;nested.hsave  with our L2 context.</p><p>如果vmrun成功，请尝试使用vmrun的另一个调用来启动“L3”来宾。如果这失败，我们在步骤2中失去了比赛，并且必须再试一次。如果vmrun成功，我们已成功覆盖SVM-＆gt;嵌套.HSAVE与我们的L2上下文。</p><p> Clear the SVME bit in MSR_EFER while still running in the “L3” context. This frees the MSR permission bitmap backing pages used by the L2 guest who is now executing again.</p><p> 清除MSR_efer中的SVME位，同时仍在“L3”上下文中运行。这使得现在正在执行的L2 Guest虚拟机使用的MSR权限位图备份页面。</p><p> Wait until the KVM host reuses the backing pages. This will potentially clear all or some of the bits, giving the guest access to host MSRs.</p><p> 等到KVM主机重用备份页面。这将可能清除全部或某些位，使访客访问主机MSR。</p><p>    When I initially discovered and reported this vulnerability, I was feeling pretty confident that this type of MSR access should be more or less equivalent to full code execution on the host. While my feeling turned out to be correct, getting there still took me multiple weeks of exploit development. In the next section I’ll describe the steps to turn this primitive into a guest-to-host escape.</p><p>    当我最初发现并报告这种漏洞时，我感到非常相信，这种类型的MSR访问应该或多或少等于主机上的完整代码执行。虽然我的感觉是正确的，但到达那里仍然花了我多周的利用发展。在下一节中，我将描述将此原语转为主机转义的步骤。</p><p>     Assuming our guest can get full unrestricted access to any MSR (which is only a question of timing thanks to init_on_alloc=1 being the default for most modern distributions), how can we escalate this into running arbitrary code in the context of the KVM host? To answer this question we first need to look at what kind of MSRs are supported on a modern AMD system. Looking at the   BIOS and Kernel Developer’s Guide  for recent AMD processors we can find a wide range of MSRs starting with well known and widely used ones such as EFER (the Extended Feature Enable Register) or LSTAR (the syscall target address) to rarely used ones like SMI_ON_IO_TRAP (can be used to generate a System Management Mode Interrupt when specific IO port ranges are accessed).</p><p>     假设我们的访客可以获得对任何MSR的完全不受限制的访问权限（这只是凭借Init_On_Alloc = 1是最现代化分布的默认值），我们如何在KVM主机的上下文中升级它在运行任意代码中？要回答这个问题，我们首先需要查看现代AMD系统支持哪种MSR。查看最近AMD处理器的BIOS和Kernel开发人员指南我们可以找到广泛的MSR，以众所周知的和广泛使用的诸如EFER（扩展功能启用寄存器）或LSTAR（SYSCALL目标地址）以很少使用与SMI_ON_IO_TRAP（可用于在访问特定IO端口范围时生成系统管理模式中断）。</p><p>  Looking at the list, several registers like LSTAR or KERNEL_GSBASE seem like interesting targets for redirecting the execution of the host kernel. Unrestricted access to these registers is actually  enabled by default , however they are automatically restored to a valid state by KVM after a vmexit so modifying them does not lead to any changes in host behavior.</p><p>  查看列表，像LSTAR或Kernel_GSBase这样的多个寄存器似乎是有趣的目标，用于重定向主机内核的执行。默认情况下，实际上启用了对这些寄存器的不受限制的访问，但是在vmexit之后，它们会自动将其恢复到有效状态，以便修改它们不会导致主机行为中的任何更改。</p><p>    Still, there is one MSR that we previously mentioned and that seems to give us a straightforward way to achieve code execution: The VM_HSAVE_PA that stores the physical address of the host save area, which is used to restore the host context when a vmexit occurs. If we can point this MSR at a memory location under our control we should be able to fake a malicious host context and execute our own code after a vmexit.</p><p>    此前，我们之前提到的一个MSR似乎为我们提供了一种直接实现代码执行的方式：存储主机保存区域物理地址的VM_HSAVE_PA，用于在发生vmexit时恢复主机上下文。如果我们可以在我们的控件下将此MSR指向内存位置，我们应该能够伪造恶意主机上下文并在VMexit后执行自己的代码。 </p><p>     AMD is pretty clear about the fact that software should not touch the host save area in any way and that the data stored in this area is CPU-dependent: “ Processor implementations may store only part or none of host state in the memory area pointed to by VM_HSAVE_PA MSR and may store some or all host state in hidden on-chip memory. Different implementations may choose to save the hidden parts of the host’s segment registers as well as the selectors. For these reasons, software must not rely on the format or contents of the host state save area, nor attempt to change host state by modifying the contents of the host save area. ” (AMD64 Architecture Programmer’s Manual, Volume 2: System Programming, Page 477). To strengthen the point, the format of the host save area is undocumented.</p><p>AMD非常清楚的是，软件不应该以任何方式触摸主机保存区域，并且存储在该区域中的数据依赖于CPU：“处理器实现可以在指向的内存区域中仅存储部分或者主状态通过vm_hsave_pa msr，可以在隐藏的片上存储器中存储一些或所有主机状态。不同的实现可以选择保存主机段寄存器的隐藏部分以及选择器。由于这些原因，软件不得依赖主机状态保存区域的格式或内容，也不会通过修改主机保存区域的内容来尝试更改主机状态。 “（AMD64架构程序员手册，第2卷：系统编程，第477页）。为了加强重点，主机保存区域的格式无证。</p><p> Debugging issues involving an invalid host state is very tedious as any issue leads to an immediate processor shutdown. Even worse, I wasn’t sure if rewriting the VM_HSAVE_PA MSR while running inside a VM can even work. It’s not really something that should happen during normal operation so in the worst case scenario, overwriting the MSR would just lead to an immediate crash.</p><p> 随着任何问题导致立即处理器关闭，调试涉及无效主机状态的问题非常乏味。更糟糕的是，我不确定是否在VM内部运行时重写VM_HSAVE_PA MSR甚至可以工作。在正常运行期间，在最坏的情况下，这并不是真正的事情，因此覆盖了MSR，只会导致立即崩溃。</p><p> Even if we can create a valid (but malicious) host save area in our guest, we still need some way to identify its host physical address (HPA). Because our guest runs with nested paging enabled, physical addresses that we can see in the guest (GPAs) are still one address translation away from their HPA equivalent.</p><p> 即使我们可以在我们的客人中创建有效（但恶意）主机保存区域，我们仍然需要某种方式来识别其主机物理地址（HPA）。由于我们的访客使用嵌套分页启用，我们可以在访客中看到的物理地址（GPAS）仍然是一个源于HPA等效的地址。</p><p>    After spending some time scrolling through AMD’s documentation, I still decided that VM_HSAVE_PA seems to be the best way forward and decided to tackle these problems one by one.</p><p>    在花一些时间滚动到AMD的文档之后，我仍然决定VM_HSAVE_PA似乎是最佳的前进方向，并决定一个接一个地解决这些问题。</p><p>    After dumping the host save area of a normal KVM guest running on an AMD EPYC 7351P CPU, the first problem goes away quickly: As it turns out, the host save area has the same layout as a normal VMCB with only a couple of relevant fields initialized. Even better, the initialized fields include all the saved host information documented in the AMD manual so the fear that a</p><p>    在倾倒在AMD EPYC 7351P CPU上运行的普通KVM Guest的主机保存区域后，第一个问题很快就会消失：事实证明，主机保存区域具有与普通VMCB相同的布局，只有几个相关字段初始化。甚至更好，初始化的字段包括在AMD手册中记录的所有已保存的主机信息，因此担心A</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://googleprojectzero.blogspot.com/2021/06/an-epyc-escape-case-study-of-kvm.html">https://googleprojectzero.blogspot.com/2021/06/an-epyc-escape-case-study-of-kvm.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/逃生/">#逃生</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/escape/">#escape</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/kvm/">#kvm</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>