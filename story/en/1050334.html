<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>重新考虑GPU加速超级计算机的MPI Rethinking MPI for GPU Accelerated Supercomputers</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Rethinking MPI for GPU Accelerated Supercomputers<br/>重新考虑GPU加速超级计算机的MPI </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-02 14:00:32</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/008abfbc309875639bd8cc3e3cb83412.jpg"><img src="http://img2.diglog.com/img/2021/3/008abfbc309875639bd8cc3e3cb83412.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In the accelerated era of exascale supercomputing, MPI is being pushed to its logical limits. No matter how entrenched it has become over the last two decades, it might be time to rethink programming for increasingly large, heterogenous systems.</p><p>在百亿亿次超级计算的加速时代，MPI被推到其逻辑极限。在过去的二十年中，无论它变得多么牢固，都可能需要重新考虑针对越来越大的异构系统的编程。</p><p> Every field has its burdens newcomers must bear and for HPC programmers, MPI is one of them. The problem is not just that writing communication code directly using MPI primitives is challenging—that is well understood, especially for codes with tricky communication patterns. The problem for the next decade is that acceleration will become the norm on most large machines and pure MPI overall never quite got with the CPU-GPU program, despite best efforts. The answer to this has been use of high-level libraries.</p><p> 每个领域都有其新手必须承担的负担，对于HPC程序员而言，MPI就是其中之一。问题不仅仅在于使用MPI原语直接编写通信代码具有挑战性-这是众所周知的，尤其是对于具有棘手通信模式的代码。下一个十年的问题是，即使有最大的努力，加速也将成为大多数大型计算机上的规范，而纯MPI总体上不会通过CPU-GPU程序实现。答案是使用高级库。</p><p> A team rooted at Argonne National Lab with support from other universities is working to set MPI on the exascale course once again and in the process, has demonstrated some notable successes in performance with relatively low overhead, scalability, and ability to use some custom asynchronous and other features. With this proven out, the plan is to keep tuning it for exascale systems with GPUs as the choice accelerator to validate the concept.</p><p> 一支扎根于阿贡国家实验室的团队，在其他大学的支持下，他们正在努力再次将MPI设置为亿万级课程，并在此过程中以较低的开销，可伸缩性以及使用某些自定义异步和自定义功能的能力证明了一些显着的成功其它功能。经过验证的结果，计划是继续针对具有GPU作为验证该概念的选择加速器的百亿分之一系统进行调整。</p><p> The effort, called  PetscSF, is being used to gradually replace the direct MPI calls in the Portable Extensible Toolkit for Scientific Computation ( PETSc) at Argonne National Lab. This is a wellspring of data structures and routines for PDE-based applications that supports MPI, GPUs (via CUDA or OpenCL) in addition to hybrid MPI-GPU parallelism accompanied by the Tao optimization library. This is application and center-specific but the concept of replacing those MPI calls on GPU supers could go far beyond Argonne and the test/development site for the work—the Summit supercomputer at Oak Ridge National Lab.</p><p> 这项名为PetscSF的工作正在逐步取代Argonne国家实验室的便携式可扩展科学计算便携式工具包（PETSc）中的直接MPI调用。这是基于PDE的应用程序的数据结构和例程的源泉，它支持MPI，GPU（通过CUDA或OpenCL），以及伴随Tao优化库的混合MPI-GPU并行性。这是针对应用程序和特定于中心的，但是替换在GPU超级计算机上进行的MPI调用的概念可能远远超出了Argonne和这项工作的测试/开发站点，即Oak Ridge国家实验室的Summit超级计算机。</p><p> PetscSF sets forth a bare-bones API for managing shared communication patterns for HPC applications using a star-forest graph representation with maneuvering room based on the target architecture (this can be used on an all-CPU machine as well as GPU accelerated). There is  extensive detail on how the network and intra-node communication are handled with use cases highlighting its functionality on parallel matrix operations and unstructured meshes, both of critical importance in large-scale HPC applications. PetscSF can handle different implementations, including those that use MPI one- and two-sided communication.</p><p> PetscSF设置了一个基本API，用于使用星型森林图形表示法以及基于目标体系结构的操纵空间来管理HPC应用程序的共享通信模式（可以在全CPU机器上使用，也可以在GPU加速下使用）。有关如何处理网络和节点内通信的详细信息，其中包括用例，突出了其在并行矩阵操作和非结构化网格上的功能，这两者在大规模HPC应用程序中都至关重要。 PetscSF可以处理不同的实现，包括那些使用MPI单向和双向通信的实现。</p><p> The creators of PetscSF are careful to note this is not a “drop-in replacement for MPI” but rather it is designed to overcome some of the time-consuming complexity challenges of working directly with MPI with GPUs on large-scale systems.</p><p> PetscSF的创建者小心地注意到，这并不是“替代MPI”，而是旨在克服在大型系统上直接使用MPI和GPU来解决一些费时的复杂性挑战。</p><p> They note that using CUDA for GPUs brings “new challenges to MPI. Users want to communicate data on the device while MPI runs on the host, but the MPI specification does not have a concept of device data or host data.” They add that “with a non-CUDA aware MPI implementation, programmers have to copy data back and forth between devices and host to do computation on the host.” While CUDA-aware MPI can address this by letting programmers pass device buffers to MPI using the same API there is still a “semantic mismatch” as CUDA kernels are executed asynchronously on CUDA streams while MPI has no idea what stream are in this regard and can’t line up its operations to streams with the right data dependence.</p><p> 他们指出，将CUDA用于GPU会给MPI带来“新挑战。当MPI在主机上运行时，用户希望在设备上进行数据通信，但是MPI规范没有设备数据或主机数据的概念。”他们补充说：“使用非CUDA感知的MPI实现，程序员必须在设备和主机之间来回复制数据，以便在主机上进行计算。”尽管支持CUDA的MPI可以通过允许程序员使用相同的API将设备缓冲区传递给MPI来解决此问题，但是仍然存在“语义不匹配”，因为CUDA内核是在CUDA流上异步执行的，而MPI不知道这是什么流，并且可以不能将其操作与具有适当数据依赖关系的流对齐。 </p><p> This is one of several issues the PetscSF has tackled with Nvidia-specific work based on Nvidia’s implementation of the OpenSHMEM spec on its GPUs (NVSHMEM). Much of the performance and scalability are based on using NVSHMEM at the core of the PetscSF work.</p><p>这是PetscSF基于Nvidia在其GPU（NVSHMEM）上实施OpenSHMEM规范而通过Nvidia特定工作解决的几个问题之一。很多性能和可伸缩性都基于在PetscSF工作的核心上使用NVSHMEM。</p><p> Nvidia GPUs are not the only ones in the exascale field these days, of course. The research team that developed PetscSF used only Nvidia’s devices. We will be curious to see what, if any differences, we might expect from AMD GPUs, which are finding their way onto several pre-exascale and exascale-class systems.</p><p> 如今，英伟达GPU并不是亿亿级领域中唯一的GPU。开发PetscSF的研究小组仅使用Nvidia的设备。我们很想知道AMD GPU会带来什么变化（如果有差异的话），AMD GPU正在将其运用于数个亿亿级和万亿级的系统上。</p><p> The team takes a look at related efforts to fill in the raw MPI gaps. They say that PGAS languages (UPC, Chapel, OpenSHMEM) that provide users the “illusion of shared memory” lead to codes that are “error prone since shared memory programming easily leads to data race conditions and deadlocks that are difficult to detect, debug and fix” and without efforts, lead to applications that don’t get max performance—and it’s not easier than just programming in MPI. Other specific efforts, including High Performance Fortran, while allowing users to write data distribution directives for their arrays “failed because compilers, even today, are not powerful enough to do this with indirectly indexed arrays.”</p><p> 团队将研究相关的工作，以填补原始的MPI差距。他们说，向用户提供“共享内存错觉”的PGAS语言（UPC，Chapel，OpenSHMEM）导致“容易出错”的代码，因为共享内存编程很容易导致数据争用情况和死锁，难以检测，调试和锁定。修复”，并且不费吹灰之力就导致应用程序无法获得最佳性能-这不仅比使用MPI编程容易。其他一些特定的工作，包括High Performance Fortran，虽然允许用户为其数组编写数据分发指令，但“仍然失败，因为即使在今天，编译器也不足以使用间接索引数组来做到这一点。”</p><p> Much more detail can be found  here, including experimental results on the Summit system, which is often used to prove out new concepts in exascale software and applications before even large systems hit the floor in the coming few years. These results include an overall PetscSF “ping pong” test to compare its performance to raw MPI along with asynchronous conjugate gradient on GPUs.</p><p> 在这里可以找到更多详细信息，包括Summit系统的实验结果，该系统通常用于证明百亿亿次软件和应用程序中的新概念，甚至在未来几年甚至大型系统问世之前。这些结果包括整体PetscSF“乒乓”测试，以将其性能与原始MPI以及GPU上的异步共轭梯度进行比较。</p><p> Featuring highlights, analysis, and stories from the week directly from us to your inbox with nothing in between.</p><p> 直接从我们到您的收件箱，包括本周的重点摘要，分析和故事，之间没有任何内容。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.nextplatform.com/2021/03/01/rethinking-mpi-for-gpu-accelerated-supercomputers/">https://www.nextplatform.com/2021/03/01/rethinking-mpi-for-gpu-accelerated-supercomputers/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/gpu/">#gpu</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/mpi/">#mpi</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>