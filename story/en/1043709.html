<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>避免指令缓存未命中（2019） Avoiding instruction cache misses (2019)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Avoiding instruction cache misses (2019)<br/>避免指令缓存未命中（2019） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-09 15:50:25</div><div class="page_narrow text-break page_content"><p>Modern processors are quite complex, with many parts having the potential to become a bottleneck. It is relatively easy to reason about the performance of short pieces of code, especially if memory effects are kept to a minimum. Both static analysis tools like LLVM MCA and microbenchmarks can provide a lot of information in such cases. However, the behaviour of the whole program is not just the sum of those small parts. As the code becomes larger and more complex other effects start appearing. One of such potential problems are excessive instruction cache misses.</p><p>现代处理器非常复杂，许多部件都有可能成为瓶颈。短代码的性能相对容易推断，尤其是在将内存影响保持在最低水平的情况下。在这种情况下，静态分析工具（如LLVM MCA和微基准）都可以提供很多信息。但是，整个程序的行为不仅仅是这些小部分的总和。随着代码变大和越来越复杂，其他效果开始出现。这种潜在问题之一是过多的指令高速缓存未命中。</p><p> Every program has different properties, and those large-scale effects will affect it differently. However, if its job is to execute complex logic on a small amount of data, the instruction cache is likely to become a problem at some point. The actual impact may vary significantly from codebase to codebase, which is why I won’t show any numbers in this article. Let’s consider this just a collection of ideas, but it is not easy to tell how much any of them will help for a given application.</p><p> 每个程序都具有不同的属性，并且那些大规模的效果将对其产生不同的影响。但是，如果其工作是对少量数据执行复杂的逻辑，则指令高速缓存可能会在某个时候成为问题。实际的影响可能因代码库而异，这就是为什么我在本文中不显示任何数字的原因。让我们考虑一下这只是一个想法的集合，但是要说出这些想法对给定应用程序有多大帮助并不容易。</p><p> First, let’s have a quick look at the processor front end. Below is a simplified diagram of how it is arranged in Skylake, the numbers between units are the maxima per cycle.</p><p> 首先，让我们快速看一下处理器前端。下面是它在Skylake中的排列方式的简化图，单位之间的数字是每个周期的最大值。</p><p>  Each cycle the processor fetches up to 16 bytes from the instruction cache using information from the Branch Prediction Unit to predict the control flow. The pre-decode unit determines instruction lengths and puts up to five of them in the Instruction Queue. From the Instruction Queue, up to 5 instructions (with macro-fusion) are brought each cycle to the decoders. There is one complex decoder that can handle instructions that translate to up to 4 µops and 3 simple decoders that can handle only single-µop instructions. In total, all decoders are limited to producing no more than 5 µops each cycle. Instructions that require more than 4 µops go through Microcode Sequence ROM, which emits 4 µops per cycle, and while it is active, the decoders are disabled. There is also Decoded ICache ( DSB) which caches decoded µops. It can emit up to 6 µops each cycle. All µops, regardless of their source, end up in the Instruction Decode Queue (IDQ). The Loop Stream Detector (LSD) detects small loops and keeps them in the queue, so that no fetched, decodes or reads from the DSB are needed during the duration of the loop. IDQ is the last part of the front end, and the queued µops continue to the back end.</p><p>  在每个周期中，处理器都会使用来自分支预测单元的信息来从指令高速缓存中提取多达16个字节，以预测控制流。预解码单元确定指令长度，并将最多五个指令长度放入指令队列中。每个周期从指令队列中将多达5条指令（带有宏融合）带到解码器。有一个复杂的解码器可以处理最多转换为4 µops的指令，还有3个简单的解码器只能处理单µopop指令。总的来说，所有解码器每个周期只能产生不超过5 µops的数据。需要超过4 µops的指令将通过Microcode Sequence ROM，它每个周期发出4 µops，并且在激活时，解码器被禁用。还有一个解码的ICache（DSB），用于缓存解码的µop。每个周期最多可发射6 µops。所有µop，无论其来源如何，都将最终出现在指令解码队列（IDQ）中。环路流检测器（LSD）会检测到较小的循环并将其保留在队列中，因此在循环期间不需要从DSB进行读取，解码或读取。 IDQ是前端的最后一部分，排队的微操作继续到后端。</p><p> From the instruction cache point of view, the front end has two weaknesses. Firstly, instructions are processed in-order which can severely limit the processor ability to hide latencies of cache misses. HyperThreading can make sure that this part of the processor still does some useful work, but it is also the source of the second problem – all resources, including the L1 instruction cache and µop cache are shared between the hardware threads.</p><p> 从指令缓存的角度来看，前端有两个缺点。首先，指令按顺序处理，这会严重限制处理器隐藏高速缓存未命中延迟的能力。 HyperThreading可以确保处理器的这一部分仍然可以做一些有用的工作，但这也是第二个问题的根源–包括L1指令高速缓存和µop高速缓存在内的所有资源都在硬件线程之间共享。</p><p> Modern processors provide various metrics that help monitor their behaviour. However, the task of extracting the relevant data requires a proper approach if it is to be done efficiently. Top-down analysis is invaluable with helping to understand microarchitectural phenomena in large codebases. The idea is to monitor program behaviour with the  PMU counters and identify the bottleneck starting with the major functional parts of the CPU and then digging deeper narrowing down on the exact source of the problem. It can be done in an automated way by tools like VTune or toplev.</p><p> 现代处理器提供了各种指标来帮助监控其行为。但是，如果要高效地完成提取相关数据的任务，则需要采取适当的方法。自上而下的分析对于帮助理解大型代码库中的微体系结构现象而言是无价的。这个想法是使用PMU计数器监视程序行为，并从CPU的主要功能部分开始识别瓶颈，然后深入挖掘问题的确切根源。可以通过VTune或toplev之类的工具以自动化方式完成此操作。</p><p> FE Frontend_Bound: 39.48 +- 0.00 % SlotsBE Backend_Bound: 16.19 +- 0.00 % Slots This category represents fraction of slots where no uops are being delivered due to a lack of required resources for accepting new uops in the Backend...FE Frontend_Bound.Frontend_Latency: 24.92 +- 0.00 % SlotsFE Frontend_Bound.Frontend_Bandwidth: 13.45 +- 0.00 % SlotsFE Frontend_Bound.Frontend_Latency.ICache_Misses: 14.45 +- 0.00 % Clocks &lt;== This metric represents fraction of cycles the CPU was stalled due to instruction cache misses... Sampling events: frontend_retired.l2_miss:pp frontend_retired.l1i_miss:ppFE Frontend_Bound.Frontend_Latency.ITLB_Misses: 8.71 +- 0.00 % Clocks This metric represents fraction of cycles the CPU was stalled due to instruction TLB misses... Sampling events: frontend_retired.stlb_miss:pp frontend_retired.itlb_miss:ppFE Frontend_Bound.Frontend_Latency.Branch_Resteers: 8.42 +- 0.00 % Clocks_Est This metric represents fraction of cycles the CPU was stalled due to Branch Resteers... Sampling events: br_misp_retired.all_branches:uFE Frontend_Bound.Frontend_Bandwidth.MITE: 31.93 +- 0.00 % CoreClocks This metric represents Core fraction of cycles in which CPU	was likely limited due to the MITE pipeline (Legacy Decode Pipeline)...</p><p> FE Frontend_Bound：39.48 +-0.00％插槽BE Backend_Bound：16.19 +-0.00％插槽此类别表示由于缺少用于在后端中接受新的uops的必需资源而导致无法交付uops的插槽比例... FE Frontend_Bound.Frontend_Latency ：：24.92 +-0.00％SlotsFE Frontend_Bound.Frontend_Bandwidth：13.45 +-0.00％SlotsFE Frontend_Bound.Frontend_Latency.ICache_Misses：14.45 +-0.00％时钟＆lt; ==此度量标准表示由于指令高速缓存未命中而导致CPU停止的周期的一部分。 。采样事件：frontend_retired.l2_miss：pp frontend_retired.l1i_miss：ppFE Frontend_Bound.Frontend_Latency.ITLB_Misses：8.71 +-0.00％Clocks该指标表示由于指令TLB丢失而导致CPU停顿的周期的一部分。 ：pp frontend_retired.itlb_miss：ppFE Frontend_Bound.Frontend_Latency.Branch_Resteers：8.42 +-0.00％Clocks_Est此指标表示由于分支Restee而使CPU停止的周期的一部分rs ...采样事件：br_misp_retired.all_branches：uFE Frontend_Bound.Frontend_Bandwidth.MITE：31.93 +-0.00％CoreClocks此指标表示由于MITE管道（传统解码管道）而可能限制CPU使用的周期的核心部分。 </p><p> Above is an example of a toplev result. We can see that the instruction cache misses were the dominating bottleneck. Unsurprisingly instruction TLB misses also show up. On the front end bandwidth side of things, toplev points to the legacy decode pipeline. That makes perfect sense if the instructions are supplied from DSB or LSD, then there are no instruction fetches, and no cache misses.</p><p>上面是toplev结果的示例。我们可以看到指令高速缓存未命中是主要的瓶颈。毫不奇怪，也出现了TLB未命中的指令。在前端带宽方面，toplev指向传统解码管道。如果指令是从DSB或LSD提供的，那么就没有任何意义，并且没有指令提取，也没有缓存未命中。</p><p> Sometimes, the final summary may not provide sufficient information if there are changes in the code behaviour during the test. When that’s the case, a graph is likely to be a much more helpful way of presenting the results.</p><p> 有时，如果测试过程中代码行为发生变化，则最终摘要可能无法提供足够的信息。在这种情况下，图形可能是显示结果的一种更有用的方法。</p><p>  Tools like toplev are great for initial identification of the problem, but once that’s done what we need is a right way for comparing different solutions. Ultimately, the most important metric is the actual performance of the program in a real-life workload. toplev still can be helpful as it shows the balance between different performance-limiting factors. What also can be useful is  perf stat which can show the performance counter statistics. The event most relevant for us is  L1-icache-load-misses, though there are more model-specific registers that may be of interest.</p><p>  toplev之类的工具非常适合初步识别问题，但一旦完成，我们需要的是比较不同解决方案的正确方法。最终，最重要的指标是程序在实际工作负载中的实际性能。 toplev仍然可以提供帮助，因为它显示了不同性能限制因素之间的平衡。性能统计也可以显示性能计数器统计信息。与我们最相关的事件是L1-icache-load-misses，尽管可能有更多特定于模型的寄存器。</p><p> Now, that we know how to diagnose excessive instruction cache misses, let’s see what can be done to deal with this problem.</p><p> 现在，我们知道了如何诊断过多的指令高速缓存未命中，让我们来看看如何解决这个问题。</p><p>  If the number of executed instructions is the problem, the most obvious solution is to try to reduce that number. Obviously, that’s much easier said than done, but there are some common patterns for dealing with this issue. One example would be prepared statements in databases. The general idea is that if a client knows it will send requests that have some commonality, it can tell the database engine early that the requests are going to match specific templates. This information allows the server to do as much work as possible during the preparation stage, thus reducing the amount of logic that needs to be executed for each individual request.</p><p>  如果执行的指令数量有问题，最明显的解决方案是尝试减少该数量。显然，这说起来容易做起来难，但是有一些解决此问题的常用模式。一个示例是数据库中的准备好的语句。通常的想法是，如果客户端知道它将发送具有某种共性的请求，则可以尽早告知数据库引擎该请求将与特定模板匹配。此信息允许服务器在准备阶段进行尽可能多的工作，从而减少需要为每个单独的请求执行的逻辑量。</p><p> Extracting common patterns doesn’t have to be explicit. A server or any other kind of an application which actions depend on the user input could attempt to look for repeating patterns and cache some common parts. This is a very vague idea and most likely won’t be easily implementable in a lot of applications, but in some cases may be quite a natural solution. It also shows the main problem with the “just do less” approach – it is very application specific. On the plus, side, if this can be done, it is likely to help with the overall performance, not just the instruction cache misses.</p><p> 提取通用模式不必很明确。操作取决于用户输入的服务器或任何其他类型的应用程序都可能尝试查找重复模式并缓存一些公用部分。这是一个非常模糊的想法，很可能在许多应用程序中都不容易实现，但在某些情况下可能是很自然的解决方案。它还显示了“少做点事”方法的主要问题-这是非常特定于应用程序的。从正面来看，如果可以做到，则可能会提高整体性能，而不仅仅是指令高速缓存未命中。</p><p> Another potential problem is making sure that the preparation phase can really do something to help during the execution phase. If that means pre-computing some values, then it’s simple. However, if the only thing that preparation gives us is the knowledge which code paths are going to be exercised and which branches taken during the execution, it is going to be harder to take benefit from it. One option is to have some specialised code for the most common paths, C++ templates may come in handy here. If it is not easy to determine what may be the most common paths, then a just-in-time compiler may be used to generate code in the preparation stage.  1</p><p> 另一个潜在的问题是确保准备阶段在执行阶段确实可以做些帮助。如果那意味着要预先计算一些值，那么这很简单。但是，如果准备工作给我们带来的唯一好处就是知道将要执行哪些代码路径以及在执行过程中将执行哪些分支，那么将很难从中受益。一种选择是为最常用的路径提供一些专用的代码，C ++模板在这里可能会派上用场。如果不容易确定最常见的路径，则可以在准备阶段使用即时编译器来生成代码。 1个 </p><p>  So far, we have been trying to reduce the number of executed instructions, by taking advantage of some earlier knowledge to avoiding repeating the same work. In other words, we have introduced two stages:</p><p>到目前为止，我们一直在尝试通过利用一些较早的知识来避免重复执行相同的工作，以减少执行指令的数量。换句话说，我们引入了两个阶段：</p><p>  The way this can help with the instruction cache misses is that the execution stage, being smaller, is more likely to fit in the instruction cache. Whether this brings any measurable benefits depends highly on the type of the application and how much logic can be moved from execution to preparation stages.</p><p>  这有助于减少指令缓存的方法是，较小的执行阶段更可能适合指令缓存。这是否带来任何可衡量的收益，在很大程度上取决于应用程序的类型以及从执行到准备阶段可以转移多少逻辑。</p><p> We have already split our processing pipeline into preparation and execution stage. If the execution stage can fit in the instruction cache, we are done. However, often, that’s not the case. What we can do to improve the situation more is to split the execution into more stages. This time the goal is not to reuse work as it was with the preparation, but to group entities that need to have the same code executed for them. In other words, if the processing pipeline consists of steps A, B and C the idea is to separate them, add a queue in front of each of those stages, and then cycle through those stages each time handling multiple elements from the queue. Connections between the stages don’t have to be one-to-one, any directed graph is fine.</p><p> 我们已经将处理流程分为准备和执行阶段。如果执行阶段可以放入指令缓存中，那么我们就完成了。但是，通常情况并非如此。我们可以做的进一步改善这种情况的方法是将执行分为多个阶段。这次的目标不是重用准备中的工作，而是将需要为其执行相同代码的实体分组。换句话说，如果处理流水线由步骤A，B和C组成，则其思想是将它们分开，在每个阶段的前面添加一个队列，然后每次处理该队列中的多个元素时在这些阶段中循环。阶段之间的连接不必一对一，任何有向图都可以。</p><p>  In the diagram above, there is one stage that feeds tasks to one of two stages. This could be, for example, a front-end of a database server. The first stage does some initial request processing and then, depending on whether it is a read or write, puts it in the appropriate queue. Each stage processes requests in batches, the first one warms up the instruction cache so that the subsequent ones can take advantage of it and hopefully won’t see any instruction cache misses. To make this work well we need to ensure that each stage fits in the cache and the control flow doesn’t diverge too much – we won’t benefit from warmed up cache if each request takes a different path through the code.</p><p>  在上图中，有一个阶段将任务提供给两个阶段之一。例如，这可能是数据库服务器的前端。第一阶段进行一些初始请求处理，然后根据是读还是写，将其放入适当的队列中。每个阶段都会批量处理请求，第一个阶段会预热指令缓存，以便随后的指令可以利用它，并希望不会看到任何指令缓存未命中。为了使这项工作顺利进行，我们需要确保每个阶段都适合缓存，并且控制流不会有太大差异–如果每个请求在代码中采用不同的路径，我们就不会从预热的缓存中受益。</p><p> The architecture I’ve just described is very similar to  SEDA. For the instruction cache purposes, there’s no need to run execution stages on different threads. That actually may be inadvisable since the increase in latency caused by excessive communication between threads is one of the SEDA significant  problems. This can be entirely avoided with an asynchronous thread-per-core architecture if tasks are batched per thread. That’s how I did that in Scylla with quite good  results.</p><p> 我刚刚描述的架构与SEDA非常相似。出于指令缓存的目的，无需在不同的线程上运行执行阶段。这实际上可能是不可取的，因为由线程之间的过度通信导致的延迟增加是SEDA的重要问题之一。如果按线程分批处理任务，则可以使用异步的每核线程体系结构完全避免这种情况。这就是我在Scylla中做到的效果，</p><p> Regardless of how batching fits in the overall program architecture, there’s still one crucial issue to watch out for. Unless the latency of an individual task is irrelevant, there needs to be a bound on how long it can stay in the queue. This is a trade-off between the throughput and latency and the right solution to this problem is going to be application specific.</p><p> 无论批处理如何适合整个程序体系结构，仍然需要注意一个关键问题。除非单个任务的等待时间无关紧要，否则它在队列中的停留时间需要有一个界限。这是吞吐量和延迟之间的折衷，针对此问题的正确解决方案将是针对特定应用的。</p><p>  We have considered relatively high-level changes to the code and its architecture that may improve the instruction cache situation. It is time now too look at the lower level side of things, and more specifically what the compiler can do for us.</p><p>  我们已经考虑了对代码及其体系结构的较高级别的更改，这些更改可能会改善指令缓存情况。现在是时候再来看底层的东西了，更具体地说是编译器可以为我们做些什么。 </p><p> When it comes to instruction cache misses one optimisation that can change a lot is function inlining. On the one hand inserting a function body in its call sites may increase the code size and increase the number of instruction cache misses, but on the other hand, it allows more optimisations, avoids calling convention overhead, and improves code locality. The point is that inlining can make a significant difference in both directions, and it is definitely something worth checking.</p><p>当涉及到指令缓存时，函数内联是一项可能会改变很多的优化。一方面，在其调用位置中插入函数体可能会增加代码大小，并增加指令高速缓存未命中的次数，但另一方面，它允许进行更多优化，避免调用约定开销，并改善代码局部性。关键是内联可以在两个方向上都产生显着差异，这绝对值得检查。</p><p> It is also important to remember that the compilers usually allow more control than just, relatively coarse-grained, flags like  -finline-functions or  -finline-functions-called-once. There is also a set of  tunable parameters that helps control the compiler behaviour. For example,  inline-unit-growth limits the growth of the compilation unit. The actual effect depends on how the source code is organised, which may lead to quite confusing compiler behaviour if a parameter like this one is limiting the function inlining.</p><p> 同样重要的是要记住，编译器通常允许的控制权不仅仅是-finline-functions或-finline-functions-once（仅一次）之类的相对粗粒度的标志。还有一组可调参数可以帮助控制编译器的行为。例如，内联单元增长限制了编译单元的增长。实际效果取决于源代码的组织方式，如果像这样的参数限制函数内联，则可能导致相当混乱的编译器行为。</p><p> Another way of influencing inlining decisions is to use attributes like  [[gnu::always_inline]] or  [[gnu::noinline]] (and  [[gnu::flatten]] for the brave). This may help in some cases, but adds clutter to the code and if the compiler is unable to make the right decisions without them then it probably means that its heuristics need more tuning.</p><p> 影响内联决策的另一种方法是使用[[gnu :: always_inline]]或[[gnu :: noinline]]（对于勇敢者，则使用[[gnu :: flatten]]）之类的属性。在某些情况下这可能会有所帮助，但会给代码增加混乱，如果编译器在没有决策的情况下无法做出正确的决定，则可能意味着其启发式方法需要更多调整。</p><p> Function cloning is also a type of optimisation that can directly affect the code size. GCC may use it to do constant propagation across functions if the callee is not inlined.</p><p> 函数克隆也是一种优化类型，可以直接影响代码大小。如果未内联被调用者，则GCC可以使用它在各个函数之间进行恒定传播。</p><p> [[gnu::noinline]]  // disable inlining to show cloning  int  callee ( int  x ,  int  y )  {  return  x  +  1000  /  y ; } int  caller ( int  x )  {  return  callee ( x ,  4 ); }</p><p> [[gnu :: noinline]] //禁用内联以显示克隆int被调用者（int x，int y）{return x + 1000 / y; } int调用者（int x）{返回被调用者（x，4）; }</p><p>  callee(int, int) [clone .constprop.0]: lea eax, [rdi+250] retcallee(int, int): mov eax, 1000 cdq idiv esi add eax, edi retcaller(int): jmp callee(int, int) [clone .constprop.0]</p><p>  callee（int，int）[clone .constprop.0]：lea eax，[rdi + 250] retcallee（int，int）：mov eax，1000 cdq idiv esi add eax，edi retcaller（int）：jmp callee（int， int）[克隆.constprop.0]</p><p> Almost always function cloning will increase the total binary size. It may not necessarily be a problem, though. If the program is going to execute in the hot path only a few cloned versions that benefited significantly from the constant propagation, then we may expect a reduction of the instruction cache misses. Like most things in this section and, actually, the majority of this article, the effect depends on the codebase.</p><p> 几乎总是函数克隆会增加二进制的总大小。但是，它不一定是问题。如果程序将仅在热路径中执行，则只有几个克隆版本从持续传播中受益匪浅，因此我们可以预期指令缓存未命中的情况会减少。像本节中的大多数内容以及本文的大部分内容一样，其效果取决于代码库。 </p><p> The function definition needs to be visible in the translation unit that contains the call site to be eligible for inlining or cloning. This, again, means that the optimisation will depend on the organisation of the source code, which isn’t really something desirable. The solution for this problem is Link-Time Optimisation, which enables inlining functions across object files as well as other optimisations.</p><p>该功能定义需要在包含调用站点的翻译单元中可见，才能进行内联或克隆。再次，这意味着优化将取决于源代码的组织，这实际上不是所希望的。解决此问题的方法是链接时间优化，它可以内联跨目标文件的功能以及其他优化。</p><p> Another kind of optimisation that may cause trouble in the CPU front end is loop unrolling. Ideally, a loop would qualify for being detected by the LSD, if not then we’d want it to at least fit in DSB. However, that’s not necessarily what the best for the back end. As a result, the compiler may be quite aggressive with loop unrolling and produce quite a lot of code. This is definitely something to watch out for.</p><p> 可能在CPU前端引起麻烦的另一种优化是循环展开。理想情况下，如果没有，则LSD可以检测到循环，那么我们希望它至少适合DSB。但是，这不一定是后端的最佳选择。结果，编译器可能会非常激进地进行循环展开并产生大量代码。这绝对是要提防的东西。</p><p> Finally, the compiler can be asked to optimise code for size. At first, it sounds like a good idea, a smaller program is more likely to fit in the instruction cache. However, this is done at the cost of not doing some performance-oriented optimisations. The end result depends highly on how much code size reduction  -Os can bring, and how vital for achieving good performance the optimisations that this flag doesn’t enable are.</p><p> 最后，可以要求编译器针对大小优化代码。起初，这听起来像是一个好主意，较小的程序更可能适合指令高速缓存。但是，这样做是以不进行一些面向性能的优化为代价的。最终结果很大程度上取决于-O可以带来多少代码大小缩减，以及该标志未启用的优化对于实现良好性能至关重要。</p><p>  While the compilers are prepared to do a wide range of quite aggressive optimisations, there are some limitations that they can’t easily overcome. That requires the behaviour mandated by the language standard (unless the “as-if” rule applies) or the ABI. To deal with that, the programmer needs to be aware of those restrictions, and, ideally, follow practices that lead to code that’s easier to optimise.</p><p>  尽管编译器已经准备好进行各种非常积极的优化，但是存在一些无法克服的局限性。这就要求语言标准（除非适用“按原样”规则）或ABI规定的行为。为了解决这个问题，程序员需要意识到这些限制，并且在理想情况下，请遵循导致代码更易于优化的做法。</p><p> For example, the function calling conventions usually require non-trivial objects to be effectively passed by reference regardless of their size. Similarly, they can’t be returned in registers, and the callee needs to write the object to a buffer provided by the caller.</p><p> 例如，函数调用约定通常要求非平凡的对象通过引用有效地传递，而不管它们的大小如何。同样，它们也无法在寄存器中返回，并且被调用方需要将该对象写入调用方提供的缓冲区中。</p><p> Let’s have a look at how passing a  std::unique_ptr&lt;int&gt; to a function looks like on  x86_64-linux-gnu:</p><p> 让我们看一下如何传递std :: unique_ptr＆lt; int＆gt;函数看起来像在x86_64-linux-gnu上：</p><p> void  takes_unique_pointer ( std :: unique_ptr &lt; int &gt; ); void  takes_unique_pointer_noexcept ( std :: unique_ptr &lt; int &gt; )  noexcept ; void  takes_raw_pointer ( int * ); std :: unique_ptr &lt; int &gt;  unique ; void  gives_unique_pointer ()  {  takes_unique_pointer ( std :: move ( unique )); } void  gives_unique_pointer_noexcept ()  {  takes_unique_pointer_noexcept ( std :: move ( unique )); } int *  raw ; void  gives_raw_pointer ()  {  takes_raw_pointer ( std :: exchange ( raw ,  {})); }</p><p> void takes_unique_pointer（std :: unique_ptr＆lt; int＆gt;）; void takes_unique_pointer_noexcept（std :: unique_ptr＆lt; int＆gt;）noexcept; void takes_raw_pointer（int *）; std :: unique_ptr＆lt;整数＆gt;独特 ; void Gives_unique_pointer（）{ta​​kes_unique_pointer（std :: move（unique））; } void Gives_unique_pointer_noexcept（）{ta​​kes_unique_pointer_noexcept（std :: move（unique））; } int * raw; void Gives_raw_pointer（）{ta​​kes_raw_pointer（std :: exchange（raw，{}））; } </p><p>  gives_unique_pointer(): push rbp sub rsp, 16 mov rax, QWORD PTR unique[rip] lea rdi, [rsp+8] mov QWORD PTR unique[rip], 0 mov QWORD PTR [rsp+8], rax call takes_unique_pointer(std::unique_ptr&lt;int&gt;) mov rdi, QWORD PTR [rsp+8] test rdi, rdi je .L17 mov esi, 4 call operator delete(void*, unsigned long).L17: add rsp, 16 pop rbp ret mov rbp, rax jmp .L7gives_unique_pointer() [clone .cold]:.L7: mov rdi, QWORD PTR [rsp+8] test rdi, rdi je .L16 mov esi, 4 vzeroupper call operator delete(void*, unsigned long).L8: mov rdi, rbp call _Unwind_Resume.L16: vzeroupper jmp .L8gives_unique_pointer_noexcept(): sub rsp, 24 mov rax, QWORD PTR unique[rip] lea rdi, [rsp+8] mov QWORD PTR unique[rip], 0 mov QWORD PTR [rsp+8], rax call takes_unique_pointer_noexcept(std::unique_ptr&lt;int&gt;) mov rdi, QWORD PTR [rsp+8] test rdi, rdi je .L24 mov esi, 4 call operator delete(void*, unsigned long).L24: add rsp, 24 retgives_raw_pointer(): mov rdi, QWORD PTR raw[rip] mov QWORD PTR raw[rip], 0 jmp takes_raw_pointer(int*)</p><p>lets_unique_pointer（）：推送rbp sub rsp，16 mov rax，QWORD PTR unique [rip] lea rdi，[rsp + 8] mov QWORD PTR unique [rip]，0 mov QWORD PTR [rsp + 8]，rax调用takes_unique_pointer（std ::: unique_ptr＆lt; int＆gt;）mov rdi，QWORD PTR [rsp + 8]测试rdi，rdi je .L17 mov esi，4个调用运算符delete（void *，unsigned long）.L17：添加rsp，16个pop rbp ret mov rbp ，rax jmp .L7gives_unique_pointer（）[clone .cold] :. L7：mov rdi，QWORD PTR [rsp + 8] test rdi，rdi je .L16 mov esi，4 vzeroupper调用运算符delete（void *，unsigned long）.L8 ：mov rdi，rbp call _Unwind_Resume.L16：vzeroupper jmp .L8gives_unique_pointer_noexcept（）：sub rsp，24 mov rax，QWORD PTR unique [rip] lea rdi，[rsp + 8] mov QWORD PTR unique [rip]，0 mov QWORD PTR [rsp + 8]，rax调用takes_unique_pointer_noexcept（std :: unique_ptr＆lt; int＆gt;）mov rdi，QWORD PTR [rsp + 8]测试rdi，rdi je .L24 mov esi，4个调用运算符delete（void *，unsigned long）。 L24：添加rsp，24 retgives_raw_pointer（）：mov rdi，QWORD PTR raw [rip] mov QWORD PTR raw [rip]，0 jmp takes_raw_point er（int *）</p><p> That’s a lot of code. Even though a  std::unique_ptr is just a single pointer with more logic associated with it, being non-trivial, it cannot be passed via registers. Moreover, the Itanium C++ ABI makes the caller responsible for destroying the objects passed by value to the callee. What that means is that this clean up needs to be repeated at each call site and can’t really be optimised since at this point the compiler doesn’t know what the callee does. Moreover, because of the exceptions, there is more than one way to leave a function via either a return or a throw. Unless the compiler can prove that no exception leaves the callee, because it is  noexcept or the implementation is available and straightforward enough, an additional handler doing the appropriate cleanup needs to be emitted.</p><p> 那是很多代码。即使std :: unique_ptr只是具有更多逻辑关联的单个指针，这也是不平凡的，但它不能通过寄存器传递。而且，Itanium C ++ ABI使调用者负责销毁按值传递给被调用者的对象。这意味着需要在每个调用站点上重复进行此清理操作，并且实际上无法对其进行优化，因为此时编译器不知道被调用者的操作。此外，由于异常，通过返回或抛出来离开函数的方法不止一种。除非编译器能够证明没有异常离开被调用者（因为没有异常），或者实现可用并且足够直接，否则就需要发出一个进行适当清理的附加处理程序。</p><p> Obviously, many more things may noticeably contribute to the code size growth: restrictions on copy elision, virtual and non-virtual thunks adjusting the object pointer in calls to virtual functions, accessing thread-local variables – just to name the few. However, there are some general guidelines. Often, a lot of complexity is caused by non-trivial objects, e.g. working with a  std::string_view instead of  std::string may be a good idea, as long as it doesn’t result in dangling pointers, that is. Incidentally, the restrictions that  constexpr imposes force the code to avoid the most expensive constructs and attempting to make as much code as possible  constexpr is very likely to result in the compiler producing simpler code that’s easier to optimise.</p><p> 显然，还有更多的事情可能明显地促进了代码大小的增长：对复制省略，虚拟和非虚拟重击的限制，在对虚拟函数的调用中调整对象指针，访问线程局部变量–仅举几例。但是，有一些通用准则。通常，很多复杂性是由非平凡的对象引起的，例如使用std :: string_view而不是std :: string可能是一个好主意，只要它不会导致指针悬空即可。顺便说一下，constexpr施加的限制迫使代码避免使用最昂贵的结构，而尝试制作尽可能多的constexpr代码很可能导致编译器生成更易于优化的简单代码。</p><p>  Compilers usually allow the programmer to provide some information about the expected characteristics of the code. For our purposes the most interesting one are  [[gnu::cold]] and  [[gnu::hot]] attributes in GCC and Clang. They allow telling the compiler whether a function is unlikely executed or a hot spot. This affects the optimisations and the location of those functions in the output binary. Cold functions are going to be grouped together in a  .text.cold section to make sure they stay out of the way of the rest of the code and hot functions are going to be grouped together in  .text.hot to improve instruction cache locality.</p><p>  编译器通常允许程序员提供一些有关代码预期特性的信息。出于我们的目的，最有趣的一个是GCC和Clang中的[[gnu :: cold]]和[[gnu :: hot]]属性。它们允许告诉编译器某个函数是不太可能执行还是热点。这会影响这些功能在输出二进制文件中的优化和位置。冷函数将在.text.cold节中组合在一起，以确保它们不影响其余代码，而热函数将在.text.hot中组合在一起，以改善指令缓存的位置。</p><p> Moreover, with  -freorder-blocks-and-partition flag GCC can split functions and put cold blocks in  .text.cold section. Block that contain calls to cold functions are considered cold, as well as, some other like exception handlers. Even though at a different location in memory, those cold blocks are still effectively considered to be a part of the same function so the control can flow to them with just a single jump instruction, without any need to obey the calling convention.</p><p> 此外，使用-freorder-blocks-and-partition标志，GCC可以拆分功能并将冷块放在.text.cold节中。包含对Cold函数的调用的块以及其他类似异常处理程序的块也被视为Cold。即使在内存中的其他位置，这些冷块仍被有效地视为同一功能的一部分，因此控件只需一条跳转指令即可流向它们，而无需遵守调用约定。</p><p> static  constexpr  int  threshold  =  100 ; void  hot_path ( int  value ); [[gnu::cold]] void  cold_path ( int  value ); void  do_work ( int  value )  {  if  ( value  &gt;  threshold )  {  hot_path ( value  /  5 );  return ;  }  cold_path ( value  /  3 ); }</p><p> 静态constexpr int阈值= 100; void hot_path（int value）; [[gnu :: cold]] void cold_path（int value）;无效do_work（int值）{if（值>阈值）{hot_path（值/ 5）;回报; } cold_path（value / 3）; }</p><p>  do_work(int): cmp edi, 100 jle .L2 movsx rax, edi sar edi, 31 imul rax, rax, 1717986919 sar rax, 33 sub eax, </p><p>  do_work（int）：cmp edi，100 jle .L2 movsx rax，edi sar edi，31 imul rax，rax，1717986919 sar rax，33 sub eax， </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/">https://paweldziepak.dev/2019/06/21/avoiding-icache-misses/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/缓存/">#缓存</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/指令/">#指令</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>