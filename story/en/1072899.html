<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>DeepETA：优步如何利用深度学习预测到达时间DeepETA: How Uber Predicts Arrival Times Using Deep Learning</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">DeepETA: How Uber Predicts Arrival Times Using Deep Learning<br/>DeepETA：优步如何利用深度学习预测到达时间</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-13 15:20:32</div><div class="page_narrow text-break page_content"><p>At Uber, magical customer experiences depend on accurate arrival time predictions (ETAs). We use ETAs to calculate fares, estimate pickup times, match riders to drivers, plan deliveries, and more. Traditional routing engines compute ETAs by dividing up the road network into small road segments represented by weighted edges in a graph. They use shortest-path algorithms to find the best path through the graph and add up the weights to derive an ETA. But as we all know, the map is not the terrain: a road graph is just a model, and it can’t perfectly capture conditions on the ground. Moreover, we may not know which route a particular rider and driver will choose to their destination. By training machine learning (ML) models on top of the road graph prediction using historical data in combination with real-time signals, we can refine ETAs that better predict real-world outcomes.</p><p>在优步，神奇的客户体验取决于准确的到达时间预测（ETA）。我们使用ETA来计算票价、估计接送时间、将乘客与司机匹配、计划交付等等。传统的路由引擎通过将道路网络划分为图中加权边表示的小路段来计算ETA。他们使用最短路径算法在图中找到最佳路径，并将权重相加得出ETA。但我们都知道，地图不是地形：道路图只是一个模型，它不能完美地捕捉地面条件。此外，我们可能不知道特定的骑手和司机会选择哪条路线到达目的地。通过使用历史数据和实时信号，在道路图预测的基础上训练机器学习（ML）模型，我们可以改进ETA，更好地预测现实世界的结果。</p><p> For several years, Uber used gradient-boosted decision tree ensembles to refine ETA predictions. The ETA model and its training dataset grew steadily larger with each release. To keep pace with this growth, Uber’s   Apache Spark  ™  team contributed upstream improvements [  1 ,   2 ] to XGBoost to allow the model to grow ever deeper, making it one of the largest and deepest XGBoost ensembles in the world at that time. Eventually, we reached a point where increasing the dataset and model size using XGBoost became untenable. To continue scaling the model and improving accuracy, we decided to explore deep learning because of the relative ease of scaling to large datasets using   data-parallel SGD  [3]. To justify switching to deep learning we needed to overcome three main challenges:</p><p>几年来，优步使用梯度增强的决策树集合来改进ETA预测。ETA模型及其训练数据集随着每次发布而稳步增长。为了跟上这一增长，优步的Apache Spark™ 该团队对XGBoost进行了上游改进[1,2]，以使模型不断深入，使其成为当时世界上最大、最深的XGBoost集成之一。最终，我们达到了一个点，即使用XGBoost增加数据集和模型大小变得不可行。为了继续扩展模型并提高准确性，我们决定探索深度学习，因为使用数据并行SGD可以相对轻松地扩展到大型数据集[3]。为了证明转向深度学习的合理性，我们需要克服三个主要挑战：</p><p> Generality:  The model must provide ETA predictions globally across all of Uber’s lines of business such as mobility and delivery.</p><p>通用性：该模型必须提供Uber所有业务线（如移动和交付）的全球ETA预测。</p><p> To meet these challenges, Uber AI partnered with Uber’s Maps team on a project called DeepETA to develop a low-latency deep neural network architecture for global ETA prediction. In this blog post, we’ll be walking you through some of the learnings and design choices that helped DeepETA become the new production ETA model at Uber.</p><p>为了应对这些挑战，优步人工智能与优步地图团队合作开展了一个名为DeepETA的项目，以开发一种用于全球ETA预测的低延迟深度神经网络体系结构。在这篇博文中，我们将带您了解帮助DeepETA成为优步新生产ETA模式的一些经验和设计选择。</p><p>   In the past few years, there has been increasing interest in systems that combine physical models of the world with deep learning. We take a similar approach to ETA prediction at Uber. Our physical model is a routing engine that uses map data and real-time traffic measurements to predict an ETA as a sum of segment-wise traversal times along the best path between two points. We then use machine learning to predict the residual between the routing engine ETA and real-world observed outcomes. We call this hybrid approach ETA post-processing, and DeepETA is an example of a  post-processing model. From a practical perspective, it’s generally easier to assimilate new data sources and accommodate fast-changing business requirements by updating the post-processing model than it is to refactor the routing engine itself.</p><p>在过去几年里，人们对将世界物理模型与深度学习相结合的系统越来越感兴趣。我们对优步的ETA预测采取了类似的方法。我们的物理模型是一个路由引擎，它使用地图数据和实时流量测量来预测ETA，作为沿两点之间最佳路径的分段遍历时间之和。然后，我们使用机器学习来预测路由引擎ETA和实际观测结果之间的残差。我们称这种混合方法为ETA后处理，DeepETA是后处理模型的一个例子。从实际角度来看，通过更新后处理模型来吸收新的数据源和适应快速变化的业务需求通常比重构路由引擎本身更容易。</p><p>   To be able to predict the ETA residual, a post-processing ML model takes into account spatial and temporal features, such as the origin, destination and time of the request, as well information about real-time traffic and the nature of the request, such as whether it is a delivery dropoff or rideshare pickup, as illustrated in Figure 1. This post-processing model is the highest QPS (queries per second) model at Uber. This model needs to be fast so as not to add too much latency to an ETA request, and they need to improve ETA accuracy as measured by mean absolute error (MAE) across different segments of the data.</p><p>为了能够预测ETA残差，后处理ML模型考虑了空间和时间特征，如请求的来源、目的地和时间，以及有关实时通信量和请求性质的信息，如图1所示，它是投递放弃还是乘坐共享拾取。这种后处理模式是优步最高的QPS（每秒查询数）模式。该模型需要快速，以免给ETA请求增加太多延迟，并且需要提高ETA的准确性，如通过不同数据段的平均绝对误差（MAE）测量的那样。</p><p>   The DeepETA team tested and tuned 7 different neural network architectures:   MLP  [4],   NODE  [5],   TabNet  [6],   Sparsely Gated Mixture-of-Experts  [7],   HyperNetworks  [8],   Transformer  [9] and   Linear Transformer  [10]. We found that an encoder-decoder architecture with self-attention provided the best accuracy. Figure 2 illustrates the high-level architecture of our design. In addition, we tested different feature encodings and found that discretizing and embedding all of the inputs to the model provided a significant lift over alternatives.</p><p>DeepETA团队测试并调整了7种不同的神经网络体系结构：MLP[4]、节点[5]、TabNet[6]、稀疏的专家组合[7]、超网络[8]、变压器[9]和线性变压器[10]。我们发现，具有自我关注的编码器-解码器架构提供了最佳的准确性。图2展示了我们设计的高层架构。此外，我们测试了不同的特征编码，发现离散化和嵌入模型的所有输入提供了显著的提升。</p><p>    Many people are familiar with the Transformer architecture because of its applications in Natural Language Processing and Computer Vision, but it might not be obvious how Transformers can be applied to tabular data problems like ETA prediction. The defining innovation of Transformer is the self-attention mechanism. Self-attention is a sequence-to-sequence operation that takes in a sequence of vectors and produces a reweighted sequence of vectors. Details can be found in the   Transformer paper  [9].</p><p>由于Transformer在自然语言处理和计算机视觉中的应用，许多人都熟悉Transformer体系结构，但Transformer如何应用于ETA预测等表格数据问题可能并不明显。变压器的定义性创新是自我关注机制。自我注意是一种序列对序列的操作，它接收向量序列，并产生一个重新加权的向量序列。有关详细信息，请参见《变压器》一文[9]。</p><p> In a language model, each vector represents a single word token, but in the case of DeepETA, each vector represents a single feature, such as the origin of the trip or the time of day. Self-attention uncovers pairwise interactions among the K features in a tabular dataset by explicitly computing a K*K attention matrix of pairwise dot products, using the softmax of these scaled dot-products to reweight the features. When the self-attention layer processes each feature, it looks at every other feature in the input for clues and outputs the representation of this feature as a weighted sum of all features. This process is illustrated in Figure 3. Through this way, we can bake the understanding of all the temporal and spatial features into the one feature currently being processed and focus on the features that matter. In contrast to language models, there is no positional encoding in DeepETA since the order of the features doesn’t matter.</p><p>在语言模型中，每个向量代表一个单词标记，但在DeepETA的情况下，每个向量代表一个单一的特征，例如旅行的起源或一天中的时间。通过显式计算成对点积的K*K注意矩阵，使用这些缩放点积的softmax重新加权特征，自我注意揭示了表格数据集中K个特征之间的成对交互。当自我注意层处理每个特征时，它会查看输入中的每个其他特征以寻找线索，并将该特征的表示输出为所有特征的加权和。这个过程如图3所示。通过这种方式，我们可以将对所有时间和空间特征的理解转化为当前正在处理的一个特征，并将重点放在重要的特征上。与语言模型相比，DeepETA中没有位置编码，因为特征的顺序无关紧要。</p><p>   Taking a trip from origin A to destination B as an example, the self-attention layer scales the importance of the features given the time of the day, origin and destination, traffic conditions etc. A visualization of the self-attention is shown in Figure 4 (generated from   Tensor2Tensor  [11]) with 8 colors corresponding to 8 attention heads and shares corresponding to random generated attention weights.</p><p>以从起点a到目的地B的旅行为例，自我关注层根据一天中的时间、起点和目的地来衡量特征的重要性，交通状况等。图4（由Tensor2Tensor[11]生成）显示了自我注意的可视化，8种颜色对应8个注意头，共享对应随机生成的注意权重。</p><p>     The DeepETA model embeds all categorical features, and bucketizes all continuous features before embedding them. Somewhat counterintuitively, bucketizing continuous features led to better accuracy than using continuous features directly. While bucketizing isn’t strictly necessary because neural networks can learn any non-linear discontinuous function, a network with bucketized features may have an advantage because it doesn’t have to spend any of its parameter budget learning to partition the input space. Like the Gradient Boosted Decision Tree Neural Network paper [12], we found that using quantile buckets provided better accuracy than equal-width buckets. We suspect that quantile buckets perform well because they maximize entropy: for any fixed number of buckets, quantile buckets convey the most information (in bits) about the original feature value compared to any other bucketing scheme.</p><p>DeepETA模型嵌入了所有分类特征，并在嵌入之前对所有连续特征进行了Bucketize处理。在某种程度上，与直接使用连续特征相比，对连续特征进行bucketizing处理会带来更好的准确性。由于神经网络可以学习任何非线性不连续函数，所以bucketizing并不是绝对必要的，但具有bucketizing特征的网络可能具有优势，因为它不必花费任何参数预算学习来划分输入空间。与梯度增强的决策树神经网络论文[12]一样，我们发现使用分位数桶比等宽桶具有更好的精度。我们怀疑分位数桶的表现很好，因为它们最大化了熵：对于任何固定数量的桶，与任何其他bucketing方案相比，分位数桶传递的关于原始特征值的信息（以位为单位）最多。</p><p>   Post-processing models receive the origin and destination of a trip as latitudes and longitudes. Because these start and end points are very important for predicting ETAs, DeepETA encodes them differently from other continuous features. Location data is distributed very unevenly over the globe and contains information at multiple spatial resolutions. We therefore quantize locations into multiple resolution grids based on latitude and longitude. As the resolution increases, the number of distinct grid cells grows exponentially and the average amount of data in each grid cell decreases proportionally. We explored three different strategies for mapping these grids to embeddings:</p><p>后处理模型以纬度和经度的形式接收旅行的起点和终点。因为这些起点和终点对于预测ETA非常重要，所以DeepETA对它们的编码不同于其他连续特征。位置数据在全球分布非常不均匀，包含多个空间分辨率的信息。因此，我们根据纬度和经度将位置量化为多分辨率网格。随着分辨率的增加，不同网格单元的数量呈指数增长，每个网格单元中的平均数据量成比例减少。我们探讨了将这些网格映射到嵌入的三种不同策略：</p><p> Exact indexing , which maps each grid cell to a dedicated embedding. This takes up the most space.</p><p>精确索引，将每个网格单元映射到专用嵌入。这占用了最多的空间。</p><p> Feature hashing  [16], which maps each grid cell into a compact range of bins using a hash function. The number of bins is much smaller than with exact indexing.</p><p>特性散列[16]，它使用散列函数将每个网格单元映射到一个紧凑的存储单元范围中。与精确索引相比，存储箱的数量要小得多。</p><p> Multiple feature hashing , which extends feature hashing by mapping each grid cell to multiple compact ranges of bins using independent hash functions. See Figure 5:</p><p>多特征散列，它通过使用独立的散列函数将每个网格单元映射到多个紧凑范围的容器来扩展特征散列。参见图5：</p><p>    Our experiments showed that while feature hashing saved space compared to exact indexing, the accuracy was the same or slightly worse depending on the grid resolution. This is likely due to hash collisions that cause some information to be lost. Multiple feature hashing provided the best accuracy and latency while still saving space compared to exact indexing. This implies that the network is able to combine the information from multiple independent hash buckets to undo the negative effects of single-bucket collisions.</p><p>我们的实验表明，虽然与精确索引相比，特征散列节省了空间，但根据网格分辨率的不同，准确度相同或稍差。这可能是由于散列冲突导致一些信息丢失。与精确索引相比，多功能哈希提供了最佳的准确性和延迟，同时还节省了空间。这意味着网络能够组合来自多个独立散列存储桶的信息，以撤销单个存储桶冲突的负面影响。</p><p>   DeepETA’s serving latency requirements are very stringent. While it’s possible to accelerate inference by using specialized hardware or post-training optimizations, in this section we discuss architectural design decisions that helped DeepETA minimize latency.</p><p>DeepETA的服务延迟要求非常严格。虽然可以通过使用专用硬件或培训后优化来加速推理，但在本节中，我们将讨论帮助DeepETA最小化延迟的体系结构设计决策。</p><p>   While the transformer-based encoder provided the best accuracy, it was too slow to meet the latency requirements for online real-time serving. The original self-attention model has quadratic complexity because it computes a K   K attention matrix from K inputs. There have been multiple research works that linearize the self-attention calculation, for example   linear transformer  [10],   linformer  [13],   performer  [14]. After experimentation, we chose the linear transformer, which uses the kernel trick to avoid calculating the attention matrix.</p><p>虽然基于变压器的编码器提供了最佳精度，但速度太慢，无法满足在线实时服务的延迟要求。原始的自我注意模型具有二次复杂性，因为它从K个输入计算K个注意矩阵。已经有多个研究工作将自我注意力计算线性化，例如线性变压器[10]、linformer[13]、performer[14]。经过实验，我们选择了线性变换器，它使用核技巧来避免计算注意矩阵。</p><p> To illustrate the time complexity, let’s use the following example: say we have K inputs with dimension d. The original transformer’s time complexity is O(K  2 d), while the linear transformer’s time complexity is O(Kd  2 ). If we have 40 features all with 8 dimensions, i.e. K = 40, d = 8, K  2 d = 12,800, while Kd  2  = 2,560. It’s clear that the linear transformer is faster whenever K&gt;d.</p><p>为了说明时间复杂度，让我们使用以下示例：假设有K个维度为d的输入。原始转换器的时间复杂度为O（k2d），而线性转换器的时间复杂度为O（kd2）。如果我们有40个特征，都有8个维度，即K=40，d=8，K2 d=12800，而Kd 2=2560。很明显，只要K&gt；D</p><p>   Another secret to making DeepETA fast is to utilize feature sparsity. While the model has hundreds of millions of parameters, any one prediction touches only a tiny fraction of them, roughly 0.25%. How did we achieve this?</p><p>使DeepETA快速的另一个秘密是利用特征稀疏性。虽然该模型有数亿个参数，但任何一个预测都只涉及其中的一小部分，约为0.25%。我们是如何做到这一点的？</p><p> First of all, the model itself is relatively shallow with just a handful of layers. The vast majority of the parameters exist in embedding lookup tables. By discretizing the inputs and mapping them to embeddings, we avoid evaluating any of the unused embedding table parameters.</p><p>首先，模型本身相对较浅，只有几层。绝大多数参数都存在于嵌入的查找表中。通过将输入离散化并将其映射到嵌入，我们避免了计算任何未使用的嵌入表参数。</p><p> Discretizing the inputs gives us a clear speed advantage at serving time compared to alternative implementations. Take the geospatial embeddings pictured in Figure 5 as an example. To map a latitude and longitude to an embedding, DeepETA simply quantizes the coordinates and performs a hash lookup, which takes O(1) time. In comparison, storing embeddings in a tree data structure would require O(log N) lookup time, while using fully-connected layers to learn the same mapping would require O(N  2 ) lookup time. Seen from this perspective, discretizing and embedding inputs is simply an instance of the classic space vs time tradeoff in computer science: by precomputing partial answers in the form of large embedding tables learned during training, we reduce the amount of computation needed at serving time.</p><p>与其他替代实现相比，离散化输入在服务时间为我们提供了明显的速度优势。以图5所示的地理空间嵌入为例。要将纬度和经度映射到嵌入，DeepETA只需量化坐标并执行哈希查找，这需要O（1）个时间。相比之下，在树数据结构中存储嵌入需要O（logn）查找时间，而使用完全连接的层来学习相同的映射需要O（n2）查找时间。从这个角度来看，离散化和嵌入输入只是计算机科学中经典的空间与时间权衡的一个例子：通过以训练期间学习的大型嵌入表的形式预先计算部分答案，我们减少了服务时间所需的计算量。</p><p>   One of the design goals of DeepETA is to provide a general ETA model that serves all of Uber’s lines of business across the globe. This can be challenging because different lines of business have different needs and different data distributions. The overall model structure is shown in Figure 6 below.</p><p>DeepETA的设计目标之一是提供一个通用的ETA模型，服务于优步在全球的所有业务线。这可能是一个挑战，因为不同的业务线有不同的需求和不同的数据分布。整体模型结构如下图6所示。</p><p>    Once we have learned meaningful feature representations, the next step is to decode them and make predictions. In our case, the decoder is a fully connected neural network with a segment bias adjustment layer. The distribution of absolute errors varies by a lot across delivery trips vs rides trips, long vs short trips, pick-up vs drop-off trips, and also across global mega-regions. Adding bias adjustment layers to adjust the raw prediction for each of the different segments can account for their natural variations and in turn improve the MAE. This approach performs better than simply adding segment features to the model. The reason we implemented a bias adjustment layer instead of a multi-task decoder is due to latency constraints. We also employ a few tricks to further improve prediction accuracy, such as using ReLU at output to force predicted ETA to be positive; clamping to reduce the effect of extreme values.</p><p>一旦我们学会了有意义的特征表示，下一步就是解码它们并做出预测。在我们的例子中，解码器是一个具有分段偏差调整层的完全连接的神经网络。绝对误差的分布在送货旅行与乘车旅行、长途旅行与短途旅行、上下车旅行以及全球大型区域之间差异很大。添加偏差调整层来调整每个不同片段的原始预测，可以解释它们的自然变化，进而改善MAE。这种方法比简单地向模型中添加线段特征效果更好。我们实现偏差调整层而不是多任务解码器的原因是延迟限制。我们还采用了一些技巧来进一步提高预测精度，例如在输出端使用ReLU强制预测ETA为正；夹紧以减少极值的影响。</p><p>   Different business use cases require different types of ETA point estimates and will also have varying proportions of outliers in their data. For example, we want to estimate a mean ETA for fare computation but need to control for the effect of outliers. Other use cases may call for a specific quantile of the ETA distribution. To accommodate this diversity, DeepETA uses a parameterized loss function, asymmetric Huber loss, which is robust to outliers and can support a range of commonly used point estimates.</p><p>不同的业务用例需要不同类型的ETA点估计，并且在它们的数据中也会有不同比例的异常值。例如，我们希望估算票价计算的平均预计到达时间，但需要控制异常值的影响。其他用例可能需要ETA分布的特定分位数。为了适应这种多样性，DeepETA使用了一个参数化的损失函数，即不对称Huber损失，它对异常值具有鲁棒性，并且可以支持一系列常用的点估计。</p><p> Asymmetric Huber loss has two parameters, delta and omega, that control the degree of robustness to outliers and the degree of asymmetry respectively. By varying delta (as shown in Figure 7), squared error and absolute error can be smoothly interpolated, with the latter being less sensitive to outliers. By varying omega, you can control the relative cost of underprediction vs overprediction, which is useful in situations where being a minute late is worse than being a minute early. These parameters not only make it possible to mimic other commonly used regression loss functions, but also make it possible to tailor the point estimate produced by the model to meet diverse business goals.</p><p>不对称Huber损失有两个参数delta和omega，分别控制对异常值的鲁棒性程度和不对称程度。通过改变增量（如图7所示），平方误差和绝对误差可以平滑地插值，后者对异常值不那么敏感。通过改变ω，你可以控制预测不足与预测过高的相对成本，这在迟到一分钟比提前一分钟更糟糕的情况下很有用。这些参数不仅可以模拟其他常用的回归损失函数，还可以定制模型生成的点估计，以满足不同的业务目标。</p><p>    We leveraged the Canvas framework from    Uber’s ML platform – Michelangelo  [15] to train and deploy the model. The exact architecture we prototyped is essentially depicted in Figure 8. Once the model is trained and deployed to Michelangelo, we need to serve those predictions to users for real-time ETA prediction. The high-level architecture is shown in Figure 9 below: Uber consumers’ requests are routed through various services to the uRoute service. The uRoute service serves as a frontend for all routing lookups. It makes requests to the routing engine to produce route-lines and ETAs. It uses this ETA and other model features to make requests to the Michelangelo Online prediction service to get predictions from the DeepETA model. Periodic auto retraining workflows are set up to retrain and validate the model.</p><p>我们利用Uber的ML平台——米开朗基罗[15]的Canvas框架来训练和部署模型。我们原型化的确切架构基本上如图8所示。一旦模型被训练并部署到米开朗基罗，我们需要为用户提供这些预测，以便实时预测埃塔。高级架构如下图9所示：Uber消费者的请求通过各种服务路由到uRoute服务。uRoute服务充当所有路由查找的前端。它向路由引擎发出生成路由线和ETA的请求。它使用此ETA和其他模型功能向米开朗基罗在线预测服务发出请求，以从DeepETA模型获得预测。定期设置自动再培训工作流，以对模型进行再培训和验证。</p><p>      We have launched this DeepETA model into production for global 4-wheel ETA prediction. The DeepETA model launch makes it both possible and efficient to train and serve large-scale Deep Learning models that predict ETAs better than XGBoost approaches. DeepETA delivers an immediate improvement to metrics in production and establishes a model foundation that can be reused for multiple consumer use cases.</p><p>我们已经将这个DeepETA模型投入生产，用于全球四轮ETA预测。DeepETA模型的推出使得培训和服务大规模深度学习模型成为可能，而且效率更高。与XGBoost方法相比，该模型能更好地预测ETA。DePETA对生产中的度量提供了即时的改进，并建立了一个可以用于多个消费者用例的模型基础。</p><p> DeepETA enables the ETA team to explore diverse model architectures and output multiple ETAs that can be tailored for each consumer. More work is underway to further expand the accuracy improvements it can deliver by looking at every aspect of the modeling process covering Dataset, Features, Transformations, Model Architectures, Training Algorithms, Loss Functions, and Tooling/Infrastructure Improvements. In the future, the team will explore enhancements such as continuous, incremental training, which will enable ETAs to be trained on fresher data.</p><p>DeepETA使ETA团队能够探索不同的模型体系结构，并输出可为每个消费者定制的多个ETA。通过查看建模过程的各个方面，包括数据集、功能、转换、模型体系结构、培训算法、损失函数和工具/基础设施改进，正在进行更多的工作，以进一步扩大其可提供的准确性改进。在未来，该团队将探索增强功能，例如持续、增量培训，这将使ETA能够在更新鲜的数据上接受培训。</p><p>   DeepETA is a product of an Uber AI &lt;&gt; Maps partnership. We’d like to thank the following people who helped make this possible:</p><p>DeepETA是Uber AI的产品&lt&gt；地图伙伴关系。我们要感谢以下帮助实现这一目标的人：</p><p> Software Engineers: Michael Albada, Apurva Bhandari, Aditya Bhave, Tanmay Binaykiya, Chongxiao Cao, Sashikanth Chandrasekaran, Eric Chen, Hao Jiang, Vladimir Kuzmin, Michael Mallory, Michael Mui, Sean Po, Rich Porter, Vivek Sah, Raajay Viswanathan, Joseph Wang, Peng Zhang</p><p>软件工程师：Michael Albada、Apurva Bhandari、Aditya Bhave、Tanmay Binaykiya、Chongxio Cao、Sashikanth Chandrasekaran、Eric Chen、Hao Jiang、Vladimir Kuzmin、Michael Mallory、Michael Mui、Sean Po、Rich Porter、Vivek Sah、Raajay Viswanathan、Joseph Wang、Peng Zhang</p><p> Apache Spark ™ and Spark ™ are trademarks of the Apache Software Foundation in the United States and/or other countries. No endorsement by The Apache Software Foundation is implied by the use of these marks.</p><p>阿帕奇星火™ 火花™ 是Apache软件基金会在美国和/或其他国家的商标。Apache软件基金会不认可使用这些标记。</p><p>   Popov, Sergei, Stanislav Morozov, and Artem Babenko. “Neural oblivious decision ensembles for deep learning on tabular data.” arXiv preprint arXiv:1909.06312 (2019).</p><p>波波夫、谢尔盖、斯坦尼斯拉夫·莫罗佐夫和阿尔特姆·巴本科。“用于表格数据深度学习的神经不经意决策集成。”arXiv预印本arXiv:1909.06312（2019）。</p><p> Arık, Sercan O., and Tomas Pfister. “Tabnet: Attentive interpretable tabular learning.” arXiv (2020).</p><p>阿尔克、塞尔坎·O.和托马斯·普菲斯特。“Tabnet：专注的可解释表格学习。”arXiv（2020年）。</p><p> Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. “Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.” arXiv preprint arXiv:1701.06538 (2017).</p><p>沙泽尔、诺姆、阿扎莉亚·米尔霍塞尼、克兹兹托夫·马齐亚兹、安迪·戴维斯、库克·勒、杰弗里·辛顿和杰夫·迪恩。“异常庞大的神经网络：专家层的稀疏混合。”arXiv预印本arXiv:1701.06538（2017）。</p><p> Ha, David, Andrew Dai, and Quoc V. Le. “Hypernetworks.” arXiv preprint arXiv:1609.09106 (2016).</p><p>哈，大卫，安德鲁·戴和库克·V·勒。“超级网络。”arXiv预印本arXiv:1609.09106（2016）。</p><p> Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. “Attention Is All You Need.” In Advances in neural information processing systems, pp. 5998-6008. 2017.</p><p>瓦斯瓦尼、阿什什、诺姆·沙泽尔、尼基·帕尔玛、雅各布·乌兹科雷特、利昂·琼斯、艾丹·N·戈麦斯、乌卡斯·凯泽和伊利亚·波洛苏金。“注意力是你所需要的。”神经信息处理系统的进展，第5998-6008页。2017</p><p> Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. “Transformers are rnns: Fast autoregressive transformers with linear attention.” In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020.</p><p>卡萨罗普洛斯、安杰洛斯、阿波罗夫·维亚斯、尼古拉斯·帕帕斯和弗朗索瓦·弗莱雷特。“变压器是RNN：具有线性注意的快速自回归变压器。”在国际机器学习会议上，第5156-5165页。PMLR，2020年。</p><p> Vaswani, Ashish, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones et al. “Tensor2tensor for neural machine translation.” arXiv preprint arXiv:1803.07416 (2018).</p><p>Vaswani、Ashish、Samy Bengio、Eugene Brevdo、Francois Chollet、Aidan N. Gomez、Stephan Gouws、Lyon等。“神经机器翻译的张量张量”。arXiv预印本arXiv:1803.07416（2018）。</p><p> Saberian, Mohammad, Pablo Delgado, and Yves Raimond. “Gradient boosted decision tree neural network.” arXiv preprint arXiv:1910.09340 (2019).</p><p>萨伯里亚人、穆罕默德、巴勃罗·德尔加多和伊夫·雷蒙德。“梯度增强的决策树神经网络。”arXiv预印本arXiv:1910.09340（2019）。</p><p> Wang, Sinong, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. “Linformer: Self-attention with linear complexity.” arXiv preprint arXiv:2006.04768 (2020).</p><p>王、思农、贝琳达·Z·李、马甸·卡萨、韩芳和马浩。“Linformer：线性复杂的自我关注。”arXiv预印本arXiv:2006.04768（2020）。</p><p> Choromanski, Krzysztof, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins et al. “Rethinking attention with performers.” arXiv preprint arXiv:2009.14794 (2020).</p><p>Choromanski、Krzysztof、Valerii Likhosherstov、David Dohan、Song Xingyou、Andreea Gane、Tamas Sarlos、Peter Hawkins等，“重新思考演员的注意力。”arXiv预印本arXiv:2009.14794（2020）。</p><p>  Weinberger, Kilian, et al. “Feature hashing for large scale multitask learning.” Proceedings of the 26th annual international conference on machine learning. 2009.</p><p>Weinberger，Kilian等人，“用于大规模多任务学习的特征哈希。”第26届国际机器学习年会论文集。2009</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/深度学习/">#深度学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/优步/">#优步</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/uber/">#uber</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>