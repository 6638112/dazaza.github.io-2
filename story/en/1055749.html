<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么机器学习与因果关系挣扎 Why machine learning struggles with causality</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Why machine learning struggles with causality<br/>为什么机器学习与因果关系挣扎 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-02 23:02:08</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/4/9805e26f8a7c35347841a708c9e67884.jpg"><img src="http://img2.diglog.com/img/2021/4/9805e26f8a7c35347841a708c9e67884.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This article is part of our  reviews of AI research papers, a series of posts that explore the latest findings in artificial intelligence.</p><p>本文是我们对AI研究论文评论的一部分，这是一系列探索人工智能最新发现的一系列帖子。</p><p> When you look at the following short video sequence, you can make inferences about causal relations between different elements. For instance, you can see the bat and the baseball player’s arm moving in unison, but you also know that it is the player’s arm that is causing the bat’s movement and not the other way around. You also don’t need to be told that the bat is causing the sudden change in the ball’s direction.</p><p> 当您查看以下短视频序列时，您可以对不同元素之间的因果关系进行推断。例如，你可以看到蝙蝠和棒球运动员的手臂齐声移动，但你也知道这是玩家的手臂，导致蝙蝠的运动，而不是相反的方式。您也不需要被告知蝙蝠导致突然变化在球的方向上。</p><p> Likewise, you can think about counterfactuals, such as what would happen if the ball flew a bit higher and didn’t hit the bat.</p><p> 同样，您可以考虑反事实，例如如果球飞得更高并且没有击中蝙蝠会发生什么。</p><p>  Such inferences come to us humans intuitively. We learn them at a very early age, without being explicitly instructed by anyone and just by observing the world. But for  machine learning algorithms, which have managed to outperform humans in complicated tasks such as go and chess, causality remains a challenge. Machine learning algorithms, especially  deep neural networks, are especially good at ferreting out subtle patterns in huge sets of data. They can transcribe audio in real-time, label thousands of images and video frames per second, and examine x-ray and MRI scans for cancerous patterns. But they struggle to make simple causal inferences like the ones we just saw in the baseball video above.</p><p>  这种推断直观地对我们的人来说。我们在一个很小的年龄地学习它们，而不被任何人明确指导，只是通过观察世界。但是对于机器学习算法，这已经设法以Go和Chess等复杂任务更优于人类，因此因果关系仍然是一个挑战。机器学习算法，尤其是深神经网络，尤其擅长在大量数据中脱颖而出。它们可以实时转录音频，每秒标记数千个图像和视频帧，并检查X射线和MRI扫描以获取癌症模式。但他们努力使我们刚刚在上面棒球视频中看到的简单因果推断。</p><p> In a  paper titled “Towards Causal Representation Learning,” researchers at the Max Planck Institute for Intelligent Systems, the Montreal Institute for Learning Algorithms (Mila), and Google Research, discuss the challenges arising from the lack of causal representations in machine learning models and provide directions for creating artificial intelligence systems that can learn causal representations.</p><p> 在一个标题为“因果代表学习”的文件中，Max Planck智能系统智能系统研究所（MILA）和谷歌研究所的Max Planck智能系统研究所和谷歌研究，讨论了机器学习模型中缺乏因果表现产生的挑战提供创建可以学习因果表现的人工智能系统的方向。</p><p> This is one of several efforts that aim to explore and solve machine learning’s lack of causality, which can be key to overcoming some of the  major challenges the field faces today.</p><p> 这是旨在探索和解决机器学习缺乏因果关系的几项努力之一，这可能是克服现场今天面临的一些主要挑战的关键。</p><p>   Why do machine learning models fail at generalizing beyond their narrow domains and training data?</p><p>   为什么机器学习模型在狭窄的域和培训数据之外都会失败？ </p><p> “Machine learning often disregards information that animals use heavily: interventions in the world, domain shifts, temporal structure — by and large, we consider these factors a nuisance and try to engineer them away,” write the authors of the causal representation learning paper. “In accordance with this, the majority of current successes of machine learning boil down to large scale pattern recognition on suitably collected  independent and identically distributed (i.i.d.) data.”</p><p>“机器学习往往无视动物使用的信息：世界上的干预措施，域名转移，时间结构 - 逐渐，我们认为这些因素是滋扰并试图将它们工程师们送走，”写下因果代表学习纸的作者。 “按照这一点，大多数当前机器学习成功的成功归结为适当收集的独立收集的大规模模式识别，并相同分布（即）数据。”</p><p> i.i.d. is a term often used in machine learning. It supposes that random observations in a problem space are not dependent on each other and have a constant probability of occurring. The simplest example of i.i.d. is flipping a coin or tossing a die. The result of each new flip or toss is independent of previous ones and the probability of each outcome remains constant.</p><p> I.I.D.是一个经常用于机器学习的术语。它假设问题空间中的随机观察不依赖于彼此并且具有恒定的发生概率。最简单的例子。正在翻转硬币或折腾模具。每个新的翻转或折腾的结果与先前的翻转或折腾的结果无关，每个结果的概率保持不变。</p><p> When it comes to more complicated areas such as  computer vision, machine learning engineers try to turn the problem into an i.i.d. domain by training the model on very large corpora of examples. The assumption is that, with enough examples, the machine learning model will be able to encode the general distribution of the problem into its parameters. But in the real world, distributions often change due to factors that cannot be considered and controlled in the training data. For instance,  convolutional neural networks trained on millions of images can fail when they see objects under new lighting conditions or from slightly different angles or against new backgrounds.</p><p> 当涉及计算机视觉等更复杂的领域时，机器学习工程师试图将问题变成I.I.D.域通过在非常大的例子上培训模型。假设是，通过足够的示例，机器学习模型将能够将问题的一般分布编码为其参数。但在现实世界中，由于在训练数据中无法考虑和控制的因素，分布通常会发生变化。例如，在数百万图像上培训的卷积神经网络可以在新照明条件下或从略微不同的角度或对新的背景下看到物体时失败。</p><p>  Efforts to address these problems mostly include training machine learning models on more examples. But as the environment grows in complexity, it becomes impossible to cover the entire distribution by adding more training examples. This is especially true in domains where AI agents must interact with the world, such as robotics and self-driving cars. Lack of causal understanding makes it very hard to make predictions and deal with novel situations. This is why you see  self-driving cars make weird and dangerous mistakes even after having trained for millions of miles.</p><p>  解决这些问题的努力主要包括更多示例的培训机器学习模型。但随着环境在复杂性的增加，通过添加更多培训例子，不可能覆盖整个分布。在域名必须与世界互动，例如机器人和自动驾驶汽车等域中尤其如此。缺乏因果理解使得难以做出预测和处理新颖的情况。这就是为什么你看到自动驾驶汽车的错误，即使在数百万英里训练过的时候也能产生奇怪和危险的错误。</p><p>  “Generalizing well outside the i.i.d. setting requires learning not mere statistical associations between variables, but an underlying causal model,” the AI researchers write.</p><p>  “概括在I.I.D之外。设置需要学习不仅仅是变量之间的统计关联，而是一个潜在的因果模型，“AI研究人员写道。</p><p> Causal models also allow humans to repurpose previously gained knowledge for new domains. For instance, when you learn a real-time strategy game such as Warcraft, you can quickly apply your knowledge to other similar games StarCraft and Age of Empires. Transfer learning in machine learning algorithms, however, is limited to very superficial uses, such as finetuning an image classifier to detect new types of objects. In more complex tasks, such as learning video games, machine learning models need huge amounts of training (thousands of years’ worth of play) and respond poorly to minor changes in the environment (e.g., playing on a new map or with a slight change to the rules).</p><p> 因果模型还允许人类重新培养以前获得了对新域的知识。例如，当您学习魔兽等实时策略游戏时，您可以快速将您的知识应用于其他类似的游戏星际争霸和帝国年龄。然而，在机器学习算法中传输学习仅限于非常肤浅的用途，例如FineTuning图像分类器以检测新类型的对象。在更复杂的任务中，如学习视频游戏，机器学习模型需要大量的培训（数千年的戏剧），并对环境的微小变化进行应对良好（例如，在新地图上或略有变化或略有变化到规则）。</p><p> “When learning a causal model, one should thus require fewer examples to adapt as most knowledge, i.e., modules, can be reused without further training,” the authors of the causal machine learning paper write.</p><p> “在学习因果模型时，因此应该需要更少的例子来适应大多数知识，即模块，可以在没有进一步培训的情况下重复使用，”因果机学习纸写的作者写道。 </p><p>   So, why has i.i.d. remained the dominant form of machine learning despite its known weaknesses? Pure observation-based approaches are scalable. You can continue to achieve incremental gains in accuracy by adding more training data, and you can speed up the training process by adding more compute power. In fact, one of the key factors behind the recent success of deep learning is the  availability of more data and stronger processors.</p><p>所以，为什么我有.D。尽管其已知的缺点，但仍然是机器学习的主导形式？纯粹的观察方法是可扩展的。您可以通过添加更多培训数据来继续实现增量增益，并且您可以通过添加更多计算电源来加快培训过程。事实上，深度学习最近成功的关键因素之一是更多数据和更强大的处理器的可用性。</p><p> i.i.d.-based models are also easy to evaluate: Take a large dataset, split it into training and test sets, tune the model on the training data, and validate its performance by measuring the accuracy of its predictions on the test set. Continue the training until you reach the accuracy you require. There are already many public datasets that provide such benchmarks, such as ImageNet, CIFAR-10, and MNIST. There are also task-specific datasets such as the COVIDx dataset for covid-19 diagnosis and the Wisconsin Breast Cancer Diagnosis dataset. In all cases, the challenge is the same: Develop a machine learning model that can predict outcomes based on statistical regularities.</p><p> I.I.D.-基于模型也很容易评估：拍摄一个大型数据集，将其分成训练和测试集，调整培训数据的模型，并通过测量测试集预测的准确性来验证其性能。继续培训，直到您达到您所需的准确性。已经有许多公共数据集提供了这样的基准，例如想象成，CiFar-10和Mnist。还有特定于特定的数据集，例如Covid-19诊断和威斯康辛乳腺癌诊断数据集的Covidx数据集。在所有情况下，挑战是相同的：开发一种机器学习模型，可以基于统计规则预测结果。</p><p> But as the AI researchers observe in their paper, accurate predictions are often not sufficient to inform decision-making. For instance, during the coronavirus pandemic, many  machine learning systems began to fail because they had been trained on statistical regularities instead of causal relations. As life patterns changed, the accuracy of the models dropped.</p><p> 但随着AI研究人员在纸质中观察到的，准确的预测通常不足以告知决策。例如，在Coronavirus大流行期间，许多机器学习系统开始失败，因为他们已经接受了统计规则而不是因果关系训练。随着生命模式改变，模型的准确性下降。</p><p> Causal models remain robust when interventions change the statistical distributions of a problem. For instance, when you see an object for the first time, your mind will subconsciously factor out lighting from its appearance. That’s why, in general, you can recognize the object when you see it under new lighting conditions.</p><p> 当干预措施改变问题的统计分布时，因果模型仍然坚固。例如，当您第一次看到一个对象时，您的思想将从其外观潜意识地进行照明。这就是为什么通常，在新的照明条件下看到它时，您可以识别该对象。</p><p> Causal models also allow us to respond to situations we haven’t seen before and think about counterfactuals. We don’t need to drive a car off a cliff to know what will happen. Counterfactuals play an important role in cutting down the number of training examples a machine learning model needs.</p><p> 因果模型也允许我们回应我们之前没有看到的情况并考虑反事实。我们不需要驾驶悬崖上的汽车了解会发生什么。反事实在减少培训示例的数量方面发挥着重要作用，例如机器学习模型需要。</p><p> Causality can also be crucial to dealing with  adversarial attacks, subtle manipulations that force machine learning systems to fail in unexpected ways. “These attacks clearly constitute violations of the i.i.d. assumption that underlies statistical machine learning,” the authors of the paper write, adding that adversarial vulnerabilities are proof of the differences in the robustness mechanisms of human intelligence and machine learning algorithms. The researchers also suggest that causality can be a possible defense against adversarial attacks.</p><p> 因果关系也可能对处理对抗性攻击至关重要，微妙的操纵，强迫机器学习系统以意想不到的方式失败。 “这些袭击显然构成了I.I.D的违规行为。假设是统计机器学习的基础，“论文写作，增加了对抗性漏洞是人类智能和机器学习算法鲁棒机制差异的证明。研究人员还表明，因果关系可能是对抗对抗攻击的可能性。</p><p>  In a broad sense, causality can address machine learning’s lack of generalization. “It is fair to say that much of the current practice (of solving i.i.d. benchmark problems) and most theoretical results (about generalization in i.i.d. settings) fail to tackle the hard open challenge of generalization across problems,” the researchers write.</p><p>  在广泛的意义上，因果关系可以解决机器学习的缺乏概括。研究人员写道，“这是公平的大多数当前的实践（解决I.I.D.基准问题）和大多数理论结果（关于I.I.D.设置的概念），”研究人员写道“ </p><p>  In their paper, the AI researchers bring together several concepts and principles that can be essential to creating causal machine learning models.</p><p>在纸质中，AI研究人员带来了几种概念和原则，这可能对创造因果机学习模型至关重要。</p><p> Two of these concepts include “structural causal models” and “independent causal mechanisms.” In general, the principles state that instead of looking for superficial statistical correlations, an AI system should be able to identify causal variables and separate their effects on the environment.</p><p> 其中两个概念包括“结构因果模型”和“独立因果机制”。通常，原则状态，而不是寻找表面统计相关性，AI系统应该能够识别因果变量并将其对环境的影响分开。</p><p> This is the mechanism that enables you to detect different objects regardless of the view angle, background, lighting, and other noise. Disentangling these causal variables will make AI systems more robust against unpredictable changes and interventions. As a result, causal AI models won’t need huge training datasets.</p><p> 这是使您能够检测不同对象的机制，无论视角，背景，照明和其他噪声如何。解开这些因果变量将使AI系统对不可预测的变化和干预措施更加强大。因此，因果AI模型不需要巨大的训练数据集。</p><p> “Once a causal model is available, either by external human knowledge or a learning process,  causal reasoning allows to draw conclusions on the effect of interventions, counterfactuals and potential outcomes,” the authors of the causal machine learning paper write.</p><p> “一旦通过外部人类知识或学习过程提供了因果模式，因果推理允许结论干预措施，反事实和潜在结果”，“因果机学习纸张写作的作者”。</p><p> The authors also explore how these concepts can be applied to different branches of machine learning, including  reinforcement learning, which is crucial to problems where an intelligent agent relies a lot on exploring environments and discovering solutions through trial and error. Causal structures can help make the training of reinforcement learning more efficient by allowing them to make informed decisions from the start of their training instead of taking random and irrational actions.</p><p> 作者还探讨了这些概念如何应用于机器学习的不同分支，包括加强学习，这对智能代理依赖于探索环境和通过试验发现解决方案的问题至关重要。由于允许他们从培训开始而不是随机和非理性行动，可以帮助培训加强学习的培训更有效。</p><p> The researchers provide ideas for AI systems that combine machine learning mechanisms and structural causal models: “To combine structural causal modeling and representation learning, we should strive to embed an SCM into larger machine learning models whose inputs and outputs may be high-dimensional and unstructured, but whose inner workings are at least partly governed by an SCM (that can be parameterized with a neural network). The result may be a modular architecture, where the different modules can be individually fine-tuned and re-purposed for new tasks.”</p><p> 研究人员为AI系统提供了联合机器学习机制和结构因果模型的理念：“结合结构因果建模和表示学习，我们应该努力将SCM嵌入到更大的机器学习模型中，其输入和输出可能是高维和非结构化的，但其内部工作至少部分由SCM（这可以用神经网络参数化）。结果可以是模块化架构，其中不同的模块可以单独微调并重新饰有新任务。“</p><p> Such concepts bring us closer to the modular approach the human mind uses (at least as far as we know) to link and reuse knowledge and skills across different domains and areas of the brain.</p><p> 这些概念使我们更接近人类思维使用的模块化方法（至少据我们所知）来链接和重用跨大脑区域和地区的知识和技能。 </p><p>  It is worth noting, however, that the ideas presented in the paper are at the conceptual level. As the authors acknowledge, implementing these concepts faces several challenges: “(a) in many cases, we need to infer abstract causal variables from the available low-level input features; (b) there is no consensus on which aspects of the data reveal causal relations; (c) the usual experimental protocol of training and test set may not be sufficient for inferring and evaluating causal relations on existing data sets, and we may need to create new benchmarks, for example with access to environment information and interventions; (d) even in the limited cases we understand, we often lack scalable and numerically sound algorithms.”</p><p>然而，值得注意的是，论文中提出的想法是在概念层面。随着作者承认的，实施这些概念面临着几种挑战：“（a）在许多情况下，我们需要从可用的低级输入功能中推断出抽象的因果变量; （b）没有达成共识的数据，揭示了因果关系; （c）培训和测试集的通常实验协议可能不足以推断和评估现有数据集的因果关系，并且我们可能需要创建新的基准，例如访问环境信息和干预措施; （d）即使在有限的情况下，我们也会理解，我们经常缺乏可扩展且数字的声音算法。“</p><p> But what’s interesting is that the researchers draw inspiration from much of the parallel work being done in the field. The paper contains references to the work done by Judea Pearl, a Turing Award–winning scientist best known for his work on  causal inference. Pearl is a vocal critic of pure deep learning methods. Meanwhile, Yoshua Bengio, one of the co-authors of the paper and another Turing Award winner, is one of the pioneers of deep learning.</p><p> 但有趣的是，研究人员从该领域所做的大部分并行工作中汲取灵感。本文包含对朱迪亚珍珠的工作的参考，这是一个为他在因果推断工作而闻名的图灵屡获殊荣的科学家。珍珠是一种纯粹深入学习方法的声音批评。与此同时，Yoshua Bengio是本文的共同作者之一和另一个图灵奖获得者，是深度学习的先驱之一。</p><p> The paper also contains several ideas that overlap with the idea of  hybrid AI models proposed by Gary Marcus, which combines the reasoning power of  symbolic systems with the pattern recognition power of neural networks. The paper does not, however, make any direct reference to hybrid systems.</p><p> 本文还包含几种与Gary Marcus提出的混合AI模型重叠的想法，这与神经网络的模式识别能力相结合的符号系统的推理能力。但是，本文没有对混合系统进行任何直接引用。</p><p> The paper is also in line with  system 2 deep learning, a concept first proposed by Bengio in a talk at the NeurIPS 2019 AI conference. The idea behind system 2 deep learning is to create a type of neural network architecture that can learn higher representations from data. Higher representations are crucial to causality, reasoning, and transfer learning.</p><p> 本文还符合System 2深度学习，这是一个概念首次由Bengio在Neurips 2019 Ai会议上进行谈话中提出的概念。系统2背后的想法深入学习是创建一种类型的神经网络架构，可以学习来自数据的更高表示。更高的表示对因果关系，推理和转移学习至关重要。</p><p> While it’s not clear which of the several proposed approaches will help solve machine learning’s causality problem, the fact that ideas from different—and often conflicting—schools of thought are coming together is guaranteed to produce interesting results.</p><p> 虽然目前尚不清楚哪种拟议方法中的哪种方法有助于解决机器学习的因果问题，但思想来自不同 - 往往思考的思想的事实是保证产生有趣的结果。</p><p> “At its core, i.i.d. pattern recognition is but a mathematical abstraction, and causality may be essential to most forms of animate learning,” the authors write. “Until now, machine learning has neglected a full integration of causality, and this paper argues that it would indeed benefit from integrating causal concepts.”</p><p> “在它的核心，I.I.D.作者写道，模式识别是一种数学抽象，而且因果关系可能对大多数形式的动画学习是必不可少的。“ “到目前为止，机器学习忽略了完全一体化因果关系，本文认为它确实可以从整合因果概念中受益。” </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://bdtechtalks.com/2021/03/15/machine-learning-causality/">https://bdtechtalks.com/2021/03/15/machine-learning-causality/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>