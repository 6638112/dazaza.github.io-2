<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>将坚持不懈的降落镜头重新注册到卫星图像上 Reprojecting the Perseverance landing footage onto satellite imagery</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Reprojecting the Perseverance landing footage onto satellite imagery<br/>将坚持不懈的降落镜头重新注册到卫星图像上 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-14 11:51:38</div><div class="page_narrow text-break page_content"><p>The landing of the Mars 2020 Perseverance rover last month amazed the world.The stunning footage of the descent shows each stage of the sequence.If you have not seen it already you can watch it here.</p><p>火星2020的着陆仍然是世界上的持久性漫游者。下降的令人惊叹的镜头展示了序列的每个阶段。如果你还没有看到它，你已经可以在这里观看。</p><p> One thing that I found remarkable was the self-similarity of the martianterrain. As the lander descends towards the ground it is hard to get asense of scale, since there is no familiar frame of reference to tell us how faraway the ground is. This led me to embark on a project in which Ireproject the footage onto a satellite image obtained from the Mars Express Orbiter,along with a scale to tell us how large features on the ground actually are:</p><p> 我发现卓越的一件事是Martianterrain的自我相似性。由于着陆器朝着地面下降，因此很难获得规模的锐阵，因为没有熟悉的参考框架，告诉我们地面有多遥远。这让我踏上了一个项目，其中将镜头从火星表达轨道器中获得的卫星图像上，以及一个规模，告诉我们实际上的地面有多大的功能是：</p><p>  In this post I am going to explain how I used Python, OpenCV, PyTorch, andBlender to make the above footage.</p><p>  在这篇文章中，我将解释我如何使用Python，OpenCV，PyTorch，AndBlender进行上述镜头。</p><p>  Producing my video involves distorting the frames of the original footage sothat each frame lines up with the satellite image. The usual way of doing thisis to:</p><p>  生成我的视频涉及将每个帧与卫星图像相结合的原始素材Sothat的框架扭曲。常用方式的方式：</p><p> find a mathematical function that maps points in the first image to those in the second image.</p><p> 找到一个数学函数，将第一个图像中的点映射到第二个图像中的那些。</p><p> The details of implementing the above are described in this OpenCV tutorial, but I will summarize the process here.</p><p> 在此OpenCV教程中描述了实现上述的详细信息，但我将总结此过程。</p><p> Breaking this down, on the left is a frame from the video that we wish to align,with the reference satellite image on the right:</p><p> 左侧打破这一点，是我们希望对齐的视频的帧，在右边的参考卫星图像： </p><p>  Firstly, we use OpenCV’s Scale Invariant Feature Transform (SIFT) keypointdetector to pull out salient keypoints from the image:</p><p>首先，我们使用OpenCV的规模不变功能转换（SIFT）键点Detector从图像中拔出突出关键点：</p><p>  Each red cross here marks a potentially “interesting” point as determined by theSIFT algorithm. Associated with each point (but not shown) is a vector of 128values which describes the part of the image that surrounds the keypoint. Theidea is that this descriptor is invariant to things like scale (as the nameimplies), rotation, and lighting differences. We can then match up points in ourpair of images with similar descriptors:</p><p>  这里的每个红十字会都标记了由截图算法确定的可能“有趣”点。与每个点相关联（但未示出）是128Values的矢量，它描述了围绕关键点的图像的一部分。 TheIDEA是该描述符是不变的，比例（作为名称阶段），旋转和点亮差异。然后，我们可以匹配具有类似描述符的图像中的点数：</p><p>   Now that we have found the keypoint pairs, the next step is to find a transformationthat maps the keypoints from the video frame onto the corresponding keypoints ofthe satellite image. To do this, we make use of a class of transformations knownas projective transformations. Projective transformations can be used todescribe how fixed points on a flat plane change apparent position when viewedfrom a different location and angle, which is useful to us since the surface of Mars can be wellapproximated by a flat plane at these distances. This is assuming that the camera conforms to a rectilinear perspective projection (i.e. without lens distortion), which appears to be the case.</p><p>   现在我们已经找到了关键点对，下一步是找到从视频帧将关键点映射到卫星图像的相应关键点。为此，我们利用了一类众多转型的历史投射转换。可以使用投影变换，以便在从这些距离的不同位置和角度对我们有用的位置和角度进行观察时，将平面上的固定点改变明显位置。这假设相机符合直线透视投影（即没有透镜失真），这似乎是这种情况。</p><p> A projective transformation is represented by a 3x3 matrix \(M\). To apply such atransformation to a 2D point \(v\) we first append a 1 to give a 3-vector, thenmultiply by the matrix:</p><p> 投影转换由3x3矩阵\（m \）表示。要将此类Atransformation应用于2D点\（v \），我们首先将A 1附加到给出3向量，然后由矩阵管道：</p><p>\[v&#39; = M \begin{bmatrix}v_x \\v_y \\1 \end{bmatrix}\] To get back to a 2D point, the result is divided by itsthird element, and truncated back to a 2-vector:</p><p>\ [v＆＃39; = m \ begin {bmatrix} v_x \\ v_y \\ 1 \ neg {bmatrix} \]返回2d点，结果由ITSThird元素划分，并截断回2向量：</p><p>\[v_\text{projected} = \begin{bmatrix}v&#39;_x / v&#39;_z \\v&#39;_y / v&#39;_z\end{bmatrix}\] This can be visualized byplotting the points on the z=1 plane, applying the transformation, and thenprojecting each point towards the origin, back onto the z=1 plane:</p><p>\ [v_ \ text {projected} = \ begin {bmatrix} v＆＃39; _x / v＆＃39; _z \\ v＆＃39; _y / v＆＃39; _z \ end {bmatrix} \]这可以是可视化的byplotting z = 1平面上的点，将变换应用，然后对每个点朝向原点，回到z = 1平面：</p><p>   When we talk about composing projective transformations, what we are actually doing ismultiplying the underlying matrices: projective transformations have the propertythat the composition of two transformations is equal to the projective transformationgiven by the matrix product of their respective matrices. Written symbolicallythis can be written as</p><p>   当我们谈论撰写投影转换时，我们实际做的是Multiply依赖底层矩阵：投影变换具有两个转换的组成等于由各自矩阵的矩阵产物的突出变换。写象征性的书面可以写成 </p><p>\[\forall x \in \mathbb{R}^2 \ \colon \ p_{M_1} ( p_{M_2} (x) ) = p_{M_1 M_2} (x)\]  Finding the transformation is done using a RANSAC approach. For more details on RANSAC please see my Pluto flyby post.</p><p>\ [\ forall x \ in \ mathbb {r} ^ 2 \ \ colon \ p_ {m_1}（p_ {m_2}（x））= p_ {m_1 m_2}（x）\]找到转换使用RANSAC完成方法。有关Ransac的更多详情，请参阅我的Pluto Flyby Post。</p><p> Once we have a transformation for each frame, we can reproject each video frameinto the frame of reference of the satellite image, thus obtaining the stablizedview.</p><p> 一旦我们对每个帧进行了转换，我们就可以恢复卫星图像的参考帧的每个视频框架，从而获得稳定的视图。</p><p>  Unfortunately it is not simply a case of repeating the above process for eachframe in order to produce a complete video, because the algorithm is not able toproduce sufficient correspondences for every frame.</p><p>  不幸的是，不仅仅是重复对每个帧的上述过程的情况，以便产生完整的视频，因为该算法不能为每个帧拓展足够的对应关系。</p><p> In order to resolve this, we also look for transformations between the video framesthemselves. The idea being that if a frame has no direct transformation linkingit to a satellite image, but we do have a transformation linking it to anotherframe that is itself connected to the satellite image, then we can simplycompose the two transformations to map the original frame onto the satellite view.</p><p> 为了解决这个问题，我们还寻找视频弗拉梅姆之间的变换。如果一个帧没有直接转换链接到卫星图像，但我们确实将其与另一个自身连接到卫星图像的帧的转换，然后我们可以简单地将两个转换映射到映射到原始帧卫星景观。</p><p> So, I labelled every thirtieth frame (ie. one frame per second) as a “keyframe”,and then exhaustively searched for transformations between each pair ofkeyframes. For the remaining frames I searched for transformations to thenearest keyframe.</p><p> 因此，我将每三十帧（即，每秒一帧）标记为“关键帧”，然后彻底搜索每对keyframe之间的变换。对于我搜索的剩余帧，用于转换为最新帧。</p><p> This results in a fairly dense graph with one node per frame, and one edge pertransformation found. Here is a simplified example, with keyframes at every 5frames rather than at every 30:</p><p> 这导致具有每帧一个节点的相当密集的图表，并且找到了一个边缘变形。这是一个简化的例子，每个5帧的关键帧而不是每30个：</p><p>  Any path from the satellite node to a particular frame’s node represents a chainof transformations that when composed will map the frame onto the satelliteview.</p><p>  卫星节点到特定帧节点的任何路径都表示转换的链条，当组合时将帧映射到SatelliteView上。 </p><p> We will begin by selecting one path for each node. Doing a  breadth-firstsearchfrom the satellite node will give us a path to each frame while alsoguaranteeing that it is the shortest possible:</p><p>我们将首先选择每个节点的一条路径。卫星节点的宽度优先考虑卫星节点将为每个帧提供一条路径，而alsoguarteeing是最短的：</p><p>       While the above method yields a decent reprojection, it is not perfect. Thereare clear mode switches around when the shortest path changes.</p><p>       虽然上述方法产生了一个不错的重点，但它并不完美。当最短路径更改时，临时清除模式会切换。</p><p> If we incorporate all correspondences, and not just those on the shortest path,this provides more information and results in smoother and more accuratetransformations.</p><p> 如果我们纳入所有通信，而不仅仅是那些在最短路径上的通信，这提供了更多信息并导致更平滑和更高的ComitateTrans。</p><p> To do this, I wrote a loss function which returns the total reprojection error,given a satellite-relative transformation for each image:</p><p> 为此，我写了一个丢失函数，返回总重注错误，给定每个图像的卫星相对转换：</p><p> def  project ( v ):  # Project onto the plane z=1  return  v  /  v [...,  - 1 ][...,  None ]  def  loss ( frame_transforms ,  src_pts ,  dst_pts ,  src_idx ,  dst_idx ):  M_src_inv  =  torch . inverse ( frame_transforms )[ src_idx ]  M_dst  =  frame_transforms [ dst_idx ]  ref_pts  =  torch . einsum ( &#39;nij,nj-&gt;ni&#39; ,  M_src_inv ,  src_pts )  reprojected_dst_pts  =  project ( torch . einsum ( &#39;nij,nj-&gt;ni&#39; ,  M_dst ,  ref_pts ))  return  torch . dist ( reprojected_dst_pts ,  dst_pts )</p><p> def项目（v）：＃投影到平面上z = 1返回v / v [...， -  1] [...，无] def丢失（frame_transforms，src_pts，dst_pts，src_idx，dst_idx）：m_src_inv =火炬。逆（frame_transform）[src_idx] m_dst = frame_transforms [dst_idx] ref_pts =火炬。 einsum（＆＃39; nij，nj-＆gt; ni＆＃39;，m_src_inv，src_pts）reprojected_dst_pts =项目（火炬。einsum（＆＃39; nij，nj-＆gt; ni＆＃39; m_dst，ref_pts）返回火炬 。 dist（reprojected_dst_pts，dst_pts）</p><p> src_pts and  dst_pts are both  N x 3 arrays, representing every pair ofpoints in the dataset.  frame_transforms is an  M x 3 x 3 array representingthe candidate transformations,  M being the number of frames in the video. frame_transforms are relative to the satellite image, which is to say a pointin the satellite image when transformed with  frame_transforms[i] shouldgive the corresponding point in frame  i.</p><p> src_pts和dst_pts都是n x 3阵列，代表数据集中的每对点。 Frame_Transforms是表示候选变换的M x 3 x 3阵列，m是视频中的帧数。 Frame_Transforms是相对于卫星图像的，这就是用Frame_Transforms变换时卫星图像的卫星图像[i]框架I中的对应点。</p><p> Since there are multiple point-pairs per frame,  src_idx and  dst_idx are usedto map each half of each point-pair to the corresponding video frame.</p><p> 由于每帧多点对，因此SRC_IDX和DST_IDX将每个点对的每一半映射到相应的视频帧。 </p><p> The  loss function proceeds by taking the first points from each pair, mappingthem back into the satellite image’s frame of reference, then mapping them intothe frame of reference of the second image. With accurate frame transformations andperfect correspondences, these transformed points should be very close to thecorresponding set of second points. The final line of the  loss function thenmeasures the Euclidean distance (sum of squares) between the reprojected firstpoints and the (unmodified) second points. The idea is that if we find a set of frame_transforms with a lower loss, then we will have a more accurate set oftransformations.</p><p>丢失函数通过从每对的第一点拍摄第一点，将映射回到卫星图像的参考框架，然后将它们映射到第二图像的参考框架。通过精确的帧变换和富合作用的对应关系，这些变换的点应该非常接近对应的第二组第二点。损失功能的最后一行释放了恢复的首发和（未改性）第二点之间的欧几里德距离（平方和）。这个想法是，如果我们发现一组具有较低损失的框架，那么我们将有一个更准确的OFFRANSEMATIONS。</p><p> loss is written using  Torch. Torch is an automaticdifferentiation framework with functionality for applying  gradientdescent (amongst other things).As such we can use it to iteratively improve our  frame_transforms:</p><p> 使用火炬写入损失。火炬是一种自动化的框架，具有用于应用渐变（在其他内容中）的功能。这样，我们可以使用它来迭代地改善我们的Frame_Transforms：</p><p> src_pts ,  dst_pts ,  src_idx ,  dst_idx  =  dataset  frame_transforms  =  initial_frame_transforms  optim  =  torch . optim . Adam ([ frame_transforms ],  lr = 1e-5 )  while  True :  optim . zero_grad ()  l  =  loss ( frame_transforms ,  src_pts ,  dst_pts ,  src_idx ,  dst_idx )  l . backward ()  optim . step ()</p><p> src_pts，dst_pts，src_idx，dst_idx = dataset frame_transforms = initial_frame_transforms Optim =火炬。 Optim。 adam（[frame_transforms]，lr = 1e-5），而true：Optim。 zero_grad（）l =损失（frame_transforms，src_pts，dst_pts，src_idx，dst_idx）l。倒退（）Optim。步 （）</p><p> dataset is constructed from the set of correspondences, and the initial_frame_transforms are those derived from composing the transformationsalong the shortest paths.</p><p> 数据集是由该组对应关系构建的，并且initial_frame_transforms是衍生自组合转换的最短路径的字母。</p><p> After running this loop for a while we obtain the final set of transformationsfor each frame. This produces a more stable set of transformations:</p><p> 运行此循环后，我们获取每个帧的最终变换集。这会产生更稳定的转换：</p><p>    To produce the final video I used the 3D modelling and rendering application Blender. I used Blender’srich Python scripting interface to animate a quad whose corners follow thereprojected video’s corners. To get the right texture for the quad I tookadvantage of Blender’s shader system:</p><p>    要生产最终视频，我使用3D建模和呈现应用程序搅拌机。我使用blender'srich python脚本界面来为quad设置一个Quad，其角落遵循其中的视频的角落。为了获得闪贷的Shader System的Quad的正确纹理：</p><p>  In general, the shader system decides how a particular point on a surface shouldbe shaded, which is typically a function of incoming light, view direction, andproperties of the surface. Here I am using it in a very simple way whichcalculates what colour the point on the quad should be, given the point’scoordinates in 3D space.</p><p>  通常，着色器系统决定了表面上的特定点应该是阴影的，这通常是光线，视图方向，表面的函数的函数。在这里，我以一种非常简单的方式使用它，它会在3D空间中赋予Quad的点应该是什么颜色的颜色。 </p><p>  Take the location of the point to be coloured, and replace the Z componentwith a 1. This is the first stage of the projective transformation where weturn the 2-vector into a 3-vector by appending a one.</p><p>拍摄点的位置彩色，并更换Z分量A 1.这是通过附加一个将2-向量迁移到3向量中的投影变换的第一阶段。</p><p>  Multiply this 3-vector by a matrix defined by the constants shown here. Theseconstants are in fact animated so that on any given frame these display frame_transforms[frame_num].</p><p>  通过由此处显示的常量定义的矩阵乘以该3矢量。事实上，这是一个动画，以便在任何给定的帧上这些显示Frame_Transforms [Frame_Num]。</p><p>    At this point the coordinates are in terms of pixels in the video frame.However the next stage needs them to be in the range 0 to 1, so divide by thevideo width and height here.</p><p>    此时，坐标在视频帧中的像素方面。然而，随着下一阶段需要它们在0到1的范围内，因此在此处划分视频宽度和高度。</p><p>   I used many satellite images rather than just one. However, I designate oneas the “reference frame” (ie. the frame with the identity transformation) andtreat the rest as if they were video key frames.</p><p>   我使用了许多卫星图像而不是一个。但是，我指定了“参考帧”（即，具有身份转换的帧）和其余的帧，就好像它们是视频键帧一样。</p><p>  During the early part of the video, the rover’s heatshield is visible. Withoutintervention, some frame correspondences track the heatshield (which is itselfmoving) rather than the terrain, causing bad tracking. So, I manuallyextracted some keypoints from the heatshield on a particular frame, andignored all keypoints that were similar to at least one of the heatsheid’skeypoints.</p><p>  在视频的早期部分期间，流动站的热屏幕是可见的。没有Intervelion，一些帧对应关系跟踪热屏幕（这是本身的移动）而不是地形，导致跟踪不良。因此，我将一些关键点从特定帧上的热屏幕中手术解压缩，同时与至少一个HeatsheID'SkyyPoints类似的关键点。</p><p>  Rarely, degenerate frame correspondences are found. When all matchingkeypoints are in a line you get multiple solutions corresponding to rotationsabout that line. Even if matching keypoints are not exactly in a line but areclose, the transformation found can be inaccurate. There was one such imagepair that caused this issue in my video, which I manually excluded.</p><p>  很少，找到退化的帧对应关系。当所有匹配标准点都处于一行中，您可以获得与旋转的多个解决方案。即使匹配的关键点不完全在一行中但areClose中，也可以不准确地进行变换。有一种这样的imagepair在我的视频中导致了这个问题，我手动排除在外。</p><p>  I have shown that the footage from the Perseverance rover’s descent can bestablized and aligned with a reference satellite image. While I am pleased withthe result and it certainly helps give context to the raw footage, there aresome ways that it could be improved, for example, during the early part of thevideo there are not many keypoints found by SIFT. This manifests itself asinaccuracy in the tracking. Perhaps experimenting with different keypointalgorithms would yield more usable keypoints.</p><p>  我已经表明，缺口Rover的镜头可以达到并与参考卫星图像进行束缚和对齐。虽然我对结果很满意，但它肯定有助于将上下文放在原始镜头上，而且可以改善它可以改进的方式，例如，在视频的早期部分，没有许多筛选的关键点。这表明了在跟踪中的贱原。也许用不同的关键点进行实验将产生更多可用的关键点。 </p><p> There may be also alternative ways to solve the problem which I have notexplored here. For example, the problem is quite similar to that of generalvideo stabilization. Perhaps I could use an off-the-shelf solver to achieve asimilar effect.</p><p>还可以解决我在这里尚未开发的问题的方法。 例如，问题与通用稳定化的问题非常相似。 也许我可以使用现成的求助主者来实现一致的效果。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://matthewearl.github.io/2021/03/06/mars2020-reproject/">https://matthewearl.github.io/2021/03/06/mars2020-reproject/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/降落/">#降落</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/视频/">#视频</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012 - 2021 diglog.com </div></div></body></html>