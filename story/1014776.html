<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>对主要的Cloud Vision AutoML工具进行基准测试</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">对主要的Cloud Vision AutoML工具进行基准测试</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-07-29 04:15:45</div><div class="story_img_container"><a href="http://img.diglog.com/img/2020/7/4f4a5347da9aaf7d608a197f565c334f.png"><img src="http://img.diglog.com/img/2020/7/4f4a5347da9aaf7d608a197f565c334f.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>三大云提供商最近都推出了用于培训自定义对象检测模型的无代码工具。但是，到目前为止，很少有关于它们性能的独立研究发表(无论是相对于彼此还是针对最先进的开源模型)。我们通过所有这三个工具运行了一个标准数据集，以查看它们之间的对比情况。</p><p>如果您正在考虑在您的产品中使用来自Google Cloud、AWS或Azure的计算机视觉服务(或者您已经训练了自己的对象检测模型，并且想知道与AutoML相比，您处于什么位置)，这是适合您的帖子。</p><p>在这篇文章中，我们回顾了Amazon Rekognition Custom Label、Azure Custom Vision和Google Cloud AutoML Vision。我们将权衡它们之间的优劣，并将它们与当代开源模型YOLOv5x的结果进行比较。</p><p>我们还深入了解了每个平台的主要限制和易用性。</p><p>我们的总体结论是：如果您更关心便利性和易用性而不是原始模型的性能，AutoML工具是一个很好的折衷方案。考虑到节省的开发时间，它可能比使用您自己的解决方案便宜许多倍。即使您计划最终构建自定义解决方案，AutoML也是评估项目可行性并获得基准性能基准的很好的第一站。</p><p>不幸的是，每个平台都使用自己的专有API和数据格式，因此入门非常麻烦，一旦重新启动并运行，就会被锁定。(幸运的是，Roboflow通过与所有这三个组件无缝集成解决了这一问题。)。</p><p>无论您选择哪种平台，您都将进入类似的工作流程来创建和部署您的计算机视觉模型。</p><p>数据准备-为了训练计算机视觉模型，您首先需要获取出现在您感兴趣的领域中的图像。域越窄，模型的性能就越好。然后，您的图像需要贴上标签以提供监督，以训练您的模型学习有问题的视觉概念。</p><p>模型训练和评估-你将开始训练，这是一个过程，计算机根据训练图像制定一个算法，应用于它从未见过的图像。然后将根据您的模型在预测期间的执行情况对其进行评估。在添加数据和调整训练长度时，这可能是一个迭代过程。</p><p>模型部署和推断-一旦您有了合适的模型，您将把此模型部署到一个计算实例，并建立一个API端点，您可以点击该端点以接收基于您的模型在生产映像中看到的预测。</p><p>云提供商将数据标注与AutoML分开，假设您已经收集了一个带标签的数据集，我们将在这篇文章中重点讨论AutoML服务。</p><p>最初的计划是在现代标准Microsoft Common Objects in Context Object Detection(COCO)数据集的基础上对这些服务进行基准测试。</p><p>不幸的是，这三个工具都没有通过此挑战，我们无法完成关于COCO的培训。</p><p>重新识别自定义标签-对每个作业72个节点小时的最长培训时间进行了硬性限制。如果数据集的收敛时间超过该时间，则作业将超时。Rekognition在超过其时间限制之前没有完成MS Coco作业，因此未能通过我们的测试。限制页面。</p><p>Google Cloud AutoML-注释上传有100MB的限制。我们可以通过将文件分成几个块来解决这个问题，但是在Google Cloud AutoML上培训MS Coco的预算估计是6000美元。很抱歉，我们绝不会把这笔钱花在博客上。“配额和限制”页。</p><p>Azure Custom Vision-最多100,000张训练图像(Coco有超过200,000张)。限制和配额页</p><p>为了比较AutoML VISION解决方案的准确性，我们退回到一个较旧的基准对象检测任务，即Pascal VOC 2012对象检测数据集。</p><p>对象检测任务-对象检测挑战计算机学习在属于感兴趣对象类集合的对象周围绘制边界框。</p><p>Pascal VOC 2012数据集有20个对象类的13,690个训练图像和3,428个验证图像。</p><p>Pascal VOC保留了数据集的测试集，因此我们使用验证集来评估云平台的性能。</p><p>一旦您将数据集加载到Amazon Rekognition、Google Cloud AutoML Vision或Azure Custom Vision中，您就可以开始训练您的模型了。</p><p>训练模型是一项计算密集的任务，当计算机迭代您的数据集时，它将优化数百万个神经网络参数。为了高效地执行此操作，AutoML服务将引导GPU节点。</p><p>Amazon Rekognition Custom Label培训成本-开始并运行培训，直到模型达到最高性能。您无法控制培训的时长。培训费用为1美元/小时/实例，Amazon可能会配置多个计算实例来培训您的模型。因此，您的培训工作可能会在两个小时内完成，但如果启动更多实例，则会向您收取更多费用。培训最长可运行72个节点小时。如果超时，您的培训费用将退还。小心，您不能停止已经开始的培训工作！定价页面。</p><p>Google Cloud AutoML Vision培训成本-Google Cloud AutoML Vision使用Tesla V100 GPU使用8个节点培训他们的模型。每个节点小时是3.15美元/小时，所以每个挂钟小时大约25美元。AutoML Vision允许您启动、停止和恢复模型培训，这可以很好地在培训工作开始之前预测成本。您可以在Google Cloud AutoML Vision中配置的培训时间没有限制。定价页面。</p><p>Azure Custom Vision培训成本-Azure Custom Vision在解释他们的培训成本时没有那么冗长-它的成本是20美元/计算小时，但底层硬件没有披露，所以还不清楚这与谷歌和亚马逊的性价比如何。定价页面。</p><p>如果不考虑培训时间，很难确定培训成本的基准。每个服务可以运行更大或更小的模型体系结构，并且可以具有不同级别的训练时间软件优化。</p><p>在开始您的培训工作时，您可以根据您在Azure和Google Cloud中的下游推理时间需求在大型或紧凑型模型之间进行选择。亚马逊Rekognition只提供一刀切的服务。</p><p>在这场对峙中，我们优化了所有模型的准确性(并试图以类似的预算进行培训)。</p><p>Amazon Rekognition Custom Label Training Time-Rekognition在26个节点小时内完成了Pascal VOC 2012培训集的培训工作，费用为26美元。为了进行公平的比较，我们试图将此作为我们跨平台的培训预算。</p><p>Google Cloud AutoML Vision培训时间-为了坚持识别培训成本基准，我们需要在Google Cloud AutoML Vision中使用9个节点小时，但我们的PascalVOC 2012数据集允许的最短时间是20小时，成本为60美元。建议的培训时间为1080个节点小时-这将花费高达3402美元。我们选择在允许的最短时间内进行培训(这仍然需要花费2倍的重新认知费)。</p><p>Azure Custom Vision培训时间--按照识别基准，我们为Azure Custom Vision提供了1小时的培训时间，费用为20美元。</p><p>值得一提的是，Google Cloud AutoML Vision和Azure Custom Vision本可以在更大的培训预算下获得更好的评估性能。Rekognition根本不允许您配置此权衡。</p><p>目标检测系统的评级是由平均平均精度指标来衡量的，这是一种衡量模型在测试图像中检测从未见过的目标的好坏的指标。</p><p>Google Cloud AutoML Vision是唯一一个报告公正评估指标的平台。蔚蓝自定义视觉和识别正在使他们的结果向有利于他们的方向倾斜。以下是血淋淋的细节。</p><p>作为入门读物，您可以考虑阅读有关对象检测的平均精度是什么的博客，或者真正深入了解YouTube的平均精度。</p><p>Google Cloud AutoML视觉评估方法-Google Cloud根据MAP@0.5报告性能。这意味着在UNION的50%交叉处计算的平均平均精度度量。</p><p>超过并集阈值的交集意味着预测长方体和地面真实值长方体相对于其并集的交集必须至少为50%。欠条越少，就越容易使MAP指标更高。</p><p>Amazon Rekognition自定义标签评估方法-Rekognition不公开用于评估的欠条指标，这是有问题的。此外，他们根据每个类别标签优化的置信度阈值计算平均精度度量。这意味着，对于绵羊，如果模型的准确率超过40%，他们就会做出预测；而对于人，如果模型的准确率超过80%，他们就会做出预测。这在应用程序设置中很有用，但对于评估而言，这完全是一帆风顺的。</p><p>Azure自定义视觉评估方法-Azure通过仅对训练图像(？！)进行评估，在所有大型云评估中危害最大(？！)。正确的做法是为您的评估创建模型没有看到的坚持测试集。否则，模型只能记忆训练集中的模式。此外，Azure Custom Vision的评估预设借条是30%，这是非常宽松的-他们确实允许你用滑块调整这个比例。</p><p>由于Azure Custom Vision和Amazon Rekognition提供的评估方法令人困惑，我们进行了自定义评估流程来逐一比较模型。为此，我们只将Pascal VOC培训集上传到Rekognition and Azure，并保留了验证集。</p><p>检测到Pascal_VOC_test_images/2009_003951_jpg.rf.84034d373c5e22f708af7c177e87e806.jpgDetected自定义标签的自定义标签用于Pascal_VOC_test_images/2009_003965_jpg.rf.4c9750ed8c6383e30f087b5d8df232d2.jpgDetected的自定义标签用于Pascal_VOC_test_images/2009_003975_jpg.rf.e93d55d198402c969db470c56bbe1432.jpgDetected的自定义标签用于Pascal_VOC_test_images/2009_003976_jpg.rf.340afb150f459810fc40e336145daf29.jpgDetected的自定义标签用于Pascal_VOC_test_images/2009_003995_jpg.rf.244ac5855fba6469ff7a0b359bde0263.jpgDetected的自定义标签用于pascal_voc_test_。Images/2009_004004_jpg.rf.59277dc0388ff956b4fcbe76cf82543c.jpg。</p><p>Azure检测到的：/Users/wolf/Downloads/Pascal_VOC_2012/val-corrupted/2009_002445_jpg.rf.afc8bb54acd8e8b2a7ec6110fc11eb06.jpgdetection_time对象0.36528491973876953 Azure检测到的对象：/Users/wolf/Downloads/Pascal_VOC_2012/val-corrupted/2011_006121_jpg.rf.08fba4e44cc22a4e81b45390b64995c2.jpgdetection_time 0.33763909339904785 Azure检测到的对象：/Users/wolf/Downloads/Pascal_VOC_2012/val-corrupted/2011_004730_jpg.rf.c083830630b8c51555a29db05a79e0f3.jpgdetection_time 0.4244537353515625 Azure检测到的对象：/Users/WOLF。/Downloads/Pascal_VOC_2012/val-corrupted/2010_003667_jpg.rf.9d48e35fe50f100e98b80daed42870a7.jpgdetection_time 0.4153890609741211。</p><p>然后，我们将这些模型推论与地面事实标签进行对比，以进行真正的MAP@0.5 IOU计算。有关代码方法，请参阅此存储库。</p><p>根据这些新的评估，我们可以看到，Amazon Rekognition模型在三大云提供商中执行共享对象检测任务时表现最好，但培训预算相似。</p><p>选择模型或服务时的一个关键考虑因素是接收预测的速度有多快。一些用例(比如识别用户上传的视频中的对象)可以在事后处理，并且可以很好地并行化。其他用例(如下一代增强现实)需要以低延迟实时运行。推断时间也可能是启动能力和基于云愿景的应用程序之间的差异，或者不是。</p><p>在我们使用Amazon Rekognition Python库和Azure Custom Vision Python库进行的实际推理中，我们发现Amazon Rekognition在每张图像上的推论时间约为2.1秒。冰川般的缓慢..。一些技术水平较小的对象检测模型正在以400FPS进行推断。</p><p>我们在400ms到600ms之间的任何地方都可以看到Azure自定义视觉推断，这是一个相当快的速度。(Azure还允许您训练较小的模型，并在许多平台架构(如Apple CoreML、TensorFlow Lite和ONNX)上导出用于设备上推理的权重。)。</p><p>对于Google Cloud AutoML Vision，本文中使用的较大模型使用AutoML Python API推断为800ms。</p><p>预测结果：ANNOTATION_SPEC_ID：&#34；4359503130992312320&#34；image_object_detection{边界框{规格化_顶点{x：0.5619745850563049 y：0.07531488686800003}规格化_顶点{x：0.7575701475143433 y：0.3709263801574707}}得分：0.973810613155365}显示名称：&#34；人员&#34；预测所用时间：0.8128330707550049。</p><p>AutoML解决方案的大部分成本将在部署用于推理的模型后长期累积。</p><p>Amazon Rekognition自定义标签推理成本-Rekognition每推理小时收费4美元。推断时间基于您的自定义模型处理图像所需的时间，并且可能取决于图像的大小以及自定义模型的复杂性-您无法对其进行太多控制，但想必较困难的任务将使用较大的计算效率较低的模型。</p><p>在我们的测试中，我们能够每2秒进行大约1次推理。因此，如果您的实例被充分利用，则每个映像的成本约为0.002美元。</p><p>如果您可以进行批处理推理，则可以在每天的指定时间启动实例，并在完成后重新启动它们。如果您需要按需推断，您将需要继续支付小时费率以保持计算资源的运行。</p><p>Google Cloud AutoML Vision推断成本-使用按需预测，您需要为每个节点支付1.82美元/小时(即使没有预测)。他们估计每个节点每秒可以进行1.5次预测。我们的测试每秒产生x次预测。因此，如果充分利用，每幅图像的成本约为0.0003美元。</p><p>AutoML Vision还支持批量预测(它们可以为您放大和缩小计算资源)，每个节点的价格为2.02美元/小时。</p><p>Azure Custom Vision推理成本-推理成本为每千笔交易2美元(每张图片0.002美元)。如果您需要按需预测，则按推理收费与按小时收费是一个巨大的优势，因为这意味着您无需为闲置的计算基础设施付费。</p><p>如果你只需要偶尔做出预测，Azure值得一看。每幅图像的成本与批量预测的Rekognition相当。</p><p>为了给我们发现的三个云AutoML服务的结果提供上下文，我们还针对开源对象检测库YOLOv5进行了性能基准测试。</p><p>我们的培训评估记录在这本关于Pascal VOC的Colab笔记本培训YOLOv5中。</p><p>我们对YOLOv5x模型进行了12小时的培训。值得注意的是，它在Pascal VOC2012验证集上获得了0.725个MAP，超过了在AWS Rekognition、Azure Custom Vision和GCP Custom Vision培训的对象探测器集。此外，它的训练速度比它的AutoML表亲快得多。</p><p>本地推理比命中AutoML API快得多。但是，请注意，这种推断并不是一个公平的比较，因为AutoML工具的大部分推理时间都包含在网络开销中。GCP和Azure(但不是Rekognition)提供了导出训练有素的权重用于设备部署的能力。</p><p>在这里，我们将详细介绍如何在您自己的自定义计算机视觉任务上运行这些基准测试。</p><p>最简单的方法是使用Roboflow进行数据转换并上传到云平台。Roboflow的Pro Tier与所有三个云AutoML平台无缝集成。否则，您将花费数天(如果不是数周)来解析完全不同的数据格式、API和手续。</p><p>接下来，为您的数据集版本选择或清除预处理和增强选项，然后生成。生成后，您可以导出到三大云提供商。</p><p>一旦您的数据进入云提供商的愿景服务，您将能够一键训练以实现类似于我们在本文中提供的基准。</p><p>我们已经全面审查了所有三大云提供商用于培训自定义对象检测模型的无代码工具，结果如下：</p><p>在选择AutoML服务时，您需要权衡许多因素，包括培训时间和成本、模型性能以及推理时间和成本。根据我们上面的探索，我们为不同的使用案例推荐以下内容：</p><p>寻求最佳性能的大预算--使用Roboflow测试所有三个，看看哪一个最适合您的数据集，并在Azure和GCP中开始长期的培训工作。</p><p>寻求可持续的长期成本的大预算-GCP或Azure基于您的推断工作负载(可批处理：GCP，按需：Azure)。</p><p>需要实时推理-从GCP Custom Vision或Azure AutoML导出模型。如果您需要更好的性能(以大量的开发时间为代价)，请考虑培训开源。</p><p>我们希望您喜欢阅读我们对云AutoML解决方案的详细分析。训练愉快！</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.roboflow.ai/automl-vs-rekognition-vs-custom-vision/">https://blog.roboflow.ai/automl-vs-rekognition-vs-custom-vision/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/vision/">#vision</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/major/">#major</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>