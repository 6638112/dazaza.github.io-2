<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>数据不足的学习第2部分：主动学习Learning with Not Enough Data Part 2: Active Learning</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Learning with Not Enough Data Part 2: Active Learning<br/>数据不足的学习第2部分：主动学习</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-23 21:50:23</div><div class="page_narrow text-break page_content"><p>The performance of supervised learning tasks improves with more high-quality labels available. However, it is expensive to collect a large number of labeled samples. Active learning is one paradigm to deal with not enough labeled data, when there are resources for labeling more data samples but under a limited budget.</p><p>有监督学习任务的性能随着更多高质量标签的提供而提高。然而，收集大量标记样本的成本很高。当有资源标记更多的数据样本但预算有限时，主动学习是一种处理标记数据不足的范式。</p><p> This is part 2 of what to do when facing a limited amount of labeled data for supervised learning tasks. This time we will get some amount of human labeling work involved, but within a budget limit, and therefore we need to be smart when selecting which samples to label.</p><p>这是在监督学习任务中面对有限数量的标记数据时该怎么做的第2部分。这一次，我们将涉及一些人类标记工作，但在预算范围内，因此我们需要在选择要标记的样本时保持明智。</p><p>     Given an unlabeled dataset \(\mathcal{U}\) and a fixed amount of labeling cost \(B\), active learning aims to select a subset of \(B\) examples from \(\mathcal{U}\) to be labeled such that they can result in maximized improvement in model performance. This is an effective way of learning especially when data labeling is difficult and costly, e.g. medical images. This classical  survey paper in 2010 lists many key concepts. While some conventional approaches may not apply to deep learning, discussion in this post mainly focuses on deep neural models and training in batch mode.</p><p>给定一个未标记的数据集\（\mathcal{U}\）和一个固定的标记成本\（B\），主动学习的目的是从\（\mathcal{U}\）中选择一个子集\（B\）示例进行标记，以便最大限度地提高模型性能。这是一种有效的学习方法，尤其是在数据标记困难且成本高昂的情况下，例如医学图像。这篇2010年的经典调查论文列出了许多关键概念。虽然一些传统方法可能不适用于深度学习，但本文的讨论主要集中在深度神经模型和批处理模式下的训练。</p><p>   To simplify the discussion, we assume that the task is a \(K\)-class classification problem in all the following sections. The model with parameters \(\theta\) outputs a probability distribution over the label candidates, which may or may not be calibrated, \(P_\theta(y \vert \mathbf{x})\) and the most likely prediction is \(\hat{y} = \arg\max_{y \in \mathcal{Y}} P_\theta(y \vert \mathbf{x})\).</p><p>为了简化讨论，我们假设在以下所有部分中，任务都是一个\（K \）类分类问题。带有参数\（\theta\）的模型在标签候选上输出概率分布，该概率分布可能会被校准，也可能不会被校准。\（P \ \theta（y\vert\mathbf{x}），最可能的预测是\（\hat{y}=\arg\max\u{y\in\mathcal{y}P\utheta（y\vert\mathbf{x}）。</p><p>  The process of identifying the most valuable examples to label next is referred to as “sampling strategy” or “query strategy”. The scoring function in the sampling process is named “acquisition function”, denoted as \(U(\mathbf{x})\). Data points with higher scores are expected to produce higher value for model training if they get labeled.</p><p>识别最有价值的示例以标记下一步的过程称为“采样策略”或“查询策略”。采样过程中的评分函数称为“采集函数”，表示为\（U（\mathbf{x}）\）。得分较高的数据点如果被标记，则有望为模型训练产生更高的价值。</p><p>   Uncertainty sampling selects examples for which the model produces most uncertain predictions. Given a single model, uncertainty can be estimated by the predicted probabilities, although one common complaint is that deep learning model predictions are often not calibrated and not correlated with true uncertainty well. In fact, deep learning models are often overconfident.</p><p>不确定性抽样选择模型产生最多不确定性预测的示例。给定一个单一的模型，不确定性可以通过预测的概率来估计，尽管一个常见的抱怨是深度学习模型的预测通常没有经过校准，并且与真实的不确定性没有很好的相关性。事实上，深度学习模式往往过于自信。</p><p> Least confident score, also known as  variation ratio: \(U(\mathbf{x}) = 1 - P_\theta(\hat{y} \vert \mathbf{x})\).</p><p>最不自信分数，也称为变异率：\（U（\mathbf{x}）=1-P_theta（\hat{y}\vert\mathbf{x}）\）。</p><p>  Margin score: \(U(\mathbf{x}) = P_\theta(\hat{y}_1 \vert \mathbf{x}) - P_\theta(\hat{y}_2 \vert \mathbf{x})\), where \(\hat{y}_1\) and \(\hat{y}_2\) are the most likely and the second likely predicted labels.</p><p>边际得分：\（U（\mathbf{x}）=P\theta（\hat{y}U 1\vert\mathbf{x}）-P\theta（\hat{y}U 2\vert\mathbf{x}），其中\（\hat{y}U 1\）和\（\hat{y}U 2\）是最可能和第二可能的预测标签。</p><p>  Entropy: \(U(\mathbf{x}) = \mathcal{H}(P_\theta(y \vert \mathbf{x})) = - \sum_{y \in \mathcal{Y}} P_\theta(y \vert \mathbf{x}) \log P_\theta(y \vert \mathbf{x})\).</p><p>熵：\（U（\mathbf{x}）=\mathcal{H}（P\theta（y\vert\mathbf{x}））=-\sum\U{y\in\mathcal{y}P\theta（y\vert\mathbf{x}）\log P\theta（y\vert\mathbf{x}）。</p><p> Another way to quantify uncertainty is to rely on a committee of expert models, known as Query-By-Committee (QBC). QBC measures uncertainty based on a pool of opinions and thus it is critical to keep a level of disagreement among committee members. Given \(C\) models in the committee pool, each parameterized by \(\theta_1, \dots, \theta_C\).</p><p>另一种量化不确定性的方法是依靠专家模型委员会，称为委员会查询（QBC）。QBC基于一系列意见来衡量不确定性，因此保持委员会成员之间的分歧水平至关重要。给定委员会池中的\（C\）模型，每个模型由\（\theta_1、\dots、\theta_C\）参数化。</p><p> Voter entropy: \(U(\mathbf{x}) = \mathcal{H}(\frac{V(y)}{C})\), where \(V(y)\) counts the number of votes from the committee on the label \(y\).</p><p>选民熵：\（U（\mathbf{x}）=\mathcal{H}（\frac{V（y）}{C}），其中\（V（y）\）统计标签\（y\）上委员会的投票数。</p><p>  Diversity sampling intend to find a collection of samples that can well represent the entire data distribution. Diversity is important because the model is expected to work well on any data in the wild, just not on a narrow subset. Selected samples should be representative of the underlying distribution. Common approaches often rely on quantifying the similarity between samples.</p><p>多样性抽样旨在找到能够很好地代表整个数据分布的样本集合。多样性很重要，因为该模型有望在野外的任何数据上都能很好地工作，而不是在一个狭窄的子集上。所选样本应代表潜在分布。常用的方法通常依赖于量化样本之间的相似性。</p><p>  Expected model change refers to the impact that a sample brings onto the model training. The impact can be the influence on the model weights or the improvement over the training loss. A  later section reviews several works on how to measure model impact triggered by selected data samples.</p><p>预期模型变更是指样本对模型培训产生的影响。影响可以是对模型权重的影响，也可以是对训练损失的改善。下一节将回顾关于如何测量选定数据样本触发的模型影响的几项工作。</p><p>  Many methods above are not mutually exclusive. A  hybrid sampling strategy values different attributes of data points, combining different sampling preferences into one. Often we want to select  uncertain but also highly representative samples.</p><p>上述许多方法并不相互排斥。混合采样策略对数据点的不同属性进行估值，将不同的采样偏好组合为一个。我们通常希望选择不确定但也具有高度代表性的样本。</p><p>   The model uncertainty is commonly categorized into two buckets ( Der Kiureghian &amp; Ditlevsen 2009,  Kendall &amp; Gal 2017):</p><p>模型不确定性通常分为两类（Der Kiureghian&Ditlevsen 2009，Kendall&Gal 2017）：</p><p> Aleatoric uncertainty is introduced by noise in the data (e.g. sensor data, noise in the measurement process) and it can be input-dependent or input-independent. It is generally considered as irreducible since there is missing information about the ground truth.</p><p>任意不确定度是由数据中的噪声（例如传感器数据、测量过程中的噪声）引入的，可以是输入相关的，也可以是输入独立的。它通常被认为是不可还原的，因为缺少关于地面真相的信息。</p><p>  Epistemic uncertainty refers to the uncertainty within the model parameters and therefore we do not know whether the model can best explain the data. This type of uncertainty is theoretically reducible given more data</p><p>认知不确定性是指模型参数内的不确定性，因此我们不知道模型是否能最好地解释数据。考虑到更多的数据，这种不确定性在理论上是可以减少的</p><p>  There is a long tradition in machine learning of using ensembles to improve model performance. When there is a significant diversity among models, ensembles are expected to yield better results. This ensemble theory is proved to be correct by many ML algorithms; for example,  AdaBoost aggregates many weak learners to perform similar or even better than a single strong learner.  Bootstrapping ensembles multiple trials of resampling to achieve more accurate estimation of metrics. Random forests or  GBM is also a good example for the effectiveness of ensembling.</p><p>机器学习有一个悠久的传统，即使用集成来提高模型性能。当模型之间存在显著差异时，集成有望产生更好的结果。许多ML算法都证明了这个系综理论的正确性；例如，AdaBoost聚合了许多弱学习者，使其表现与单个强学习者相似，甚至更好。Bootstrapping集成了多次重采样试验，以实现更准确的度量估计。随机森林（Random forests）或GBM也是一个很好的例子，说明了加密的有效性。</p><p> To get better uncertainty estimation, it is intuitive to aggregate a collection of independently trained models. However, it is expensive to train a single deep neural network model, let alone many of them. In reinforcement learning, Bootstrapped DQN ( Osband, et al. 2016) is equipped with multiple value heads and relies on the uncertainty among an ensemble of Q value approximation to guide  exploration in RL.</p><p>为了获得更好的不确定性估计，可以直观地聚合一组经过独立训练的模型。然而，训练单个深层神经网络模型的成本很高，更不用说训练其中的许多模型了。在强化学习中，自举DQN（Osband等，2016）配备了多个值头，并依靠Q值近似集合中的不确定性来指导RL中的探索。</p><p> In active learning, a commoner approach is to use  dropout to “simulate” a probabilistic Gaussian process ( Gal &amp; Ghahramani 2016). We thus ensemble multiple samples collected from the same model but with different dropout masks applied during the forward pass to estimate the model uncertainty (epistemic uncertainty). The process is named  MC dropout (Monte Carlo dropout), where dropout is applied before every weight layer, is approved to be mathematically equivalent to an approximation to the probabilistic deep Gaussian process ( Gal &amp; Ghahramani 2016). This simple idea has been shown to be effective for classification with small datasets and widely adopted in scenarios when efficient model uncertainty estimation is needed.</p><p>在主动学习中，更常见的方法是使用辍学学生“模拟”概率高斯过程（Gal&amp；Ghahramani 2016）。因此，我们将从同一模型中采集的多个样本进行整合，但在正向传递过程中使用不同的退出遮罩，以估计模型的不确定性（认知不确定性）。该过程被称为MC辍学（Monte Carlo辍学），在每个权重层之前应用辍学，被批准在数学上等同于概率深高斯过程的近似值（Gal&Ghahramani 2016）。这种简单的想法已经被证明对小数据集的分类是有效的，并且在需要有效的模型不确定性估计的场景中被广泛采用。</p><p> DBAL (Deep Bayesian active learning;  Gal et al. 2017) approximates Bayesian neural networks with MC dropout such that it learns a distribution over model weights. In their experiment, MC dropout performed better than random baseline and mean standard deviation (Mean STD), similarly to variation ratios and entropy measurement.</p><p>DBAL（Deep Bayes active learning；Gal et al.2017）使用MC辍学近似贝叶斯神经网络，从而学习模型权重的分布。在他们的实验中，MC辍学表现优于随机基线和平均标准偏差（mean STD），类似于变异率和熵测量。</p><p>   Beluch et al. (2018) compared ensemble-based models with MC dropout and found that the combination of naive ensemble (i.e. train multiple models separately and independently) and variation ratio yields better calibrated predictions than others. However, naive ensembles are  very expensive, so they explored a few alternative cheaper options:</p><p>Beluch等人（2018年）将基于集合的模型与MC辍学模型进行了比较，发现朴素集合（即单独和独立地训练多个模型）和变异率的组合产生了比其他模型更好的校准预测。然而，naive套装非常昂贵，因此他们探索了几种更便宜的选择：</p><p> Snapshot ensemble: Use a cyclic learning rate schedule to train an implicit ensemble such that it converges to different local minima.</p><p>快照集成：使用循环学习速率计划来训练隐式集成，使其收敛到不同的局部极小值。</p><p>  Diversity encouraging ensemble (DEE): Use a base network trained for a small number of epochs as initialization for \(n\) different networks, each trained with dropout to encourage diversity.</p><p>多样性鼓励集成（DEE）：使用一个经过少量训练的基本网络作为\（n \）个不同网络的初始化，每个网络都经过辍学训练，以鼓励多样性。</p><p> Unfortunately all the cheap implicit ensemble options above perform worse than naive ensembles. Considering the limit on computational resources, MC dropout is still a pretty good and economical choice. Naturally, people also try to combine ensemble and MC dropout ( Pop &amp; Fulop 2018) to get a bit of additional performance gain by stochastic ensemble.</p><p>不幸的是，上面所有廉价的隐式合奏选项都比天真的合奏效果差。考虑到计算资源的限制，MC辍学仍然是一个非常好且经济的选择。当然，人们也会尝试将合奏和MC辍学（Pop&amp；Fulop 2018）结合起来，通过随机合奏获得一些额外的性能增益。</p><p>  Bayes-by-backprop ( Blundell et al. 2015) measures weight uncertainty in neural networks directly. The method maintains a probability distribution over the weights \(\mathbf{w}\), which is modeled as a variational distribution \(q(\mathbf{w} \vert \theta)\) since the true posterior \(p(\mathbf{w} \vert \mathcal{D})\) is not tractable directly. The loss is to minimize the KL divergence between \(q(\mathbf{w} \vert \theta)\) and \(p(\mathbf{w} \vert \mathcal{D})\),</p><p>backprop的Bayes（Blundell等人，2015）直接测量神经网络中的权重不确定性。该方法在权重\（\mathbf{w}\）上保持概率分布，该概率分布被建模为变分分布\（q（\mathbf{w}\vert\theta）\），因为真实后验分布\（p（\mathbf{w}\vert\mathcal{D}）不可直接处理。损失是为了最小化\（q（\mathbf{w}\vert\theta）\）和\（p（\mathbf{w}\vert\mathcal{D}）\）之间的KL差异，</p><p>\[\begin{aligned}\mathcal{L}(\theta)&amp;= \text{KL}[q(\mathbf{w}\vert\theta) \| p(\mathbf{w} \vert \mathcal{D})] \\ &amp;= \int q(\mathbf{w}\vert\theta) \log \frac{q(\mathbf{w}\vert\theta)}{p(\mathbf{w}) p(\mathcal{D}\vert \mathbf{w})} d\mathbf{w} \\ &amp;= \text{KL}[q(\mathbf{w}\vert\theta) \| p(w)] - \mathbb{E}_{q(\mathbf{w}\vert\theta)} [\log p(\mathcal{D} \vert \mathbf{w})] \\&amp;\approx \log q(\mathbf{w} \vert \theta) - \log p(\mathbf{w}) p(\mathcal{D}\vert \mathbf{w}) &amp; \text{; monte carlo sampling; }q(\mathbf{w} \vert \theta)\text{ &amp; }p(\mathbf{w})\text{ are close.}\end{aligned}\] The variational distribution \(q\) is typically a Gaussian with diagonal covariance and each weight is sampled from \(\mathcal{N}(\mu_i, \sigma_i^2)\). To ensure non-negativity of \(\sigma_i\), it is further parameterized via softplus, \(\sigma_i = \log(1 + \exp(\rho_i))\) where the variational parameters are \(\theta = \{\mu_i , \rho_i\}^d_{i=1}\).</p><p>\[\begin{aligned}\mathcal{L}（\theta）&amp；=\text{KL}[q（\mathbf{w}\vert\theta）\\124; p（\mathbf{w}\vert\mathcal{D}）]\&amp；=\int q（\mathbf{w}\vert\theta）\log\frac{q（\mathbf{w}\vert\theta）}{p（\mathbf{w}）p（\mathcal{D}\vert\mathbf{w}）D\mathbf{w}\\\\&amp；=\文本{KL}[q（\mathbf{w}\vert\theta）\\ p（w）]-\mathbb{E}{uq（\mathbf{w}\vert\theta）}[\log p（\mathcal{D}\vert\mathbf{w}）]\&amp；\大约\logq（\mathbf{w}\vert\theta）-\logp（\mathbf{w}）p（\mathcal{D}\vert\mathbf{w}）&amp；\文本{；蒙特卡罗抽样；}q（\mathbf{w}\vert\theta）\text{&amp；}p（\mathbf{w}）\text{接近。}\结束{aligned}\]变分分布\（q\）通常是具有对角协方差的高斯分布，每个权重从\（\mathcal{N}（\mu_i，\sigma_i^2）\）中采样。为了确保\（\sigma_i\）的非负性，它通过softplus进一步参数化，\（\sigma_i=\log（1+\exp（\rho_i）），其中变分参数为\（\theta=\{\mu_i\rho_i\}^d_{i=1}）。</p><p>  Calculate the gradient of \(f(\mathbf{w}, \theta)\) w.r.t. to \(\mu\) and \(\rho\) and then update \(\theta\).</p><p>计算\（f（\mathbf{w}，\theta）\）w.r.t.到\（\mu\）和\（\rho\）的梯度，然后更新\（\theta\）。</p><p>  The loss objective guides model training. A low loss value indicates that a model can make good and accurate predictions.  Yoo &amp; Kweon (2019) designed a  loss prediction module to predict the loss value for unlabeled inputs, as an estimation of how good a model prediction is on the given data. Data samples are selected if the loss prediction module makes uncertain predictions (high loss value) for them. The loss prediction module is a simple MLP with dropout, that takes several intermediate layer features as inputs and concatenates them after a global average pooling.</p><p>损失目标指导模型培训。较低的损失值表明模型可以做出良好且准确的预测。Yoo&amp；Kweon（2019）设计了一个损失预测模块，用于预测未标记输入的损失值，作为对给定数据的模型预测效果的估计。如果损失预测模块对数据样本进行不确定预测（高损失值），则选择数据样本。损耗预测模块是一个简单的带丢失的MLP，它将几个中间层特征作为输入，并在全局平均池后将它们连接起来。</p><p>   Let \(\hat{l}\) be the output of the loss prediction module and \(l\) be the true loss. When training the loss prediction module, a simple MSE loss \(=(l - \hat{l})^2\) is not a good choice, because the loss decreases in time as the model learns to behave better. A good learning objective should be independent of the scale changes of the target loss. They instead rely on the comparison of sample pairs. Within each batch of size \(b\), there are \(b/2\) pairs of samples \((\mathbf{x}_i, \mathbf{x}_j)\) and the loss prediction model is expected to correctly predict which sample has a larger loss.</p><p>假设\（\hat{l}\）是损失预测模块的输出，\（l\）是真正的损失。在培训损失预测模块时，简单的MSE损失“（=（l-\hat{l}）^2”）不是一个好的选择，因为随着模型学习到更好的行为，损失会随着时间的推移而减少。一个好的学习目标应该独立于目标损失的规模变化。相反，他们依赖于样本对的比较。在每批大小为\（b\）的样品中，有\（b/2\）对样品\（（\mathbf{x}u i、\mathbf{x}u j）\），预计损失预测模型能够正确预测哪个样品的损失更大。</p><p>\[\begin{aligned}\mathcal{L}_\text{loss}(\mathbf{x}_i, \mathbf{x}_j) &amp;= \max\big( 0, -\mathbb{1}(l(\mathbf{x}_i), l(\mathbf{x}_j)) \cdot (\hat{l}(\mathbf{x}_i) - \hat{l}(\mathbf{x}_j)) + \epsilon \big) \\ \text{where } \mathbb{1}(l_i, l_j) &amp;= \begin{cases} +1 &amp; \text{if }l_i &gt; l_j \\ -1 &amp; \text{otherwise} \end{cases} \end{aligned}\]  In experiments on three vision tasks, active learning selection based on the loss prediction performs better than random baseline, entropy based acquisition and  core-set.</p><p>\[bebe开始[[[be开始[[be开始[be开始[be开始[be开始[[be开始[[对齐]的[[[be开始[be开始[[be开始[[be开始[be开始[[[[[对齐]]]的[[be开始[[be开始[[[be开始[[be开始[be开始[be开始[[[be开始[[be开始[[[be开始[[[be开始[[[[be开始]]]]]可能[[[[[[[be开始[[[[be开始]]]]]]]可能[[[[[be开始[[[[[[[be开始[[[[[[be开始[[[[[be开始[[[[[[be开始]]]]]]]]]]]]可能[[[[[[[[[[[[[be开始]]]]]]]]]]]]]]]]]]的[[[[[[[[[[[[[[[[[[i，L_j）&amp；=\begin{cases}+1&amp；\text{if}L_i&gt；L_j\-1&amp；\text{other}\end{cases}\end{aligned}\]在三项视觉任务的实验中，基于损失预测的主动学习选择优于随机基线、基于熵的获取和核心集。</p><p>    Sinha et al. (2019) proposed a GAN-like setup, named  VAAL (Variational Adversarial Active Learning), where a discriminator is trained to distinguish unlabeled data from labeled data. Interestingly, active learning acquisition criteria does not depend on the task performance in VAAL.</p><p>Sinha等人（2019年）提出了一种类似于GAN的设置，名为VAAL（变分对抗式主动学习），其中训练了一个鉴别器来区分未标记数据和标记数据。有趣的是，主动学习习得标准并不取决于VAAL中的任务表现。</p><p>   The \(\beta\)-VAE learns a latent feature space \(\mathbf{z}^l \cup \mathbf{z}^u\), for labeled and unlabeled data respectively, aiming to  trick the discriminator \(D(.)\) that all the data points are from the labeled pool;</p><p>\（\beta\）-VAE分别学习标记和未标记数据的潜在特征空间\（\mathbf{z}^l\cup\mathbf{z}^u\），目的是欺骗鉴别器\（D（.）\）所有数据点都来自标记的池；</p><p>  The discriminator \(D(.)\) predicts whether a sample is labeled (1) or not (0) based on a latent representation \(\mathbf{z}\). VAAL selects unlabeled samples with low discriminator scores, which indicates that those samples are sufficiently different from previously labeled ones.</p><p>鉴别器\（D（.）\）根据潜在表示\（\mathbf{z}\）预测样本是否标记为（1）或（0）。VAAL选择具有低鉴别器分数的未标记样本，这表明这些样本与之前标记的样本有足够的差异。</p><p> The loss for VAE representation learning in VAAL contains both a reconstruction part (minimizing the ELBO of given samples) and an adversarial part (labeled and unlabeled data is drawn from the same probability distribution \(q_\phi\)):</p><p>VAAL中VAE表示学习的损失包含重建部分（最小化给定样本的ELBO）和对抗部分（标记和未标记的数据来自相同的概率分布\（q_ \ phi\）：</p><p>\[\begin{aligned}\mathcal{L}_\text{VAE} &amp;= \lambda_1 \mathcal{L}^\text{rec}_\text{VAE} + \lambda_2 \mathcal{L}^\text{adv}_\text{VAE} \\\mathcal{L}^\text{rec}_\text{VAE} &amp;= \mathbb{E}[\log p_\theta(\mathbf{x}^l \vert \mathbf{z}^l)] - \beta \text{KL}(q_\phi(\mathbf{z}^l \vert \mathbf{x}^l) \| p(\mathbf{\tilde{z}})) + \mathbb{E}[\log p_\theta(\mathbf{u} \vert \mathbf{z}^u)] - \beta \text{KL}(q_\phi(\mathbf{z}^u \vert \mathbf{u}) \| p(\mathbf{\tilde{z}})) \\\mathcal{L}^\text{adv}_\text{VAE} &amp;= - \mathbb{E}[\log D(q_\phi (\mathbf{z}^l \vert \mathbf{x}^l))] - \mathbb{E}[\log D(q_\phi(\mathbf{z}^u \vert \mathbf{u}))]\end{aligned}\] where \(p(\mathbf{\tilde{z}})\) is a unit Gaussian as a predefined prior and \(\beta\) is the Lagrangian parameter.</p><p>\[开始[[[[be开始[[[[be开始[[[[[be开始[[[[[[[对齐]}}}{{[[[be开始[[[be开始[[[be开始[[[[[[[对齐]]]]可能[[be开始[[[[be开始[[[[[be开始[[[[对齐]]]]可能[[[[[[be开始[[[[[[[[be开始]]]]]]]]可能[[[[[[[[be开始]]]]]]]可能可能[[[[[[[[[[[[[[[be开始]]]]]]]]]]可能可能可能可能可能可能[[[[[[[[[[[[[[[[[[be开始]]]]]]]]]]]]]]]]]]]可能可能可能[[[[..........................^L]-\beta\text{KL}（q\phi（\mathbf{z}^L\vert\mathbf{x}^L）\\124p（\mathbf{\tilde{z}））+\mathbb{E}[\log p\uTheta（\mathbf{u}\vert\mathbf{z}^u）]-\beta\text{KL}（q{phi（\mathbf{z}^u\vert\mathbf{u}）\|p（\mathbf{\tilde{z}））\\\mathcal{L}^\text{adv}\mathbb{E}[\logd（q_\phi（\mathbf{z}^l\vert\mathbf{x}^l））-\mathbb{E}[\logd（q_\phi（\mathbf{z}^u\vert\mathbf{u}））\end{aligned}]其中\（p（\mathbf{tilde{z}}）是一个预先定义的高斯单位，\）是拉格朗日参数。</p><p> \[\mathcal{L}_D = -\mathbb{E}[\log D(q_\phi (\mathbf{z}^l \vert \mathbf{x}^l))] - \mathbb{E}[\log (1 - D(q_\phi (\mathbf{z}^u \vert \mathbf{u})))]\]   Ablation studies showed that jointly training VAE and discriminator is critical. Their results are robust to the biased initial labeled pool, different labeling budgets and noisy oracle.</p><p>\消融研究表明，联合训练VAE和鉴别器是至关重要的。他们的结果对有偏差的初始标记池、不同的标记预算和嘈杂的oracle具有鲁棒性。</p><p> MAL (Minimax Active Learning;  Ebrahimiet al. 2021) is an extension of VAAL. The MAL framework consists of an entropy minimizing feature encoding network \(F\) followed by an entropy maximizing classifier \(C\). This minimax setup reduces the distribution gap between labeled and unlabeled data.</p><p>MAL（Minimax主动学习；Ebrahimiet al.2021）是VAAL的一个扩展。MAL框架由熵最小化特征编码网络\（F\）和熵最大化分类器\（C\）组成。这种minimax设置减少了标记和未标记数据之间的分布差距。</p><p>   A feature encoder \(F\) encodes a sample into a \(\ell_2\)-normalized \(d\)-dimensional latent vector. Assuming there are \(K\) classes, a classifier \(C\) is parameterized by \(\mathbf{W} \in \mathbb{R}^{d \times K}\).</p><p>特征编码器\（F \）将样本编码为\（ell_2 \）归一化\（d \）维潜在向量。假设存在\（K\）类，分类器\（C\）由\（\mathbf{W}\in\mathbb{R}^{d\times K}\）参数化。</p><p> (1) First \(F\) and \(C\) are trained on labeled samples by a simple cross entropy loss to achieve good classification results,</p><p>（1） 首先通过简单的交叉熵损失对标记样本进行训练，以获得良好的分类结果，</p><p>\[\mathcal{L}_\text{CE} = -\mathbb{E}_{(\mathbf{x}^l, y) \sim \mathcal{X}} \sum_{k=1}^K \mathbb{1}[k=y] \log\Big( \sigma(\frac{1}{T} \frac{\mathbf{W}^\top F\big(\mathbf{x}^l)}{\|F(\mathbf{x}^l)\|}\big) \Big)\] \[\begin{aligned}\mathcal{L}_\text{Ent} &amp;= -\sum^K_{k=1} p(y=k \vert \mathbf{u}) \log p(y=k\vert \mathbf{u}) \\\theta^*_F, \theta^*_C &amp;= \min_F\max_C \mathcal{L}_\text{Ent} \\\theta_F &amp;\gets \theta_F - \alpha_1 \nabla \mathcal{L}_\text{Ent} \\\theta_C &amp;\gets \theta_C + \alpha_2 \nabla \mathcal{L}_\text{Ent}\end{aligned}\]  First, minimizing the entropy in \(F\) encourages unlabeled samples associated with similar predicted labels to have similar features.</p><p>\[[[mathbf{x}^L，y\[[[[[[mathbf{[x}^1，1，y]\sim\Math[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[10]数学b[[[10}}}}{{1，1，1，1，1，1，1，1，1，1，1，1，1，1，1，y，1，y，y，y，y]概概概概概概概概概概概概概概概概概概概[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[[3]学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校学校]的[[[[[[[[[[[[[[[[[}\mathcal{L}\text{Ent}&amp；=-\sum^k{k=1}p（y=k\vert\mathbf{u}）\log p（y=k\vert\mathbf{u}）\\\theta^*\u F、\theta^*\u C&amp；=\min_F\max_C\mathcal{L}\text{Ent}\\\ theta_F&amp；\获取\theta_F-\alpha_1\nabla\mathcal{L}\uu\text{Ent}\\\ theta_C&amp；\首先获取\theta_C+\alpha_2\nabla\mathcal{L}\text{Ent}\end{aligned}\]，最小化\（F\）中的熵会鼓励与类似预测标签关联的未标记样本具有类似的功能。</p><p>  Maximizing the entropy in \(C\) adversarially makes the prediction to follow a more uniform class distribution.  (My understanding here is that because the true label of an unlabeled sample is unknown, we should not optimize the classifier to maximize the predicted labels just yet.)</p><p>在\（C \）中最大化熵会使预测遵循更均匀的类分布。（我在这里的理解是，由于未标记样本的真实标签未知，我们不应该优化分类器以最大化预测标签。）</p><p>   Diversity: the score of \(D\) indicates how similar a sample is to previously seen examples. A score closer to 0 is better to select unfamiliar data points.</p><p>多样性：分数为\（D\）表示样本与之前看到的样本有多相似。分数接近0时，最好选择不熟悉的数据点。</p><p>  Uncertainty: use the entropy obtained by \(C\). A higher entropy score indicates that the model cannot make a confident prediction yet.</p><p>不确定性：使用\（C\）获得的熵。熵得分越高，表明该模型尚不能做出可靠的预测。</p><p> The experiments compared MAL to random, entropy, core-set, BALD and VAAL baselines, on image classification and segmentation tasks. The results look pretty strong.</p><p>在图像分类和分割任务上，实验将MAL与随机、熵、核心集、BALD和VAAL基线进行了比较。结果看起来相当强劲。</p><p>   CAL (Contrastive Active Learning;  Margatina et al. 2021) intends to select  contrastive examples. If two data points with different labels share similar network representations \(\Phi(.)\), they are considered as contrastive examples in CAL. Given a pair of contrastive examples \((\mathbf{x}_i, \mathbf{x}_j)\), they should</p><p>CAL（对比主动学习；Margatina et al.2021）打算选择对比例子。如果具有不同标签的两个数据点共享相似的网络表示\（\Phi（.）\），它们在CAL中被视为对比示例。给出一对对比示例\（\mathbf{x}u i、\mathbf{x}u j）\），它们应该</p><p>\[d(\Phi(\mathbf{x}_i), \Phi(\mathbf{x}_j)) &lt; \epsilon \quad\text{and}\quad \text{KL}(p(y\vert \mathbf{x}_i) \| p(y\vert \mathbf{x}_j)) \rightarrow \infty\]  Select the top \(k\) nearest neighbors in the model feature space among the labeled samples, \(\{(\mathbf{x}^l_i, y_i\}_{i=1}^M \subset \mathcal{X}\).</p><p>\[d（p（mathbf{x}{x}{x}{x}}{U j）和[d（p（mathbf{x}{x}{x}{x}{y（p（p（y\mathbf{x}{x{x{x{x}}i）和[p（p（mathbf{x}{x}{x}{x}{x}{x}j）的[j）和[p（p（p（p（y（y\mathbf{x{x{x{x{x{x}{x{x}{x}{x{x}{x}{x}x}x}}x}x}x}x}{x}}x}x}}x}}j）的[j）的[j\）。</p><p>  Compute the KL divergence between the model output probabilities of \(\mathbf{x}\) and each in \(\{\mathbf{x}^l\}\). The contrastive score of \(\mathbf{x}\) is the average of these KL divergence values: \(s(\mathbf{x}) = \frac{1}{M} \sum_{i=1}^M \text{KL}(p(y \vert \mathbf{x}^l_i \| p(y \vert \mathbf{x}))\).</p><p>计算\（\{mathbf{x}\）和\（\{mathbf{x}^l}\）中每个模型输出概率之间的KL散度。\（\mathbf{x}\）的对比分数是这些KL散度值的平均值：\（s（\mathbf{x}）=\frac{1}{M}\sum{i=1}^M\text{KL}（p（y\vert\mathbf{x}^l\u i\\ 124p（y\vert\mathbf{x}））。</p><p> On a variety of classification tasks, the experiment results of CAL look similar to the entropy baseline.</p><p>在各种分类任务上，CAL的实验结果与熵基线相似。</p><p>   A  core-set is a concept in computational geometry, referring to a small set of points that approximates the shape of a larger point set. Approximation can be captured by some geometric measure. In the active learning, we expect a model that is trained over the core-set to behave comparably with the model on the entire data points.</p><p>核心集是计算几何中的一个概念，指的是近似于较大点集形状的一小组点。近似可以通过一些几何度量来捕捉。在主动学习中，我们期望在核心集合上训练的模型在整个数据点上的表现与模型相当。</p><p> Sener &amp; Savarese (2018) treats active learning as a core-set selection problem. Let’s say, there are \(N\) samples in total accessible during training. During active learning, a small set of data points get labeled at every time step \(t\), denoted as \(\mathcal{S}^{(t)}\). The upper bound of the learning objective can be written as follows, where the  core-set loss is defined as the difference between average empirical loss over the labeled samples and the loss over the entire dataset including unlabelled ones.</p><p>Sener&amp；Savarese（2018）将主动学习视为核心集合选择问题。比如说，在培训期间总共有\（N\）个样本可访问。在主动学习过程中，每个时间步\（t\）都会标记一小部分数据点，表示为\（\mathcal{S}^{（t）}\）。学习目标的上界可以写成如下，其中核心集损失定义为标记样本的平均经验损失与整个数据集（包括未标记样本）的损失之间的差值。</p><p>\[\begin{aligned}\mathbb{E}_{(\mathbf{x}, y) \sim p} [\mathcal{L}(\mathbf{x}, y)]\leq&amp; \bigg\vert \mathbb{E}_{(\mathbf{x}, y) \sim p} [\mathcal{L}(\mathbf{x}, y)] - \frac{1}{N} \sum_{i=1}^N \mathcal{L}(\mathbf{x}_i, y_i) \bigg\vert &amp; \text{; Generalization error}\\+&amp; \frac{1}{\vert \mathcal{S}^{(t)} \vert} \sum_{j=1}^{\vert \mathcal{S}^{(t)} \vert} \mathcal{L}(\mathbf{x}^l_j, y_j) &amp; \text{; Training error}\\+&amp; \bigg\vert \frac{1}{N} \sum_{i=1}^N \mathcal{L}(\mathbf{x}_i, y_i) - \frac{1}{\vert \mathcal{S}^{(t)} \vert} \sum_{j=1}^{\vert \mathcal{S}^{(t)} \vert} \mathcal{L}(\mathbf{x}^l_j, y_j) \bigg\vert &amp; \text{; Core-set error}\end{aligned}\] \[\min_{\mathcal{S}^{(t+1)} : \vert \mathcal{S}^{(t+1)} \vert \leq b} \bigg\vert \frac{1}{N}\sum_{i=1}^N \mathcal{L}(\mathbf{x}_i, y_i) - \frac{1}{\vert \mathcal{S}^{(t)} \cup \mathcal{S}^{(t+1)} \vert} \sum_{j=1}^{\vert \mathcal{S}^{(t)} \cup \mathcal{S}^{(t+1)} \vert} \mathcal{L}(\mathbf{x}^l_j, y_j) \bigg\vert\] It is equivalent to  the \(k\)-Center problem: choose \(b\) center points such that the largest distance between a data point and its nearest center is minimized. This problem is NP-hard. An approximate solution depends on the greedy algorithm.</p><p>\[\begin{aligned}\mathb{E}{（\mathbf{x}，y）\sim p}[\mathcal{L}（\mathbf{x}，y）]\leq&amp；\bigg\vert\mathbb{E}{（\mathbf{x}，y）\sim p}[\mathcal{L}（\mathbf{x}，y）]-\frac{1}{N}\sum{i=1}^N\mathcal{L}（\mathbf{x}，y}）bigg vert&amp；\文本{；泛化错误}\\\+&amp；\frac{1}{\vert\mathcal{S}{（t）}\vert}\sum{j=1}{\vert\mathcal{S}{（t）}\vert}\mathcal{L}（\mathbf{x}^lj，yj）&amp；\文本{；训练错误}\\\+&amp；\bigg\vert\frac{1}{N}\sum{i=1}^N\mathcal{L}（\mathbf{x}i，y{u i）-\frac{1}{\vert\mathcal{S}\vert}\sum{j=1}{\vert\mathcal{S}{t}\vertj}；\（t+1）以下的：：：：：{核心组错误；核心组错误；核心组错误：{{核心组错误：{核心组错误：{核心组错误：{{核心组错误：{核心组错误：{{核心组错误（t+1）}\核心组错误[[[[[[[[[5 5]的主要组组错误]缅甸缅甸[[[[[[5]主要主要主要组组组组组组错误：{{（t+1）1）的[t+1}}}}}\ \ \倒倒倒倒倒倒倒倒倒倒倒基基..............基基.............................................................................1）}\vert}\sum{j=1}{\vert\mathcal{S}{（t）}\cup\mathcal{S}{（t+1）}\vert}\mathcal{L}（\mathbf{x}^L{j，yj）\bigg\vert\]这相当于\（k \）中心问题：选择\（b \）中心点，使数据点与其最近中心之间的最大距离最小化。这个问题是NP难问题。近似解取决于贪婪算法。</p><p>   It works well on image classification tasks when there is a small number of classes. When the number of classes grows to be large or the data dimensionality increases (“curse of dimensionality”), the core-set method becomes less effective ( Sinha et al. 2019).</p><p>当类数较少时，它可以很好地用于图像分类任务。当类的数量增加或数据维度增加（“维度诅咒”）时，核心集方法的效率就会降低（Sinha等人，2019年）。</p><p> Because the core-set selection is expensive,  Coleman et al. (2020) experimented with a weaker model (e.g. smaller, weaker architecture, not fully trained) and found that empirically using a weaker model as a proxy can significantly shorten each repeated data selection cycle of training models and selecting samples, without hurting the final error much. Their method is referred to as  SVP (Selection via Proxy).</p><p>由于核心集选择成本高昂，Coleman等人（2020年）对较弱的模型进行了实验（例如，较小、较弱的体系结构，未完全训练），并发现经验上使用较弱的模型作为代理可以显著缩短训练模型和选择样本的每个重复数据选择周期，不会对最后的错误造成太大伤害。他们的方法被称为SVP（通过代理选择）。</p><p>  BADGE (Batch Active learning by Diverse Gradient Embeddings;  Ash et al. 2020) tracks both model uncertainty and data diversity in the gradient space. Uncertainty is measured by the gradient magnitude w.r.t. the final layer of the network and diversity is captured by a diverse set of samples that span in the gradient space.</p><p>BADGE（通过不同梯度嵌入的批量主动学习；Ash等人，2020）跟踪梯度空间中的模型不确定性和数据多样性。不确定性通过梯度幅值w.r.t.来测量。网络的最后一层和多样性由梯度空间中的一组不同样本捕获。</p><p> Uncertainty. Given an unlabeled sample \(\mathbf{x}\), BADGE first computes the prediction \(\hat{y}\) and the gradient \(g_\mathbf{x}\) of the loss on \((\mathbf{x}, \hat{y})\) w.r.t. the last layer’s parameters. They observed that the norm of \(g_\mathbf{x}\) conservatively estimates the example’s influence on the model learning and high-confidence samples tend to have gradient embeddings of small magnitude.</p><p>不确定性。给定一个未标记的样本\（\mathbf{x}\），BADGE首先计算最后一层的参数在\（\mathbf{x}、\hat{y}）上的损失的预测\（\hat{y}\）和梯度\（g\mathbf{x}）。他们观察到，\（g_\mathbf{x}\）的范数保守地估计了示例对模型学习的影响，并且高置信度样本往往具有小幅度的梯度嵌入。</p><p>  Diversity. Given many g</p><p>差异考虑到很多g</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/mathbf/">#mathbf</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>