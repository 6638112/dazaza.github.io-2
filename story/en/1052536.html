<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>谁确保A.I. 机器不是种族主义者？ Who Is Making Sure the A.I. Machines Aren’t Racist?</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Who Is Making Sure the A.I. Machines Aren’t Racist?<br/>谁确保A.I. 机器不是种族主义者？ </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-16 00:39:02</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/ed5c037d22aa60f71a9f6c3b0f8104e6.jpg"><img src="http://img2.diglog.com/img/2021/3/ed5c037d22aa60f71a9f6c3b0f8104e6.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Hundreds of people gathered for the first lecture at what had become the world’s most important conference on artificial intelligence — row after row of faces. Some were East Asian, a few were Indian, and a few were women. But the vast majority were white men. More than 5,500 people attended the meeting, five years ago in Barcelona, Spain.</p><p>数百人聚集在一起的第一次讲座，以后成为世界上最重要的人工智能大会 - 脸上的行。有些是东亚，少数是印度人，少数女性。但绝大多数是白人。五年前在西班牙巴塞罗那参加了5,500多名会议。</p><p> Timnit Gebru, then a graduate student at Stanford University, remembers counting only six Black people other than herself, all of whom she knew, all of whom were men.</p><p> Timnit Gebru，然后是斯坦福大学的研究生，记得只有六名黑人以外的六名黑人，所有她所知道的，所有人都是男人。</p><p> The homogeneous crowd crystallized for her a glaring issue. The big thinkers of tech say A.I. is the future. It will underpin everything from search engines and email to the software that drives our cars, directs the  policing of our streets and  helps create our vaccines.</p><p> 均匀的人群为她的一个耀眼的问题而结晶。 Tech的大思想家说A.I.是未来。它将支持从搜索引擎和电子邮件到推动我们汽车的软件的一切，指导我们的街道警务并帮助创建我们的疫苗。</p><p> But it is being built in a way that replicates the biases of the almost entirely male, predominantly white work force making it.</p><p> 但它正在建立一种复制几乎完全男性的偏差，主要是白色的劳动力制造。</p><p> In the nearly 10 years I’ve written about artificial intelligence, two things have remained a constant: The technology relentlessly improves in fits and sudden, great leaps forward. And bias is a thread that subtly weaves through that work in a way that tech companies are reluctant to acknowledge.</p><p> 在我对人工智能写的近10年来，两件事仍然是一个常数：这种技术无情地改善了适合和突然的跳跃。并且偏见是一个巧妙地编织的线程，以这种工作，以这种方式是技术公司不愿意承认。</p><p> On her first night home in Menlo Park, Calif., after the Barcelona conference, sitting cross-​legged on the couch with her laptop, Dr. Gebru described the A.I. work force conundrum in a Facebook post.</p><p> 在她的第一个晚上在Menlo Park，加利福尼亚州的第一个晚上回家。，在巴塞罗那会议之后，用她的笔记本电脑坐在沙发上坐在沙发上，Gebru博士描述了A.I.工作部队在Facebook帖子中康明。</p><p> “I’m not worried about machines taking over the world. I’m worried about groupthink, insularity and arrogance in the A.I. community —  especially with the current hype and demand for people in the field,” she wrote. “The people creating the technology are a big part of the system. If many are actively excluded from its creation, this technology will benefit a few while harming a great many.”</p><p> “我并不担心机器接管世界。我担心A.I中的集团，绝对和傲慢。社区 - 特别是当前对该领域人民的炒作和需求，“她写道。 “创建技术的人是系统的重要组成部分。如果许多人被积极地排除在其创造之外，这项技术将受益一些，同时伤害很多。“ </p><p> The A.I. community buzzed about the mini-manifesto. Soon after, Dr. Gebru helped create a new organization, Black in A.I. After finishing her Ph.D., she was hired by Google.</p><p>A.I.社区对迷你宣言嗡嗡作响。之后，Gebru博士帮助创建一个新的组织，在A.I中创建一个新的组织。完成她的博士后，她被谷歌聘用了。</p><p> She teamed with Margaret Mitchell, who was building a group inside Google dedicated to “ethical A.I.” Dr. Mitchell had previously worked in the research lab at Microsoft. She had grabbed attention when she told Bloomberg News in 2016 that A.I. suffered from a  “sea of dudes” problem. She estimated that she had worked with hundreds of men over the previous five years and about 10 women.</p><p> 她与玛格丽特米切尔合作，他在谷歌内部建立了一个小组，致力于“道德A.I”。米切尔博士此前曾在微软的研究实验室工作过。当她在2016年在2016年讲布鲁姆伯格新闻时，她抓住了注意力。遭受了“帅哥海”问题。她估计，她在过去五年和大约10名女性中曾在数百名男子上工作过。</p><p> Their work was hailed as groundbreaking. The nascent A.I. industry, it had become clear, needed minders and people with different perspectives.</p><p> 他们的工作被誉为突破性。 nascent a.i.行业，它已经清晰，需要不同的观点的思维和人。</p><p> About six years ago, A.I. in a Google online photo service organized photos of Black people into a folder called “gorillas.” Four years ago, a researcher at a New York start-up noticed that the A.I. system she was working on was egregiously biased against Black people. Not long after, a Black researcher in Boston discovered that an A.I. system couldn’t identify her face — until she put on a white mask.</p><p> 大约六年前，A.I.在Google Online照片服务中有组织的黑人照片进入一个名为“Gorillas”的文件夹。四年前，纽约的一名研究员初创公司注意到了A.I.她正在努力的系统是对黑人偏见的偏见。不久之后，波士顿的黑色研究员发现了一个A.I.系统无法识别她的脸 - 直到她穿上白色面具。</p><p> In 2018, when I told Google’s public relations staff that I was working on a book about artificial intelligence, it arranged a long talk with Dr. Mitchell to discuss her work. As she described how she built the company’s Ethical A.I. team — and brought Dr. Gebru into the fold — it was refreshing to hear from someone so closely focused on the bias problem.</p><p> 2018年，当我告诉谷歌的公共关系人员时，我正在努力一本关于人工智能的书，它安排了米切尔博士讨论她的工作。正如她描述了她如何建造公司的道德A.I.团队 - 并将Gebru博士进入折叠 - 从那些密切关注偏见问题的人听到的人令人耳目一新。</p><p> But nearly three years later, Dr. Gebru was pushed out of the company without a clear explanation. She  said she had been fired after criticizing Google’s approach to minority hiring and, with a  research paper, highlighting the harmful biases  in the A.I. systems that underpin Google’s search engine and other services.</p><p> 但近三年后，Gebru博士没有明确解释就被推出了公司。她说她在批评谷歌对少数民族招聘方面的方法后被解雇了，并在研究论文中突出了A.I的有害偏见。支持谷歌搜索引擎和其他服务的系统。</p><p> “Your life starts getting worse when you start advocating for underrepresented people,” Dr. Gebru said in an email before her firing. “You start making the other leaders upset.”</p><p> “当你开始倡导尊重的人时，你的生活开始变得更糟，”Gebru博士在她发射前在一封电子邮件中说。 “你开始让其他领导者心烦意乱。” </p><p> As Dr. Mitchell defended Dr. Gebru, the company removed her, too. She had searched through her own Google email account for material that would support their position and forwarded emails to another account, which somehow got her into trouble. Google declined to comment for this article.</p><p>正如米切尔博士捍卫Gebru博士，该公司也删除了她。她已经搜索了自己的谷歌电子邮件帐户，以获得支持他们的职位和转发电子邮件的材料，以某种方式让她陷入困境。谷歌拒绝评论这篇文章。</p><p> Their departure became a point of contention for A.I. researchers and other tech workers. Some saw a giant company no longer willing to listen, too eager to get technology out the door without considering its implications. I saw an old problem — part technological and part sociological — finally breaking into the open.</p><p> 他们的出发成为A.I的争论点。研究人员和其他技术人员。有些人看到一家巨大的公司不再愿意倾听，太渴望得到了技术，而不考虑其含义。我看到了一个旧的问题 - 部分技术和部分社会学 - 终于进入了开放。</p><p>   In June 2015, a friend sent Jacky Alciné, a 22-year-old software engineer living in Brooklyn, an internet link for snapshots the friend had posted to the new Google Photos service. Google Photos could analyze snapshots and automatically sort them into digital folders based on what was pictured. One folder might be “dogs,” another “birthday party.”</p><p>   2015年6月，一位朋友发给JackyAlciné，这是一个居住在布鲁克林的22岁的软件工程师，这是一个互联网链接，用于朋友发布到新的Google照片服务。 Google照片可以分析快照，并根据图片的内容自动对数字文件夹进行排序。一个文件夹可能是“狗，”另一个“生日聚会”。</p><p> When Mr. Alciné clicked on the link, he noticed one of the folders was labeled “gorillas.” That made no sense to him, so he opened the folder. He found more than 80 photos he had taken nearly a year earlier of a friend during a concert in nearby Prospect Park. That friend was Black.</p><p> 当Alciné先生点击了该链接时，他注意到其中一份文件夹被标记为“大猩猩”。对他没有意义，所以他打开了这个文件夹。在附近的展望公园的音乐会期间，他发现了超过80张朋友近一年的朋友。那个朋友很黑。</p><p> He might have let it go if Google had mistakenly tagged just one photo. But 80? He posted a screenshot on Twitter. “Google Photos, y’all,” messed up, he wrote, using much saltier language. “My friend is not a gorilla.”</p><p> 如果谷歌错误地标记了一张照片，他可能会放手。但是80？他在Twitter上发布了一个屏幕截图。 “谷歌照片，y'all，”他搞砸了，他写道，使用太多的口语。 “我的朋友不是大猩猩。”</p><p> Like facial recognition services,  talking digital assistants and  conversational “chatbots,” Google Photos relied on an A.I. system that learned its skills by analyzing enormous amounts of digital data.</p><p> 喜欢面部识别服务，谈论数字助理和会话“聊天”，谷歌照片依靠A.I.通过分析大量数字数据来学习其技能的系统。</p><p> Called a “ neural network,” this mathematical system could learn tasks that engineers could never code into a machine on their own. By analyzing thousands of photos of gorillas, it could learn to recognize a gorilla. It was also capable of egregious mistakes. The onus was on engineers to choose the right data when training these mathematical systems. (In this case, the easiest fix was to eliminate “gorilla” as a photo category.)</p><p> 这个数学系统可以学习工程师永远无法自行编码的任务，从而呼叫“神经网络”。通过分析大猩猩的数千张照片，可以学会识别大猩猩。它也有能力的错误。 ONU在培训这些数学系统时选择合适的数据。 （在这种情况下，最简单的修复是消除“大猩猩”作为照片类别。） </p><p> As a software engineer, Mr. Alciné understood the problem. He compared it to making lasagna. “If you mess up the lasagna ingredients early, the whole thing is ruined,” he said. “It is the same thing with A.I. You have to be very intentional about what you put into it. Otherwise, it is very difficult to undo.”</p><p>作为软件工程师，Alciné先生了解了这个问题。他比较了让烤宽面条。 “如果你早期搞砸了烤宽面条成分，整件事都被毁了，”他说。 “它与a是一样的。你必须非常有意地了解你的东西。否则，很难撤消。“</p><p> In 2017, Deborah Raji, a 21-​year-​old Black woman from Ottawa, sat at a desk inside the New York offices of Clarifai, the start-up where she was working. The company built technology that could automatically recognize objects in digital images and planned to sell it to businesses, police departments and government agencies.</p><p> 2017年，来自渥太华的21岁的黑人女子，德国·拉吉坐在纽约克拉迪瓦纽约办公室里的一张桌子上，这是她正在工作的初创公司。该公司建立了可以自动识别数字图像中对象的技术，并计划将其销售给企业，警察部门和政府机构。</p><p> She stared at a screen filled with faces — images the company used to train its facial recognition software.</p><p> 她盯着充满了面孔的屏幕 - 用来训练其面部识别软件的公司。</p><p> As she scrolled through page after page of these faces, she realized that most — more than 80 percent — were of white people. More than 70 percent of those white people were male. When Clarifai trained its system on this data, it might do a decent job of recognizing white people, Ms. Raji thought, but it would fail miserably with people of color, and probably women, too.</p><p> 当她在这些面孔的页面后滚动页面时，她意识到最多 - 超过80％ - 是白人。超过70％的白人是男性。当Clarifai培训了它的系统上的系统时，它可能会做出识别白人的体面，raji女士思想，但它与颜色人物悲惨地失败，也可能是女性。</p><p>  Clarifai was also building a “content moderation system,” a tool that could automatically identify and remove pornography from images people posted to social networks. The company trained this system on two sets of data: thousands of photos pulled from online pornography sites, and thousands of G‑rated images bought from stock photo services.</p><p>  Clarifai还建立了一个“内容审核系统”，一个可以自动识别和删除从发布到社交网络的图像中的色情内容的工具。该公司在两套数据上培训了这个系统：来自在线色情网站的数千张照片，以及从库存照片服务购买的数千个G级图像。</p><p> The system was supposed to learn the difference between the pornographic and the anodyne. The problem was that the G‑rated images were dominated by white people, and the pornography was not. The system was learning to identify Black people as pornographic.</p><p> 该系统应该学习色情和肛门之间的差异。问题是，G级图像由白人主导，色情内容不是。该系统正在学习将黑人识别为色情。</p><p> “The data we use to train these systems matters,” Ms. Raji said. “We can’t just blindly pick our sources.”</p><p> “我们用来训练这些系统的数据很重要，”Raji女士说。 “我们不能只是盲目地选择我们的来源。” </p><p> This was obvious to her, but to the rest of the company it was not. Because the people choosing the training data were mostly white men, they didn’t realize their data was biased.</p><p>这对她来说很明显，但到了公司的其余部分并非如此。因为选择培训数据的人大多是白人，所以他们没有意识到他们的数据被偏见。</p><p> “The issue of bias in facial recognition technologies is an evolving and important topic,” Clarifai’s chief executive, Matt Zeiler, said in a statement. Measuring bias, he said, “is an important step.”</p><p> “面部识别技术的偏见问题是一个不断发展和重要的话题，”克拉迪瓦的首席执行官Matt Zeiler在一份声明中表示。他说，衡量偏见，“是一个重要的一步。</p><p>  Before joining Google, Dr. Gebru collaborated on a study with a young computer scientist, Joy Buolamwini. A graduate student at the Massachusetts Institute of Technology, Ms. Buolamwini, who is Black, came from a family of academics. Her grandfather specialized in medicinal chemistry, and so did her father.</p><p>  在加入谷歌之前，Gebru博士在与年轻电脑科学家的一项研究中合作，欢乐Buolamwini。 Massachusetts理工学院的研究生Buolamwini女士来自黑人，来自一家学者。她的祖父专业从事药用化学，她的父亲也是如此。</p><p> She gravitated toward facial recognition technology. Other researchers believed it was reaching maturity, but when she used it, she knew it wasn’t.</p><p> 她渴望面部识别技术。其他研究人员认为它正在达到成熟，但当她用它时，她知道这不是。</p><p> In October 2016, a friend invited her for a night out in Boston with several other women. “We’ll do masks,” the friend said. Her friend meant skin care masks at a spa, but Ms. Buolamwini assumed Halloween masks. So she carried a white plastic Halloween mask to her office that morning.</p><p> 2016年10月，一位朋友邀请她在波士顿的夜晚与其他几个女性。 “我们会做面具，”朋友说。她的朋友在水疗中心意味着皮肤护理面具，但Buolamwini女士假设万圣节面具。所以她早上带着一件白色的塑料万圣节面具给她的办公室。</p><p> It was still sitting on her desk a few days later as she struggled to finish a project for one of her classes. She was trying to get a detection system to track her face. No matter what she did, she couldn’t quite get it to work.</p><p> 几天后，她仍然坐在她的桌子上，因为她努力为她的一个班级完成一个项目。她试图获得一个检测系统来跟踪她的脸。无论她做了什么，她都无法努力工作。</p><p> In her frustration, she picked up the white mask from her desk and pulled it over her head. Before it was all the way on, the system recognized her face — or, at least, it recognized the mask.</p><p> 在她的沮丧中，她从桌子上拿起白色面具，把它拉过她的头部。在这一路一路之前，系统识别她的面部 - 或者至少，它识别出掩码。 </p><p> “Black Skin, White Masks,” she said in an interview, nodding to the 1952 critique of historical racism from the psychiatrist Frantz Fanon. “The metaphor becomes the truth. You have to fit a norm, and that norm is not you.”</p><p>“黑色皮肤，白面具，”她在接受采访时表示，向1952年的历史种族主义点头，从精神科医生Frantz Fanon。 “隐喻成为真相。你必须适应常态，那个规范不是你。“</p><p> Ms. Buolamwini started exploring commercial services designed to analyze faces and identify characteristics like age and sex, including tools from Microsoft and IBM.</p><p> Buolamwini女士开始探索旨在分析面孔的商业服务，并确定年龄和性等特征，包括来自微软和IBM的工具。</p><p> She found that when the services read photos of lighter-​skinned men, they misidentified sex about 1 percent of the time. But the darker the skin in the photo, the larger the error rate. It rose particularly high with images of women with dark skin. Microsoft’s error rate was about 21 percent. IBM’s was 35.</p><p> 她发现，当服务读取较轻皮肤的男性照片时，他们的性别误解了1％的时间。但是照片中的皮肤越深，错误率越大。它特别高，用黑皮肤的妇女图像升高。微软的错误率约为21％。 IBM是35岁。</p><p> Published in the winter of 2018,  the study drove a backlash against facial recognition technology and, particularly, its use in law enforcement. Microsoft’s chief legal officer said the company had turned down sales to law enforcement when there was concern the technology could unreasonably infringe on people’s rights, and he made a public call for government regulation.</p><p> 该研究在2018年冬季发表了反对面部识别技术的反对，特别是在执法方面使用。微软的首席法律官表示，当有担心这项技术有不合理地侵犯人民的权利时，该公司已经向执法人员拒绝了销售人员，他召开了政府监管的公开电话。</p><p> Twelve months later, Microsoft backed a bill in Washington State that would require notices to be posted in public places using facial recognition and ensure that government agencies obtained a court order when looking for specific people. The bill passed, and it takes effect later this year. The company, which did not respond to a request for comment for this article, did not back other legislation that would have provided stronger protections.</p><p> 十二个月后，微软在华盛顿州支持了一项法案，要求通知在使用面部认可的公共场所发布，并确保在寻找具体人员时获得法院命令。截至今年晚些时候，账单通过了。该公司没有回复对本文评论的请求，并没有退回其他立法，该立法将提供更强有力的保护。</p><p> Ms. Buolamwini began to collaborate with Ms. Raji, who moved to M.I.T. They started testing facial recognition technology from a third American tech giant: Amazon. The company had started to market its technology to police departments and government agencies under the name Amazon Rekognition.</p><p> Buolamwini女士开始与Raji女士合作，搬到M.I.T.他们开始从第三届美国科技巨头测试面部识别技术：亚马逊。该公司在亚马逊重新识别的名称下开始向警方部门和政府机构推出技术。</p><p> Ms. Buolamwini and Ms. Raji published a study showing that an Amazon face service also had trouble identifying the sex of female and darker-​skinned faces. According to the study, the service mistook women for men 19 percent of the time and misidentified darker-​skinned women for men 31 percent of the time. For lighter-​skinned males, the error rate was zero.</p><p> Buolamwini女士和Raji女士发表了一项研究表明，亚马逊面部服务也难以识别女性和较暗的面孔的性别。根据该研究的说法，将误认为男性的服务误认为是31％的人为男性的时间19％和较暗皮肤的女性。对于较轻的雄性，错误率为零。 </p><p> Amazon called for government regulation of facial recognition. It also attacked the researchers in private emails and public blog posts.</p><p>亚马逊呼吁政府监管面部识别。它还攻击了私人电子邮件和公共博客帖子的研究人员。</p><p> “The answer to anxieties over new technology is not to run ‘tests’ inconsistent with how the service is designed to be used, and to amplify the test’s false and misleading conclusions through the news media,” an Amazon executive, Matt Wood, wrote in a blog post that disputed the study and a  New York Times article that described it.</p><p> “对新技术令人焦虑的答案不是与服务如何使用的”测试“不一致，并通过新闻媒体放大测试的虚假和误导性结论，”亚马逊行政，亚光木，写在一个博客帖子，争论研究和纽约时报文章描述了它。</p><p> In  an open letter, Dr. Mitchell and Dr. Gebru rejected Amazon’s argument and called on it to stop selling to law enforcement. The letter was signed by 25 artificial intelligence researchers from Google, Microsoft and academia.</p><p> 在一个公开的信中，米切尔博士和Gebru博士拒绝了亚马逊的论点，并要求停止销售执法。这封信由谷歌，微软和学术界的25名人工智能研究人员签字。</p><p> Last June, Amazon backed down. It  announced that it would not let the police use its technology for at least a year, saying it wanted to give Congress time to create rules for the ethical use of the technology. Congress has yet to take up the issue. Amazon declined to comment for this article.</p><p> 去年6月，亚马逊退缩了。它宣布不会让警察使用其技术至少一年，称其希望提供国会时间来创造伦理使用规则。国会尚未接受该问题。亚马逊拒绝评论这篇文章。</p><p>  Dr. Gebru and Dr. Mitchell had less success fighting for change inside their own company. Corporate gatekeepers at Google were heading them off with a new review system that had lawyers and even communications staff vetting research papers.</p><p>  Gebru博士和米切尔博士在其本公司内部的变革较少成功。谷歌的企业门卫将他们与律师甚至通信人员审查论文的新审查制度一起离开。</p><p> Dr. Gebru’s dismissal in December stemmed, she said, from the company’s treatment of a research paper she wrote alongside six other researchers, including Dr. Mitchell and three others at Google. The paper discussed ways that a new type of language technology, including a system built by Google that underpins its search engine, can show bias against women and people of color.</p><p> 她说，Gebru博士的解雇源于该公司的待遇，她与六名其他研究人员一起写作，包括Mitchell博士和谷歌的三个。本文讨论了一种新型语言技术的方法，包括由谷歌建造的系统建造的系统，可以向妇女和颜色人民表达偏见。</p><p> After she submitted the paper to an academic conference, Dr. Gebru said, a Google manager demanded that she either retract the paper or remove the names of Google employees. She said she would resign if the company could not tell her why it wanted her to retract the paper and answer other concerns.</p><p> 德布鲁博士表示，在向学术会议提交到学术会议之后，谷歌经理要求她撤回纸纸或删除谷歌员工的姓名。她说，如果公司无法讲述为什么它希望她撤回纸张并回答其他问题，她会辞职。 </p><p> The response: Her resignation was accepted immediately, and Google revoked her access to company email and other services. A month later, it removed Dr. Mitchell’s access after she searched through her own email in an effort to defend Dr. Gebru.</p><p>回应：她立即接受了辞职，谷歌撤销了她对公司电子邮件和其他服务的访问。一个月后，它在她通过自己的电子邮件中搜索了米切尔博士的访问，以努力捍卫Gebru博士。</p><p> In a Google staff meeting last month, just after the company fired Dr. Mitchell, the head of the Google A.I. lab, Jeff Dean, said the company would create strict rules meant to limit its review of sensitive research papers. He also defended the reviews. He declined to discuss the details of Dr. Mitchell’s dismissal but said she had violated the company’s code of conduct and security policies.</p><p> 在上个月的谷歌员工会议上，刚刚在谷歌A.I的负责人发射Mitchell博士之后。 Lab jeff Dean表示，该公司将创造严格的规则，以限制对敏感性研究论文的审查。他还捍卫了评论。他拒绝讨论米切尔解雇博士的细节，但表示她违反了公司的行为准则和安全政策。</p><p> One of Mr. Dean’s new lieutenants, Zoubin Ghahramani, said the company must be willing to tackle hard issues. There are “uncomfortable things that responsible A.I. will inevitably bring up,” he said. “We need to be comfortable with that discomfort.”</p><p> 院长曾先生的新副职位，邹斌，Ghahramani表示，该公司必须愿意解决难题。负责a.i有“不舒服的事情。将不可避免地提起，“他说。 “我们需要对这种不适感到舒服。”</p><p> But it will be difficult for Google to regain trust — both inside the company and out.</p><p> 但谷歌很难重新获得信任 - 在公司内部和外面。</p><p> “They think they can get away with firing these people and it will not hurt them in the end, but they are absolutely shooting themselves in the foot,” said Alex Hanna, a longtime part of Google’s 10-member Ethical A.I. team. “What they have done is incredibly myopic.”</p><p> “他们认为他们可以逃避这些人，最后不会伤害他们，但他们绝对在脚下射击自己，”谷歌的10人道德A.I的长期部分是一个长期的一部分。团队。 “他们所做的是令人难以置信的近视。”</p><p> Cade Metz is a technology correspondent at The Times and the author of “ Genius Makers: The Mavericks Who Brought A.I. to Google, Facebook, and the World,” from which this article is adapted.</p><p> Cade Metz是“天才制造商的时代和作者：带来的小牛克利克斯的技术记者。到谷歌，Facebook和世界“，”这篇文章自适应。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.nytimes.com/2021/03/15/technology/artificial-intelligence-google-bias.html">https://www.nytimes.com/2021/03/15/technology/artificial-intelligence-google-bias.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/确保/">#确保</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/aren/">#aren</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/谷歌/">#谷歌</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012 - 2021 diglog.com </div></div></body></html>