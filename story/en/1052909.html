<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>找到机器学习的标准数据集格式（2020） Finding a standard dataset format for machine learning (2020)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Finding a standard dataset format for machine learning (2020)<br/>找到机器学习的标准数据集格式（2020） </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-17 10:48:08</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/3/f169e3a7bd77e6ac93a421576cc2cde1.jpg"><img src="http://img2.diglog.com/img/2021/3/f169e3a7bd77e6ac93a421576cc2cde1.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Machine learning data is commonly shared in whatever form it comes in (e.g. images, logs, tables) without being able to make strict assumptions on what it contains or how it is formatted. This makes machine learning hard because you need to spend a lot of time figuring out how to parse and deal with it. Some datasets are accompanied with loading scripts, which are language-specific and may break, and some come with their own server to query the dataset. These do help, but are often not available, and still require us to handle every dataset individually.</p><p>机器学习数据通常以其进入的任何形式共享（例如，图像，日志，表），而不能够对其包含的错误进行严格的假设，或者它是如何格式化的。这使得机器学习困难，因为您需要花费很多时间来解析如何解析和处理它。有些数据集伴随着加载脚本，它是特定于语言的，可能会破坏，有些则带有自己的服务器来查询数据集。这些确实有助于，但通常不可用，并且仍然需要我们单独处理每个数据集。</p><p> With OpenML, we aim to take a stress-free, &#39;zen&#39;-like approach to working with machine learning datasets. To make training data easy to use, OpenML serves thousands of datasets in the same format, with the same rich meta-data, so that you can directly load it (e.g. in numpy,pandas,…) and start building models without manual intervention. For instance, you can benchmark algorithms across hundreds of datasets in a simple loop.</p><p> 使用OpenML，我们的目标是采取无压力，＆＃39; ZEN＆＃39;  - 与机器学习数据集一起使用的方法。为了使培训数据易于使用，OpenML以相同的格式提供数千个数据集，具有相同的富含元数据，使您可以直接加载它（例如Numpy，Pandas，......）并开始构建模型而无需手动干预。例如，您可以在简单的循环中跨数百个数据集进行基准测试算法。</p><p> For historical reasons, we have done this by internally storing all data in the  ARFF data format, a CSV-like text-based format that includes meta-data such as the correct feature data types. However, this format is loosely defined, causing different parsers to behave differently, and the current parsers are memory-inefficient which inhibits the use of large datasets. A more popular format these days is  Parquet, a binary single-table format. However, many current machine learning tasks require multi-table data. For instance, image segmentation or object detection tasks have both images and varying amounts of annotations per image.</p><p> 出于历史原因，我们通过内部存储了ARFF数据格式的所有数据，包括基于CSV的文本的格式，该格式包括诸如正确的特征数据类型的元数据。然而，这种格式是松散定义的，导致不同的解析器以不同方式行事，并且当前解析器是抑制使用大型数据集的存储器效率的存储器效率。这些天是一种更流行的格式是镶木地板，是二进制单表格格式。但是，许多当前机器学习任务需要多表数据。例如，图像分割或对象检测任务具有图像和每个图像的变化量的涂布量。</p><p> In short, we are looking the best format to  internally store machine learning datasets in the foreseeable future, to extend OpenML towards all kinds of modern machine learning datasets and serve them in a uniform way. This blog post presents out process and insights. We would love to hear your thoughts and experiences before we make any decision on how to move forward.</p><p> 简而言之，我们正在预见的未来在内部存储机器学习数据集的最佳格式，将OpenML扩展到各种现代机器学习数据集，并以统一的方式为它们提供服务。此博客文章呈现出过程和见解。在我们对如何向前迈进的决定之前，我们很乐意听到您的想法和经验。</p><p>   It should be  useful for data storage and transmission. We can always convert data during upload or download in OpenML&#39;s client APIs. For instance, people may upload a Python pandas dataframe to OpenML, and later get the same dataframe back, without realizing or caring how the data was stored in the meantime. If people want to store the data locally, they can download it in the format they like (e.g. a memory-mapped format like Arrow/Feather for fast reading or TFRecords for people using TensorFlow). Additional code can facilitate such conversions.</p><p>   它对于数据存储和传输应该是有用的。我们可以随时在上传期间转换数据或在OpenML＆＃39;客户端下载。例如，人们可以将Python Pandas DataFrame上传到OpenML，稍后获得相同的DataFrame返回，而不实现或关心数据的应用程序。如果人们想要在本地存储数据，他们可以以它们所喜欢的格式下载（例如，像箭头/羽毛一样的内存映射格式，用于使用Tensorflow的人员的快速阅读或TFRecords）。附加代码可以促进此类转换。</p><p>  There should be a  standard way to represent specific types of data, i.e. a fixed schema that can be verified. For instance, all tabular data should be stored in a uniform way. Without it, we would need dataset-specific code for loading, which requires maintenance, and it will be harder to check quality and extract meta-data.</p><p>  应该有一个标准的方法来表示特定类型的数据，即可以验证的固定模式。例如，所有表格数据应以统一的方式存储。没有它，我们需要特定于数据集的加载代码，需要维护，并且需要检查质量并提取元数据。</p><p>  The format should  allow storing most (processed) machine learning datasets, including images, video, audio, text, graphs, and multi-tabular data such as object recognition tasks and relational data. Data such as images can be converted to numeric formats (e.g. pixel values) for storage in this format (and usage in machine learning).</p><p>  该格式应允许存储大多数（已处理的）机器学习数据集，包括图像，视频，音频，文本，图形和多表格数据，例如对象识别任务和关系数据。诸如图像的数据可以转换为以这种格式存储的数字格式（例如，像素值）（以及机器学习中的使用）。 </p><p>  Since OpenML is a community project, we want to keep it as easy as possible to use and maintain:</p><p>由于OpenML是一个社区项目，我们希望保持尽可能简单地使用和维护：</p><p> We need  machine-readable schemas (in a specific language) that describe how a certain type of data is formatted. Examples would be a schema for tabular data, a schema for annotated image data, etc. Every dataset should specify the schema it satisfies, and we should be able to  validate this. We aim to gradually roll out support form different types of data, starting with tabular, and including others only after schemas are defined.</p><p> 我们需要机器可读模式（以特定语言）描述如何格式化某种类型的数据。示例将是表格数据的模式，用于注释图像数据的模式等。每个数据集应指定它满足的模式，我们应该能够验证这一点。我们的目标是逐步推出支持表格不同类型的数据，从表格开始，并且仅在定义模式后仅包括其他类型。</p><p>  We need to support batch data now, but ideally the format should allow data appending (streaming) in the future.</p><p>  我们现在需要支持批处理数据，但理想情况下，格式应该允许将来附加数据（流）。</p><p> When no agreed upon schema exists, we could offer a forum for the community to discuss and agree on a standard schema, in collaboration with other initiatives (e.g.  frictionlessdata). For instance, new schemas could be created in a github repo to allow people to do create pull requests. They could be effectively used once they are merged.</p><p> 当不同意架构时，我们可以在与其他举措合作（例如FrictionLesddata）的合作中向社区提供讨论和同意标准架构的论坛。例如，可以在GitHub Repo中创建新模式，以允许人们进行拉拔请求。一旦合并，它们就可以有效地使用。</p><p>   If possible, support for multiple ‘resources’ (e.g. collections of files or multiple relational tables).</p><p>   如果可能，支持多个“资源”（例如，文件或多个关系表的集合）。</p><p>       Not stable enough yet and not ideal for long-term storage. The authors also discourage it for long-term storage.</p><p>       尚未足够稳定，不适合长期存储。作者也向长期存储劝阻。</p><p>  Limited to one data structure per file, but that data structure can be complex (e.g. dict).</p><p>  限于每个文件的一个数据结构，但数据结构可以复杂（例如，dict）。 </p><p>   Has parsers in different languages, but not all Parquet features are supported in every library (see below).</p><p>具有不同语言的解析器，但每个库中都不支持所有地板功能（见下文）。</p><p>  The Python libraries ( Arrow,  fastparquet)  do not support partial read/writes. The Java/Go implementations do. Splitting up parquet files into many small files can be cumbersome.</p><p>  Python库（箭头，FastParquet）不支持部分读/写入。 Java / Go实现执行。将镶木地板文件分成许多小文件可能是麻烦的。</p><p>  No version control, no meta-data storage, no schema enforcement. There are layers on top (e.g. delta lake) that do support this. Simple file versioning can also be done with S3.</p><p>  没有版本控制，没有元数据存储，无模式强制执行。顶部（例如Delta Lake）的层数都支持这一点。简单的文件版本控制也可以使用S3完成。</p><p>  The different parsers (e.g.  Parquet support inside Arrow,  fastparquet) implement different parts of the Parquet format and different set of compression algorithms. Hence, parquet files  may not be compatible between parsers (see  here and  here.</p><p>  不同的解析器（例如箭头内部镶木地板，FastParquet）实施镶木地板格式的不同部分和不同的压缩算法。因此，Parquet文件可能不在解析器之间兼容（请参阅此处和此处。</p><p>  Support  limited to single-table storage. For instance, there doesn’t seem to be an apparent way to store an object detection dataset (with images and annotations) as a single parquet file.</p><p>  支持仅限于单表存储。例如，似乎没有作为单个条形文件存储对象检测数据集（用图像和注释）的明显方法。</p><p>   Very flexible access to parts of the data. SQL queries can be used to select any subset of the data.</p><p>   非常灵活地访问数据部分。 SQL查询可用于选择数据的任何子集。</p><p>  It supports  only 2000 columns, and we have quite a few datasets with more than 2000 features. Hence, storing large tabular data will require mapping data differently, which would add a lot of additional complexity.</p><p>  它仅支持2000列，我们拥有超过2000多个功能的数据集。因此，存储大型表格数据将需要不同的映射数据，这将增加大量的额外复杂性。 </p><p>   Very good support in all languages. Has well-tested parsers, all using the same C implementation.</p><p>对所有语言的支持非常好。具有良好的解析器，所有这些都使用相同的C实现。</p><p>            Self-descriptive: the structure of the data can be easily read programmatically. For instance, ‘dump -H -A 0 mydata.hdf5’ will give you a lot of detail on the structure of the dataset.</p><p>            自我描述性：可以以编程方式轻松读取数据的结构。例如，'dump -h -a 0 mydata.hdf5'将为您提供大量详细信息数据集的结构。</p><p>  Complexity. We  cannot make any a priori assumptions about how the data is structured. We need to define schema and implement code that automatically validates that a dataset follows a specific schema (e.g. using h5dump to see whether it holds a single dataframe that we could load into pandas). We are unaware of any initiatives to define such schema.</p><p>  复杂。我们不能提出关于数据如何构造的先验假设。我们需要定义模式和实现自动验证数据集遵循特定架构的代码（例如，使用H5dump查看它是否包含我们可以加载到Pandas的单个数据帧）。我们不知道任何措施来定义这种模式。</p><p>  The format has a  very long and detailed specification. While parsers exist we don’t really know whether they are fully compatible with each other.</p><p>  格式具有很长且详细的规范。虽然存在解析器，但我们真的不知道它们是否完全彼此兼容。</p><p>   Text-based, so easy versioning with git LFS. Changes in different versions can be observed with a simple git diff.</p><p>   基于文本的，因此使用Git LFS简单的版本控制。通过简单的Git Diff，可以观察到不同版本的变化。</p><p>  Many different dialects exist. We need to decide on a standardized dialect and enforce that only that dialect is used on OpenML ( https://frictionlessdata.io/specs/csv-dialect/). The dialect specified in  RFC4180, which uses the comma as delimiter and the quotation mark as quote character, is often recommended.</p><p>  存在许多不同的方言。我们需要决定标准化的方言，并强制执行该方言仅在OpenML上使用（https://frictionlessdata.io/specs/csv-dialect/）。 RFC4180中指定的方言通常建议使用逗号作为分隔符和引号作为报价字符。</p><p>    There exist some prior benchmarks ( here and  here) on storing dataframes. These only consider single-table datasets. For reading/writing, CSV is clearly slower and Parquet is clearly faster. For storage, Parquet is most efficient but zipped CSV as well. HDF requires a lot more disk space. We also ran our own  benchmark to compare the writing performance of those data formats for very large and complex machine learning datasets, but could not find a way to store these in one file in Parquet.</p><p>    在存储DataFrame上存在一些先前的基准（这里和此）。这些只考虑单表数据集。为了阅读/写作，CSV显然较慢，木质镶木地板显然更快。对于储存，镶木地板最有效但却是压缩的CSV。 HDF需要更多的磁盘空间。我们还耗尽了自己的基准测试，可以比较那些数据格式的写作性能为非常大而复杂的机器学习数据集，但无法找到在地板中的一个文件中存储这些方法。 </p><p>  Version control for large datasets is tricky. For text-based formats (CSV), we could use  git LFS store the datasets and have automated versioning of datasets. We found it quite easy to export all current OpenML dataset to GitLab:  https://gitlab.com/data/d/openml.</p><p>大型数据集的版本控制很棘手。对于基于文本的格式（CSV），我们可以使用Git LFS存储数据集并具有数据集的自动版本控制。我们发现它很容易将所有当前OpenML数据集导出到Gitlab：https://gitlab.com/data/d/openml。</p><p> The binary formats do not allow us to track changes in the data, only to recover the exact versions of the datasets you want (and their metadata). Potentially, extra tools could still be used to export the data to dataframes or text and then compare them. Delta Lake has version history support, but seemingly only for Spark operations done on the datasets.</p><p> 二进制格式不允许我们跟踪数据的更改，仅恢复所需数据集的确切版本（及其元数据）。可能，额外的工具仍可用于将数据导出到DataFrames或文本，然后比较它们。 Delta Lake具有版本历史支持，但似乎仅用于在数据集上完成的Spark操作。</p><p> We need your help!If we have missed any format we should investigate, or misunderstood those we have investigated, or missed some best practice, please tell us.You are welcome to comment below, or send us an email at openmlhq@googlegroups.com</p><p> 我们需要你的帮助！如果我们错过了我们应该调查的任何格式，或者误解了我们调查的那些，或错过了一些最佳实践，请告诉我们。您欢迎您在下面发表评论，或者在OpenMLHQ@GoogleGroups.com发送电子邮件</p><p> Contributors to this blog post:Mitar Milutinovic, Prabhant Singh, Joaquin Vanschoren, Pieter Gijsbers, Andreas Mueller, Matthias Feurer, Jan van Rijn, Marcus Weimer, Marcel Wever, Gertjan van den Burg, Nick Poorman</p><p> 撰稿人帖子：Mitar Milutinovic，Prabhant Singh，Joaquin Vanschoren，Pieter Gijsbers，Andreas Mueller，Matthias Feurer，Jan Van Rijn，Marcus Weimer，Marcus Wever，Gertjan Van Den Burg，Nick Poorman </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://openml.github.io/blog/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html">https://openml.github.io/blog/openml/data/2020/03/23/Finding-a-standard-dataset-format-for-machine-learning.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/2020/">#2020</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/机器/">#机器</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/standard/">#standard</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012 - 2021 diglog.com </div></div></body></html>