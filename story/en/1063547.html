<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>查询您的数据库无法回答 The Query Your Database Can’t Answer</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">The Query Your Database Can’t Answer<br/>查询您的数据库无法回答 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-05 00:59:00</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/5957214543e61b2009e2522cc6045172.png"><img src="http://img2.diglog.com/img/2021/6/5957214543e61b2009e2522cc6045172.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>What if I told you there is a query your database can’t answer? That would probably surprise you. With decades of effort behind them, databases are one of the most robust pieces of software in existence. Because they have remained enormously popular in their current form, many engineers assume that their database can answer any query they have if they’re willing to wait long enough.</p><p>如果我告诉你有一个查询你的数据库无法回答怎么办？这可能会让你感到惊讶。随着数十年的努力，数据库是存在最强大的软件之一。因为他们在目前的形式中保持极大地流行，所以许多工程师假设他们的数据库可以回答他们拥有的任何查询，如果他们愿意等待足够长。</p><p> This, however, isn’t a great assumption. Why? Nearly all databases are linked by one common trait: They exclusively query data at rest. When you issue a query, the database executes it by scanning the data and returning the result. The problem is that this architecture inherently puts some queries out of reach. Namely, you can’t write a query whose result updates every time the underlying data changes. You’re blind to everything that happens in between two query invocations. And that is important if  what you are doing is driven by software.</p><p> 然而，这不是一个伟大的假设。为什么？几乎所有数据库都是由一个常见的特征链接的：它们完全查询休息时的数据。发出查询时，数据库通过扫描数据并返回结果来执行它。问题是，这种架构本身就是遥不可及的疑问。即，您无法编写一个查询，其结果更新每次底层数据更改。您对两个查询调用之间发生的所有内容都盲目。如果您正在做的是由软件驱动的那么重要。</p><p>    If you had a database that could update query results every time the data changed, you might say it’s reactive. Although reactivity is a missing piece in many databases, it’s the heart of event streams and stream processing—as new events are received, applications immediately react by processing data to take some action.</p><p>    如果您有一个数据库，可以每次数据更改时都可以更新查询结果，您可能会说它是有动的。虽然反应性是许多数据库中缺失的作品，但它是事件流和流处理的核心 - 随着收到新事件，应用程序立即通过处理数据来采取一些动作进行反应。</p><p> Stream processing’s reactivity nearly makes it an ideal fit to serve queries for applications. Although this can work, it has a serious weakness—it detonates the tidy abstraction of the database. Databases—in particular, SQL—hide all the underlying complexity of query execution, indexing, concurrency control, crash recovery, replication, and so on. When you make a stream processor responsible for serving queries, you take components that used to be neatly hidden by your database and graft them onto your application. You’re now on the hook to make a lot more things work correctly.</p><p> 流处理的反应性几乎使其成为为应用程序查询提供理想的拟合。虽然这可以工作，但它有一个严重的弱点 - 它引爆了数据库的整洁抽象。数据库 - 特别是SQL-WATE查询执行，索引，并发控制，崩溃恢复，复制等的所有基础复杂性。当您制作负责服务查询的流处理器时，您可以使用数据库整齐地隐藏的组件并将其移植到您的应用程序上。你现在正在勾手上，让更多的事情正常工作。</p><p> I think the better path is to recognize a hidden relationship: Stream processing is the other half of databases. Databases manage data at rest; stream processing manages data in motion. They just don’t yet fit together.</p><p> 我认为更好的路径是识别隐藏的关系：流处理是另一半数据库。数据库在休息时管理数据;流处理管理运动中的数据。他们只是不合适。</p><p>   Confluent has been working on a new kind of database named  ksqlDB that tries to do exactly that. It helps you both query your data and react to changes. But instead of managing all of the complexity yourself, you interact with it solely through the abstraction of a relational database—like Postgres. For its transaction log, it relies exclusively on Apache Kafka ®.</p><p>   Confluent一直在研究一个名为KSQLDB的新类型数据库，它试图完成这一点。它可以帮助您对数据进行查询并对变化作出反应。但是，您可以完全通过类似于关系数据库的Postgres的抽象来管理所有的复杂性。对于其事务日志，它仅依赖于ApacheKafka®。</p><p>  Let me be up front: Although ksqlDB has a strong feature set today, completing its larger vision of marrying databases and stream processing remains an ambitious work in progress. That progress—the subject of this post—relies on a lot of deep work happening in the Kafka ecosystem.</p><p>  让我走了前线：虽然ksqldb今天有一个强大的功能，但完成了它的结婚数据库的更大愿景，并且流处理仍然是一个雄心勃勃的工作。这进步 - 这次职位的主题依赖于Kafka生态系统的大量深入工作。 </p><p> So let’s look at that deep work in each layer of the stack—from data capture, to storage, to processing, to query execution—to get an idea of what it will take to answer the unanswerable query.</p><p>因此，让我们看看堆栈从数据捕获的每层深度工作，以存储到处理，查询执行 - 以了解回答不可批售查询所需的内容。</p><p>   Whenever you imagine building an event-driven system, it’s easy to make one crucial, yet incorrect, assumption about how it will work: that the events you want will already be captured in a stream.</p><p>   每当您想象建立一个事件驱动的系统时，它很容易创建一个至关重要的，但对其运作方式的假设是一个至关重要的假设：您想要的事件将已在流中捕获。</p><p> But that’s not how it works in practice. Events are born outside of streams: in other databases, other web apps, other servers. Some piece of code, somewhere, needs to be smart enough to send data to (or receive data from) a stream. This is almost always overlooked because it intuitively feels like a solved problem; all the cool computer science work you hear about related to event streaming tends to be about processing or storage. You tend to hear less about byte shoveling.</p><p> 但这不是它在实践中的工作原理。事件出生在流之外：在其他数据库中，其他Web应用程序，其他服务器。某些代码，某个代码需要足够智能，可以将数据发送到流（或从）流的数据。这几乎总是被忽视，因为它直觉感觉就像一个解决的问题;您听到与事件流相关的所有酷计算机科学工作往往是关于处理或存储。你倾向于听到更少的字节铲。</p><p> But like anything in life, nothing is free. In the beginning, there was no built-in software that did this with Kafka. You had to write your own specialized programs to transport data in a scalable, fault-tolerant, correct way. You had to do this for every data system that you wanted to integrate with, source and sink. This isn’t just annoying grunt work; it is quite hard to build for systems that don’t natively expose a change data capture mechanism.</p><p> 但就像生活中的任何东西一样，没有什么是自由的。一开始，没有内置的软件与Kafka这样做。您必须编写自己的专业程序，以以可扩展，容错，正确的方式运输数据。您必须为您想要与来源和汇总的每个数据系统执行此操作。这不仅仅是令人讨厌的咕噜声;对于未自然地公开更改数据捕获机制的系统，建立系统非常困难。</p><p> If this part didn’t work well, nothing else mattered. Without events, what is the point of any of this? The first order of business was to make a framework so that we could isolate all of the common complexity for communicating with external systems.  Kafka Connect was added to Apache Kafka in 2015 to do just that.</p><p> 如果这部分不好，那就没有别的重要。没有事件，这一切是什么点？第一阶的业务是制作框架，以便我们可以隔离所有与外部系统通信的共同复杂性。 Kafka Connect于2015年添加到Apache Kafka进行。</p><p> Ever since then, Connect’s mission has been delightfully single minded: Build a connector to and from  everything. There are now  more than 200 connector integrations, many of which are pearls of engineering. Consider the suite of  Debezium connectors that do change data capture against MySQL, Postgres, MongoDB, and friends. Or take the connectors that integrate with SaaS apps like  Salesforce. There’s even a connector to source data from  Oracle.</p><p> 从那时起，连接的使命已经令人愉快地单身令人愉悦：将一个连接器从一切中建造。现在有超过200个连接器集成，其中许多是工程的珍珠。考虑Debezium连接器的套件，这些连接器会更改针对MySQL，Postgres，MongoDB和朋友的数据捕获。或者拍摄与Salesforce等SaaS应用集成的连接器。甚至还有一个连接器来源来自Oracle的数据。</p><p> Out of focus comes power. ksqlDB builds on this to make it easy to source and sink events with connector management. Using SQL, you can interactively create connector objects:</p><p> 焦点是力量。 ksqldb构建它可以轻松地使用连接器管理来源于源和汇。使用SQL，您可以交互式创建连接器对象： </p><p> CREATE SOURCE CONNECTOR riders WITH ( &#39;connector.class&#39; = &#39;JdbcSourceConnector&#39;, &#39;connection.url&#39; = &#39;jdbc:postgresql://...&#39;, &#39;topic.prefix&#39; = &#39;rider&#39;, &#39;table.whitelist&#39; = &#39;geoEvents, profiles&#39;, &#39;key&#39; = &#39;profile_id&#39;, ...);</p><p>创建源连接器骑手（ ＆＃39;连接器.Class＆＃39; =＆＃39; jdbcsourceConnector＆＃39;， ＆＃39; connection.url＆＃39; =＆＃39; jdbc：postgreSQL：//...&#39;， ＆＃39;主题.prefix＆＃39; =＆＃39;骑手＆＃39;， ＆＃39; table.whitelist＆＃39; =＆＃39;地理位置，概况＆＃39;， ＆＃39; key＆＃39; =＆＃39; profile_id＆＃39; ......）;</p><p> Break out your shovels—bytes are now in motion. The connectors run in a fault-tolerant, scalable manner across ksqlDB’s servers, all thanks to Kafka Connect.</p><p> 突破你的铲子字节现在正常。连接器以容错，可扩展的方式运行，横跨ksqldb的服务器，都感谢Kafka Connect。</p><p>  Once you’ve managed to capture events, you need a home for them. In databases, we call that the log. If the log, stored as a Kafka stream, is going to be the backbone of your architecture, there can be no doubt in your mind about its stability.</p><p>  一旦您设法捕获事件，您需要为他们提供一个家。在数据库中，我们称之为日志。如果将日志存储为Kafka流，将成为架构的骨干，您的思想毫无疑问是它的稳定性。</p><p> But a decade ago, that wasn’t exactly the case. There were lots of unanswered questions about what it would be like to use Kafka for mission-critical use cases.</p><p> 但十年前，情况并非如此。在关键任务用例中使用Kafka，有很多未解决的问题。</p><p> Yes, it’s cool, we said,  But can it store all my data? Kafka used to store data only on the brokers’ disks, which meant that it was storage bound against the broker with the least amount of capacity. You had to choose between provisioning expensive, high-capacity disks and storing less data. But it was worse than that. Many use cases, like auditing and online machine learning, can’t work with streams that have short retention.</p><p> 是的，它很酷，我们说，但它可以存储所有的数据吗？ Kafka用于仅在经纪人的磁盘上存储数据，这意味着它是符合最少容量的存储器的存储空间。您必须在供应昂贵的高容量磁盘之间进行选择，并存储更少的数据。但它比那更糟糕。许多用例，如审计和在线机器学习，不能与具有短暂保留的流。</p><p> What might be better? Object stores, like Amazon S3 and Google Cloud Storage, offer effectively infinite storage capacity. If Kafka were to use those services as storage backends, it could be made to store huge volumes of data. In 2020, Confluent introduced  Tiered Storage to make that a reality. And for Apache Kafka,  KIP-405 aims to introduce similar functionality. This technique has made it possible to store limitless amounts of data in streams for absurdly cheap costs.</p><p> 什么可能更好？像Amazon S3和Google云存储等对象存储，提供有效的无限存储容量。如果Kafka要将这些服务用作存储后端，可以存储大量数据。在2020年，汇合引入了分层存储，以使现实。而对于Apache Kafka，KIP-405旨在引入类似的功能。这种技术使得可以将无限量的数据存储在流中以获得荒谬廉价的成本。</p><p> Sure, but can it survive a regional outage? We live in a world that is always on, 24×7. A datacenter outage isn’t just inconvenient anymore—it can be damaging. If Kafka is going to be used for anything critical, you need to be able to count on your data being available, even when an entire availability zone is lost. Until recently, the conventional technique for making Kafka immune from outages was to simply “stretch” the cluster’s nodes across data centers. This idea worked somewhat, but it was far from perfect. The further the nodes move away from each other geographically, the harder it becomes to operate the cluster.</p><p> 当然，但它可以在区域中断吗？我们生活在一个永远在的世界，24×7。数据中心中断不再是不方便的 - 它可能会损坏。如果Kafka将用于任何关键的任何问题，即使在整个可用性区域丢失时，您需要计算您的数据。直到最近，用于使Kafka免受停电的传统技术是简单地“拉伸”跨数据中心的集群节点。这个想法有点努力，但它远非完美。该节点的另一个节点地理位置地远离彼此移动，操作群集越硬。 </p><p> There is not a soul who has worked with Kafka and didn’t want an easier way to do cross-datacenter replication. And in 2020, it got one—two, actually. Apache Kafka added  geo-replication (under the project name MirrorMaker2), which is a piece of software that sits in the middle of two Kafka clusters, replicating between them (it’s cleverly built on Kafka Connect). The second was an addition to Confluent, named  Cluster Linking—instead of using a middleman for replication, it uses Kafka’s native replication protocol to copy data directly.</p><p>没有一个与Kafka合作的灵魂，并且不希望更容易做跨数据中心复制。在2020年，实际上有一二。 Apache Kafka添加了Geo-Replication（在项目名称MirrorMaker2下），这是一块位于两个Kafka集群中间的软件，它们之间复制（它巧妙地构建在Kafka Connect上）。第二个是与Confluent的补充，命名群集链接 - 而不是使用Middleman进行复制，它使用Kafka的本机复制协议直接复制数据。</p><p> Okay, but can I keep it up? When it comes to infrastructure, everything is riding on your confidence in keeping it online. If it’s hard to operate, it won’t stay up, and it will give you no value. Running Kafka yourself used to be hard. It wasn’t as battle-tested as it is today, and no one had time to specialize in operating it. You had to carefully choose your cluster size, then police everyone who used it to play by your administrative rules.</p><p> 好的，但我可以留下来吗？谈到基础设施时，一切都在骑自信地骑在网上保持信心。如果它很难运作，它不会留下来，它不会给你一个价值。运行Kafka自己曾经很难过。它并不像今天那样战斗，没有人有时间专注于操作它。您必须仔细选择您的群集大小，然后警察每个使用它的行政规则播放的人。</p><p> Software as a service in the cloud has completely changed this. It’s no longer the case that Kafka is only suitable for big projects or high-volume workloads. You can get an inexpensive Kafka cluster from many cloud vendors, like  Confluent Cloud—with an SLA. You are shielded from all of the hard work of keeping brokers healthy. Running Kafka is someone else’s problem now.</p><p> 软件作为云中的服务完全改变了这一点。 Kafka仅适用于大项目或大批量工作负载，不再是这种情况。您可以从许多云供应商那样获得一个廉价的Kafka集群，如Confluent云 - 带有SLA。你被掩负的所有艰苦的工作都是保持经纪人健康的。运行Kafka现在是别人的问题。</p><p> Simply put, a database is nothing without its storage engine. Likewise, ksqlDB is nothing without Kafka. It’s only useful because its transaction log is capacious, shareable, durable, and inexpensive. But what do we do with that data after we’ve stored it?</p><p> 简单地，数据库没有存储引擎。同样，KSQLDB没有Kafka。它只有用，因为它的交易日志是宽敞的，可共同，耐用的，廉价的。但在我们存储后，我们如何处理该数据？</p><p>  A stream full of events is only useful if it can be processed. This is the heart of where stream processing meets databases—you write queries whose results are revised as soon as new information is received.</p><p>  充满事件的流仅在处理它时才有用。这是流处理符合数据库的核心 - 您写入查询后，只要收到新信息就会修改结果。</p><p> For newcomers, this idea sounds exotic, but it’s easy to understand with an analogy: Processing data is like solving a jigsaw puzzle. With traditional database query processing, every query is like a brand new puzzle. You are given all the pieces up front, but you can’t see the picture the puzzle will reveal until all of its pieces have been fit together.</p><p> 对于新人来说，这个想法听起来很异落，但很容易用类比理解：处理数据就像求解拼图拼图。通过传统的数据库查询处理，每个查询都像一个全新的拼图。你把所有的碎片放在前面，但你看不到图片拼图将揭示，直到所有的碎片都很合适。</p><p> With stream processing, queries run indefinitely—it is like solving one puzzle continuously, where you are given only a few pieces at a time. You immediately fit the piece you are given into the puzzle as best as you can. By contrast to traditional query processing, stream processing lets you see the puzzle’s picture as it evolves. This is just like solving a real puzzle—you don’t need to wait until the end to learn what it will reveal.</p><p> 通过流处理，查询无限期地运行 - 它就像是连续解决一个拼图，在那里你一次只给出几件。你立即适合你可以尽可能地放入拼图的作品。与传统查询处理相比，流处理允许您在演变时看到拼图的图片。这就像解决真正的拼图 - 你不需要等到结束，了解它将揭示什么。 </p><p>  The analogy works because seeing the face of the puzzle is just like seeing the results of a query. But this isn’t just attractive because you can receive query results early. It’s attractive because the intermediary states of a query often have value in themselves.</p><p>类比的作品是因为看到拼图的面部就像看到查询的结果一样。但这不仅仅是有吸引力的，因为您可以早期接收查询结果。它具有吸引力，因为查询的中介状态通常具有价值。</p><p> The utility of stream processing is clear—but how do you build a system to do that? Before Kafka, the answer wasn’t obvious. Stream processing was and still is a new paradigm of programming. Back then, it had limited APIs, and the theory behind it was still being figured out. Let’s look at three of the most important concepts that have evolved since then: time, scale, and synchronization.</p><p> 流处理的效用是明确的 - 但是如何构建系统要做的？在Kafka之前，答案并不明显。流处理仍然是一个新的编程范例。然后，它有限的API，它背后的理论仍然被弄清楚。让我们看看以来的三个最重要的概念：时间，规模和同步。</p><p>  First, time. To depart from the metaphor, a stream processing puzzle is never finished because you don’t know if you have all the pieces yet. This is reflective of real life—it is hard to know if you ever have all the data that you will get. In 2015, the  Google Dataflow paper explored this tension, framed as a trade-off between correctness, latency, and cost.</p><p>  第一次。要离开隐喻，流程处理拼图从未完成，因为您是否不知道您是否拥有所有碎片。这是反映的现实生活 - 很难知道你是否拥有你将得到的所有数据。 2015年，谷歌数据流纸张探讨了这一紧张局势，在正确性，延迟和成本之间造成权衡。</p><p> One consequence of not knowing if you have all the data yet is dealing with receiving data out of order. Event data, by definition, carries a timestamp—the time at which the event happened. What happens if you receive “older” events after “newer” events? There are plenty of situations where this can happen.</p><p> 不了解您是否拥有所有数据的后果，尚未处理从订单中接收数据。根据定义，事件数据携带时间戳 - 事件发生的时间。如果您在“较新”事件之后收到“较旧”事件会发生什么？有很多情况会发生这种情况。</p><p> Here’s a quick example. Consider a simple stream processing program that tallies how much money a business is making each day. As orders flow in, the revenue is summed up for whatever day today is. But now imagine that a batch of orders was submitted late. The orders were placed yesterday, but you only received them today. The appropriate action is to adjust the daily revenue totals for yesterday. But if all your program knows how to do is add to today’s total, you’re toast.</p><p> 这是一个快速的例子。考虑一个简单的流处理程序，略高于每天制作多少钱。随着订单流入，收入总结为今天的任何日子。但现在想象一批订单迟到了。订单昨天被下了，但你今天只收到了​​他们。适当的行动是昨天调整日常收入总额。但是，如果您的所有计划都知道如何做到今天的总数，您就会增加今天。</p><p> In the past, you’d have to pick your poison: either throw away the data, or use a batch processor to mop up the problems later. The latter, known as the lambda architecture, was particularly pernicious because it meant that you had to write your program twice—in both your stream processing and your batch processing framework.</p><p> 在过去，你必须选择你的毒药：要么扔掉数据，要么使用批处理器来稍后搞定问题。后者，被称为Lambda架构，特别是有害的，因为它意味着您必须在您的流处理和批处理框架中两次编写您的程序。</p><p> There is no single thing that changed to make out-of-order stream processing easier. Rather, a collection of solutions evolved over the course of 2014 to present day.</p><p> 没有单一的东西改变为使超级流处理更容易。相反，在2014年到现在的过程中，一系列解决方案的集合也在演变。 </p><p> Kafka Streams,  Flink, and  Dataflow all built APIs that mostly handle out-of-order data by default. This means that other than hinting to the stream processing framework where to find the timestamp, you don’t have to do anything to get correct answers. Most APIs accomplish this with  windowing—tumbling, hopping, sliding, and session windows help you slice time in different ways, but all handle disorder transparently.</p><p>Kafka Streams，Flink和Dataflow所有内置API，默认情况下主要处理无序数据。这意味着除了暗示流处理框架，在哪里找到时间戳，您不必做任何事情来获得正确的答案。大多数API通过窗口翻滚，跳跃，滑动和会话窗口完成了这一点，帮助您以不同方式切片时间，但所有处理障碍都透明地。</p><p>  At the same time, we had to deal with data being “too late” to be useful. Kafka Streams pioneered the concepts of stream time, grace periods, and retention. Flink spearheaded watermarks and a unified API for batch and stream processing.</p><p>  与此同时，我们必须处理“太晚”的数据是有用的。 Kafka Streams开创了流时间，宽限期和保留的概念。 Flink Spearhead水印和用于批量和流处理的统一API。</p><p>   Now, scale. What is it that motivates someone to architect with event streams? One of the reasons is often scalability. When you maintain state with a stream processor, you decentralize where your queries run; instead of having them all run together in your database servers, you place them onto different application services. This allows you to adjust how many resources are dedicated to each query. If you have a query that needs to process a lot of data, you can add more machines to parallelize the work.</p><p>   现在，规模。将某人与事件流有人激励某人是什么？其中一个原因通常是可扩展性。使用流处理器保持状态时，您将在查询运行的位置分散;您将它们放在不同的应用程序服务上，而不是将它们全部运行在一起。这允许您调整有多少资源专用于每个查询。如果您有一个需要处理大量数据的查询，则可以添加更多计算机来并行化工作。</p><p>   Imagine that you have a cluster of two nodes processing a stateful query. The two nodes divide the keyspace and create localized, aggregated state. Now, what will happen if you add a third node to your cluster? Early on, this was a recipe for pain. What you need to do is redivide the keyspace and shuffle the data evenly across the three nodes. Data shuffling has always been a hard problem, but it’s even more challenging in an always-on stream system. Until recently, Kafka Streams’ solution to this problem was to stop the world. A global rebalance would take place, and availability was only restored when the rebalance completed. The more state you had, the longer this took.</p><p>   想象一下，您有一个处理有状态查询的两个节点集群。两个节点划分keyspace并创建本地化，聚合状态。现在，如果将第三个点添加到群集时会发生什么？早期，这是一个痛苦的配方。您需要做的是冗余键空间并均匀地将数据均匀地跨越三个节点。数据洗牌一直是一个难题，但它在始终是流动系统中更具挑战性。直到最近，Kafka Streams对这个问题的解决方案是阻止世界。全球重新平衡将发生，并且在重新平衡完成时才会恢复可用性。你所拥有的州越多，越长。</p><p> Global rebalancing—goodbye. Say hello to  cooperative, incremental rebalancing. Added in 2020, incremental rebalancing helps Kafka Streams shuffle the minimum amount of data across nodes when a cluster changes size. There is no avoiding shuffling in distributed processing systems, but you can minimize the amount of pausing and data movement. Kafka Streams does this by default now.</p><p> 全球重新平衡 - 再见。打招呼，合作，渐进重新平衡。添加在2020年，增量重新平衡有助于Kafka Streams在群集更改大小时将跨节点跨节点的最小数据量进行破坏。分布式处理系统中没有避免随机播放，但您可以最大限度地减少暂停和数据移动量。默认情况下，Kafka Streams会这样做。</p><p>   Finally, synchronization. If events are the currency of your system, it’s important that the teller be precise when following your instructions. But whenever distributed systems are involved, there’s sure to be easy ways to make mistakes. In particular, when distributed nodes exchange messages, failures can create retries, which in turn can create duplicate messages, loss of messages, or partial execution of instructions.  Kyle Kingsbury has made a living showing what a mess this can be.</p><p>   最后，同步。如果事件是您系统的货币，则在遵循指示时，出纳员必须精确。但只要涉及分布式系统，肯定会有很简单的方法来犯错误。特别是，当分布式节点交换消息时，失败可以创建重试，这又可以创建重复的消息，丢失消息或部分执行指令。 Kyle Kingsbury展现了一个乱七八糟的生活。</p><p> When Kafka took off, it couldn’t make strong guarantees about how messages would be exchanged under failure scenarios. There wasn’t a way to work around this—these are low-level guarantees that need to be made for you by the infrastructure.</p><p> 当Kafka起飞时，它无法担心如何在故障情景下兑换消息。没有办法解决这个问题 - 这些是基础架构需要为您提供的低级保证。 </p><p> If Kafka was going to be used for mission-critical workloads, this needed to change. So in 2017, Apache Kafka added  exactly-once semantics (EOS). EOS means three things: idempotent message sending, transactions, and idempotent stateful processing. You need all three of these to completely solve the problem of reliable message processing. With this in place, Kafka became trusted for situations where there just isn’t room for error.</p><p>如果Kafka将用于关键任务工作负载，这需要改变。因此，2017年，Apache Kafka完全添加了一旦语义（EOS）。 EOS意味着三件事：IDEMPotent消息发送，事务和幂等的状态处理。您需要所有三个来完全解决可靠的消息处理问题。在此到位，Kafka是值得信赖的，其中没有错误的空间。</p><p> Where is ksqlDB in all this? Happily solving jigsaw puzzles. Out-of-order data is handled transparently thanks to Kafka Streams’ API. Cluster scaling works without much of a thought. And exactly-once semantics can be activated with a single server flag.</p><p> 所有这些在哪里ksqldb？愉快地解决七巧板拼图。由于Kafka Streams的API，透明地处理了无序数据。群集缩放工作没有大部分思考。正好 - 一旦可以使用单个服务器标志激活一次语义。</p><p> We’ve worked our way across the stack: from data acquisition, to storage, to processing. What’s left? The most striking feature of databases is what’s left.</p><p> 我们已经在堆栈中工作了：从数据采集，存储到处理。还剩什么？数据库最醒目的功能是剩下的。</p><p>  Close your (figurative) eyes. Think about sitting down at your laptop to use a database. What do you see?</p><p>  关闭你的（比喻）的眼睛。想想坐在笔记本电脑上使用数据库。你看到了什么？</p><p> You probably see yourself typing some text into a console. That text is probably SQL.</p><p> 您可能会看到自己将一些文本键入控制台。该文本可能是SQL。</p><p> SQL-based databases have dominated for decades, and rightly so. Their query models are easy to use and easy to understand. If stream processing is to become the other half of databases, we need to pursue a similar abstraction that encompasses everything we’ve talked about so far.</p><p> 基于SQL的数据库已经占据了几十年，而且正确的数据库。他们的查询型号易于使用，易于理解。如果流处理是成为其他一半的数据库，我们需要追求类似的抽象，这些抽象包括到目前为止所讨论的一切。</p><p>  Its query layer, built around SQL, uses a simple client/server architecture—just like how Postgres works. Instead of communicating directly with the underlying components, you use SQL to create streams, inspect tables, derive materializations, and issue queries. You see only two layers: storage (through Kafka) and compute (through ksqlDB). Both elastically scale independently from one another.</p><p>  它的查询层围绕SQL构建，使用简单的客户端/服务器架构 - 就像Postgres如何工作一样。您可以使用SQL来创建流，检查表，导出verticizations并发出查询来而不是直接与底层组件进行通信。您只看到两层：存储（通过Kafka）并计算（通过KSQLDB）。彼此独立弹性地缩放。 </p><p>  When you want to query your data, accessing your tables of state is simple. Because all of the work to update your query’s state (fitting each puzzle piece) happens at processing time, reading the results of a query (seeing the face of the puzzle) is predictably fast. There is no work to be done. Although this idea is straightforward in theory, building your architecture around it is more challenging.</p><p>当您想查询数据时，访问状态表很简单。因为更新查询的状态（拟合每个拼写件）的所有工作发生在处理时间时，读取查询的结果（看到拼图的面部）是可预测的快速。没有工作要做。虽然这个想法在理论上是直截了当的，但在它周围建立你的建筑是更具挑战性的。</p><p> For example, in a distributed cluster, how will you find the right server to issue your query against? And what should happen if the data you’re looking for has recently moved to another server? How do you make the networking between the client and servers not a problem for everyone who has a query?</p><p> 例如，在分布式群集中，您如何找到正确的服务器来发出查询的？如果您要查找的数据最近移动到另一台服务器，那么应​​该发生什么？您如何在客户端和服务器之间进行网络，这对于有查询的每个人都没有问题？</p><p> ksqlDB makes this easy with its dual query support. When you issue queries to ksqlDB’s servers, it’s able to automatically route your query to the right node—even in the face of failures. This is all thanks to  KIP-535 in Kafka Streams, which augmented its state store APIs to support replica information. Using this, ksqlDB can serve pull queries (request/response style) and push queries (streaming style).</p><p> KSQLDB使用双查询支持轻松实现此功能。当您向KSQLDB的服务器发出查询时，它能够自动将查询路由到右侧节点 - 即使在故障面上也是如此。这一切都感谢Kip-535在Kafka Streams中，这增加了其州商店API支持副本信息。使用此，ksqldb可以提供拉出查询（请求/响应样式）和推送查询（流样式）。</p><p>    For all the progress we’ve made, we’re not there yet. A book could be filled with the remaining gaps, but for now I’ll briefly touch on three of the larger ones. In a follow-up post, I’ll discuss more of them in detail.</p><p>    对于我们所取得的所有进展，我们还没有那里。一本书可以充满剩下的差距，但现在我将简要触及三个较大的差距。在后续帖子中，我将详细讨论更多。</p><p> The first area that has a range of open problems is at the top of the stack, the query layer. To support request-response style queries, ksqlDB ought to handle ad hoc SQL. Today, it has more limited expressivity that’s bounded by its ability to generate additional indexes. And to support streaming style queries, the SQL language needs to be rethought and extended. There’s an ongoing standards committee ( DM32 Technical Committee of INCITS), made up of companies like Microsoft, Google, Oracle, IBM, and Confluent, taking a shot at that problem.</p><p> 具有一系列打开问题的第一个区域位于堆栈的顶部，查询层。为了支持请求 - 响应样式查询，KSQLDB应该处理ad hoc sql。今天，它具有更多有限的富有关键型，这是其产生额外索引的能力。为了支持流式样式查询，需要将SQL语言重新升级和扩展。有一个正在进行的标准委员会（DM32佣金技术委员会），由Microsoft，Google，Oracle，IBM和Confluent等公司组成，在那个问题上拍摄。</p><p> Another part of the picture that needs more thought is consistency with fault tolerance, both across replicas and between the source/sinks. All things being equal, it’s easier to build applications against data systems with stronger consistency guarantees than weaker ones. Today, ksqlDB is a fault-tolerant, eventually consistent system. Other projects are approaching this in the reverse direction by starting with strong consistency and no fault tolerance. The end goal is clearly to get both.</p><p> 需要更多思想的图片的另一部分是跨副本和源/下沉之间的容错容差的一致性。所有的东西都是平等的，它更容易构建与数据系统的应用程序，比较弱的一致性保证。如今，KSQLDB是一个容错的，最终一致的系统。其他项目通过以强的一致性和没有容错来开始反向沿反向方向接近这一点。最终目标显然是为了获得两者。</p><p> Lastly, there’s a hidden gem in the world of stream processing that’s worth remembering. One of the perks of using stream processing to materialize state is that you can choose your storage backend. If you need to do fast lookups, you can use a key/value store. If you need to do analytical queries, you can plug in a columnar store. Relational databases support swappable storage engines to some degree, but stream processors traditionally give you far more latitude. Today, ksqlDB bakes in the choice of RocksDB. B</p><p> 最后，流处理世界上有一个隐藏的宝石，值得记住。使用流处理以实现状态的特权是您可以选择存储后端。如果您需要快速查找，可以使用键/值存储。如果需要进行分析查询，则可以插入柱状商店。关系数据库支持可交换的存储引擎到某种程度，但传统上的流处理器传统上给您更加纬度。如今，KSQLDB在选择RocksdB中烘焙。 B. </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.confluent.io/blog/ksqldb-streaming-sql-the-query-your-database-cant-answer/">https://www.confluent.io/blog/ksqldb-streaming-sql-the-query-your-database-cant-answer/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/database/">#database</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/查询/">#查询</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>