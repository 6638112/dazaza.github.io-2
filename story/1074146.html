<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>登录Twitter</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">登录Twitter</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-20 09:34:21</div><div class="page_narrow text-break page_content"><p>虽然之前Twitter上存在集中式日志记录，但它受到低摄取能力和有限查询能力的限制，这导致采用率低下，未能实现我们希望的价值。为了解决这个问题，我们采用了Splunk Enterprise并将集中式日志记录迁移到它。现在我们接收到的日志数据是原来的4倍，并且有了更好的查询引擎和更好的用户接受度。我们向Splunk Enterprise的迁移总体上为我们提供了一个更强大的日志平台，但这一过程并非没有挑战和经验教训，我们将在本博客中更详细地分享这些挑战和经验教训。</p><p>在我们在Twitter上采用Splunk Enterprise之前，我们的中心日志平台是一个名为Loglens的自制系统。这个最初的系统解决了我们的计算平台上容器中的日志短暂性带来的问题，以及在调查正在进行的事件时浏览不同服务的不同实例中的不同日志文件的困难。该记录系统的设计目标如下：</p><p>Loglens专注于从构成Twitter的服务中获取日志。在解决摆在它面前的问题方面，由于成本和时间限制，Loglens是一个相当成功的解决方案。</p><p>毫不奇怪，一个资源投入如此有限的日志系统最终没有达到用户的预期，而且该系统在Twitter上被开发者采用的情况也不理想。关于Loglens的更多信息可以在我们之前的帖子中找到。下面是Loglens摄食系统的简图。日志被写入本地Scribe守护进程，转发到Kafka，然后被摄入Loglens索引系统。</p><p>在运行Loglens作为我们的日志后端时，我们在每个数据中心每秒接收大约6000K个事件。然而，只有大约10%的日志被提交，剩下的90%被速率限制器丢弃。</p><p>鉴于这篇博文的标题，说我们用Splunk Enterprise取代Loglens并不是一个颠覆。我们之所以采用Splunk Enterprise，是因为我们努力寻找一个中央日志系统，该系统可以处理更广泛的用例集，包括Twitter车队中数十万台服务器的系统日志和网络设备的设备日志。我们最终选择了Splunk，因为它可以扩展以满足容量需求，并提供灵活的工具，可以满足大多数用户的日志分析需求。</p><p>我们向Splunk Enterprise的过渡简单而直接。随着系统日志和网络设备日志成为我们可以接受的新类别，我们能够重新开始。我们在车队中的每台服务器上安装了Splunk Universal Forwarder，包括一些运行rsyslog的新专用服务器，以将日志从网络设备中继到Universal Forwarder。</p><p>转换过程中最复杂的部分是将现有的应用程序日志从Loglens迁移到Splunk Enterprise。幸运的是，由于Loglens管道的松散耦合设计，我们团队的大部分工作都是在简单的任务中创建一个新服务，订阅Loglens已经使用的Kafka主题，然后将这些日志转发到Splunk Enterprise。</p><p>我们将这项新服务命名为应用程序日志转发器（ALF），这可能有点缺乏想象力。我们在设计ALF时的主要目标如下：</p><p>阿尔夫应该有韧性。也就是说，当遇到间歇性错误时，事件提交应该具有弹性。</p><p>阿尔夫应该是可控的。如果一个行为不端的服务产生过多的日志量，我们应该能够删除事件，以保持每日配额和索引器集群的稳定性。</p><p>ALF是一项非常简单的服务。它从Kafka读取事件，并使用HTTP事件收集器将它们提交给Splunk Enterprise，并根据服务名称和日志级别进行一些基本的速率限制。尽管它很简单，但ALF占所有摄入日志的85%以上。通用转发器转发的系统日志或通过rsyslog+通用转发器接收的网络设备日志不到15%。</p><p>我们新的应用程序日志记录管道看起来非常相似，如下图所示。唯一的变化是在蓝色虚线下方。</p><p>我们当前的拓扑结构涉及在每个主要数据中心的索引器、搜索头、部署器和集群管理器组成的一个基本独立的集群，以接收和索引来自该数据中心的服务器和服务的日志。每个数据中心集群之间的唯一交互是在其中一个部署器上运行的索引器和许可证管理器之间，以及配置为搜索所有索引器集群的搜索头集群之间。我们不使用多站点索引复制或多站点搜索头群集。</p><p>目前，我们每天在每个数据中心收集大约42 TB的数据。这不包括Splunk Enterprise的内部日志记录，以及来自最嘈杂的服务的一些日志，这些服务的速率是有限的。超过三分之二的流量来自Splunk Enterprise HTTP事件收集器，在正常操作期间，在不到10秒内从日志语句传输到磁盘存储。我们在每个数据中心每秒接收大约500万个事件。这比我们使用Loglens作为后端时每秒接收的事件数高出4倍多，而且我们仍然有足够的磁盘空间、磁盘IO和CPU空间来接收更多日志。</p><p>虽然向Splunk Enterprise的过渡极大地改善了我们在Twitter上的日志服务，但这并非没有挑战。</p><p>我们遇到的一个基本挑战是管理Splunk企业服务器的配置。虽然使用Puppet或Chef等配置管理工具的标准解决方案涵盖了我们需要自动化和管理的大部分配置，但这种方法仍然不能满足我们管理索引和访问控制的需要。</p><p>当我们使用Puppet管理索引及其访问策略时，整个过程最终受到以下限制：</p><p>这种方法不仅需要工程师的参与，并且限制了我们创建索引的速度，而且还没有很好地集成到Twitter上的其他资源调配工具中。</p><p>我们找到的解决方案是创建一个生成索引的新服务。确认和认证。conf文件并将其部署到正确的服务器。这项服务提供了一个API，可以对网络的其余部分进行基于角色的访问控制，使我们能够将索引创建与现有的资源调配工具集成在一起。</p><p>该解决方案受到Splunk Enterprise API中某些限制的阻碍。虽然有一个API端点可以创建索引，但它只适用于独立的索引器。远程管理配置文件也有其他限制。因此，我们将文件复制到带外以分发配置文件。此外，重新加载配置文件的文档是有限的，我们依靠一个未记录的端点来重新加载身份验证和授权文件，而无需重新启动搜索头。</p><p>我们的方法为我们提供了及时的索引创建，在正常操作下，日志团队的工程师完全不参与。我们预计，在未来一年，这一过程将动态创建600多个指数。</p><p>Splunkbase包含一系列令人印象深刻的附加组件。其中许多定义了模块化输入，以从第三方应用程序（例如Github、Slack或Jenkins）的API收集数据。这些附加组件通常很容易设置（尽管集群环境通常会使设置复杂化），几分钟内您就可以收集到不可用或难以收集的数据。</p><p>分布式资源，如Splunk Enterprise Key-Value Store，没有锁定或信号量，容易受到竞争条件的影响</p><p>在任何一种情况下，在多个节点上配置模块化输入都会导致重复查询上游API，并重复提交给Splunk Enterprise的数据。这意味着大多数模块化输入必须在Splunk企业基础设施中的单个节点上运行，从而造成单点故障。如果该节点出现故障，则需要手动干预或配置管理中的额外复杂性，以允许自动选择新节点。归根结底，大多数Splunkbase附加组件的设计并不是为了能够适应意外的服务器故障。</p><p>相反，我们正在为一项服务实施插件，该服务由我们的计算基础设施计划并在其上运行。我们的插件定期从这些第三方应用程序加载数据，并使用ALF将其提交给我们的索引器集群。这种方法最大的缺点是我们必须自己重新实现每个插件。这是必需的，因为现有的模块化输入需要修改才能在这个新服务下运行，而且，考虑到不同模块化输入的写入方式和存储游标的方式不同，这些修改最终将是每个模块化输入唯一的。似乎编写行为一致的新插件会更容易。</p><p>我们的大部分数据来自ALF。如果某个服务行为不当，并向系统中注入足够多的日志事件，从而威胁到我们Splunk企业集群的稳定性，我们可以按日志级别或原始服务丢弃事件。这使我们能够优先考虑更重要的服务和索引器集群的整体健康状况，而不是来自每个严重性的服务的每条消息的交付。这反过来让我们有时间调查原木量的突然增加，而不会对所有客户造成更大的事故。</p><p>虽然这在我们的环境中不太常见，但跨车队运行的通用转发器有时也会开始转发足够的日志量，从而威胁集群的稳定性。然而，环球货运代理缺乏限制或丢弃事件的灵活性。诚然，这可能是因为环球货运公司的设计决策以适用于大多数原木的假设为中心。然而，这些假设并不适用于我们的许多应用程序日志。</p><p>服务器维护，就像定期重启一样，在我们的环境中是一个巨大的挑战。在实时应用操作系统更新时，固件更新或内核补丁需要重新启动。这主要给索引器带来了一个问题，因为搜索头的状态不太好，集群管理器可以短暂关闭，而不会影响索引器集群或搜索头集群。这一点因集群的大小而变得复杂，三个索引器集群中的每个集群都有几百个索引器。一次重新启动过多服务器或连续快速重新启动服务器会导致搜索中断，新数据的接收率也很低。我们也看到了不寻常的稳定性问题，直到我们执行了一个干净的滚动重启。我们的正常运行时间和可靠性目标不允许我们为这种持续数小时进行定期维护或完全停机的降级服务留出空间。</p><p>等待cluster manager上丢失的存储桶和挂起的存储桶修复指标稳定下来，然后再进入下一批</p><p>这一过程可能需要长达一周的时间，因为它是由工程师驱动的，因此只有在工作日才会取得进展。虽然它不需要工程师的持续关注，但它确实给我们的团队成员带来了相当大的干扰和负担。</p><p>未来的工作包括使用Twitter上现有的工作调度和自动化服务，以智能的方式驱动这些任务。</p><p>将我们的集中式日志服务迁移到Splunk Enterprise使我们的工程师能够保留更多的日志，接收更多类型的日志数据，并对日志执行更复杂的查询和分析。安装和迁移相当简单，导致日志量增加了400%，同时将从日志语句到磁盘上的典型接收时间保持在10秒以下。它也带来了新的挑战。我们能够解决运行Splunk Enterprise时遇到的挑战，该企业具有大型集群（200多个节点）和高数据量（每个集群每秒500万个事件），有时需要编写定制软件来实现这一点。最终，这种迁移导致了集中式日志记录的采用增加，包括在核心应用程序团队和运营部门之间</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/登录/">#登录</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/logging/">#logging</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/日志/">#日志</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>