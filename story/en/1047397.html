<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>深度学习表达钢琴演奏 Deep Learning for Expressive Piano Performances</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Deep Learning for Expressive Piano Performances<br/>深度学习表达钢琴演奏 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-09 20:24:08</div><div class="page_narrow text-break page_content"><p>For the last four years, a small team at Popgun has been studying the application of deep learning to music analysisand generation. This research has culminated in the release of  Splash Pro - a free, AI-powered plugin for Digital Audio Workstations (DAWs). With the release of this blog, we hope to provide an accessible introduction to deep learning with music, by explaining some core projects we have worked on.</p><p>在过去的四年中，Popgun的一个小团队一直在研究深度学习在音乐分析和生成中的应用。这项研究的最终成果是发布了Splash Pro，Splash Pro是一款免费的，由AI驱动的数字音频工作站（DAW）插件。随着此博客的发布，我们希望通过解释我们已经从事的一些核心项目，为音乐深度学习提供一个易于访问的介绍。</p><p>  In this post I want to describe the design of a model for expressive piano synthesis. The model, which we call ‘BeatNet’, is capable of analysing a piano performance and generating new pieces that imitate the original playing style. Before I get into the technical workings, here are a few samples generated by BeatNet:</p><p>  在这篇文章中，我想描述表达钢琴综合模型的设计。该模型（我们称为“ BeatNet”）能够分析钢琴演奏并生成模仿原始演奏风格的新作品。在开始技术工作之前，下面是BeatNet生成的一些示例：</p><p>   In the style of “Chopin - Nocturne in E-flat major, Op. 9, No. 2”</p><p>   风格为“肖邦-E-flat专业夜曲，作品。 9号2号”</p><p>    BeatNet is able to replicate many of the characteristics present in human piano performances, albeit withsome weaknesses and limitations that I will address. To explain how this model works, I have split the remainder of the blog into two sections. Part I will discuss musical representations, while Part II will explore the model design and showcase more of its capabilities.</p><p>    BeatNet能够复制人类钢琴演奏中存在的许多特征，尽管我要解决一些弱点和局限性。为了解释该模型的工作原理，我将博客的其余部分分为两部分。第一部分将讨论音乐表现，而第二部分将探索模型设计并展示其更多功能。</p><p> *Note: BeatNet does not model sustain pedal events. Sustain was manually added to this piece.</p><p> *注意：BeatNet不对延音踏板事件建模。延音是手动添加到此作品中的。</p><p>  In designing a model for music generation, it is critical to choose an appropriate representation. This choicedetermines which data can be faithfully encoded, the modelling techniques that are available, and the efficacy of the overall system. For this reason, I have decided to begin with a brief explanation of some common musical representations. If this all feels familiar to you, feel free to skip ahead to part II.</p><p>  在设计音乐生成模型时，选择合适的表现形式至关重要。此选择确定可以忠实地编码哪些数据，可用的建模技术以及整个系统的效力。因此，我决定首先简要解释一些常见的音乐表现形式。如果您对这一切感到熟悉，请随时跳到第二部分。</p><p>  When we talk about audio in machine learning, we are typically referring to an array of time-domain samples. Thesesamples are a digital approximation to the physical sound pressure wave. The quality of this approximation isdetermined by the sample rate and the bit-depth (see:  PCM). In research, it is common to see sample rates of 16-22kHz. However, to capture the full range of human audible frequencies,most HiFi applications (e.g. music, podcasts, etc) use rates of around 44kHz (see:  Nyquist Theorem).</p><p>  当我们谈论机器学习中的音频时，通常是指一组时域样本。这些样本是物理声压波的数字近似值。这种近似的质量取决于采样率和位深度（请参阅：PCM）。在研究中，通常会看到16-22kHz的采样率。但是，为了捕获整个人类可听到的频率，大多数HiFi应用程序（例如音乐，播客等）使用大约44kHz的频率（请参阅：奈奎斯特定理）。 </p><p> Because of its high dimensionality, modelling raw audio is extremely challenging. For a generative model to reproduce audio structure on a timescale of a few seconds, it must capture the relationships between tens of thousands of samples. While it is becoming increasingly feasible to do this (see  [1]), there are more efficient ways to encode piano performances.</p><p>由于其高维度，对原始音频进行建模非常具有挑战性。对于要在几秒钟的时间尺度上重现音频结构的生成模型，它必须捕获成千上万个样本之间的关系。尽管执行此操作变得越来越可行（请参见[1]），但是有更有效的方法来编码钢琴演奏。</p><p> Note: For an example of music modelling directly in the audio domain, see  OpenAI’s Jukebox  [2].</p><p> 注意：有关直接在音频域中进行音乐建模的示例，请参见OpenAI的Jukebox [2]。</p><p>  MIDI is a technical standard allowing electronic instruments and computers to communicate. It comprisesa sequence of ‘messages’, each describing a particular musical event or instruction. Instructions relating to the note timings, pitch and loudness (i.e. velocity) can be transmitted and stored. For example, a piano performance encoded as MIDI is a record of which notes were pressed, how forcefully and at what time.</p><p>  MIDI是一项允许电子乐器和计算机进行通信的技术标准。它由一系列“消息”组成，每个消息描述一个特定的音乐事件或指令。可以传送和存储与音符时间，音高和响度（即速度）有关的指令。例如，编码为MIDI的钢琴演奏可以记录按下音符的力度，力度和时间。</p><p> To convert a MIDI file back into listenable audio, a piece of software replays these instructions andsynthesizes the relevant notes. MIDI does not encode acoustic information, such as the instrument sound or the recordingenvironment. For this reason it is drastically more efficient than audio, and it is a good starting point for our exploration.</p><p> 要将MIDI文件转换回可听的音频，一段软件会重放这些说明并合成相关音符。 MIDI不对声学信息进行编码，例如乐器的声音或录音环境。因此，它比音频要高效得多，这是我们探索的一个很好的起点。</p><p> When working with MIDI we use the terrific  PrettyMIDI library by Colin Raffel. This abstracts away manycomplexities of the raw format. See Colin Raffel’s  thesis fora more in-depth explanation of the MIDI file format.</p><p> 使用MIDI时，我们使用Colin Raffel的出色的PrettyMIDI库。这消除了原始格式的许多复杂性。有关MIDI文件格式的更详细说明，请参阅Colin Raffel的论文。</p><p>   Notice how succinctly we can represent Für Elise this way, and consider that the equivalent audio section contains approximately 50k samples (for a 44kHz recording).</p><p>   请注意，我们用这种方式表示FürElise的方式非常简洁，并考虑到等效音频部分包含大约50k样本（对于44kHz录音）。</p><p>  This will look familiar to anyone who has used a Digital Audio Workstation (DAW) such as GarageBand or Ableton.The vertical axis denotes pitch, and the horizontal axis denotes time. The velocity of each note is encodedby its magnitude. As well as providing an intuitive way to visualize MIDI, piano roll has some interesting properties for machine learning.</p><p>  对于使用GarageBand或Ableton等数字音频工作站（DAW）的任何人来说，这看起来都很熟悉。垂直轴表示音高，水平轴表示时间。每个音符的速度由其大小编码。除了提供直观的方式来可视化MIDI之外，钢琴卷帘还具有一些有趣的机器学习属性。 </p><p> For example, consider the effect of raising the song’s pitch by one semitone, or delaying its onset by a few seconds. These transpositions preserve the spatial structure of the piano roll. When designing a model we can exploit this property by applying 2D Convolutional Neural Networks (CNN). Early experiments at Popgun tried exactlythis, as did  this project using PixelCNN. However, there are some serious drawbacks to this approach.</p><p>例如，考虑将歌曲的音调提高一个半音或将其开始延迟几秒钟的效果。这些换位保留了钢琴卷的空间结构。设计模型时，我们可以通过应用2D卷积神经网络（CNN）来利用此属性。 Popgun的早期实验正是使用PixelCNN进行了此尝试。但是，这种方法有一些严重的缺点。</p><p>   A typical song encoded as piano roll is incredibly sparse: nearly all entries are zero. Naively applying a CNN wastes a large amount of computation on entirely empty regions. It is also difficultto choose a suitable resolution for the time axis. If the original data is quantised, such that each note alignsperfectly with a uniform musical grid, we can choose the resolution based on the quantisation strength. However,this breaks down for expressive piano performances, where natural variations in timing are critical to the musicality.</p><p>   编码为钢琴卷的典型歌曲非常稀疏：几乎所有条目均为零。天真地应用CNN会浪费大量的时间在完全空白的区域上。为时间轴选择合适的分辨率也很困难。如果对原始数据进行量化，以使每个音符与统一的音乐网格完美对齐，则可以基于量化强度选择分辨率。但是，这对于具有表现力的钢琴演奏来说很困难，在这种情况下，时间的自然变化对音乐性至关重要。</p><p> Note: Many works have tried to model piano roll, using a broad range of neural net architectures. For those interested to learn more,  this paper is a good starting point  [16].</p><p> 注意：许多作品都尝试使用广泛的神经网络架构来模拟钢琴卷。对于那些有兴趣了解更多信息的人，本文是一个很好的起点[16]。</p><p>  In 2017  Google Magenta released a model called Performance RNN  [3], which demonstrated the ability to model expressive piano performances with high fidelity. The key innovation was a new format that is highly suited to this task. For reasons that will become apparent, we refer to this “performance representation” simply as “time-shift”. Like MIDI, time-shift encodes music with a sequence of discrete musical events. What sets it apart is the unique way it encodes the progression of time. Instead of representing time along a specific axis (piano roll), or as a property of each individual note (MIDI), in time-shift there is a specific event that indicates the advancement of the piece.</p><p>  2017年，Google Magenta发布了一个名为Performance RNN [3]的模型，该模型展示了能够以高保真度对富有表现力的钢琴演奏进行建模的功能。关键的创新是非常适合此任务的新格式。出于显而易见的原因，我们将这种“绩效表示”简称为“时移”。与MIDI一样，时移通过一系列离散的音乐事件对音乐进行编码。它与众不同的是它编码时间进程的独特方式。代替沿特定轴表示时间（钢琴滚动）或作为每个单独音符（MIDI）的属性，在时移中有一个特定事件指示乐曲的前进。</p><p>  272 - Set Velocity = 6476 - Turn On MIDI Note 76 (E5)317 - Advance Time by 0.3 Seconds204 - Turn Off MIDI Note 76 (E5)271 - Set Velocity = 6075 - Turn On MIDI Note 75 (D#5)317 - Advance Time by 0.3 Seconds203 - Turn Off MIDI Note 75 (D#5)274 - Set Velocity = 7276 - Turn On MIDI Note 76 (E5)317 - Advance Time by 0.3 Seconds204 - Turn Off MIDI Note 76 (E5)273 - Set Velocity = 6875 - Turn On MIDI Note 75 (D#5)317 - Advance Time by 0.3 Seconds...</p><p>  272-设置速度= 6476-开启MIDI音符76（E5）317-提前0.3秒204-关闭MIDI音符76（E5）271-设置速度= 6075-开启MIDI音符75（D＃5）317-提前0.3秒203-关闭MIDI Note 75（D＃5）274-设置速度= 7276-开启MIDI Note 76（E5）317-提前0.3秒204-关闭MIDI Note 76（E5）273-设置速度= 6875-打开MIDI Note 75（D＃5）317-提前0.3秒...</p><p> This one-dimensional sequence of tokens is similar to the encodings used in language models.This means that modern advances in NLP (e.g. ByteNet  [4], Transformers  [5]) can be readily applied to the time-shift format.</p><p> 这种一维标记序列类似于语言模型中使用的编码。这意味着NLP的现代发展（例如ByteNet [4]，Transformers [5]）可以很容易地应用于时移格式。</p><p>  0 - 127: Note on events 128 - 255: Note off events 256 - 287: 32 velocity change events 288 - 387: 100 quantized time-shift values (10ms -&gt; 1000ms)</p><p>  0-127：记录事件128-255：记录事件256-287：32个速度变化事件288-387：100个量化的时移值（10ms-> 1000ms） </p><p> Note: We have not accounted for sustain pedal events, however it might be interesting to incorporate themin future work.</p><p>注意：我们尚未考虑延音踏板事件，但是纳入未来的最小工作可能很有趣。</p><p>  In 2016, researchers at Google’s DeepMind lab released the now-famous WaveNet paper  [1]. The core insight was that 1D convolutions could produce efficient sequence models, by eliminating the need for expensive recurrent computations during training. The efficacy was proven with a  demo of raw audio generation. The publication of ByteNet  [4] shortly afterwards, extended this idea to the domain of natural language processing.</p><p>  2016年，谷歌公司DeepMind实验室的研究人员发布了现在著名的WaveNet论文[1]。核心见解是，一维卷积可以消除训练过程中对昂贵的重复计算的需求，从而可以生成有效的序列模型。原始音频生成的演示证明了其有效性。不久之后，ByteNet [4]的发布将这一思想扩展到自然语言处理领域。</p><p> Inspired by this work, Popgun developed its own ByteNet model for symbolic piano music. We refer to this work as“BeatNet”. The choice to use ByteNet over traditionally dominant RNNs was motivated by a few factors:</p><p> 受这项工作的启发，Popgun开发了自己的ByteNet模型用于符号钢琴音乐。我们将此工作称为“ BeatNet”。在传统上占主导地位的RNN上使用ByteNet的选择是出于以下几个因素：</p><p> Hype. WaveNet and ByteNet were the first papers to threaten the dominance of RNNs, before Transformer basedarchitectures  [5] exploded in popularity.</p><p> 炒作。在基于Transformer的体系结构[5]迅速普及之前，WaveNet和ByteNet是最早威胁RNN统治地位的论文。</p><p>  We curated a MIDI dataset by drawing from a number of online sources. One key source was the Yamaha e-Piano Competition Dataset, consisting of jazz and classical music performances. Another source was the  Lakh MIDI Dataset, which contains a broader range of genres, including many contemporary pieces.</p><p>  我们通过从许多在线资源中绘制来策划MIDI数据集。一个重要的来源是雅马哈电子钢琴比赛数据集，其中包括爵士和古典音乐表演。另一个来源是Lakh MIDI数据集，其中包含更广泛的流派，包括许多现代作品。</p><p> To create a model which generates highly expressive performances, we processed the data to filter out ‘low quality’ items. Anyone who has worked with MIDI datasets will appreciate that many songs sound ‘bad’.This raises some difficult problems: What is musical quality? How can we account for subjective preferences? Are some songs so weird we can confidently deem them ‘not musical’? Suffice to say, these problems are beyondthe scope of this post.</p><p> 为了创建一个能够表现出高表现力的模型，我们处理了数据以滤除“低质量”的商品。使用MIDI数据集的任何人都会欣赏很多歌曲听起来“不好”的声音。这会带来一些困难的问题：什么是音乐质量？我们如何解释主观偏好？有些歌曲很奇怪，我们可以自信地认为它们不是“音乐剧”吗？可以说，这些问题超出了本文的范围。</p><p> Ultimately, our working solution can be summarised as this: An engineer listened to some data items, in each case proclaiming “Yep, this sounds musical” or “Nope, I don’t like it”. In the process, they developed automated heuristicsto filter unwanted items, biasing the dataset towards songs that are subjectively ‘musical’. In the future it would be interesting to crowd-source these assessments, or to devise heuristics based on music theory instead.</p><p> 最终，我们的工作解决方案可以总结为：工程师听了一些数据项，每种情况下都说“是的，听起来很音乐”或“不，我不喜欢”。在此过程中，他们开发了自动启发式过滤器来过滤不需要的项目，从而使数据集偏向于主观“音乐”的歌曲。将来，将这些评估进行众包或在音乐理论的基础上设计启发式方法将很有趣。 </p><p>  We completed our first working prototype in July 2017. It is essentially an unconditional ByteNet  [4] decoder, operating over time-shift format sequences. Figure 5 illustrates the basic generation procedure.</p><p>我们在2017年7月完成了我们的第一个工作原型。它实质上是无条件的ByteNet [4]解码器，可按时移格式序列运行。图5说明了基本的生成过程。</p><p>   At each step, BeatNet outputs a probability distribution over the next time-shift token, based on the historical context. To generate a song, we prime the model with a sequence from the held-out test set, and sample a predicted continuation. Here is a sample from this model:</p><p>   在每个步骤中，BeatNet都会根据历史情况在下一个时移令牌上输出概率分布。要生成一首歌曲，我们使用来自测试集的序列对模型进行预填充，然后对预测的延续进行采样。这是此模型的示例：</p><p>   Notice the presence of natural variations in timing and loudness, mimicking the kinds of features present in a human performance. Despite some interesting flourishes, the model tends to wander aimlessly, with limited evidence of planning or long term structure. The timing is also unusual, since the model has no concept of time signatures orrhythms.</p><p>   请注意，时间和响度存在自然变化，模仿了人类表演中存在的各种功能。尽管有一些有趣的繁荣发展，但该模型倾向于漫无目的地徘徊，而规划或长期结构的证据有限。时序也不常见，因为该模型没有拍号节奏的概念。</p><p> For user-facing applications, this kind of ‘unconditional’ model has very limited control; only the priming sequence (and a few minor generation settings) can be adjusted. One way to improve this is to introduce a latent variable.</p><p> 对于面向用户的应用程序，这种“无条件”模型的控制非常有限。只能调整启动顺序（和一些次要生成设置）。改善此问题的一种方法是引入潜在变量。</p><p>  Variational Autoencoders (VAEs)  [6] are a popular generative modelling technique, with applications in an increasing number of domains, such as images  [7], molecule synthesis  [8], symbolic music [9] [10] and speech synthesis  [11]. A lot has been written about VAEs by other authors  [12] [13] [14], so I will not give a comprehensive explanation here. What’s important is this: VAEs provide a principled way to learn the unspecified factors of variation in a dataset. To illustrate why this is useful, consider two pathways to control in our piano model:</p><p>  变分自动编码器（VAE）[6]是一种流行的生成建模技术，其应用领域越来越多，例如图像[7]，分子合成[8]，符号音乐[9] [10]和语音合成[11] ]。其他作者[12] [13] [14]撰写了很多有关VAE的文章，因此在这里我不做全面的解释。重要的是：VAE提供了一种原则性的方法来学习数据集中不确定的变化因素。为了说明为什么这样做有用，请考虑在我们的钢琴模型中进行控制的两个途径：</p><p> The desired model controls (e.g. “loudness”, “tempo”, “key”) are specified in advance. For each dataset item (or a large subset) we collect labels corresponding to these controls. We provide these labels as an additional input to the model during training. When generating a new song these features can be manipulated to steer the output.</p><p> 预先指定所需的模型控制（例如“响度”，“速度”，“键”）。对于每个数据集项目（或较大的子集），我们收集与这些控件相对应的标签。我们提供这些标签作为训练期间模型的额外输入。生成新歌曲时，可以操纵这些功能来控制输出。</p><p>  A VAE is trained to automatically discover the latent factors of variation in the data. After training, a musically trained listener explores these factors (e.g. by interpolating on each dimension) and observes the effect on the modeloutput. Based on this exploration, we devise a post-hoc interpretation of each latent dimension. By exposing these features to the user they can steer the model as per (1).</p><p>  对VAE进行了培训，可以自动发现数据变化的潜在因素。训练后，受过音乐训练的听众会探索这些因素（例如通过对每个维度进行插值）并观察对模型输出的影响。基于此探索，我们为每个潜在维度设计了事后解释。通过向用户展示这些功能，他们可以按照（1）指导模型。 </p><p> There are trade-offs between these approaches. If the desired control features can be readily extracted foreach data item then (1) becomes feasible. However, in our experiments we observed that features defined this waytend to have natural correlations in the dataset. This can negatively impact the output quality if the userdefines an implausible configuration at generation time. Contrast this with the approach in (2), whereby the VAE objective encourages independence of the learned control features.</p><p>这些方法之间需要权衡取舍。如果可以为每个数据项轻松提取所需的控制特征，则（1）变得可行。但是，在我们的实验中，我们观察到此特征定义的方式倾向于在数据集中具有自然相关性。如果用户在生成时定义了不合理的配置，则可能对输出质量产生负面影响。与（2）中的方法相反，VAE目标鼓励学习的控制功能的独立性。</p><p> Furthermore, consider that we might like to control certain abstract features, such as ‘style’ or ‘genre’, which are difficult to formally define. The VAE sidesteps this issue, by simply learning features that explain the variability in the data. It is possible that a certain latent dimension or subspace will map to human notions of ‘genre’ or ‘style’, but there are no guarantees; it can be difficult to interpret the learned features. In practice, we make two observations:</p><p> 此外，请考虑我们可能希望控制难以正式定义的某些抽象功能，例如“样式”或“风格”。 VAE通过简单地学习可以解释数据可变性的功能来避开此问题。某个潜在的维度或子空间可能会映射到人类的“体裁”或“风格”概念，但并不能保证；可能很难解释学习到的功能。在实践中，我们有两个观察结果：</p><p> It is often possible to find an interpretable subset of latent dimensions to expose as user controls.</p><p> 通常可以找到潜在维度的可解释子集，以暴露为用户控件。</p><p>  Irrespective of 1., a latent space provides novel mechanisms for user control, such as musical interpolationand style transfer.</p><p>  与1.无关，潜在空间为用户控制提供了新颖的机制，例如音乐插值和样式转移。</p><p>  In BeatNet VAE we introduce a sequence of latent variables to help guide the model generation. The architectureis very similar to  Google Magenta’s NSynth  [15]. A convolutional encoder embeds the input into a compressedlatent representation, which is then provided as additional context to the ByteNet decoder. The VAE objective encouragesthese vectors to follow a simple distribution (i.e. standard normal), and provides a penalty on the amount of informationthey can contain. By tuning the size of the latent vectors and the regularisation term, it is possible to learn a representation that explains some (but importantly not all) of the variation in the sequence. By holding this latentvariable constant, we can sample many diverse outputs which inherit the ‘style’ of a given input.</p><p>  在BeatNet VAE中，我们引入了一系列潜在变量来帮助指导模型的生成。该架构与Google Magenta的NSynth [15]非常相似。卷积编码器将输入嵌入到压缩的潜在表示中，然后将其作为附加上下文提供给ByteNet解码器。 VAE目标鼓励这些向量遵循简单的分布（即标准正态），并对其可能包含的信息量进行惩罚。通过调整潜矢量的大小和正则项，可以学习一种表示，它解释了序列中的某些（但重要的不是全部）变异。通过保持此潜在变量常数，我们可以对许多不同的输出进行采样，这些输出继承了给定输入的“样式”。</p><p> A Continuation of Für Elise with Beatnet VAE   Für Elise MIDI sourced from  8notes with permission.</p><p> Beatnet VAE的FürElise的延续作品经许可，源自8notes的FürElise MIDI。</p><p>  This snippet demonstrates how BeatNet VAE can generate continuations of given input sequence. The input runs for 18s, with the model response following. Latent variables derived from the original piece help to guide the continuation,transferring qualities such as the dynamics, key and playing style. Note that compared with the unconditional model,BeatNet VAE does a much better job of playing consistently.</p><p>  此代码段演示了BeatNet VAE如何生成给定输入序列的延续。输入运行18s，随后出现模型响应。源自原始作品的潜在变量有助于指导延续性，传递动感，调子和演奏风格等品质。请注意，与无条件模型相比，BeatNet VAE在持续播放方面做得更好。 </p><p>   Figure 6 shows the probability BeatNet assigns to each token during the first 200 steps of generation. The y-axis tracks the different event types, starting with ‘note on’ events From y=0 to y=127. ‘Note off’ events are above that, and so on according to the time-shift specification. The dark horizontal bands correspond to notes that are outside the range of piano music in the training data.</p><p>图6显示了BeatNet在生成的前200个步骤中分配给每个令牌的概率。 y轴跟踪不同的事件类型，从y = 0到y = 127的“ note on”事件开始。 “音符关闭”事件位于该事件的上方，依时间平移规范依此类推。深色水平带对应于训练数据中钢琴音乐范围之外的音符。</p><p>   Figure 7 gives a zoomed-in picture of the same probabilities. We are focusing on the ‘note on’ events for the first40 generation steps. Figure 8 (below) presents the same phrase as piano roll for comparison. The tokens selected by BeatNet at each step have been highlighted in purple (remember that only ‘note on’ tokens are shown).</p><p>   图7给出了相同概率的放大图。我们专注于前40代步骤中的“注意事项”事件。为了比较，图8（下图）显示了与钢琴卷相同的短语。 BeatNet在每个步骤中选择的令牌都以紫色突出显示（请记住，仅显示“ note on”令牌）。</p><p> To understand what’s going on lets consider one column in isolation. At  x=20 BeatNet elects to play the note  A3 (token  57). Given that we are in A-minor it seems reasonable that BeatNet has assigned a high probability to  B3 (a diatonic note), and a low probability to the  A#3 (a dissonant minor 2nd). At each step a multitude of branching trajectories are possible, though some are much more likely than others.</p><p> 要了解发生了什么，请单独考虑一列。 BeatNet在x = 20时选择弹奏音符A3（令牌57）。考虑到我们处于A小调，BeatNet将高概率分配给B3（全音阶音符），将低概率分配给A＃3（不和谐的小二音）似乎是合理的。在每个步骤中，都有许多分支轨迹是可能的，尽管有些分支的可能性要大得多。</p><p>    For more applications of machine learning to creative tasks, the  Google Magenta blog contains many interesting projects.</p><p>    对于将机器学习应用于创造性任务的更多应用，Google Magenta博客包含许多有趣的项目。</p><p>  This project was a joint effort from the  Popgun team. I would like to especiallythank  Adam Hibble for leading the team during this project. If you haveany technical questions feel free to reach out on  twitter. For other inquiries email info@popgun.ai .</p><p>  这个项目是Popgun团队的共同努力。我要特别感谢Adam Hibble在这个项目中领导团队。如果您有任何技术问题，请随时与Twitter联络。如有其他查询，请发送电子邮件至info@popgun.ai。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html">https://popgun-labs.github.io/ml-blog/generative_models/piano/symbolic_music/2020/02/01/beatnet.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/深度学习/">#深度学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/音乐/">#音乐</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>