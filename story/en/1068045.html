<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>如何智能地做多任务学习 How to Do Multi-Task Learning Intelligently</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">How to Do Multi-Task Learning Intelligently<br/>如何智能地做多任务学习 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-25 01:06:14</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/2bdcdad40b8404768ec30cc3190a5847.jpg"><img src="http://img2.diglog.com/img/2021/6/2bdcdad40b8404768ec30cc3190a5847.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>During the past decade, machine learning has exploded in popularity and is now being applied to problems in many fields. Traditionally, a single machine learning model is devoted to one task, e.g. classifying images, which is known as  single-task learning (STL). There are some advantages, however, to training models to make multiple kinds of predictions on a single sample, e.g. image classification and semantic segmentation. This is known as  Multi-task learning (MTL). In this article, we discuss the motivation for MTL as well as some use cases, difficulties, and recent algorithmic advances.</p><p>在过去十年中，机器学习已经爆发了流行，现在正在应用于许多领域的问题。传统上，单机学习模型专门用于一项任务，例如一项任务。分类图像，称为单任务学习（STL）。然而，培训模型存在一些优点，以制造单个样本的多种预测，例如，图像分类和语义分割。这称为多任务学习（MTL）。在本文中，我们讨论了MTL的动机以及一些用例，困难和最近的算法进步。</p><p>  There are various reasons that warrant the use of MTL. We know machine learning models generally require a large volume of data  for training. However, we often end up with  many tasks for which the individual datasets are  insufficiently sized to achieve good results. In this case, if some of these tasks are related, e.g. predicting many diseases and outcomes from a patient’s profile, we can merge the features and labels into a single larger training dataset, so that we can take advantage of shared information from related tasks to build a sufficiently large dataset.</p><p>  有各种原因，保证使用MTL。我们知道机器学习模型通常需要大量的培训数据。但是，我们经常最终有许多任务，其中各个数据集不足以实现良好的效果。在这种情况下，如果这些任务中的一些是相关的，例如，从患者的个人资料中预测许多疾病和结果，我们可以将功能和标签合并到一个较大的训练数据集中，以便我们可以利用相关任务的共享信息来构建一个足够大的数据集。</p><p> MTL also improves the  generalization  of the model. Using MTL, the information learned from related tasks improves the model’s ability to learn a useful representation of the data, which reduces overfitting and enhances generalization. MTL can also  reduce training time because instead of investing time training many models on multiple tasks, we train a single model.</p><p> MTL还提高了模型的泛化。使用MTL，相关任务中学到的信息提高了模型学习数据的有用表示的能力，这减少了过度装备和增强了泛化。 MTL还可以减少培训时间，因为我们在多个任务上投资时间培训许多模型，我们训练一个型号。</p><p> MTL is crucial in some cases, such as when the model will be deployed in an environment with  limited computational power. Since machine learning models often have many parameters that need to be stored in memory, for applications where the computational power is limited (e.g. edge devices), it is preferable to have a single MTL network with some shared parameters, as opposed to multiple STL models doing related tasks. For example, in self-driving cars, we need  multiple tasks to be done in  real-time, including object detection and depth estimation. Having multiple neural networks doing these tasks individually requires computational power that might not be available. Instead, using a single model trained with MTL reduces the memory requirements and speeds up inference.</p><p> MTL在某些情况下至关重要，例如当模型将部署在具有有限的计算能力的环境中。由于机器学习模型通常具有许多需要存储在存储器中的参数，对于计算功率受到限制的应用（例如，边缘设备），因此优选地具有具有一些共享参数的单个MTL网络，而不是多个STL模型做相关任务。例如，在自动驾驶汽车中，我们需要实时完成多个任务，包括对象检测和深度估计。拥有多个神经网络，以便单独执行这些任务，需要可能无法使用的计算能力。相反，使用具有MTL培训的单个模型可降低存储器要求并加速推断。</p><p>  Despite the advantages of MTL, there are some cases where the approach can actually hurt performance. During the training of an MTL network, tasks can compete with each other in order to achieve a better learning representation i.e. one or more tasks can dominate the training process. For example, when instance segmentation (segmenting a separate mask for each individual object in an image) is trained alongside semantic segmentation (classifying of objects at pixel level) in an MTL setting, the latter task often dominates the learning process unless some task balancing mechanism is employed [1].</p><p>  尽管MTL的优势，但在某些情况下，该方法实际上可以损害性能。在培训MTL网络期间，任务可以彼此竞争，以实现更好的学习表示即，一个或多个任务可以主导培训过程。例如，当在MTL设置中，当与MTL设置中的语义分割（在图像中的每个单独对象分割图像中的每个单独对象的单独对象）进行培训时，除非一些任务平衡机制，否则后者任务通常会主导学习过程就业[1]。</p><p> Furthermore, the loss function of MTL may also be more complex as a result of multiple summed losses, thereby making the optimization more difficult. In these cases, there is a negative effect of cooperating on multiple tasks, and individual networks that are trained on single tasks may perform better.</p><p> 此外，由于多个总和损耗，MTL的损耗功能也可能更复杂，从而使优化更加困难。在这些情况下，存在对多个任务的合作的负面影响，并且在单个任务上培训的单个网络可能更好地执行。</p><p> So when should we multitask? Answering that question is difficult, but in the last few years, there have been a series of important papers that propose algorithms to  learn what and when tasks should be learned together, and when tasks should be learned separately. Here are three important papers towards that end:</p><p> 所以我们什么时候应该多任务？回答这个问题很困难，但在过去的几年里，有一系列重要的论文提出了学习应该在一起学习的算法和当应单独学习任务时的算法。以下是其中的三篇重要论文： </p><p>  Motivation: Generally in MTL, one of two approaches is used. One is  hard parameter sharing, in which initial layers are shared up until a certain point after which the network branches out to make predictions for individual tasks. The problem with this approach is that it forces the machine learning practitioner to specify which layers are to be shared, which may not be optimal for the tasks at hand. In  soft parameter sharing, each task is learned with  separate models and weights, but the objective function includes a loss term that encourages the parameters of the models to be similar. The downside of soft parameter sharing is the large number of parameters, especially as the number of tasks grows.</p><p>动机：通常在MTL中，使用两种方法中的一种。一个是硬参数共享，其中初始层被共享，直到某个点之后，网络分支出用于对各个任务进行预测。这种方法的问题在于它强制机器学习从业者指定要共享哪些层，这可能对手的任务可能不是最佳的。在软参数共享中，每个任务都是通过单独的型号和权重学习，但目标函数包括丢失术语，鼓励模型的参数是相似的。软参数共享的缺点是大量参数，特别是随着任务数量的增加。</p><p> To combine the benefits of these approaches, X. Sun et al. proposed, “ AdaShare: Learning What to Share for Efficient Deep Multi-Task Learning” (2020) [2]. The primary goal of the researchers was to specify a single multi-layer architecture for MTL and train a  policy that determines which layers to share by multiple tasks, which layers to use for specific tasks and which layers to skip for all tasks while ensuring the model provides highest performance at its most compact form.</p><p> 结合这些方法的好处，X. Sun等人。建议，“Adashare：学习分享有效的深度多任务学习”（2020）[2]。研究人员的主要目标是为MTL指定单层架构，并培训一个策略，该策略确定要通过多个任务共享哪些层，这些层将用于特定任务以及哪些图层在确保模型时跳过所有任务的图层以最紧凑的形式提供最高性能。</p><p>  Method: The method proposed by the authors jointly optimizes the network parameters and a binary random variable  ul,k for each layer l and each task in  Tk, where  Tk represents a set of k tasks. Here, the binary random variable or policy represents which tasks are shared, skipped, or done individually by a particular block for multiple tasks. Since the policy variable is non-differentiable, the Gumbel-Softmax sampling [3] approach is used to  optimize it. The loss function proposed in the paper is the sum of the task-specific losses, a sparsity loss to encourage model compactness, and a sharing loss that encourages block sharing across tasks.</p><p>  方法：作者提出的方法共同优化了网络参数和二进制随机变量UL，对于每个层L和TK中的每个任务，其中TK代表一组K任务。这里，二进制随机变量或策略表示由特定块进行共享，跳过或单独完成多个任务的任务。由于策略变量是不可差异的，因此Gumbel-SoftMax采样[3]方法用于优化它。本文提出的损失函数是任务特定损失的总和，鼓励模型紧凑的稀疏性损失以及鼓励跨任务分享的共享损失。</p><p> Limitations: The main limitation of the proposed method is that it requires the model to have skip connections between layers. While such architectures have been used in prior work (e.g. ResNets), the proposed approach cannot directly be generalized to other network architectures.</p><p> 限制：所提出的方法的主要限制是它需要模型在图层之间跳过连接。虽然这些架构已经在事先工作（例如Resnets）中使用，但是所提出的方法不能直接推广到其他网络架构。</p><p>  Motivation: In order to do MTL effectively, a network needs to share related information from the input features between tasks, while also balancing the learning rates of individual tasks. In “ End-to-End Multi-Task Learning with Attention” [4], S. Liu et al. introduce a unified approach which employs both task sharing and task balancing schemes in the learning process.</p><p>  动机：为了有效地进行MTL，网络需要与任务之间的输入功能共享相关信息，同时平衡各个任务的学习率。在“注意到与注意力的结束多任务学习”[4]，刘等人。介绍一个统一的方法，可以在学习过程中使用任务共享和任务平衡方案。</p><p>  Method:  This approach proposed by the authors divides a neural network architecture into two parts. The first part are standard shared layers trained on all of the features and tasks. Following these shared layers, a soft-attention mechanism collects task-specific features from the shared layers, allowing the tasks to be learned end-to-end in a self-supervised fashion. In other words, these attention modules act as feature selectors for each specific task, which are fed into the second, task-specific part of the network.</p><p>  方法：作者提出的这种方法将神经网络架构划分为两部分。第一部分是在所有功能和任务中培训的标准共享图层。在这些共享层之后，软关注机制从共享层收集特定于任务特征，允许以自我监督的方式将任务学习到结束。换句话说，这些注意力模块作为每个特定任务的特征选择器，它被馈送到网络的第二个任务专用部分中。</p><p> In addition, to balance the learning rates of different tasks, the authors also propose a “dynamic weight averaging” technique.  At the very beginning, for the first two iterations of training, the weight for each task’s loss is initialized to 1. After each iteration, the weights are first adjusted to be the ratio of the losses over the previous two iterations for that task, and then soft-maxed so that they are between 0 and 1. With this technique, the weights adapt so that the tasks that are hardest to learn are given more weight during training.</p><p> 此外，为了平衡不同任务的学习率，作者还提出了一种“动态重量平均”技术。在一开始，对于训练的前两个迭代，每个任务丢失的权重初始化为1.在每次迭代之后，首先将权重调整为对该任务的前两次迭代中的损耗与损失的比率。然后软最大化使它们在0和1之间。通过这种技术，权重适应，使得最难学习的任务在训练期间给予更多重量。 </p><p> Limitations: Although this method is seemingly effective, the experiments run by the authors of this paper (as well as the other two papers we review) are mostly limited to a small number of computer vision tasks. Although the authors try the method on one dataset with up to 10 tasks, the results are not compared with other state-of-the-art MTL methods. Further evaluation is needed to understand how this method scales with increasing number and diversity of tasks.</p><p>限制：虽然这种方法看似有效，但是本文作者经营的实验（以及我们审查的其他两篇论文）主要限于少数计算机视觉任务。虽然作者在一个数据集上尝试了最多10个任务的方法，但结果与其他最先进的MTL方法没有比较。需要进一步评估来了解如何在越来越多的任务数量和多样性的情况下缩放。</p><p>  Motivation: In  this paper by Trevor Standley et al. [5], the authors consider not only how to group tasks together for MTL, but also explore how much computational budget should be assigned to each group of tasks. The authors introduce a new learning framework for multi-task learning which maximizes the performance on the tasks within a given computational budget.</p><p>  动机：在Trevor Standley等中的本文中。 [5]，作者不仅考虑如何将任务组合在一起，还要探讨应为每组任务分配多少计算预算。作者为多任务学习推出了一个新的学习框架，最大化了给定的计算预算中的任务的性能。</p><p>  Method: To study the relationship between the sets of tasks, the authors carry out empirical studies of model performance in different settings on the Taskonomy dataset [6]. The results of the study highlight the influence of network capacity, auxiliary tasks, and the amount of training data on the task relationships and overall MTL performance.</p><p>  方法：要研究任务集之间的关系，作者对Taskomaty DataSet的不同设置进行了模型性能的实证研究[6]。该研究的结果突出了网络容量，辅助任务以及培训数据的影响以及整体MTL性能的影响。</p><p> Based on the results of the study, the authors propose three techniques for MTL: optimal solution (OS), early stopping approximation (ESA) and higher order approximation (HOA). The first approach is based on the branch-and-bound algorithm, which uses a combinatorial approach to choose the optimal solution in the space of all the fully-trained network-task combinations, based on performance and inference time.</p><p> 基于该研究的结果，作者提出了三种MTL：最佳解决方案（OS），早期停止近似（ESA）和更高阶近似（HOA）的技术。第一种方法是基于分支和绑定算法，它使用组合方法基于性能和推理时间选择所有完全训练的网络任务组合的空间中的最佳解决方案。</p><p> Since OS can take a significant amount of time to run, the latter approaches are faster approximations. ESA reduces runtime by estimating the task relationships using results from the early stage of training and then training the chosen network configuration until convergence. HOA calculates per-task loss estimates (based on the individual tasks) and uses this estimate to approximate the performance of network configurations.</p><p> 由于OS可以花费大量运行，因此后一种方法越快近似。 ESA通过从训练早期阶段估计任务关系来减少运行时，然后培训所选择的网络配置直到收敛。 HOA计算每个任务丢失估计（基于各个任务），并使用此估计来近似网络配置的性能。</p><p> Limitations: Since the optimal solution is based on the branch-and-bound algorithm, it has a runtime that may be infeasible as the number of tasks increases. In ESA, the correlation between the early training and final training performance is not necessarily high and hence gives misleading results for the task relationships. HOA completely ignores task interactions and nonlinear effects associated with grouping tasks together. As a consequence, both ESA and HOA suffer a degradation in prediction performance.</p><p> 限制：由于最佳解决方案基于分支和绑定算法，因此它具有运行时可能是不可行的，因为任务数量增加。在ESA中，早期培训和最终培训表现之间的相关性并不一定高，因此为任务关系提供了误导性结果。 HOA完全忽略与分组任务一起关联的任务交互和非线性效果。因此，ESA和HOA都遭受预测性能的降级。</p><p>  Because of the increased importance of multi-task learning, a large number of methods have been proposed to automatically learn which tasks should be jointly learned. However, these methods have not been exhaustively evaluated, especially on large numbers of tasks and on domains outside of computer vision. New methods may be needed to scale these methods to tens or hundreds of tasks and on other domains where multitasking is important, such as natural language processing and biomedical data.</p><p>  由于多任务学习的重要性增加，已经提出了大量方法来自动了解应该共同学习哪些任务。然而，这些方法尚未被详尽地评估，特别是在大量任务和计算机视觉之外的域名上。可能需要新方法来扩展这些方法到数十或数百个任务以及多任务处理很重要的其他域，例如自然语言处理和生物医学数据。 </p><p>   [1] Alex Kendall, Yarin Gal, Roberto Cipolla (2018). Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018.</p><p>[1] Alex Kendall，Yarin Gal，Roberto Cipolla（2018）。多任务学习利用不确定性来称量场景几何和语义的损失。在计算机愿景和模式识别（CVPR）2018年IEEE会议中。</p><p> [2] Sun, X., Panda, R., Feris, R., &amp; Saenko, K. (2020). Adashare: Learning what to share for efficient deep multi-task learning.  Annual Conference on Neural Information Processing Systems, NeurIPS, December 6-12, 2020.</p><p> [2] Sun，X.，Panda，R.，Feris，R.，＆amp; Saenko，K。（2020）。 Adashare：学习如何分享高效的深度多任务学习。神经信息处理系统年会，神经潮端，12月6日至12日。</p><p> [3] Jang, E., Gu, S., &amp; Poole, B. (2016). Categorical reparameterization with gumbel-softmax.  arXiv preprint arXiv:1611.01144.</p><p> [3] Jang，E.，Gu，S.，＆amp; Poole，B.（2016）。 Gumbel-Softmax的分类Reparameterization。 Arxiv预印迹arxiv：1611.01144。</p><p> [4] Liu, S., Johns, E., &amp; Davison, A. J. (2019). End-to-end multi-task learning with attention. In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 1871-1880).</p><p> [4]刘，S.，约翰，E.，＆amp;达维森，A. J.（2019）。紧随关注的端到端多任务学习。在IEEE / CVF会议上关于计算机愿景和模式识别（PP.1871-1880）的诉讼程序。</p><p> [5] Standley, T., Zamir, A., Chen, D., Guibas, L., Malik, J., &amp; Savarese, S. (2020). Which tasks should be learned together in multi-task learning?. In International Conference on Machine Learning (pp. 9120-9132). PMLR.</p><p> [5] Standley，T.，Zamir，A.，Chen，D.，Guibas，L.，Malik，J.，＆amp; Savarese，S。（2020）。哪个任务应该在多任务学习中学到？在国际机会学习会议中（第9120-9132页）。 PMLR。</p><p> [6] Zamir, A. R., Sax, A., Shen, W. B., Guibas, L. J., Malik, J.,and Savarese, S. Taskonomy: Disentangling task transfer learning.  In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018.</p><p> [6] Zamir，A. R.，SAX，A.，Shen，W. B.，Guibas，L. J.，Malik，J.和Savarese，S. Taskomy：解开任务转移学习。在计算机视觉和模式识别（CVPR）的IEEE会议中。 IEEE，2018。</p><p>  Aminul Huq is a Master&#39;s degree student at Tsinghua University. His research interest lies in computer vision and adversarial machine learning.</p><p>  Aminul Huq是清华大学的硕士学位＆＃39; S学士学位。他的研究兴趣位于计算机视觉和对抗机器学习。 </p><p> Mohammad Hanan Gani is an ML Engineer at Harman International Inc. where he works in the R&amp;D team to build AI solutions to solve the challenging problems in the domain of automation. His research interests lie in the unsupervised deep learning, Few shot learning and Multi-task learning in computer vision.</p><p>Mohammad Hanan Gani是Harman International Inc.的ML工程师，他在R＆amp; D团队中工作，以建立AI解决方案来解决自动化领域的挑战性问题。他的研究兴趣位于无人监督的深度学习，很少有射击学习和计算机愿景中的多任务学习。</p><p> Ammar Sherif is a Teaching and Research Assistant at Nile University. He is doing research related to learning efficiently including topics from Multi-Task Learning and Uncertainty Estimation.</p><p> Ammar Sherif是尼罗大学的教学和研究助理。他正在进行与学习有效的研究，包括来自多任务学习和不确定性估计的主题。</p><p> Abubakar Abid is the CEO/cofounder of Gradio, where he builds tools to explore and explain machine learning models. He also researches applications of machine learning to medicine as a researcher at Stanford University.</p><p> Abubakar Abid是Gradio的首席执行官/ Cofounder，在那里他建造了探索和解释机器学习模型的工具。他还研究了机器学习到医学作为斯坦福大学研究员的应用。</p><p>     Aminul Huq, Mohammad Hanan Gani, Ammar Sherif, Abubakar Abid, &#34;How to Do Multi-Task Learning Intelligently&#34;, The Gradient,  2021.</p><p>     Aminul Huq，Mohammad Hanan Gani，Ammar Sherif，Abubakar Abid，＆＃34;如何智能地进行多任务学习＆＃34;，梯度，2021。</p><p>  @article{aminulmultitask2021, author = {Huq, Aminul and Gani, Mohammad Hanan, and Sherif, Ammar and Abid, Abubakar}, title = {How to Do Multi-Task Learning Intelligently}, journal = {The Gradient}, year = {2021}, howpublished = { https://thegradient.pub/how-to-do-multi-task-learning-intelligently}, }</p><p>  @Article {aminulmultAstask2021，作者= {Huq，Aminul和Gani，Mohammad Hanan和Sherif，Ammar和Abid，Abubakar}，Title = {如何做多任务学习智能}，期刊= {梯度}，年= { 2021}，Hopublished = {https://thegradient.pub/how-to-do-multi-task-learning-intelligpless}，} </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://thegradient.pub/how-to-do-multi-task-learning-intelligently/">https://thegradient.pub/how-to-do-multi-task-learning-intelligently/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/多任务/">#多任务</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/task/">#task</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/任务/">#任务</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>