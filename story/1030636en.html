<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Played解析事务数据</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Played解析事务数据</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-23 02:56:16</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/ce433d97af5491e8cde5ea4d7fcc1a9d.png"><img src="http://img2.diglog.com/img/2020/10/ce433d97af5491e8cde5ea4d7fcc1a9d.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>At Plaid, we link financial accounts to applications, removing the need for our customers to build individual connections to financial institutions and standardizing the data that’s used across multiple accounts. That means developers can focus on creating innovative products and services.</p><p>在Play，我们将金融账户与应用程序联系起来，消除了我们的客户与金融机构建立个人连接的需要，并标准化了跨多个账户使用的数据。这意味着开发者可以专注于创造创新的产品和服务。</p><p> One of the most interesting challenges we face is the aforementioned data standardization, or normalization: how can we simplify data across thousands of different formats to be used in thousands of different ways, and why should we?</p><p>我们面临的最有趣的挑战之一是前面提到的数据标准化或标准化：我们如何简化数千种不同格式的数据，以便以数千种不同的方式使用，我们为什么要这样做？</p><p> The ‘why’ is easy: by changing the value of transaction data from a mere record of financial activity to a building block of deep user understanding, we provide our customers with the ability to glean meaningful insights. In turn, they can help their users make better financial decisions.</p><p>“为什么”很简单：通过将交易数据的价值从单纯的财务活动记录转变为用户深入理解的基础，我们为客户提供了收集有意义的见解的能力。反过来，他们可以帮助用户做出更好的财务决策。</p><p> The ‘how’ is more complex: that’s because the data financial institutions natively provide is messy and convoluted—and far from normalized. A look over your last bank statement will likely leave you pondering at least a few transactions. For example, take the transaction below, displayed via three separate financial institutions:</p><p>“如何”更为复杂：这是因为金融机构本身提供的数据杂乱无章，令人费解--而且远未规范化。看一看你上一次的银行对账单，你可能会考虑至少几笔交易。例如，以下面的交易为例，该交易通过三个独立的金融机构显示：</p><p>  This is a single merchant across just three banks. Imagine millions of merchants across thousands of banks. How can we normalize this?</p><p>这是只有三家银行的单一商家。想象一下，数以千计的商家横跨数千家银行。我们怎么才能使这件事正常化呢？</p><p>  Location parsing and merchant parsing are the two most important and impactful challenges we face when enriching transaction descriptions. They are examples of a common research topic in machine learning, named-entity recognition (NER), in which we seek to locate named entities in unstructured text and classify them into predefined categories (in our case, location and merchant). Here’s an example:</p><p>在丰富交易描述时，位置解析和商家解析是我们面临的两个最重要和最有影响力的挑战。它们是机器学习中一个常见的研究主题-命名实体识别(NER)的示例，在NER中，我们寻求在非结构化文本中定位命名实体，并将它们分类到预定义的类别(在我们的情况下，位置和商家)。这里有一个例子：</p><p>  Although NER problems are often solved with machine learning approaches, a complex model might not be needed for every instance. For example, a transaction description like the one above immediately signals a McDonald’s transaction made in New York. That means we can skip the computationally intensive model and directly return the result. To achieve this, we built a light fuzzy matching algorithm to extract the location and merchant information directly from the transaction description.</p><p>虽然NER问题通常用机器学习方法来解决，但并不是每个实例都需要复杂的模型。例如，上面这样的交易描述会立即发出麦当劳在纽约进行交易的信号。这意味着我们可以跳过计算密集型模型，直接返回结果。为此，我们构建了一种轻型模糊匹配算法，直接从交易描述中提取位置和商家信息。</p><p> Unfortunately, not all location strings appear in the tidy format above, and there will always be new merchants that we’ve not previously seen. Here’s an example:</p><p>不幸的是，并不是所有的位置字符串都以上面的整洁格式出现，而且总会有我们以前没有见过的新商家。这里有一个例子：</p><p>  Let’s say this is a new restaurant called SAPPS, which just opened in Bedminster, NJ. The Naive Location Matcher would not be able to determine that Bedminster, NJ represents a location since the token ‘BEDMINSTER’ is concatenated with a numeric string, and the string ‘US’ is also appended to ‘NJ’. Moreover, since SAPPS is a new business not included in our merchant dataset, we’re unable to identify it as the merchant.</p><p>假设这是一家名为Sapps的新餐厅，刚刚在新泽西州贝德明斯特开业。朴素的位置匹配器将无法确定新泽西州贝德明斯特代表一个位置，因为标记‘BEDMINSTER’与数字字符串连接，字符串‘US’也附加到‘NJ’。此外，由于Sapps是一项新业务，没有包括在我们的商家数据集中，我们无法将其识别为商家。</p><p> Plaid has therefore developed a solution to tackle these NER challenges using a language model and bidirectional long short-term memory.</p><p>因此，Play开发了一种解决方案，使用语言模型和双向长期短期记忆来应对这些NER挑战。</p><p>  A statistical  language model is a probability distribution over sequences of words, in which the model assigns a probability to each word or token in a given sequence. More practically, it encodes the internal meaning of a word with the information contained in its neighbors. There are two major categories of language model approaches:</p><p>统计语言模型是单词序列上的概率分布，其中该模型为给定序列中的每个单词或标记分配概率。更实际的是，它用相邻单词中包含的信息来编码单词的内部含义。有两大类语言模型方法：</p><p> Masked language model (MLM) approaches, which predict a [MASK] token using all tokens in a sentence</p><p>掩码语言模型(MLM)方法，它使用句子中的所有标记来预测[掩码]标记。</p><p> Typically the MLM approach works better for natural language understanding tasks (e.g. named-entity recognition, text classification), while the AR approach performs well for language generation tasks due to its sequential nature. Using the [MASK] token in MLM enables us to model the meaning of a word using all the surrounding words save for the word itself (otherwise, the model would learn each word from its own embeddings and ignore the contextual information).</p><p>通常，MLM方法更适合于自然语言理解任务(例如，命名实体识别、文本分类)，而AR方法由于其顺序性，对于语言生成任务执行得更好。在MLM中使用[MASK]标记使我们能够使用除单词本身之外的所有周围单词对单词的含义进行建模(否则，模型将从其自己的嵌入中学习每个单词，而忽略上下文信息)。</p><p> Plaid uses an MLM model similar to BERT (Bidirectional Encoder Representations from Transformers) to help tackle the location and merchant parsing problem. BERT ( BERT Paper) is one of the most well known and high-performing masked language representation models and is designed to pre-train deep bidirectional representations of natural language by using Transformer Encoders to encode contextual information of the input sequences.</p><p>Play使用类似于BERT(来自Transformers的双向编码器表示)的MLM模型来帮助解决位置和商家解析问题。BERT(Bert Paper)是最广为人知的高效掩蔽语言表示模型之一，旨在通过使用Transform Encoders对输入序列的上下文信息进行编码来预先训练自然语言的深层双向表示。</p><p> Before we dive deeper into the language model, let’s take a small detour to explore the idea behind Transformer Encoders, which are a vital component of the BERT model.</p><p>在我们更深入地研究语言模型之前，让我们先绕个小弯路，探索一下Transformer Encoders背后的思想，它是BERT模型的一个重要组件。</p><p>  The Transformer architecture was proposed in the paper  Attention is All You Need, and is essentially a Sequence-to-Sequence (Seq2Seq) encoder combined with an Attention Mechanism.</p><p>Transformer架构是在“注意就是您所需要的一切”一文中提出的，本质上是一个结合了注意机制的Sequence-to-Sequence(Seq2Seq)编码器。</p><p> A Seq2Seq encoder takes in a sequence of items (in our case, words) and outputs another sequence in which each item is encoded with the information from the surrounding items. The Attention Mechanism helps to decide which other item(s) in the sequence are important, while encoding and understanding the information of a specific item. Take the following sentence: Jack won the championship and he felt so proud of it. The Attention Mechanism would understand that he refers to the person Jack and therefore assign more significant attention to the token Jack.</p><p>Seq2Seq编码器接收一个项目序列(在我们的例子中是单词)，并输出另一个序列，在该序列中，每个项目都用周围项目的信息进行编码。注意机制在对特定项目的信息进行编码和理解的同时，帮助确定序列中的哪些其他项目是重要的。以下面这句话为例：杰克赢得了冠军，他为此感到非常自豪。注意机制将理解他指的是人杰克，因此将更重要的注意力分配给令牌杰克。</p><p>    Let’s use the transaction description ‘McDonald’s New York NY’ to illustrate the model’s behavior at a high level. The model would:</p><p>让我们使用交易描述“麦当劳纽约纽约”从较高的层面来说明该模型的行为。该模型将：</p><p> Tokenize the transaction description.  [‘McDonald’s’, ‘New’, ‘York’, ‘NY’]</p><p>将交易描述标记化。[‘麦当劳’，‘纽约’]。</p><p>  3.  Encode the embeddings input with contextual information through a set of 		 	 Transformer Encoder layers.</p><p>3.利用上下文信息通过一组Transform Encoding层对嵌入输入进行编码。</p><p>       The trained MLM is effectively a Seq2Seq encoder that takes in a textual sequence and emits another sequence. Each element of the latter is encoded with the information of its surrounding elements.</p><p>经过训练的MLM实际上是一个接受文本序列并发出另一个序列的Seq2Seq编码器。后者的每个元素用其周围元素的信息进行编码。</p><p> Because the MLM is an unsupervised learning approach, we’re not limited by the amount of labeled data when building the model. By feeding it the sea of Plaid-managed transactions, we end up with a language model embedded with the meaning of transaction descriptions.</p><p>因为MLM是一种无监督的学习方法，所以我们在建立模型时不受标签数据量的限制。通过向它提供Play-Managed事务的海洋，我们最终得到了一个嵌入了事务描述含义的语言模型。</p><p>  Once the encoded sequences are sent from the MLM, they are fed into a downstream parser to recognize the target entities (merchant / location). For the downstream bidirectional parser, we leveraged the Bidirectional LSTM (long short-term memory) model, a state-of-the-art approach for entity recognition problems. The high-level model structure is as follows:</p><p>一旦编码序列从传销发送，它们就被馈送到下游解析器以识别目标实体(商家/位置)。对于下游的双向解析器，我们利用了双向LSTM(Long Short-Term Memory)模型，这是一种解决实体识别问题的最先进的方法。高级模型结构如下：</p><p>    The  Bidirectional LSTM model is an extension of the Unidirectional LSTM, itself a member of the RNN (Recurrent Neural Network) family. The Unidirectional LSTM is designed to recognize patterns in sequential data, such as time series and human language. It does so by extending the contextual meaning of the preceding text into a target word. Bidirectional LSTMs go one step further, by understanding contextual information both forwards and backwards, rather than only the former.</p><p>双向LSTM模型是单向LSTM的扩展，它本身就是RNN(递归神经网络)家族的成员。单向LSTM旨在识别序列数据(如时间序列和人类语言)中的模式。它通过将前面文本的上下文含义扩展到目标单词来实现这一点。双向LSTM更进一步，通过向前和向后理解上下文信息，而不仅仅是前者。</p><p> By leveraging the Bidirectional LSTM framework, we effectively train two separate LSTM neural networks—one that takes the original copy of the text sequence and the other that takes the reversed copy—and eventually aggregate the results together. In this way, each token in the sequence encapsulates the information from both directions. A final prediction can thus be made having a holistic view of the text sequence.</p><p>通过利用双向LSTM框架，我们有效地训练了两个独立的LSTM神经网络-一个采用文本序列的原始副本，另一个采用反向副本-并最终将结果聚合在一起。这样，序列中的每个令牌都封装了来自两个方向的信息。因此，可以在具有文本序列的整体视图的情况下做出最终预测。</p><p>  This combination of our string matching / regex rules and our Neural Networks has yielded promising results for our location and merchant parsing product. As of today, we are able to correctly identify 95% of merchant and location information in transaction descriptions when present.</p><p>我们的字符串匹配/正则表达式规则和我们的神经网络的这种组合已经为我们的位置和商家解析产品产生了令人振奋的结果。到今天为止，我们能够正确识别交易描述中95%的商家和位置信息(如果存在)。</p><p> Moving forward, we’re eager to explore additional improvements, such as how model performance might vary with different sets of hyperparameters or how a combination of character-level CNNs (Convolutional Neural Networks)—known for capturing the semantic information of unfamiliar words—and word embeddings might produce even better results.</p><p>展望未来，我们渴望探索其他改进，例如模型性能如何随不同的超参数集而变化，或者字符级CNN(卷积神经网络)(以捕获陌生单词的语义信息而闻名)与单词嵌入的组合如何产生更好的结果。</p><p> If you’d like to help us find answers to these questions and many others, or if you’re interested in learning more about the ways we use data science to empower financial services, email me directly at  cjin@plaid.com or check out  plaid.com/careers.</p><p>如果您想帮助我们找到这些问题和其他许多问题的答案，或者如果您有兴趣了解更多关于我们使用数据科学增强金融服务能力的方式，请直接给我发电子邮件至cjin@plaid.com或查看plaid.com/areers。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://blog.plaid.com/how-plaid-parses-transaction-data/">https://blog.plaid.com/how-plaid-parses-transaction-data/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/解析/">#解析</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/parses/">#parses</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>