<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习需要大量精力 It takes a lot of energy for machines to learn</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">It takes a lot of energy for machines to learn<br/>机器学习需要大量精力 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-01 08:48:27</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/1/21e735e5c447d0e212be6c0fde13073a.jpeg"><img src="http://img2.diglog.com/img/2021/1/21e735e5c447d0e212be6c0fde13073a.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>This month, Google forced out a prominent AI ethics researcher after she voiced frustration with the company for making her  withdraw a research paper. The paper pointed out the risks of language-processing artificial intelligence, the type used in Google Search and other text analysis products.</p><p>本月，谷歌因对该公司撤回研究论文表示不满，迫使该公司退出了一位著名的AI伦理研究人员。本文指出了语言处理人工智能的风险，在Google搜索和其他文本分析产品中使用的类型。</p><p> Among the risks is the large carbon footprint of developing this kind of AI technology.  By some estimates, training an AI model generates as much carbon emissions as it takes to build and drive five cars over their lifetimes.</p><p> 开发此类AI技术的巨大风险是其中的风险。据一些估计，训练AI模型产生的碳排放量与在其生命周期内制造和驾驶五辆汽车所需的碳排放量一样多。</p><p> I am a researcher who  studies and develops AI models, and I am all too familiar with the skyrocketing energy and financial costs of AI research. Why have AI models become so power hungry, and how are they different from traditional data center computation?</p><p> 我是一名研究和开发AI模型的研究员，而且我对AI研究的飞速发展的能源和财务成本非常熟悉。为什么AI模型变得如此耗电，它们与传统数据中心计算有何不同？</p><p>  Traditional data processing jobs done in data centers include video streaming, email and social media. AI is more computationally intensive because it needs to read through lots of data until it learns to understand it – that is, is trained.</p><p>  在数据中心完成的传统数据处理工作包括视频流，电子邮件和社交媒体。 AI的计算量更大，因为它需要通读大量数据，直到学会理解它为止，即经过培训。</p><p> This training is very inefficient compared to how people learn. Modern AI uses  artificial neural networks, which are mathematical computations that mimic neurons in the human brain. The strength of connection of each neuron to its neighbor is a parameter of the network called weight. To learn how to understand language, the network starts with random weights and adjusts them until the output agrees with the correct answer.</p><p> 与人们的学习方式相比，这种培训效率很低。现代AI使用人工神经网络，这是模拟人脑神经元的数学计算。每个神经元与其邻居的连接强度是网络的一个参数，称为权重。为了学习如何理解语言，网络从随机权重开始并进行调整，直到输出同意正确答案为止。</p><p>  A common way of training a language network is by feeding it lots of text from websites like Wikipedia and news outlets with some of the words masked out, and asking it to guess the masked-out words. An example is “my dog is cute,” with the word “cute” masked out. Initially, the model gets them all wrong, but, after many rounds of adjustment, the connection weights start to change and pick up patterns in the data. The network eventually becomes accurate.</p><p>  训练语言网络的一种常见方法是，从Wikipedia和新闻媒体等网站向其提供大量文本，并在其中掩盖一些单词，并要求其猜测被掩盖的单词。一个例子是“我的狗很可爱”，而“可爱”一词被遮盖了。最初，该模型将它们全部弄错了，但是，经过多轮调整后，连接权重开始改变并拾取数据中的模式。网络最终变得准确。</p><p> One  recent model called Bidirectional Encoder Representations from Transformers (BERT) used 3.3 billion words from English books and Wikipedia articles. Moreover, during training BERT read this data set not once, but 40 times. To compare, an average child learning to talk might hear 45 million words by age five, 3,000 times fewer than BERT.</p><p> 最近的一种名为“变压器双向编码器表示（BERT）”的模型使用了英语书籍和维基百科文章中的33亿个单词。此外，在训练期间BERT读取此数据集的次数不是一次，而是40次。相比之下，到五岁时，平均一个学习说话的孩子可能会听到4500万个单词，比BERT少3000倍。 </p><p>  What makes language models even more costly to build is that this training process happens many times during the course of development. This is because researchers want to find the best structure for the network – how many neurons, how many connections between neurons, how fast the parameters should be changing during learning and so on. The more combinations they try, the better the chance that the network achieves a high accuracy. Human brains, in contrast, do not need to find an optimal structure – they come with a prebuilt structure that has been honed by evolution.</p><p>使语言模型的构建成本更高的原因是，这种培训过程在开发过程中会发生很多次。这是因为研究人员希望找到网络的最佳结构-多少个神经元，神经元之间的连接数，学习过程中参数更改的速度等等。他们尝试的组合越多，网络获得高精度的机会就越大。相比之下，人脑不需要找到最佳结构-它们带有经过进化而磨练的预构建结构。</p><p> As companies and academics compete in the AI space, the pressure is on to improve on the state of the art. Even achieving a 1% improvement in accuracy on difficult tasks like machine translation is considered significant and leads to good publicity and better products. But to get that 1% improvement, one researcher might train the model thousands of times, each time with a different structure, until the best one is found.</p><p> 随着公司和学术界在AI领域竞争，不断提高技术水平的压力越来越大。即使在诸如机器翻译之类的艰巨任务上，即使将精度提高1％也被认为是重要的，并且可以带来良好的宣传和更好的产品。但是要获得1％的改进，一位研究人员可能会对模型进行数千次训练，每次使用不同的结构，直到找到最佳模型为止。</p><p> Researchers at the University of Massachusetts Amherst  estimated the energy cost of developing AI language models by measuring the power consumption of common hardware used during training. They found that training BERT once has the carbon footprint of a passenger flying a round trip between New York and San Francisco. However, by searching using different structures – that is, by training the algorithm multiple times on the data with slightly different numbers of neurons, connections and other parameters – the cost became the equivalent of 315 passengers, or an entire 747 jet.</p><p> 麻省大学阿默斯特分校的研究人员通过测量训练期间使用的通用硬件的功耗来估算开发AI语言模型的能源成本。他们发现，训练BERT曾经具有乘飞机往返纽约和旧金山之间的乘客的碳足迹。但是，通过使用不同的结构进行搜索（即通过对神经元，连接和其他参数的数量略有不同的数据多次训练算法），成本相当于315名乘客或一架整个747飞机。</p><p>  AI models are also much bigger than they need to be, and growing larger every year. A more recent language model similar to BERT,  called GPT-2, has 1.5 billion weights in its network. GPT-3, which  created a stir this year because of its high accuracy, has 175 billion weights.</p><p>  AI模型也比它们所需的要大得多，并且每年都在增长。与BERT类似的最新语言模型称为GPT-2，其网络中的权重为15亿。 GPT-3由于其高精度而在今年引起轰动，它的重量为1750亿磅。</p><p> Researchers discovered that having larger networks leads to better accuracy, even if only a tiny fraction of the network ends up being useful. Something similar happens in children’s brains when  neuronal connections are first added and then reduced, but the biological brain is much more energy efficient than computers.</p><p> 研究人员发现，即使只有一小部分网络最终有用，拥有更大的网络也会带来更高的准确性。当先添加然后减少神经元连接时，儿童的大脑也会发生类似的情况，但是生物大脑比计算机更节能。</p><p> AI models are trained on specialized hardware like graphics processor units, which draw more power than traditional CPUs. If you own a gaming laptop, it probably has one of these graphics processor units to create advanced graphics for, say, playing Minecraft RTX. You might also notice that they generate a lot more heat than regular laptops.</p><p> AI模型是在专用硬件（例如图形处理器单元）上训练的，该硬件比传统CPU消耗更多功率。如果您拥有游戏笔记本电脑，则它可能具有这些图形处理器单元之一，可以为玩Minecraft RTX创建高级图形。您可能还会注意到，它们产生的热量比普通笔记本电脑多得多。</p><p> All of this means that developing advanced AI models is adding up to a large carbon footprint. Unless we switch to 100% renewable energy sources, AI progress may stand at odds with the goals of cutting greenhouse emissions and slowing down climate change. The financial cost of development is also becoming so high that only a few select labs can afford to do it, and they will be the ones to set the agenda for what kinds of AI models get developed.</p><p> 所有这些意味着开发先进的AI模型将增加大量的碳足迹。除非我们转向100％可再生能源，否则AI的进展可能与减少温室气体排放和减缓气候变化的目标相抵触。开发的财务成本也变得如此之高，以至于只有少数选定的实验室能够负担得起，而这些实验室将成为制定哪种AI模型的议程。 </p><p> [ The Conversation’s science, health and technology editors pick their favorite stories.  Weekly on Wednesdays.]</p><p>[对话的科学，健康和技术编辑选择了他们喜欢的故事。每周三。]</p><p>  What does this mean for the future of AI research? Things may not be as bleak as they look. The cost of training might come down as more efficient training methods are invented. Similarly, while data center energy use was predicted to explode in recent years, this has not happened due to improvements in data center efficiency, more efficient hardware and cooling.</p><p>  这对人工智能研究的未来意味着什么？事情可能不像看上去那样凄凉。随着发明更有效的培训方法，培训成本可能会下降。同样，尽管近年来预计数据中心的能源使用量会激增，但由于数据中心效率的提高，更高效的硬件和冷却功能的出现，这种情况并未发生。</p><p> There is also a trade-off between the cost of training the models and the cost of using them, so spending more energy at training time to come up with a smaller model might actually make using them cheaper. Because a model will be used many times in its lifetime, that can add up to large energy savings.</p><p> 在训练模型的成本和使用模型的成本之间也需要权衡取舍，因此在训练时花费更多的精力来想出一个较小的模型实际上可能会使使用它们更便宜。由于模型在其生命周期内将被多次使用，因此可以节省大量能源。</p><p> In  my lab’s research, we have been looking at ways to make AI models smaller by sharing weights, or using the same weights in multiple parts of the network. We call these  shapeshifter networks because a small set of weights can be reconfigured into a larger network of any shape or structure. Other researchers have shown that weight-sharing  has better performance in the same amount of training time.</p><p> 在我实验室的研究中，我们一直在研究通过共享权重或在网络的多个部分使用相同权重来缩小AI模型的方法。我们称这些为Shapeshifter网络是因为一小组权重可以重新配置为任何形状或结构的更大网络。其他研究人员表明，在相同的训练时间下，负重共享具有更好的表现。</p><p> Looking forward, the AI community should invest more in developing energy-efficient training schemes. Otherwise, it risks having AI become dominated by a select few who can afford to set the agenda, including what kinds of models are developed, what kinds of data are used to train them and what the models are used for.</p><p> 展望未来，人工智能界应该在开发节能培训计划上投入更多。否则，有可能使AI受到少数人的支配，这些人无力承担制定议程的任务，其中包括开发了哪些模型，使用了哪些数据来训练它们以及使用了哪些模型。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://theconversation.com/it-takes-a-lot-of-energy-for-machines-to-learn-heres-why-ai-is-so-power-hungry-151825">https://theconversation.com/it-takes-a-lot-of-energy-for-machines-to-learn-heres-why-ai-is-so-power-hungry-151825</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/lot/">#lot</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>