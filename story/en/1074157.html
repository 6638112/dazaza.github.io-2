<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习算法备忘单Machine Learning Algorithms Cheat Sheet</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Machine Learning Algorithms Cheat Sheet<br/>机器学习算法备忘单</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-20 10:36:51</div><div class="page_narrow text-break page_content"><p>Machine learning is a subfield of artificial intelligence (AI) and computer science that focuses on using data and algorithms to mimic the way people learn, progressively improving its accuracy. This way, Machine Learning is one of the most interesting methods in Computer Science these days, and it&#39;s being applied behind the scenes in products and services we consume in everyday life.</p><p>机器学习是人工智能（AI）和计算机科学的一个分支，其重点是使用数据和算法模拟人们的学习方式，逐步提高其准确性。这样一来，机器学习是当今计算机科学中最有趣的方法之一，它&#39；它在我们日常生活中消费的产品和服务中的幕后应用。</p><p> In case you want to know what Machine Learning algorithms are used in different applications, or if you are a developer and you’re looking for a method to use for a problem you are trying to solve, keep reading below and use these steps as a guide.</p><p>如果你想知道在不同的应用程序中使用了什么机器学习算法，或者如果你是一名开发人员，并且正在寻找一种方法来解决你试图解决的问题，请继续阅读下面的内容，并将这些步骤作为指导。</p><p>  Machine Learning can be divided into three different types of learning: Unsupervised Learning, Supervised Learning, and Semi-supervised Learning.</p><p>机器学习可以分为三种不同类型的学习：无监督学习、监督学习和半监督学习。</p><p> Unsupervised learning uses information data that is not labeled, that way the machine should work with no guidance according to patterns, similarities, and differences.</p><p>无监督学习使用未标记的信息数据，这样机器就可以根据模式、相似性和差异在没有指导的情况下工作。</p><p> On the other hand,  supervised learning has a presence of a “teacher”, who is in charge of training the machine by labeling the data to work with. Next, the machine receives some examples that allow it to produce a correct outcome.</p><p>另一方面，监督学习有一位“老师”，负责通过标记要处理的数据来训练机器。接下来，机器会收到一些示例，使其能够产生正确的结果。</p><p> But there’s a hybrid approach for these types of learning, this  Semi-supervised learning works with both labeled and unlabeled data. This method uses a tiny data set of labeled data to train and label the rest of the data with corresponding predictions, finally giving a solution to the problem.</p><p>但是对于这些类型的学习有一种混合方法，这种半监督学习同时适用于标记和未标记的数据。这种方法使用一个由标记数据组成的小数据集来训练其余的数据，并用相应的预测来标记，最后给出问题的解决方案。</p><p> To begin, you need to know the number of dimensions you’re working with, it means the number of inputs in your problem (also known as features). If you’re working with a large dataset or many features, you can opt for a Dimension Reduction algorithm.</p><p>首先，你需要知道你正在处理的维度的数量，这意味着你的问题中输入的数量（也称为特征）。如果您使用的是大型数据集或许多功能，可以选择降维算法。</p><p>   A large number of dimensions in a data collection can have a significant influence on machine learning algorithms&#39; performance. The &#34;curse of dimensionality&#34; is a term used to describe the troubles large dimensionality might cause, for example, the “Distance Concentration” problem in Clustering, where the different data points will have the same value as the dimensionality of the data increases.</p><p>数据收集中的大量维度会对机器学习算法产生重大影响#39；表演34岁；维度诅咒&#34；是一个用于描述大维度可能导致的问题的术语，例如，聚类中的“距离集中”问题，当数据的维度增加时，不同的数据点将具有相同的值。</p><p> Techniques for minimizing the number of input variables in training data are referred to as “Dimension Reduction”.</p><p>将训练数据中输入变量的数量最小化的技术称为“降维”。</p><p>  Now you need to be familiar with the concept of Feature Extraction and Feature Selection to keep going. The process of translating raw data into numerical features that can be processed while keeping the information in the original data set is known as  feature extraction. It produces better outcomes than applying machine learning to raw data directly.</p><p>现在，您需要熟悉特征提取和特征选择的概念才能继续。将原始数据转换为数字特征的过程称为特征提取，这些特征可以被处理，同时将信息保留在原始数据集中。它比直接将机器学习应用于原始数据产生更好的结果。</p><p> It’s used for three known algorithms for dimensionality reduction including Principal Component Analysis, Singular Value Decomposition, and Linear  Discriminant Analysis, but you need to know exactly which tool you want to use to find patterns or infer new information from the data.</p><p>它用于三种已知的降维算法，包括主成分分析、奇异值分解和线性判别分析，但您需要确切知道要使用哪个工具来查找模式或从数据中推断新信息。</p><p> If you’re not looking to combine the variables of your data, instead you want to remove unneeded features by just keeping the important ones, then you can use the Principal Component Analysis algorithm.</p><p>如果您不希望组合数据的变量，而是希望通过保留重要的特征来删除不需要的特征，那么您可以使用主成分分析算法。</p><p>   It&#39;s a mathematical algorithm for reducing the dimension of data sets to simplify the number of variables while retaining most of the information. This trade-off of accuracy for simplicity is extensively used to find patterns in large data sets.</p><p>它&#39；这是一种数学算法，用于降低数据集的维数，以简化变量的数量，同时保留大部分信息。这种精确性与简单性之间的权衡被广泛用于在大型数据集中寻找模式。</p><p>  In terms of linear connections, it has a wide range of applications when large amounts of data are present, such as media editing, statistical quality control, portfolio analysis, and in many applications such as face recognition and image compression.</p><p>在线性连接方面，当存在大量数据时，它有着广泛的应用，例如媒体编辑、统计质量控制、投资组合分析，以及在许多应用中，例如人脸识别和图像压缩。</p><p> Alternatively, if you want an algorithm that works by combining variables of the data you’re working with, a simple PCA may not be the best tool for you to use. Next, you can have a probabilistic model or a non-probabilistic one. Probabilistic data is data that involves a random selection and is preferred by most scientists for more accurate results. While non-probabilistic data doesn’t involve that randomness.</p><p>或者，如果你想要一个算法，通过组合你正在使用的数据变量来工作，一个简单的PCA可能不是你使用的最佳工具。接下来，您可以使用概率模型或非概率模型。概率数据是指涉及随机选择的数据，大多数科学家更喜欢这种数据，以获得更准确的结果。而非概率数据并不包含这种随机性。</p><p> If you are working with non-probabilistic data, you should use the Singular Value Decomposition algorithm.</p><p>如果使用的是非概率数据，则应使用奇异值分解算法。</p><p>  In the realm of machine learning, SVD allows data to be transformed into a space where categories can be easily distinguished. This algorithm decomposes a matrix into three different matrices. In image processing, for example, a reduced number of vectors are used to rebuild a picture that is quite close to the original.</p><p>在机器学习领域，SVD允许将数据转换为一个易于区分类别的空间。该算法将一个矩阵分解为三个不同的矩阵。例如，在图像处理中，使用减少的向量数来重建非常接近原始图像的图片。</p><p>  Compared with the PCA algorithm, both can make a dimension reduction of the data. But while PCA skips the less significant components, the SVD just turns them into special data, represented as three different matrices, that are easier to manipulate and analyze.</p><p>与PCA算法相比，两者都可以对数据进行降维。但是，虽然主成分分析忽略了不太重要的成分，但奇异值分解只是将它们转换为特殊数据，以三种不同的矩阵表示，更易于操作和分析。</p><p> When it comes to probabilistic approaches, it’s better to use the Linear Discriminant Analysis algorithm for more abstract problems.</p><p>对于概率方法，最好使用线性判别分析算法来解决更抽象的问题。</p><p>  Linear Discriminant Analysis (LDA) is a classification approach in which two or more groups have previously been identified, and fresh observations are categorized into one of them based on their features. It’s different from PCA since LDA discovers a feature subspace that optimizes group separability while the PCA ignores the class label and focuses on capturing the dataset&#39;s highest variance direction.</p><p>线性判别分析（LDA）是一种分类方法，在这种方法中，两个或两个以上的组之前已经被识别，新的观察结果根据其特征被分类为其中一个组。它不同于PCA，因为LDA发现了一个优化组可分性的特征子空间，而PCA忽略了类标签，专注于捕获数据集#39；最大方差方向。</p><p> This algorithm uses Bayes’ Theorem, a probabilistic theorem used to determine the likelihood of an occurrence based on its relationship to another event. It is frequently used in face recognition, customer identification, and medical fields to identify the patient’s disease status.</p><p>该算法使用贝叶斯定理，这是一个概率定理，用于根据事件与另一事件的关系确定事件发生的可能性。它经常用于人脸识别、客户识别和医疗领域，以识别患者的疾病状态。</p><p>  The next step is to select whether or not you want your algorithm to have responses, which means if you want to develop a predictive model based on labeled data to teach your machine. You may use the Clustering techniques if you’d rather use non-labeled data so your machine can work with no guidance and search for similarities.</p><p>下一步是选择是否希望算法有响应，这意味着您是否希望基于标记数据开发预测模型来教授机器。如果您希望使用未标记的数据，那么您可以使用聚类技术，这样您的机器就可以在没有指导的情况下工作并搜索相似性。</p><p> On the other hand, the process of picking a subset of relevant features (variables, predictors) for use in model creation is known as feature selection. It helps in the simplicity of models to make them easier to comprehend for researchers and users, as well as the reduction of training periods and the avoidance of the dimensionality curse.</p><p>另一方面，选择相关特征子集（变量、预测值）用于模型创建的过程称为特征选择。它有助于简化模型，使研究人员和用户更容易理解模型，减少训练周期，避免维数灾难。</p><p>   Clustering is a technique for separating groups with similar characteristics and assigning them to clusters. If you&#39;re looking for a hierarchical algorithm:</p><p>聚类是一种分离具有相似特征的组并将其分配给簇的技术。如果你&#39；我们正在寻找分层算法：</p><p>  This type of clustering is one of the most popular techniques in Machine Learning. Hierarchical Clustering assists an organization to classify data to identify similarities, and different groupings and features, so their pricing, goods, services, marketing messages, and other aspects of the business are targeted. Its hierarchy should show the data similar to a tree data structure, known as a Dendrogram. There are two ways of grouping the data: agglomerative and divisive.</p><p>这种类型的聚类是机器学习中最流行的技术之一。层次聚类有助于组织对数据进行分类，以确定相似性、不同分组和特征，从而有针对性地确定其定价、商品、服务、营销信息和业务的其他方面。它的层次结构应该显示类似于树数据结构的数据，称为树状图。有两种方法可以对数据进行分组：聚集和分裂。</p><p> Agglomerative clustering is a &#34;bottom-up&#34; approach. To put it another way, each item is first thought of as a single-element cluster (leaf). The two clusters that are the most comparable are joined into a new larger cluster at each phase of the method (nodes). This method is repeated until all points belong to a single large cluster (root).</p><p>凝聚聚类是a&#34；自下而上&#34；方法换句话说，每个项目首先被认为是一个单一元素簇（叶）。在方法的每个阶段（节点），最具可比性的两个簇被合并成一个新的更大的簇。重复此方法，直到所有点都属于单个大簇（根）。</p><p> Divisive clustering works in a “top-down” way. It starts at the root, where all items are grouped in a single cluster, then separates the most diverse into two at each iteration phase. Iterate the procedure until all of the items are in their group.</p><p>分裂聚类以“自上而下”的方式工作。它从根开始，在根中，所有项目都被分组在一个集群中，然后在每个迭代阶段将最多样化的项目分成两个。重复该过程，直到所有项目都在其组中。</p><p> In case you’re not looking for a hierarchical solution, you must determine whether your method requires you to specify the number of clusters to be used. You can utilize the Density-based Spatial Clustering of Applications with Noise algorithm if you don&#39;t need to define it.</p><p>如果您不是在寻找分层解决方案，则必须确定您的方法是否需要指定要使用的集群数量。如果不&#39；不需要定义它。</p><p>   When it comes to arbitrary-shaped clusters or detecting outliers, it’s better to use Density-Based Clustering. DBSCAN is a method for detecting those arbitrary-shaped clusters and the ones with noise by grouping points close to each other based on two parameters: eps and minPoints.</p><p>当涉及到任意形状的聚类或检测异常值时，最好使用基于密度的聚类。DBSCAN是一种通过基于两个参数（eps和minPoints）对彼此接近的点进行分组来检测任意形状的簇和有噪声的簇的方法。</p><p> The eps tells us the distance that needs to be between two points to be considered a cluster. While the minPoints are the minimum number of points to create a cluster. We use this algorithm in the analysis of Netflix Servers outliers. The streaming service runs thousands of servers, and normally less than one percent it’s capable of becoming unhealthy, which degrades the performance of the streaming. The real problem is that this problem isn’t easily visible, to solve it, Netflix uses DBSCAN specifying a metric to be monitored, then collects data and finally is passed to the algorithm for detecting the servers outliers.</p><p>eps告诉我们两个点之间的距离需要被视为一个簇。而最小点是创建簇的最小点数。我们使用该算法分析Netflix服务器的异常值。流媒体服务运行数千台服务器，通常不到1%的服务器会变得不健康，这会降低流媒体的性能。真正的问题是，这个问题不容易被发现，为了解决这个问题，Netflix使用DBSCAN指定要监控的指标，然后收集数据，最后传递给检测服务器异常值的算法。</p><p>  One daily usage can be when e-commerce makes a product recommendation to its customers. Applying DBSCAN on the data of products the user has bought before.</p><p>当电子商务向其客户推荐产品时，可以每天使用一次。在用户之前购买的产品数据上应用DBSCAN。</p><p> In case you need to specify the number of clusters, there are three existing algorithms you could use, including K-Modes, K-Means, and Gaussian Mixture Model. Next, you need to know if you’re going to work with categorical variables, which are discrete variables that capture qualitative consequences by grouping observations (or levels). If you’re going to use them, you may opt for K-Modes.</p><p>如果需要指定簇的数量，可以使用三种现有算法，包括K模式、K均值和高斯混合模型。接下来，您需要知道是否要使用分类变量，这是一种离散变量，通过分组观察（或水平）来捕获定性结果。如果你打算使用它们，你可以选择K模式。</p><p>   This approach is used to group categorical variables. We determine the total mismatches between these types of data points. The fewer the differences between our data points, the more similar they are. The main difference between K-Modes and K-Means is that for categorical data points we can’t calculate the distance since they aren’t numeric values.</p><p>这种方法用于对分类变量进行分组。我们确定这些类型的数据点之间的总不匹配。我们的数据点之间的差异越小，它们就越相似。K-模式和K-均值之间的主要区别在于，对于分类数据点，我们无法计算距离，因为它们不是数值。</p><p> This algorithm is used for text mining applications, document clustering, topic modeling (where each cluster group represents a specific subject), fraud detection systems, and marketing.</p><p>该算法用于文本挖掘应用、文档聚类、主题建模（每个聚类组代表一个特定主题）、欺诈检测系统和营销。</p><p>   Data is clustered into a k number of groups in such a manner that data points in the same cluster are related while data points in other clusters are further apart. This distance is frequently measured with the Euclidean distance. In other words, the K-Means algorithm tries to minimize distances within a cluster and maximize the distance between different clusters.</p><p>数据以这样的方式聚集到k个组中，即同一集群中的数据点相互关联，而其他集群中的数据点相距更远。这种距离通常用欧几里德距离来测量。换句话说，K-Means算法试图最小化簇内的距离，并最大化不同簇之间的距离。</p><p> Search engines, consumer segmentation, spam/ham detection systems, academic performance, defects diagnosis systems, wireless communications, and many other industries use k-means clustering.</p><p>搜索引擎、消费者细分、垃圾邮件/火腿检测系统、学术表现、缺陷诊断系统、无线通信和许多其他行业都使用k均值聚类。</p><p> If the intended result is based on probability, then the Gaussian Mixture Model should be used.</p><p>如果预期结果基于概率，则应使用高斯混合模型。</p><p>  This approach implies the presence of many Gaussian distributions, each of which represents a cluster. The algorithm will determine the probability of each data point belonging to each of the distributions for a given batch of data.</p><p>这种方法意味着存在许多高斯分布，每个分布代表一个簇。该算法将确定属于给定批次数据的每个分布的每个数据点的概率。</p><p> GMM differs from K-means since in GMM we don’t know if a data point belongs to a specified cluster, we use probability to express this uncertainty. While the K-means method is certain about the location of a data point and starts to iterate over the whole data set. The Gaussian Mixture Model is frequently used in signal processing, language recognition, anomaly detection, and genre classification of music.</p><p>GMM不同于K-means，因为在GMM中，我们不知道数据点是否属于特定的簇，我们使用概率来表示这种不确定性。而K-means方法确定了数据点的位置，并开始迭代整个数据集。高斯混合模型常用于信号处理、语言识别、异常检测和音乐体裁分类。</p><p> In the event that you use labeled data to train your machine, first, you need to specify if it is going to predict numbers, this numerical prediction will help the algorithm to solve the problem. In case it does, you can choose Regression Algorithms.</p><p>如果您使用带标签的数据来训练您的机器，首先，您需要指定它是否将预测数字，这种数值预测将帮助算法解决问题。如果有，你可以选择回归算法。</p><p>   Regression is a machine learning algorithm in which the outcome is predicted as a continuous numerical value. This method is commonly used in banking, investment, and other fields.</p><p>回归是一种机器学习算法，其结果被预测为一个连续的数值。这种方法通常用于银行、投资和其他领域。</p><p> Here, you need to decide whether you rather have speed or accuracy. In case you’re looking for speed, you can use a Decision Tree algorithm or a Linear Regression algorithm.</p><p>在这里，你需要决定你更喜欢速度还是准确性。如果你在寻找速度，你可以使用决策树算法或线性回归算法。</p><p>   A decision tree is a flowchart like a tree data structure. Here, the data is continuously split according to a given parameter. Each parameter is allowed in a tree node, while the outcomes of the whole tree are located in the leaves. There are two types of decision trees:</p><p>决策树是类似于树数据结构的流程图。这里，数据根据给定的参数被连续分割。树节点中允许每个参数，而整个树的结果位于叶子中。有两种类型的决策树：</p><p>  When there are intricate interactions between the features and the output variables, decision trees come in handy. When there are missing features, a mix of category and numerical features, or a large variance in the size of features, they perform better in comparison to other methods.</p><p>当特征和输出变量之间存在复杂的交互时，决策树就派上了用场。当存在缺失特征、类别特征和数字特征的混合，或特征大小的巨大差异时，与其他方法相比，它们的性能更好。</p><p> This algorithm is used to enhance the accuracy of promotional campaigns, detection of fraud, and detection of serious or preventable diseases on patients.</p><p>该算法用于提高宣传活动的准确性，检测欺诈行为，以及检测患者身上的严重或可预防疾病。</p><p>  Based on a given independent variable, this method predicts the value of a dependent variable. As a result, this regression approach determines if there is a linear connection between the input (independent variable) and the output (dependent variable). Hence, the term Linear Regression was coined.</p><p>该方法基于给定的自变量，预测因变量的值。因此，这种回归方法确定输入（自变量）和输出（因变量）之间是否存在线性关系。因此，线性回归一词应运而生。</p><p> Linear regression is ideal for datasets in which the features and the output variable have a linear relationship. It&#39;s usually used for forecasting (which is particularly useful for small firms to understand the sales effect), understanding the link between advertising expenditure and revenue, and in the medical profession to understand the correlations between medicine dose and patient blood pressure.</p><p>线性回归是特征和输出变量之间存在线性关系的数据集的理想选择。它&#39；s通常用于预测（这对小公司了解销售效果特别有用），了解广告支出和收入之间的联系，以及在医学界了解药物剂量和患者血压之间的相关性。</p><p> Alternatively, if you need accuracy for your algorithm you can use the following three algorithms: Neural Network, Gradient Boosting Tree, and Random Forest.</p><p>或者，如果你的算法需要精确性，你可以使用以下三种算法：神经网络、梯度提升树和随机森林。</p><p>   A Neural Network is required to learn the intricate non-linear relationship between the features and the target. It’s an algorithm that simulates the workings of neurons in the human brain. There are several types of Neural Networks, including the Vanilla Neural Network (that handles structured data only), as well as Recurrent Neural Network and Convolutional Neural Network which both can work with unstructured data.</p><p>需要一个神经网络来学习特征和目标之间复杂的非线性关系。这是一种模拟人脑神经元工作的算法。有几种类型的神经网络，包括普通神经网络（仅处理结构化数据），以及可处理非结构化数据的递归神经网络和卷积神经网络。</p><p> When you have a lot of data (and processing capacity), and accuracy is important to you, you&#39;ll almost certainly utilize a neural network. This algorithm has many applications, such as paraphrase detection, text classification, semantic parsing, and question answering.</p><p>当你拥有大量数据（和处理能力），并且准确性对你来说很重要时，你&#39；我几乎肯定会使用神经网络。该算法有很多应用，如释义检测、文本分类、语义分析和问答。</p><p>   Gradient Boosting Tree is a method for merging the outputs of separate trees to do regression or classification. Both supervised learning incorporates a large number of decision trees to lessen the danger of overfitting (a statistical modeling mistake that happens when a function is too tightly matched to a small number of data points, making it possible to reduce the predictive power of the model) that each tree confronts alone. This algorithm employs  Boosting, which entails consecutively combining weak learners (typically decision trees with just one split, known as decision stumps) so that each new tree corrects the preceding one&#39;s faults.</p><p>梯度推进树是一种将独立树的输出进行合并以进行回归或分类的方法。这两种监督学习都结合了大量决策树，以减少每棵树单独面对的过度拟合危险（当函数与少量数据点过于紧密匹配时，会发生统计建模错误，从而可能降低模型的预测能力）。该算法采用了Boosting，这需要连续组合弱学习者（通常是只有一个分裂的决策树，称为决策树桩），以便每个新树纠正前一个#39；这是她的缺点。</p><p> When we wish to reduce the Bias error, which is the amount whereby a model&#39;s prediction varies from the target value, we usually employ the Gradient Boosting Algorithm. When there are fewer dimensions in the data, a basic linear model performs poorly, interpretability is not critical, and there is no stringent latency limit, gradient boosting is most beneficial.</p><p>当我们希望减少偏差误差时，即模型#39；s预测值与目标值不同，我们通常采用梯度推进算法。当数据中的维数较少时，基本线性模型的性能较差，可解释性不重要，并且没有严格的延迟限制，梯度增强是最有利的。</p><p> It’s used in many studies, such as a gender prediction algorithm based on the motivation of masters athletes, using gradient boosted decision trees, exploring their capacity to predict gender based on psychological dimensions evaluating reasons to participate in masters sports as statistical methodologies.</p><p>它被用于许多研究中，例如基于大师运动员动机的性别预测算法，使用梯度增强决策树，探索他们基于心理维度预测性别的能力，评估参加大师运动的原因，作为统计方法。</p><p>  Random Forest is a method for resolving regression and classification problems. It makes use of ensemble learning, which is a technique for solving complicated problems by combining several classifiers. It consists of many decision trees, where the outcomes of every one of them will throw the final result taking the average or mean decisions. The greater the number of trees, the better precision of the outcome.</p><p>随机森林是解决回归和分类问题的一种方法。它利用集成学习，这是一种通过组合多个分类器来解决复杂问题的技术。它由许多决策树组成，每一个决策树的结果都会将最终结果转化为平均或平均决策。树的数量越多，结果的精确度就越高。</p><p> Random Forest is appropriate when we have a huge dataset and interpretability is not a key problem, as it becomes increasingly difficult to grasp as the dataset grows larger. This algorithm is used in stock market analysis, diagnosis of patients in the medical field, to predict the creditworthiness of a loan applicant, and in fraud detection.</p><p>当我们有一个巨大的数据集时，随机森林是合适的，可解释性不是一个关键问题，因为随着数据集变得越来越大，它变得越来越难以理解。该算法用于股市分析、医疗领域患者的诊断、预测贷款申请人的信用度以及欺诈检测。</p><p>    Alike to the regression methods, you need to choose if you would rather speed or accuracy for your outcomes.</p><p>和回归方法一样，你需要选择你想要的结果是快速还是准确。</p><p> If you’re looking for accuracy, you not only may opt for the Kernel Support-Vector Machine, but you can use other algorithms that were mentioned previously, such as Neural Network, Gradient Boosting Tree, and Random Forest. Now, let’s introduce this new algorithm.</p><p>如果你在寻找准确度，你不仅可以选择核支持向量机，还可以使用前面提到的其他算法，比如神经网络、梯度提升树和随机森林。现在，让我们介绍一下这个新算法。</p><p>  To bridge linearity and non-linearity, the kernel technique is commonly utilized in the Support-Vector Machine model. To understand this, it is essential to know that the SVM method learns how to separate different groups by forming decision boundaries.</p><p>在支持向量机模型中，通常使用核技术来桥接线性和非线性。要理解这一点，必须知道SVM方法通过形成决策边界来学习如何分离不同的组。</p><p> But when we’re in front of a data set of higher dimensions and the costs are expensive, it is recommended to use this kernel method. It enables us to work in the original feature space without having to compute the data&#39;s coordinates in a higher-dimensional space.</p><p>但是，当我们面对的是更高维度的数据集，而且成本很高时，建议使用这种内核方法。它使我们能够在原始特征空间中工作，而无需计算数据#39；s坐标在更高维空间中。</p><p> It’s mostly used in text classification problems since most of them can be linearly separated.</p><p>它主要用于文本分类问题，因为它们中的大多数可以线性分离。</p><p> When speed is needed, we need to see if the technique we&#39;re going to employ is explainable, which implies it can explain what happens in your model from start to finish. In that case, we might use a Decision Tree algorithm or a Logistic Regression.</p><p>当需要速度时，我们需要看看我们的技术是否&#39；我们将要使用的是可解释的，这意味着它可以解释从开始到结束在模型中发生的事情。在这种情况下，我们可以使用决策树算法或逻辑回归。</p><p>  Logistic Regression is used when the dependent variable is categorical. Through probability estimate, it aids in understanding the link between dependent variables and one or more independent variables.</p><p>当因变量为分类变量时，使用逻辑回归。通过概率估计，它有助于理解因变量和一个或多个自变量之间的联系。</p><p>   The Logistic Regression algorithm is widely used in hotel booking, it shows you (through statistical research) the options you may want to have in your bookings, such as the hotel room, some journeys in the area, and more.</p><p>Logistic回归算法广泛应用于酒店预订中，它（通过统计研究）向您显示您可能希望在预订中拥有的选项，例如酒店房间、该地区的一些行程等等。</p><p> If you’re only interested in the input and output of your problem, you can check if the data you’re working with is too large. If the number is huge, you can use a Linear Support-Vector Machine.</p><p>如果你只对问题的输入和输出感兴趣，你可以检查你处理的数据是否太大。如果数量很大，可以使用线性支持向量机。</p><p>   Linear SVM is used for linearly separable data. It works in data with different variables (linearly separable data) that can be separated with a simple straight line (linear SVM classifier). This straight line represents the user behavior or outcome through a stated problem.</p><p>线性支持向量机用于线性可分数据。它适用于具有不同变量（线性可分离数据）的数据，这些变量可以用简单的直线（线性SVM分类器）分离。这条直线代表用户通过指定问题的行为或结果。</p><p> Since texts are often linearly separable and have a lot of features, the Linear SVM is the best option to use in its classification. In the case of our next algorithm, you can use it either if the data is large or not.</p><p>由于文本通常是线性可分的，并且有很多特征，线性支持向量机是用于分类的最佳选择。在我们的下一个算法中，如果数据大或小，都可以使用它。</p><p>   This algorithm is based on Bayes Theorem. It consists of predictions through objects’ probabilities. It’s called Naïve because it assumes that the appearance of one feature is unrelated to the appearance of other characteristics.</p><p>该算法基于贝叶斯定理。它包括通过物体的概率进行预测。之所以称之为天真，是因为它假设一个特征的外观与其他特征的外观无关。</p><p> This method is well-liked because it can surpass even the most sophisticated classification approaches. Furthermore, it is simple to construct and may be built rapidly. Due to its easy use and efficiency, it’s used to make real-time decisions. Along with that, Gmail uses this algorithm to know if a mail is Spam or not.</p><p>这种方法很受欢迎，因为它甚至可以超过最复杂的分类方法。此外，它构造简单，可以快速建造。由于它的易用性和高效性，它被用来做出实时决策。此外，Gmail还使用这种算法来判断邮件是否是垃圾邮件。</p><p> The Gmail spam detection picks a set of words or ‘tokens’ to identify spam email (this method is also used in text classification and it’s commonly known as a bag of words). Next, they use those tokens and compare them to spam and non-spam emails. Finally, using the Naive Bayes algorithm, they calculate the probability that the email is spam or not.</p><p>Gmail垃圾邮件检测选择一组单词或“标记”来识别垃圾邮件（这种方法也用于文本分类，通常被称为单词包）。接下来，他们使用这些代币并将其与垃圾邮件和非垃圾邮件进行比较。最后，使用朴素贝叶斯算法，他们计算电子邮件是否为垃圾邮件的概率。</p><p>  We find that Machine Learning is a widely utilized technology with many applications unrecognized by us because it&#39;s a regular occurrence. In this article, we have not only distinguished between the different approaches of machine learning but how to use them according to the data we’re working with and the problem we want to solve.</p><p>我们发现机器学习是一种被广泛使用的技术，有许多应用我们还没有认识到，因为它&#39；这是常有的事。在本文中，我们不仅区分了机器学习的不同方法，还根据我们正在处理的数据和我们想要解决的问题，区分了如何使用它们。</p><p> To learn Machine Learning, you have to have some knowledge of calculus, linear algebra, statistics, and programming skills. You can use different programming languages to implement one of these algorithms, from Python to C++, and R language. It’s up to you to make the best de</p><p>要学习机器学习，你必须具备微积分、线性代数、统计学和编程技能。您可以使用不同的编程语言来实现这些算法中的一种，从Python到C++，以及R语言。这取决于你做出最好的决定</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/算法/">#算法</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>