<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>用傅里叶变换取代伯特自我关注：精度为92％，更快7倍 Replacing Bert Self-Attention with Fourier Tranform: 92% Accuracy, 7X Faster</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Replacing Bert Self-Attention with Fourier Tranform: 92% Accuracy, 7X Faster<br/>用傅里叶变换取代伯特自我关注：精度为92％，更快7倍 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-15 03:19:38</div><div class="page_narrow text-break page_content"><p>Transformer architectures have come to dominate the natural language processing (NLP) field since their 2017 introduction. One of the only limitations to transformer application is the huge computational overhead of its key component — a self-attention mechanism that scales with quadratic complexity with regard to sequence length.  New research from a Google team proposes replacing the self-attention sublayers with simple linear transformations that “mix” input tokens to significantly speed up the transformer encoder with limited accuracy cost. Even more surprisingly, the team discovers that replacing the self-attention sublayer with a standard, unparameterized Fourier Transform achieves 92 percent of the accuracy of BERT on the GLUE benchmark, with training times that are seven times faster on GPUs and twice as fast on TPUs.</p><p>自2017年介绍以来，变压器架构已占据自然语言处理（NLP）字段。变压器应用程序的唯一限制是其关键组件的巨大计算开销 - 一种自我关注机制，其在序列长度方面具有二次复杂性。 Google团队的新研究提出使用简单的线性变换更换自我关注子层，即“混合”输入令牌，以显着加快变压器编码器，精度具有有限的成本。更令人惊讶的是，团队发现用标准替换自我关注子层，未分辨的傅里叶变换达到粘合力基准的伯特精度的92％，训练时间速度快7倍，而TPU上的两倍于快速。</p><p>  Transformers’ self-attention mechanism enables inputs to be represented with higher-order units to flexibly capture diverse syntactic and semantic relationships in natural language. Researchers have long regarded the associated high complexity and memory footprint as an unavoidable trade-off on transformers’ impressive performance. But in the paper  FNet: Mixing Tokens with Fourier Transforms, the Google team challenges this thinking with FNet, a novel model that strikes an excellent balance between speed, memory footprint and accuracy.</p><p>  变形金刚的自我关注机制使得输入的输入可以用更高阶单元表示，以灵活地以自然语言捕获不同的句法和语义关系。研究人员长期以来将相关的高复杂性和内存足迹视为变形金刚令人印象深刻的绩效的不可避免的权衡。但在论文FNET中：将令牌与傅里叶变换混合，谷歌团队用FNET挑战这一思路，这是一种小说速度，内存足迹和准确性之间存在出色的平衡。</p><p>  FNet is a layer normalized ResNet architecture with multiple layers, each of which consists of a Fourier mixing sublayer followed by a feedforward sublayer. The team replaces the self-attention sublayer of each transformer encoder layer with a Fourier Transform sublayer. They apply 1D Fourier Transforms along both the sequence dimension and the hidden dimension. The result is a complex number that can be written as a real number multiplied by the imaginary unit (the number “i” in mathematics, which enables solving equations that do not have real number solutions). Only the result’s real number is kept, eliminating the need to modify the (nonlinear) feedforward sublayers or output layers to handle complex numbers.  The team decided to replace self-attention with Fourier Transform — based on 19th century French mathematician Joseph Fourier’s technique for transforming a function of time to a function of frequency — because they found it a particularly effective mechanism for mixing tokens, enabling it to provide the feedforward sublayers sufficient access to all tokens.  In their evaluations, the team compared multiple models, including BERT-Base, an FNet encoder (replace every self-attention sublayer with a Fourier sublayer), a Linear encoder (replace each self-attention sublayer with linear sublayers), a Random encoder (replace each self-attention sublayer with constant random matrices) and a Feed Forward-only encoder (remove the self-attention sublayer from the Transformer layers).</p><p>  FNET是具有多个图层的层标准化Reset架构，每个层由傅里叶混合子层组成，然后是馈电子层。该团队用傅里叶变换子层替换每个变压器编码器层的自我关注子层。它们沿序列尺寸和隐藏的尺寸施加1D傅里叶变换。结果是一个复杂的数字，可以作为实地数乘以虚部的实数（数学数字“i”，这使得解决没有实数解的方程式）。仅保留结果的实数，消除了修改（非线性）馈电子层或输出层以处理复数的需要。该团队决定用傅立叶变换来取代自我关注 - 基于19世纪的法国数学家Joseph Fourier的技术，用于将时间变换为频率的函数 - 因为它们发现了混合令牌的特别有效的机制，使其能够提供馈线子层充分访问所有令牌。在他们的评估中，团队比较了多种型号，包括BERT基础，F形式编码器（用傅立叶子层替换每个自我关注子层），一个线性编码器（用线性子层替换每个自我关注子层），一个随机编码器（用恒定随机矩阵替换每个自我关注子层）和仅馈送前进的编码器（从变压器层上删除自我注意子层）。</p><p>       By replacing the attention sublayer with standard, unparameterized Fourier Transform, FNet achieves 92 percent of the accuracy of BERT in a common classification transfer learning setup on the GLUE benchmark, but training is seven times as fast on GPUs and twice as fast on TPUs.</p><p>       通过用标准的普通分类转移学习设置替换标准的注意子宫内置的傅立叶变换，FNET在粘合基准测试中的常见分类转移学习设置中实现了92％，但培训是GPU上的七倍，TPU上的两倍于快速。</p><p> An FNet hybrid model containing only two self-attention sublayers achieves 97 percent of BERT accuracy on the GLUE benchmark, but trains nearly six times as fast on GPUs and twice as fast on TPUs.</p><p> 仅包含两个自我关注子层的FNET混合模型在胶水基准测试中达到了97％的BERT精度，但在GPU上的速度速度速度近六倍，并且在TPU上的两倍。</p><p> FNet is competitive with all the “efficient” transformers evaluated on the Long Range Arena benchmark while having a lighter memory footprint across all sequence lengths.</p><p> FNET对在长距离竞技场基准测试中进行评估的所有“高效”变压器具有竞争力，同时在所有序列长度上具有较轻的内存占地面积。</p><p> The study shows that replacing a transformer’s self-attention sublayers with FNet’s Fourier sublayers achieves remarkable accuracy while significantly speeding up training time, indicating the promising potential of using linear transformations as a replacement for attention mechanisms in text classification tasks.  The paper  FNet: Mixing Tokens with Fourier Transforms is on  arXiv.</p><p> 该研究表明，用FNET的傅立叶子层面取代变压器的自我关注子层实现了显着的准确性，同时显着加速了培训时间，表明使用线性变换作为文本分类任务中的注意机制的替代潜力。纸FNET：在Arxiv上使用傅里叶变换混合令牌。 </p><p>     We know you don’t want to miss any news or research breakthroughs.  Subscribe to our popular newsletter    Synced Global AI Weekly  to get weekly AI updates.</p><p>我们知道您不想错过任何新闻或研究突破。 订阅我们的流行时事通讯同步全球ai每周获得每周AI更新。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/">https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/变换/">#变换</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/bert/">#bert</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/fnet/">#fnet</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>