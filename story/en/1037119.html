<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>间隔树时钟Interval Tree Clocks</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Interval Tree Clocks<br/>间隔树时钟</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-29 17:01:44</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/3bef2263cc8dc2abc1d97c86c4cac205.png"><img src="http://img2.diglog.com/img/2020/11/3bef2263cc8dc2abc1d97c86c4cac205.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>I wanted to gather some notes I&#39;ve kept in my head over the years about Interval Tree Clocks. Interval Tree clocks have been presented in  a 2008 paper by Paulo Sérgio Almeida, Carlos Baquero, and Victor Fonte. It&#39;s one of the most interesting and readable papers I&#39;ve seen, but I don&#39;t recall seeing many production systems using the mechanism, and I&#39;m not fully sure why. When discussing it over the years I realized a few times that I had come up with a pattern that made it possibly a bit more practical than what others had in mind, but never really wrote it down.</p><p>我想收集一些有关间隔树时钟的笔记，这些年来我一直在脑海里。 PauloSérgioAlmeida，Carlos Baquero和Victor Fonte在2008年的论文中介绍了间隔树时钟。这是我所见过的最有趣，最易读的论文之一，但我不记得有很多生产系统都在使用该机制，而且我也不完全清楚为什么。这些年来，当我讨论它时，我几次意识到我提出了一种模式，该模式可能使它比其他人所想的更实用，但从未真正写下来。</p><p> I&#39;m not an academic, and I put zero formalism into any of this. The stuff I&#39;m writing here is just me going &#34;that sounds like it should work&#34; and that&#39;s as far as I&#39;m gonna go with it.</p><p> 我不是学术人员，因此我对其中的任何形式都采用零形式主义。我在这里写的东西只是我在“听起来像应该工作”，这是我要去做的。</p><p>  The paper is really great because the authors were not content with only the math proving its soundness, they also use a great visual representation of the data structure to help understand it:</p><p>  这篇论文真的很棒，因为作者不仅仅满足于证明其合理性的数学，他们还使用数据结构的直观表示来帮助理解它：</p><p>  The way interval tree clocks work is fairly equivalent to vector clocks in that you can track causality, figure out conflicts, and so on. It however differs in one major way, which is that it is intended to be used in very dynamic environments where cluster membership may change constantly.</p><p>  间隔树时钟的工作方式与矢量时钟相当，因为您可以跟踪因果关系，找出冲突等。但是，它的主要不同之处在于，它旨在用于群集成员可能会不断变化的非常动态的环境中。</p><p> The trick for that is to base the clock on an Id that is divisible between nodes, such that any single member of the cluster can subdivide its own key space and hand a fraction of it to another one (as a fork), or to reunite any two of them together (as a join).</p><p> 这样做的技巧是将时钟基于节点之间可分割的ID，以便集群的任何单个成员都可以细分其自己的密钥空间，并将其中的一小部分交给另一个（作为fork），或者实现团聚他们中的任何两个在一起（作为联接）。</p><p>       {{1,0},0} {{0,1},0} {0, 1} █▒▒▒ ▒█▒▒ ▒▒██</p><p>       {{1,0}，0} {{0,1}，0} {0，1}█▒▒▒▒█▒▒▒▒██</p><p>  These three Ids are still unique, and remain perfectly mergeable or subdivisible again. I could, for example, join the first ID with the third one and obtain:</p><p>  这三个ID仍然是唯一的，并且仍然可以完美地合并或细分。例如，我可以将第一个ID与第三个ID结合起来并获得：</p><p>     The ID is only half of the clock, though. The clock works by incrementing counters by basing yourself off the Id. Only when both components are around do you have a full interval tree clock timestamp.</p><p>但是，该ID仅是时钟的一半。时钟通过根据ID递增计数器来工作。仅当两个组件都在附近时，您才具有完整的间隔树时钟时间戳。</p><p>   You can see the Id in dark grey below the dividing line, and the event counter on top of it. At step  2, both nodes increment their counter because they have seen an event or modified a thing.</p><p>   您可以在分隔线下方看到深灰色的ID，并在其顶部看到事件计数器。在第2步，两个节点都增加了计数器，因为它们已看到事件或修改了事物。</p><p> The paper defines the following operations on a timestamp, which may help explain how the previous image works:</p><p> 本文在时间戳上定义了以下操作，这可能有助于解释先前的图像如何工作：</p><p> fork: takes a clock and makes a new representation of it where the event counter is the same, but the ID part is divided as above. This is what happens at steps  1,  3, and  7</p><p> fork：使用一个时钟，并在事件计数器相同的情况下对其进行新表示，但是ID部分如上所述。这是在步骤1、3和7发生的情况</p><p> event: increments the counter on top of the clock, based off the clock&#39;s id. This is what happens at  2,  4 (bottom), and  6 (top).</p><p> 事件：根据时钟的ID在时钟顶部增加计数器。这就是在2、4（底部）和6（顶部）发生的情况。</p><p> join: merge two timestamps and get a new one. This is what happens at  5 and  8</p><p> join：合并两个时间戳并获取一个新的时间戳。这是在5和8发生的情况</p><p> peek: creates a copy of the timestamp, but with a NULL id, such that the event counter may be read, but not incremented. Not shown on the diagram.</p><p> 窥视：创建时间戳的副本，但具有NULL ID，以便可以读取事件计数器，但不能递增事件计数器。在图中未显示。</p><p>   sync: atomic composition of join+fork. This is not shown on the diagram, but can be thought of as leaving the IDs as they are, but merging the event counter.</p><p>sync：join + fork的原子组成。这未在图中显示，但可以认为是保留它们的ID，但合并事件计数器。</p><p>  The algorithm becomes more efficient since it can handle its own equivalences in the clock component to reduce its complexity and size without loss of information:</p><p>  该算法变得更有效，因为它可以处理自己在时钟组件中的等效项，以减少其复杂性和大小，而不会丢失信息：</p><p>    Because the Id space is assumed to be non-overlapping between cluster members, the last image shows that you do not need to increment  all the space matching an ID, just part of it. In this case, the algorithm ensures that the data set&#39;s numerical representation is more compact  while incrementing the counter value and adding information to it. Here&#39;s another example from the paper:</p><p>    由于假定ID空间在集群成员之间是不重叠的，因此最后一个图像显示您不需要增加与ID匹配的所有空间，而只是其中一部分。在这种情况下，该算法可确保在增加计数器值并为其添加信息时，数据集的数字表示更加紧凑。这是本文的另一个示例：</p><p>   The problem with ITCs is that it&#39;s kind of difficult to translate the data structure and its operations into a functional system if you&#39;ve never dealt with that stuff.</p><p>   ITC的问题在于，如果您从未处理过数据结构及其操作，就很难将其转换为功能系统。</p><p> The first time I looked at them, I was discussing it with a friend on IRC and we were just scratching our heads trying to figure out how we should actually make use of that stuff with a simple key/value store, since those are generally easy to reason about. Here&#39;s a few questions we asked.</p><p> 第一次查看它们时，我是在IRC上与一位朋友讨论的，我们只是摸索着试图弄清楚我们应该如何通过简单的键/值存储实际使用这些东西，因为这些通常很容易推理一下。这是我们问的几个问题。</p><p> Do you use one id per record and each record and then increment the events from there? This could seemingly work in this scenario with nodes A and B, and a user interacting with any of them:</p><p> 您是否为每条记录和每条记录使用一个ID，然后从那里递增事件？在节点A和B以及用户与其中任何一个进行交互的情况下，这似乎可以正常工作：</p><p> The user writes the data to  A, which creates ID  1 = ████, increments the counter to  1 = ████ also.</p><p> 用户将数据写入A，这将创建ID 1 =████，并将计数器也增加到1 =████。</p><p> A synchronizes with  B by forking the stamp, giving IDs  {1,0} = ██▒▒ and  {0,1} = ▒▒██. The counter remains  1 = ████.</p><p>A通过分叉戳记与B同步，并提供ID {1,0} =██▒▒和{0,1} =▒▒██。计数器仍为1 =████。</p><p> Then we&#39;re able to sync. The problem happens whenever we swap steps 2 and 3 however:</p><p> 这样我们就可以同步了。每当我们交换步骤2和3时，都会发生此问题：</p><p> The user writes the data to A, which creates ID  1 = ████, increments the counter to  1 = ████ also.</p><p> 用户将数据写入A，这将创建ID 1 =████，并将计数器也增加到1 =████。</p><p> The user writes data to B, which creates ID  1 = ████, increments the counter to  1 = ████ also.</p><p> 用户将数据写入B，B创建ID 1 =████，计数器也增加到1 =████。</p><p> The system can only assume both data sets are equivalent since the clocks are the same.</p><p> 由于时钟相同，系统只能假设两个数据集相等。</p><p> Clearly we can&#39;t dynamically create the ID for each record if the interaction isn&#39;t done strictly with a single node to which to write (sticky). The scheme may be good to track the progression of a log through systems (since there&#39;s usually a single entry point), but not general enough for a key/value store, as one of the explicit requirements of the algorithm is that the ID space does not overlap and we can easily break that requirement!</p><p> 显然，如果没有严格地与要写入的单个节点（粘性）进行交互，则我们无法为每个记录动态创建ID。该方案可能会很好地跟踪整个系统的日志进度（因为通常只有一个入口点），但是对于键/值存储来说不够通用，因为该算法的显式要求之一是ID空间不重叠，我们可以轻松突破这一要求！</p><p> So what we need is to guarantee a rule stating that only distinct IDs exist in the system, which means we cannot just create IDs whenever we want. We concluded that the ID division has to match each node&#39;s life cycle rather than a record&#39;s.</p><p> 因此，我们需要确保有一条规则指出系统中仅存在不同的ID，这意味着我们不能只在需要时创建ID。我们得出结论，ID划分必须匹配每个节点的生命周期，而不是记录的生命周期。</p><p> So we could just attach a seed ID to the node itself and ask a peer to fork off their ID. One has to be careful that the cluster is first initialised with a single root node at first, to avoid the case where we boot a new cluster with many nodes, two of which start at the same time and both assume the same ID.</p><p>因此，我们可以将种子ID附加到节点本身，然后要求对等方派生它们的ID。必须注意，首先首先使用单个根节点初始化集群，以避免出现这样的情况：我们启动具有许多节点的新集群，其中两个节点同时启动，并且都采用相同的ID。</p><p> Once a root node is booted, any other number of nodes can start joining by asking any other peer for a fork of their ID.</p><p> 引导根节点后，其他任何数量的节点都可以通过向其他任何对等方请求其ID的派生来开始加入。</p><p> This logically works well, but has a hidden cost: if you keep timestamps as described in the paper (id + event counter) and the node splitting its own ID has 10 million records, you have to rewrite these 10 million timestamps right away. If you don&#39;t, the following scenario is possible:</p><p> 这在逻辑上可以很好地工作，但是有一个隐藏的代价：如果您按照白皮书（id +事件计数器）中所述保留时间戳，并且拆分自己的ID的节点有1000万条记录，则必须立即重写这1000万个时间戳。如果您不这样做，则可能出现以下情况：</p><p> A: {1,0} = ██▒▒ B: {0,{1,0} = ▒▒█▒ C: {0,{0,1}} = ▒▒▒█--------------- ------------------ -------------------K1: ████ K1: ████ K1: ████ ██▒▒ ██▒▒ ██▒▒... ... ...KN: ▒▒██ KN: ▒▒██ KN: ▒▒██ ▒▒██ ▒▒██ ▒▒██</p><p> A：{1,0} =██▒▒B：{0，{1,0} =▒▒█▒C：{0，{0,1}} =▒▒▒█------ -------- ------------------ ------------------- K1：███ █K1：████K1：██████▒▒██▒▒██▒▒... ... ... KN：▒▒██KN：▒▒██KN：▒▒ ██▒▒██▒▒██▒▒██</p><p> So what&#39;s up there? Well each entry in the table that is synchronized has not been forked and shares the same ID. Any write would reuse the same  ██▒▒ node ID and increment events in ways that do not track causality. Whenever  A increments  K1 things work, but if it starts writing to  KN, it will look like an old version of  B wrote to it, which would clash with what  B and  C could currently do. Rewriting each entry after forks (either right away or before access time at a later point) would yield a safer:</p><p> 那那里是什么？那么，同步表中的每个条目都没有被分叉，并且共享相同的ID。任何写操作都将重复使用相同的██▒▒节点ID，并以不跟踪因果关系的方式递增事件。每当A递增K1时，事情就起作用了，但是如果它开始写入KN，它将看起来像是B的旧版本向其写入的操作，这将与B和C当前的操作发生冲突。在派生之后（立即或在稍后的访问时间之前）重写每个条目将产生一个更安全的方法：</p><p> A: {1,0} = ██▒▒ B: {0,{1,0} = ▒▒█▒ C: {0,{0,1}} = ▒▒▒█--------------- ------------------ -------------------K1: ████ K1: ████ K1: ████ ██▒▒ ▒▒█▒ ▒▒▒█... ... ...KN: ▒▒██ KN: ▒▒██ KN: ▒▒██ ▒▒██ ▒▒█▒ ▒▒▒█</p><p> A：{1,0} =██▒▒B：{0，{1,0} =▒▒█▒C：{0，{0,1}} =▒▒▒█------ -------- ------------------ ------------------- K1：███ █K1：████K1：██████▒▒▒▒█▒▒▒▒█... ... ... KN：▒▒██KN：▒▒██KN：▒▒ ██▒▒██▒▒█▒▒▒▒█</p><p> This now lets each node independently modify timestamps while tracking causality, but rewriting the whole table each time you fork a node would be extremely costly and annoying to keep track of.</p><p> 现在，这可以使每个节点在跟踪因果关系时独立地修改时间戳，但是每次您派生一个节点时都要重写整个表，这将是非常昂贵的，并且很烦人。</p><p> A far more elegant solution is to break down the clock into its two independent components: the node stores the ID globally, and each record only contains the event counter. At each operation, we dynamically reconstruct the timestamp by submitting the node&#39;s ID on each operation, and the event counter on the rest. Since the event counter is monotonic and that the node ID changes rather rarely, we can get something that is a ton cheaper and simpler to manage that way.</p><p>一个更好的解决方案是将时钟分解为两个独立的组件：节点全局存储ID，每个记录仅包含事件计数器。在每个操作中，我们通过在每个操作上提交节点的ID，并在其余操作上提交事件计数器，来动态地重建时间戳。由于事件计数器是单调的，并且节点ID很少更改，因此我们可以以这种方式获得更便宜，更简单的方法。</p><p> This requires us to extend the design of the interval tree clock a bit to support &#39;explode&#39; and &#39;rebuild&#39; operations, which split up and rejoin a single ID+event counter part of a clock. With this design, we can move ahead and define the following (informal) protocol.</p><p> 这需要我们将间隔树时钟的设计稍微扩展一点，以支持“爆炸”和“重建”操作，这些操作可以拆分并重新加入时钟的单个ID +事件计数器部分。通过这种设计，我们可以前进并定义以下（非正式）协议。</p><p>  In order to be effective, the protocol asks us to break a few layers of abstraction. This is usually a bad idea, but the tradeoff of having all layers know they&#39;re carrying interval tree clocks tends to outweigh rewriting all entries on each ID split.</p><p>  为了有效，该协议要求我们打破几层抽象。这通常是一个坏主意，但是让所有层都知道它们正在携带间隔树时钟的折衷往往会超过重写每个ID拆分中的所有条目的代价。</p><p> The first step to handle is ID propagation, where a given node in the cluster allows another one to join in and start incrementing events.</p><p> 要处理的第一步是ID传播，其中群集中的给定节点允许另一个节点加入并开始递增事件。</p><p>  Any node joining the cluster after the fact must contact at least one other peer and ask for a fork of its ID. Ideally the peer is chosen randomly to avoid subdividing the same part of the ID indefinitely, giving suboptimal id spaces.  The forking node picks either side, and overwrites its existing ID value in permanent storage (it should then wait for all ongoing operations with the old ID to terminate)</p><p>  事实结束后加入群集的任何节点都必须联系至少另一个对等节点，并要求提供其ID的分支。理想情况下，对等体是随机选择的，以避免无限期地细分ID的相同部分，从而给出次优的ID空间。分叉节点选择任一侧，并覆盖其在永久存储区中的现有ID值（然后，它应等待所有使用旧ID进行的操作终止）</p><p> Any node leaving the cluster may, in order to help reduce the ID and event space:  synchronize its data set with the rest (or part) of the cluster (see  Data propagation below)</p><p> 为了帮助减少ID和事件空间，离开群集的任何节点都可以：将其数据集与群集的其余（或部分）同步（请参见下面的数据传播）。</p><p>      Pick one peer node at random and give its ID back to it for it to be joined into its own. The other node can accept the ID directly. In case of failure or timeout, do not retry to send the ID; it is simply lost. You may want to log it somewhere so that an administrator can reconcile values manually later on.</p><p>      随机选择一个对等节点，并为其返回ID，以便将其加入自己的节点。另一个节点可以直接接受该ID。如果发生故障或超时，请不要重试发送ID。它只是迷路了。您可能希望将其记录在某处，以便管理员稍后可以手动调整值。</p><p> Step 2 has to happen in that order to prevent the case where the forking node sends a split id to the remote end, then crash-restarts or loses track of the change. If that happens, causality for the whole cluster is thrown off the mark as ID spaces now overlap; causality is corrupted.</p><p>为了防止分叉节点将拆分ID发送到远端，然后崩溃重新启动或丢失更改，必须执行步骤2。如果发生这种情况，则整个集群的因果关系将被忽略，因为ID空间现在重叠了。因果关系已损坏。</p><p> Step 3 has to see steps c, d, and e happen in that specific order to avoid crash-restart scenarios possibly asking the same ID to be joined on various remote nodes at once. This would create conflicts and corrupt causal relationships to come. Steps a and b are optional (or could be interspersed with the other steps), but nevertheless good ideas to avoid data loss, if any.</p><p> 步骤3必须看到步骤c，d和e以该特定顺序发生，以避免崩溃重新启动的情况可能要求立即将相同的ID加入到各个远程节点上。这将导致冲突和因果关系的恶化。步骤a和b是可选的（或可以与其他步骤一起使用），但是，如果有的话，可以避免数据丢失。</p><p>  From this point on there&#39;s a fork in the road depending on whether you want equivalence with version vectors or vector clocks. They&#39;re  the same but different.</p><p>  从这一点开始，取决于您要与版本向量还是向量时钟等效，这条路就已经存在。它们相同但不同。</p><p> Basically, the same logical clock mechanism can be used for either purpose. The distinction is in the intent of what you want to track:</p><p> 基本上，相同的逻辑时钟机制可用于任何目的。区别在于您要跟踪的内容：</p><p> If you want to track the causal ordering of  modifications to a data set (i.e. data in a database), you want version vectors</p><p> 如果要跟踪对数据集（即数据库中的数据）的修改的因果顺序，则需要版本向量</p><p> If you want to track the causal ordering of  events in a system (i.e. track the information flow of &#34;who has heard about this?&#34;), you want vector clocks.</p><p> 如果要跟踪系统中事件的因果顺序（即跟踪“谁听说过此信息？”的信息流），则需要矢量时钟。</p><p> The distinction is fairly simple: for equivalence with a version vector, you increment the event-counter when data is modified. For vector clock equivalence you must increment the event-counter whenever a message is sent or received.</p><p> 区别非常简单：为了与版本向量等效，在修改数据时增加事件计数器。对于等效向量时钟，无论何时发送或接收消息，都必须增加事件计数器。</p><p> Even though the distinction is simple, it can have important consequences. If you only need a version vector, you can skip over a few update operations and gain some efficiency, or even look into variations such as  dotted version vectors (DVVs) to account for changes on behalf of clients. Do note that although ITCs can be used as version vectors, they do not allow DVV-like usage. On the other hand, version vectors carry less information: if you ever want to make a  Consistent Cut, they won&#39;t be as &#34;accurate&#34; as a vector clock would be.</p><p>即使区分很简单，也可能产生重要的后果。如果只需要版本向量，则可以跳过一些更新操作并获得一定的效率，甚至可以查看诸如虚线版本向量（DVV）之类的变体来代表客户端进行更改。请注意，尽管ITC可以用作版本向量，但它们不允许类似DVV的用法。另一方面，版本向量携带的信息较少：如果您要进行“一致剪切”，它们将不会像向量时钟那样“准确”。</p><p>   When pushing data to synchronize it, only send the event counter to the remote node for each entry, but not the ID.</p><p>   推送数据以使其同步时，仅将事件计数器的每个条目发送到远程节点，而不发送ID。</p><p> When receiving data: if the current stamp is &lt; the received one, store the new one with the new piece of data</p><p> 接收数据时：如果当前戳记小于接收到的戳记，则将新的戳记与新的数据一起存储</p><p>  in case of conflict, two options exist, either on resolving conflicts on the spot, or tracking conflicts. Both options can be tracked under a single clock however. To run that one, simply join both clocks. Because we only have remote events, we can simply call: {_,  EventCounter }  =  explode (  % only grab the event, we know the ID already  event (  % increment the event counter  join (  % merge the event stamps  peek ( rebuild ( LocalId ,  RemoteEvent )),  % complete the remote stamp as read-only  rebuild ( LocalId ,  LocalEvent )  % make a complete local stamp  )) ).</p><p>  在发生冲突的情况下，存在两种选择，要么是现场解决冲突，要么是跟踪冲突。但是，两个选项都可以在单个时钟下跟踪。要运行该时钟，只需同时连接两个时钟。因为我们只有远程事件，所以我们可以简单地调用：{_，EventCounter} = explode（％仅抓取事件，我们知道ID已经为事件（％增加事件计数器的联接（％合并事件标记peek（重建（LocalId ，RemoteEvent）），将远程图章作为只读重建（LocalId，LocalEvent）％完成一个完整的本地图章）））。</p><p> For step 2, one thing to pay attention to is the client. The client is part of the distributed system if it is external to the node. If writes come from such a party with no clock information, we cannot necessarily assume that it is aware of an existing piece of data or conflict unless it submits an event counter with it. We essentially have to consider a client like that to be sort of a rogue participant into the system. To make the client behave, it has to be able to submit a stamp, and to have the client submit one, it must first read the data, where we may assume an acknowledgement of its content. If an event counter is submitted back when writing, we can do conflict detection at write time. If none is submitted, we can either always crush the entry (last-write-wins), or declare a conflict to be resolved later.</p><p> 对于步骤2，要注意的一件事是客户。如果客户端在节点外部，则它是分布式系统的一部分。如果写入来自没有时钟信息的一​​方，则除非它与事件一起提交事件计数器，否则我们不一定可以假定它知道已有的数据或冲突。从本质上讲，我们必须考虑将这样的客户视为系统中的流氓参与者。为了使客户行事，它必须能够提交邮票，并且要使客户提交邮票，它必须首先读取数据，在此我们可以假定其内容已得到确认。如果在写入时提交了事件计数器，我们可以在写入时进行冲突检测。如果未提交任何内容，则我们可以始终粉碎该条目（last-write-wins），也可以声明冲突以待日后解决。</p><p> The last sequence, described in 4.d, is interesting. Because we only have three pieces of data (local id, local event, remote event) and are missing one (remote id), we substitute in our own local id to go with the remote event, and  then use the &#39;peek&#39; primitive to give us back a mergeable event clock with a null ID. We then join them back together which gives a full local clock that is up to date. Because we either picked a winner or started tracking conflicts, the data has changed and we must increment the event counter. We can then extract that part to store it.</p><p> 在4.d中描述的最后一个序列很有趣。因为我们只有三部分数据（本地ID，本地事件，远程事件）而缺少一个（远程ID），所以我们用自己的本地ID代替远程事件，然后使用“ peek”原语来还给我们一个ID为空的可合并事件时钟。然后，我们将它们重新连接在一起，以提供最新的完整本地时钟。由于我们要么选择了获胜者，要么开始跟踪冲突，所以数据已更改，因此我们必须增加事件计数器。然后，我们可以提取该部分进行存储。</p><p> This stamp is now greater than either of the basic ones, and the increment will ensure that any third party that had either copy will now get theirs crushed (or will now see a conflict too, if they had made modifications).</p><p> 现在，此印章比任何一个基本印章都要大，并且加价幅度将确保拥有任一副本的任何第三方现在都将被粉碎（或者，如果他们进行了修改，现在也将看到冲突）。</p><p>  For vector clock equivalence, only steps 3 and 4 need to change compared to the version vector approach:</p><p>对于矢量时钟等效，与版本矢量方法相比，只需更改步骤3和4：</p><p> When pushing data to synchronize it, increment the event counter and store it. Then only send the event counter to the remote node for each entry, but not the ID.</p><p> 当推送数据以使其同步时，增加事件计数器并存储它。然后仅将事件计数器的每个条目发送到远程节点，而不发送ID。</p><p> When receiving data:  if the current stamp is &gt;= the received one, merge both counters and increment it with the local id before storage</p><p> 接收数据时：如果当前戳记> =接收到的戳记，则合并两个计数器并在存储之前将其与本地标识一起递增</p><p>  if the current stamp is &lt; the received one, store the new one with the new piece of data, but merge both counters and increment it with the local id before storage</p><p>  如果当前标记小于<接收到的标记，则将新标记与新数据一起存储，但是合并两个计数器并在存储之前使用本地ID对其进行递增</p><p>   Restoring from backups isn&#39;t something I have given much thought in such a case. The data set should be safe to restore, as event clocks are monotonic and should be able to stand on their own for comparisons (as shown in step 4.d of data sync). However, it is risky for any node to be restored if it had previously removed itself from the cluster since it could start corrupting timelines by using either an id that is either a duplicate or a subset of an existing one.</p><p>   在这种情况下，我没有考虑过从备份还原。数据集应该可以安全恢复，因为事件时钟是单调的，并且应该能够独立进行比较（如数据同步的步骤4.d所示）。但是，如果任何节点先前已将自己从集群中删除，则恢复该节点存在风险，因为它可能会通过使用既有ID或现有ID的子集开始破坏时间表。</p><p> If it is not possible to say with full certainty whether the ID was still active and not retired, abandon the ID. The id space will grow fairly indefinitely, but at a much smaller space than it would have had with a version vector or a vector clock anyway.</p><p> 如果无法完全确定该ID是否仍然有效且尚未停用，请放弃该ID。 id空间将无限期地增长，但是其空间比版本向量或向量时钟本来要小得多。</p><p> My assumption is that at that point in time, you&#39;d want some auditing mechanism that can find IDs that were allocated but are no longer used, and fold them back into the ID of a running instance. To keep backups functional, I&#39;d expect someone to wait for a given delay before reaping the IDs. For example, if you take daily backups, it might make sense to assume that you wouldn&#39;t bring back an instance that is a week old when you have half a dozen new snapshots, and it would be safe to reap their IDs. In the case where you still need access, you&#39;d fall into a pattern of data-recovery where you would have to do some fancy stuff by hand (i.e. rebooting the instance with a new forked ID while keeping the old events and seeing how many conflicts that creates).</p><p> 我的假设是，到那时，您需要一种审核机制来查找已分配但不再使用的ID，并将其折回到正在运行的实例的ID中。为了保持备份正常运行，我希望有人在获得ID之前先等待给定的延迟。例如，如果您进行每日备份，则可以合理地假设，当您有六个新快照时，您不会带回一个星期的实例，因此可以安全地获取其ID。在仍然需要访问的情况下，您将陷入一种数据恢复模式，在这种情况下，您将不得不手工做一些花哨的事情（例如，使用新的派生ID重新启动实例，同时保留旧事件并查看有多少事件）。产生的冲突）。</p><p> I haven&#39;t checked really what would make sense there, but I&#39;d have to assume that someone shipping such a system might want to support &#34;consume-only&#34; modes where a node brought back from backups will try to sync data from peers locally only to check for conflicts, before actually trying to ship all that data to other nodes and propagating potential clashes.</p><p>我还没有真正检查过那里有什么意义，但是我不得不假设运送这种系统的人可能希望支持“仅使用”模式，其中从备份中带回的节点将尝试本地同步来自对等方的数据仅在实际尝试将所有数据运送到其他节点并传播潜在冲突之前检查冲突。</p><p>  I don&#39;t expect a massive pick up of what this paper proposes now that I&#39;ve written this—there would be a need for far more formalism than what I know how to do for that—but I&#39;d like to see it read more often. One of the really great things it promises is the option to do eventual consistency in &#34;mostly offline&#34; settings with a very limited ability to know how large the actual cluster is.</p><p>  我写完这篇文章后，我不希望本文提出的内容有太多—而不是我所需要的形式主义，这需要更多的形式主义—但我希望看到它读起来更频繁。它所承诺的真正伟大的事情之一是可以在“大多数为脱机”设置中实现最终的一致性，而该选项具有非常有限的能力来知道实际群集的大小。</p><p> CRDT research is where most of these efforts went over the last few years, but I don&#39;t know that it&#39;s always the safest way to handle things. It certainly makes sense when you&#39;re building a collaborative tool where you control the experience and artifacts end-to-end. In many cases, changesets are not necessarily mergeable in a monotonic way and  detecting conflicts is a good thing. One example of that might be artifacts produced by systems that do not care about structurally being mergeable, such as versions of rendered documents or videos, or form data where conflicting entries by different people can&#39;t be safely reconciled and require domain-specific resolution, such as what I imagine could happen with prescription drugs.</p><p> 在过去的几年中，CRDT研究是其中大部分努力的方向，但我不知道这始终是处理事情的最安全方法。当您构建一个协作工具来端到端控制体验和工件时，这当然很有意义。在许多情况下，变更集不一定能以单调的方式合并，并且检测冲突是一件好事。其中一个例子可能是由不关心在结构上可合并的系统产生的工件，例如渲染文档或视频的版本，或者形成了格式数据，其中不同人的冲突条目无法安全地调和并需要特定于域的解析，例如我想象的处方药可能会发生的情况。</p><p> I&#39;ve personally been toying with the idea of making a sort of peer-to-peer dropbox on the side for years (on-and-off) and never really getting far because I keep being distracted into doing other stuff, but I always imagined that interval tree clocks would be the right data structure for that.</p><p> 我个人一直在想办法在侧面（连续不断）制作一个点对点的保管箱，但从未真正走得太远，因为我一直被分散在做其他事情上的注意力，但我总是想象间隔树时钟将是正确的数据结构。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://ferd.ca/interval-tree-clocks.html">https://ferd.ca/interval-tree-clocks.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/间隔/">#间隔</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/tree/">#tree</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/id/">#id</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>