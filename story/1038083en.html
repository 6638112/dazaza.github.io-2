<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>使用Vitess在Slack扩展数据存储 Scaling Datastores at Slack with Vitess</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Scaling Datastores at Slack with Vitess<br/>使用Vitess在Slack扩展数据存储 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-07 00:08:57</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/f67874730a38157b7a9cfaeac5634bad.png"><img src="http://img2.diglog.com/img/2020/12/f67874730a38157b7a9cfaeac5634bad.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>From the very beginning of Slack, MySQL was used as the storage engine for all our data. Slack operated MySQL servers in an active-active configuration. This is the story of how we changed our data storage architecture from the active-active clusters over to Vitess — a horizontal scaling system for MySQL .  Vitess is the present and future of Datastores for Slack and continues to be a major success story for us. From the solid scalability fundamentals, developer ergonomics, and the thriving community, our bet on this technology has been instrumental for Slack’s continued growth  .</p><p>从Slack开始，MySQL就被用作所有数据的存储引擎。 Slack在双活配置中运行MySQL服务器。这是关于我们如何将数据存储架构从主动-主动式群集更改为Vitess（用于MySQL的水平扩展系统）的故事。 Vitess是Slack数据存储的现在和将来，对我们来说仍然是一个成功的大故事。从扎实的可伸缩性基础，开发人员的人体工程学以及蓬勃发展的社区中，我们对这项技术的赌注对Slack的持续增长至关重要。</p><p> Our migration to Vitess began in 2017 and Vitess now serves 99% of our overall query load. We expect to be fully migrated by the end of 2020. In this post, we will discuss the design considerations and technical challenges that went into choosing and adopting Vitess, as well as an overview of our current Vitess usage.</p><p> 我们从2017年开始迁移到Vitess，现在Vitess可以满足我们整体查询负载的99％。我们预计将在2020年底之前完全迁移。在本文中，我们将讨论选择和采用Vitess时遇到的设计注意事项和技术挑战，并概述我们当前的Vitess用法。</p><p> Availability, performance, and scalability in our datastore layer is critical for Slack. As an example, every message sent in Slack is persisted  before  it’s sent across the real-time websocket stack and shown to other members of the channel. This means that storage access needs to be   very  fast and   very  reliable.     In addition to providing a critical foundation for message sending, over the last three years Vitess has given us the flexibility to ship new features with complex data storage needs, including   Slack Connect  and   international data residency .     Today, we serve 2.3 million QPS at peak. 2M of those queries are reads and 300K are writes. Our median query latency is 2 ms, and our p99 query latency is 11 ms.     The beginning</p><p> 数据存储层中的可用性，性能和可伸缩性对于Slack至关重要。例如，在Slack中发送的每条消息在通过实时Websocket堆栈发送并显示给频道的其他成员之前都会保留下来。这意味着存储访问需要非常快速且非常可靠。在过去的三年中，Vitess除了为消息发送提供了重要的基础之外，还使我们能够灵活地交付具有复杂数据存储需求的新功能，包括Slack Connect和国际数据驻留。如今，我们在高峰期可提供230万个QPS。这些查询中有2M是读操作，300K是写操作。我们的查询延迟中位数是2毫秒，而p99查询延迟是11毫秒。开始</p><p> Slack started as a simple LAMP stack: Linux, Apache, MySQL, and PHP. All our data was stored on three primary database clusters based on MySQL:</p><p> Slack从一个简单的LAMP堆栈开始：Linux，Apache，MySQL和PHP。我们所有的数据都存储在基于MySQL的三个主要数据库集群中：</p><p> Shards:  These virtually contained all the customer data tied to using Slack, such as messages, channels, and DMs. The data was partitioned and scaled horizontally by workspace id (a workspace is the specific Slack domain you login into). All the data for a given workspace was stored on the same shard, so the application just needed to connect to that one database.</p><p> 碎片：这些碎片实际上包含了与使用Slack关联的所有客户数据，例如消息，通道和DM。数据已按工作空间ID进行分区和水平扩展（工作空间是您登录的特定Slack域）。给定工作区的所有数据都存储在同一个分片上，因此应用程序只需要连接到该数据库即可。</p><p> Metadata cluster : The metadata cluster was used as a lookup table to map a workspace id to the underlying shard id. This means that to find the shard for a particular Slack domain to a workspace, we had to lookup the record in this metadata cluster first.</p><p> 元数据集群：元数据集群用作查找表，以将工作空间ID映射到基础分片ID。这意味着要找到工作区中特定Slack域的分片，我们必须首先在此元数据集群中查找记录。</p><p> Kitchen sink cluster:  This cluster stored all the other data not tied to a specific workspace, but that was still important Slack functionality. Some examples included the app directory. Any tables that did not have records associated with a workspace id would have gone into this cluster.</p><p> 厨房水槽群集：此群集存储了所有其他数据，这些数据没有绑定到特定的工作空间，但这仍然是重要的Slack功能。一些示例包括应用程序目录。任何没有与工作区ID关联的记录的表都将进入此集群。 </p><p> The sharding was managed and controlled by our  monolith application, “webapp”. All data access was managed by webapp, which contained the logic to look up metadata for a given workspace, and then create a connection to the underlying database shard.</p><p>分片由我们的整体应用程序“ webapp”进行管理和控制。所有数据访问均由webapp进行管理，其中包含逻辑以查找给定工作区的元数据，然后创建与基础数据库分片的连接。</p><p> From a dataset layout perspective, the company started out using a workspace-sharded model. Each database shard contained all of a workspace’s data, with each shard housing thousands of workspaces and all their data including messages and channels.</p><p> 从数据集布局的角度来看，该公司开始使用工作区分割模型。每个数据库碎片都包含工作区的所有数据，每个碎片都包含数千个工作区及其所有数据，包括消息和通道。</p><p> From an infrastructure point of view, all those clusters  were made up of one or more shards where each shard was provisioned with at least  two  MySQL instances located in different datacenters, replicating to each other using asynchronous replication. The image below shows an overview of the original database architecture.</p><p> 从基础结构的角度来看，所有这些集群都是由一个或多个分片组成的，每个分片都配备有位于不同数据中心的至少两个MySQL实例，并使用异步复制相互复制。下图显示了原始数据库体系结构的概述。</p><p>   There are many advantages to this active-active configuration, which allowed us to successfully scale the service. Some reasons why this worked well for us:</p><p>   这种主动-主动配置具有许多优势，这使我们能够成功扩展服务。这对我们来说很好的一些原因：</p><p> High availability : During normal operations, the application will always prefer to query one of the two sides based on a simple hashing algorithm. When there are failures connecting to one of the hosts, the application could retry a request to the other host without any visible customer impact, since both nodes in a shard can take reads  and  writes.</p><p> 高可用性：在正常操作期间，应用程序将始终倾向于基于简单的哈希算法查询两侧之一。当连接到其中一台主机发生故障时，由于分片中的两个节点都可以进行读取和写入，因此应用程序可以重试对另一台主机的请求，而不会对客户产生任何明显影响。</p><p> High product-development velocity : Designing new features with the model of having all the data for a given workspace stored on a single database host was intuitive, and easily extensible to new product functionality.</p><p> 高产品开发速度：使用将给定工作区的所有数据存储在单个数据库主机中的模型设计新功能非常直观，并且可以轻松扩展到新产品功能。</p><p> Easy to debug : An engineer at Slack could connect a customer report to a database host within minutes. This allowed us to debug problems quickly.</p><p> 易于调试：Slack的工程师可以在几分钟之内将客户报告连接到数据库主机。这使我们能够快速调试问题。 </p><p> Easy to scale : As more teams signed up for Slack, we could simply provision more database shards for new teams and keep up with the growth. However, there was a fundamental limitation with the scaling model.  What if a single team and all of their Slack data doesn’t fit our largest shard?</p><p>易于扩展：随着越来越多的团队签约Slack，我们可以简单地为新团队提供更多的数据库碎片，并跟上增长的步伐。但是，缩放模型存在根本限制。如果一个团队及其所有Slack数据都不适合我们最大的碎片怎么办？</p><p>    As the company grew, so did the number of product teams working on building new Slack features. We found that our development velocity was slowing down significantly in trying to fit new product features into this very specific sharding scheme. This led to some challenges:</p><p>    随着公司的发展，致力于构建新的Slack功能的产品团队数量也随之增加。我们发现，在尝试将新产品功能融入此非常具体的分片方案时，我们的开发速度大大降低了。这带来了一些挑战：</p><p> Scale limits:  As we onboarded larger and larger individual customers, their designated shard reached the largest available hardware and we were regularly hitting the limits of what that single host could sustain.</p><p> 规模限制：当我们加入越来越多的个人客户时，他们指定的分片达到了可用的最大硬件，并且我们经常达到单个主机可以承受的极限。</p><p> Stuck to one data model : As we grew, we launched new products such as Enterprise Grid and Slack Connect, both of which challenge the paradigm that all data for a team will be on the same database shard. This architecture not only added complexity to developing these features, but also a performance penalty in some cases.</p><p> 停留在一个数据模型上：随着我们的成长，我们推出了新产品，例如Enterprise Grid和Slack Connect，这两个产品都挑战了一个范例，即团队的所有数据都将位于同一数据库分片上。这种体系结构不仅增加了开发这些功能的复杂性，而且在某些情况下还会降低性能。</p><p> Hot spots:  We found that we were hitting some major hotspots, while also massively underutilizing the majority of our database fleet. As we grew, we onboarded more and more enterprise customers with large teams, consisting of thousands of Slack users. An unfortunate outcome with this architecture was that we were unable to spread the load of these large customers across the fleet and we ended up with a few hot spots in our database tier. Because it was challenging to split shards and move teams, and difficult to predict Slack usage over time, we over provisioned most of the shards, leaving the long tail underutilized.</p><p> 热点：我们发现我们遇到了一些主要的热点，同时也严重利用了我们大部分数据库资源。随着我们的成长，我们加入了由数千个Slack用户组成的大型团队的越来越多的企业客户。使用此体系结构的不幸结果是，我们无法将这些大客户的负载分散到整个机队中，并且最终在数据库层中出现了一些热点。由于拆分碎片和移动团队具有挑战性，并且随着时间的推移很难预测Slack的使用情况，因此我们过度配置了大多数碎片，从而使长尾巴的利用不足。</p><p> Workspace and shard availability concerns : All core features, such as login, messaging, and joining channels, required the database shard that housed the team’s data to be available .  This meant that when a database shard experienced an outage, every single customer whose data was on that shard also experienced a full Slack outage. We wanted an architecture where we can both spread the load around to reduce the hot spots, and  isolate different workloads so that a unavailable second tier feature couldn’t potentially impact critical features like message sending</p><p> 工作空间和分片可用性的问题：所有核心功能（例如登录，消息传递和加入渠道）都需要存储团队数据的数据库分片可用。这意味着，当数据库分片出现故障时，每个数据都在该分片上的单个客户也将遭受完全的Slack中断。我们需要一种架构，既可以分散负载以减少热点，又可以隔离不同的工作负载，以使不可用的第二层功能不会潜在地影响诸如消息发送之类的关键功能</p><p> Operations:  This is a not standard MySQL configuration. It required us to write a significant amount of internal tooling to be able to operate this configuration at scale. In addition, given that in this setup we didn’t have replicas in our topology and the fact that the application routed directly to the database hosts, we couldn’t safely use replicas without reworking our routing logic.</p><p> 操作：这不是标准的MySQL配置。需要我们编写大量的内部工具，才能大规模运行此配置。此外，鉴于在此设置中，我们的拓扑中没有副本，而且应用程序直接路由到数据库主机，因此如果不重新设计路由逻辑，我们将无法安全地使用副本。 </p><p> What to do?      In the fall of 2016, we were dealing with hundreds of thousands of MySQL queries per second and thousands of sharded MySQL hosts in production. Our application performance teams were regularly running into scaling and performance problems and having to design workarounds for the limitations of the workspace sharded architecture.— we needed a new approach to scale and manage databases for the future.</p><p>该怎么办？在2016年秋天，我们每秒处理数十万个MySQL查询以及生产中的数千个分片MySQL主机。我们的应用程序性能团队经常遇到扩展和性能问题，并且必须针对工作区分片架构的局限性设计解决方法。我们需要一种新的方法来扩展和管理数据库，以便将来使用。</p><p> From the early stages of this project, there was a question looming in our heads:  should we evolve our approach in place or replace it?  We needed a solution that could provide a flexible sharding model to accommodate new product features and meet our scale and operational requirements.</p><p> 从该项目的早期阶段开始，脑海中浮现出一个问题：我们应该发展还是替代我们的方法？我们需要一种能够提供灵活的分片模型以适应新产品功能并满足我们的规模和运营要求的解决方案。</p><p> For example, instead of putting all the messages from every channel and DM on a given workspace into the same shard, we wanted to shard the message data by the unique id of the channel. This would spread the load around much more evenly, as we would no longer be forced to serve all message data for our largest customer on the same database shard.</p><p> 例如，我们不想将来自给定工作区中每个通道和DM的所有消息都放入同一个分片中，而是希望通过通道的唯一ID对消息数据进行分片。这将使负载分散得更加均匀，因为我们将不再被迫在同一数据库分片上为我们最大的客户提供所有消息数据。</p><p> We still had a strong desire to continue to use MySQL running on our own cloud servers. At the time there were thousands of distinct queries in the application, some of which used MySQL-specific constructs. And at the same time we had years of built up operational practices for deployment, data durability, backups, data warehouse ETL, compliance, and more, all of which were written for MySQL.</p><p> 我们仍然强烈希望继续使用在我们自己的云服务器上运行的MySQL。当时，应用程序中有成千上万个不同的查询，其中一些使用MySQL特定的结构。同时，我们针对部署，数据持久性，备份，数据仓库ETL，法规遵从性等等建立了多年的运营实践，所有这些都是针对MySQL编写的。</p><p> This meant that moving away from the relational paradigm (and even from MySQL specifically) would have been a much more disruptive change, which meant we pretty much ruled out NoSQL datastores like DynamoDB or Cassandra, as well as NewSQL like Spanner or CockroachDB.</p><p> 这意味着离开关系范式（甚至是从MySQL移开）将带来更大的破坏性变化，这意味着我们几乎排除了NoSQL数据存储区（如DynamoDB或Cassandra）以及NewSQL（如Spanner或CockroachDB）。</p><p> In addition, historical context is always important to understand how decisions are made. Slack is generally conservative in terms of adopting new technologies, especially for mission-critical parts of our product stack. At the time, we wanted to continue to devote much of our engineering energy to shipping product features, and so the small datastores and infrastructure team valued simple solutions with few moving parts.</p><p> 此外，历史背景对于理解如何制定决策始终很重要。就采用新技术而言，Slack通常是保守的，尤其是对于我们产品栈中关键任务的部分。当时，我们希望继续将我们的大部分工程精力投入到产品功能的交付中，因此小型数据存储和基础架构团队非常重视具有很少活动部件的简单解决方案。</p><p> A natural way forward could have been to build this new flexible sharding model within our application. Since our application was already involved with database shard routing, we could just bake in the new requirements such as sharding by channel id into that layer. This option was given consideration, and some prototypes were written to explore this idea more fully. It became clear that there was already quite a bit of coupling between the application logic and how the data was stored. It also became apparent that it was going to be time consuming to untangle that problem, while also building the new solution.</p><p> 一种自然的前进方式可能是在我们的应用程序中构建此新的灵活分片模型。由于我们的应用程序已经涉及数据库分片路由，因此我们可以满足新的要求，例如按通道ID在该层中分片。考虑了该选项，并编写了一些原型来更全面地探索这一思想。显然，应用程序逻辑与数据存储方式之间已经存在相当多的耦合。同样很明显，解决这个问题并建立新的解决方案将非常耗时。 </p><p> For example, something like fetching the count of messages in a channel was tightly coupled to assumptions about what team the channel was on, and many places in our codebase worked around assumptions for organizations with multiple workspaces by checking multiple shards explicitly.</p><p>例如，诸如获取通道中的消息数之类的事情与关于通道所在团队的假设紧密相关，并且在代码库中的许多地方都通过显式检查多个分片来解决具有多个工作区的组织的假设。</p><p>  On top of this, building sharding awareness into the application didn’t address any of our operational issues or allow us to use read replicas more effectively. Although it would solve the immediate scaling problems, this approach seemed positioned to run into the very same challenges in the long term.  For instance, if a single team’s shard got surprisingly hot on the write path, it was not going to be straightforward to horizontally scale it.</p><p>  最重要的是，在应用程序中建立分片意识并不能解决我们的任何操作问题，也无法使我们更有效地使用只读副本。尽管它可以解决当前的扩展问题，但从长远来看，这种方法似乎可以应对同样的挑战。例如，如果一个团队的分片在写入路径上出奇地热，那么对其进行水平扩展就不是一件容易的事。</p><p>  Around this time we became aware of the Vitess project. It seemed like a promising technology since at its core, Vitess provides a database clustering system for horizontal scaling of MySQL.</p><p>  大约在这个时候，我们意识到了Vitess项目。自从其核心Vites提供了用于MySQL的水平扩展的数据库集群系统以来，这似乎是一项很有前途的技术。</p><p> At a high level Vitess ticked all the boxes of our application and operational requirements.</p><p> 在较高的级别上，Vitess勾选了我们的应用程序和操作要求的所有框。</p><p> MySQL Core : Vitess is built on top of MySQL, and as a result leverages all the years of reliability, developer understanding, and confidence that comes from using MySQL as the actual data storage and replication engine.</p><p> MySQL Core：Vitess建立在MySQL之上，因此利用了多年来使用MySQL作为实际数据存储和复制引擎的可靠性，开发人员理解和信心。</p><p> Scalability : Vitess combines many important MySQL features with the scalability of a NoSQL database. Its built-in sharding features lets you flexibly shard and grow your database without adding logic to your application.</p><p> 可伸缩性：Vitess将许多重要的MySQL功能与NoSQL数据库的可伸缩性结合在一起。它的内置分片功能使您可以灵活地分片和扩展数据库，而无需向应用程序中添加逻辑。</p><p> Operability : Vitess automatically handles functions like primary failovers and backups. It uses a lock server to track and administer servers, letting your application be blissfully ignorant of database topology. Vitess keeps track of all of the metadata about your cluster configuration so that the cluster view is always up-to-date and consistent for different clients.</p><p> 可操作性：Vitess自动处理诸如主要故障转移和备份之类的功能。它使用锁定服务器来跟踪和管理服务器，使您的应用程序对数据库拓扑一无所知。 Vitess会跟踪有关集群配置的所有元数据，以便集群视图始终是最新的，并且对于不同的客户端是一致的。 </p><p> Extensibility : Vitess is built 100% in open source using golang with an extensive set of test coverage and a thriving and open developer community. We felt confident that we would be able to make changes as needed to meet Slack’s requirements (which we did!).</p><p>可扩展性：Vitess是使用golang在开放源代码中100％构建的，它具有广泛的测试范围以及一个繁荣的开放开发者社区。我们有信心，我们将能够根据需要进行更改以满足Slack的要求（我们做到了！）。</p><p>   We decided to build a prototype demonstrating that we can migrate data from our traditional architecture to Vitess and that Vitess would deliver on its promise. Of course, adopting a new datastore at Slack scale is not an easy task. It required a significant amount of effort to set up all the new infrastructure in place.</p><p>   我们决定构建一个原型，以证明我们可以将数据从传统架构迁移到Vitess，并且Vitess将兑现其承诺。当然，采用Slack规模的新数据存储并非易事。它需要大量的精力来建立所有新的基础架构。</p><p> Our goal was to build a working end-to-end use case of Vitess in production for a small feature: integrating an RSS feed into a Slack channel. It required us to rework many of our operational processes for provisioning deployments, service discovery, backup/restore, topology management, credentials, and more. We also needed to develop new application integration points to route queries to Vitess, a generic backfill system for cloning the existing tables while performing double-writes from the application, and a parallel double-read diffing system so we were sure that the Vitess-powered tables had the same semantics as our legacy databases. However, it was worth it: the application performed correctly using the new system, it had much better performance characteristics, and operating and scaling the cluster was simpler. Equally importantly, Vitess delivered on the promise of resilience and reliability. This initial migration gave us the confidence we needed to continue our investment in the project.</p><p> 我们的目标是为生产中的一个小功能构建一个工作中的Vitess端到端用例：将RSS feed集成到Slack频道中。它要求我们对许多操作流程进行重新设计，以进行部署部署，服务发现，备份/还原，拓扑管理，凭据等。我们还需要开发新的应用程序集成点，以将查询路由到Vitess，这是一个通用的回填系统，用于在从应用程序执行两次写入操作时克隆现有表，还需要一个并行的双读取差异系统，因此我们可以确保使用Vitess技术表具有与我们的旧数据库相同的语义。但是，这是值得的：应用程序使用新系统可以正确执行，性能要好得多，并且群集的操作和扩展也更简单。同样重要的是，Vitess兑现了弹性和可靠性的承诺。最初的迁移使我们有信心继续进行该项目的投资。</p><p> At the same time, it is still important to call out that during this initial prototype and continuing for the years since, we have identified gaps in Vitess in ways that it would not work for some of Slack-specific needs out of the box. As the technology showed promise at solving the core challenges we were facing, we decided it was worth the engineering investment to add-in the missing functionality.</p><p> 同时，重要的是要指出，在这个初始原型期间并持续了数年，我们已经确定了Vitess中的不足之处，以至于无法满足某些特定于Slack的需求。由于该技术在解决我们面临的核心挑战方面显示出了希望，因此我们认为值得我们在工程上进行投资以补充缺少的功能。</p><p>  Closing some of the gaps in full MySQL query compatibility. [  a ], [  b ], [  c ], [  d ], [  e ].</p><p>  弥补完全MySQL查询兼容性方面的一些空白。 [a]，[b]，[c]，[d]，[e]。</p><p> Today, it is not an overstatement to say that some of the folks in the open source community are an extended part of our team, and since adopting Vitess, Slack has become and continues to be one of the biggest contributors to the open source   project .</p><p> 如今，可以肯定地说开源社区中的某些人是我们团队的一部分。自采用Vitess以来，Slack已成为并将继续成为开源项目的最大贡献者之一。</p><p> Now, exactly three years into this migration, we are sitting at 99% of all Slack MySQL traffic having been migrated to Vitess.  We are on track to finish the remaining 1% in the next two months. We’ve wanted to share this story for a long time, but we waited until we had full confidence that this project was a success.</p><p> 现在，距这次迁移仅三年时间，已经有99％的Slack MySQL流量迁移到了Vitess。我们有望在未来两个月内完成剩余的1％。我们一直想分享这个故事很久了，但是我们一直等到我们完全有信心这个项目成功了。 </p><p> Here’s a graph showing the migration progression and a few milestones over the last few years:</p><p>这是一张图表，显示了过去几年的迁移进度和一些里程碑：</p><p> There are many other stories to tell in these 3 years of migrations. Going from 0% to 99% adoption also meant going from 0 QPS to the 2.3 M QPS we serve today. Choosing appropriate sharding keys, retrofitting our existing application to work well with Vitess, and changes to operate Vitess at scale were necessary and each step along the way we learned something new. We break down a specific migration of a table that comprises 20% of our overall query load in a case study in  Refactoring at Scale , written with Maude Lemaire, a Staff Engineer at Slack. We also plan on writing about our change in migration strategy and technique to move whole shards instead of tables in a future blog post.</p><p> 在这三年的迁移中，还有许多其他故事要讲。从0％提高到99％的采用率也意味着从0 QPS到我们今天服务的2.3 M QPS。选择适当的分片键，对我们现有的应用程序进行改造以使其能够与Vitess一起正常工作，并且有必要进行大规模操作Vitess的更改，这是我们学习新知识的每个步骤。在Slack的一名高级工程师Maude Lemaire撰写的Scale Rescaleing案例研究中，我们分解了一个表的特定迁移，该表占我们整体查询负载的20％。我们还计划在未来的博客文章中介绍有关迁移策略和技术的更改，以移动整个分片而不是表格。</p><p>  Today, we run multiple Vitess clusters with dozens of keyspaces in different geographical regions around the world. Vitess is used by both our main webapp monolith as well as other services. Each keyspace is a logical collection of data that roughly scales by the same factor — number of users, teams, and channels. Say goodbye to only sharding by team, and to team hot-spots! This flexible sharding provided to us by Vitess has allowed us to scale and grow Slack.</p><p>  今天，我们在全球不同地理区域中运行带有多个键空间的多个Vitess集群。我们的主要Web应用程序整体以及其他服务都使用Vitess。每个键空间都是数据的逻辑集合，可以按相同的因素（用户，团队和渠道数）大致扩展。与仅按团队分片以及对团队热点说再见！ Vitess提供给我们的灵活分片使我们能够扩展和扩展Slack。</p><p> During March 2020, as our CEO Stewart Butterfield   tweeted , we saw an unprecedented increased usage of Slack as the reality of the COVID-19 pandemic hit the U.S. and work/school shifted out of offices and became distributed. On the datastores side, in just one week we saw query rates increase by 50%. In response to this, we scaled up one of our busiest keyspaces horizontally using Vitess’s splitting workflows. Without resharding and moving to Vitess, we would’ve been unable to scale at all for our largest customers, leading to downtime.     As product teams at Slack started writing new services, they were able to use the same storage technology we use for the webapp. Choosing Vitess instead of building a new sharding layer inside our webapp monolith has allowed us to leverage the same technology for all new services at Slack. Vitess is also the storage layer for our International Data Residency product, for which we run Vitess clusters in six total regions. Using Vitess here was instrumental to being able to ship this feature in record time. It enabled our product engineering team to focus on the core business logic, while the actual region locality of the data was abstracted from their efforts. When we chose Vitess, we didn’t expect to be writing new services or shipping a multi-region product, but as a result of Vitess’s suitability and our investment in it over the last few years, we’ve been able to leverage the same storage technology for these new product areas.</p><p> 2020年3月，随着我们的首席执行官Stewart Butterfield发推文，随着COVID-19大流行的现实席卷美国，工作/学校搬离办公室并分布起来，我们看到了Slack的使用空前增加。在数据存储方面，仅一周时间，我们就发现查询率提高了50％。为此，我们使用Vites的拆分工作流程水平扩展了我们最繁忙的键空间之一。如果不进行分片和迁移到Vitess，我们将根本无法为最大的客户扩展规模，从而导致停机。当Slack的产品团队开始编写新服务时，他们能够使用与Webapp相同的存储技术。选择Vitess而不是在我们的Webapp整体中建立新的分片层，使我们能够将相同的技术用于Slack的所有新服务。 Vitess还是我们的国际数据驻留产品的存储层，为此，我们在总共六个区域中运行Vitess群集。在这里使用Vitess有助于在创纪录的时间内发布此功能。它使我们的产品工程团队能够专注于核心业务逻辑，而数据的实际区域局部性则是从他们的努力中抽象出来的。当我们选择Vitess时，我们没想到会写新服务或交付多地区产品，但是由于Vitess的适用性以及我们过去几年对它的投资，我们已经能够利用相同的这些新产品领域的存储技术。</p><p> Now that the migration is complete, we look forward to leveraging more capabilities of Vitess. We have been already investing in   VReplication , a feature that allows you to hook into MySQL replication to materialize different views of your data.</p><p> 现在迁移已完成，我们期待利用Vitess的更多功能。我们已经在VReplication上进行了投资，该功能使您可以加入MySQL复制以实现数据的不同视图。</p><p> The picture below shows a simplified version of what our Vitess deployment at Slack looks like.</p><p> 下图显示了我们在Slack的Vitess部署的简化版本。</p><p>   This success still begs the question: Was this the right choice? In Spanish, there is a saying that states: “Como anillo al dedo”. It is often used when a solution fits with great exactitude. We think that even with the benefit of hindsight, Vitess was the right solution for us. This doesn’t mean that if Vitess didn’t exist, we would have not figured out how to scale our datastores. Rather, that with our requirements, we would have landed on a solution that would be very similar to Vitess .  In a way, this story is not only about how Slack scaled its datastores.  It is also a story that tells the importance of collaboration in our industry .</p><p>   这种成功仍然引出一个问题：这是正确的选择吗？在西班牙语中，有一句话说：“ Como anillo al dedo”。通常在解决方案非常精确时使用。我们认为，即使有了事后的见识，Vitess仍然是我们的正确解决方案。这并不意味着如果Vitess不存在，我们就不会想出如何扩展数据存储区。相反，根据我们的要求，我们本可以找到与Vitess非常相似的解决方案。在某种程度上，这个故事不仅涉及Slack如何扩展其数据存储。这也是一个讲述我们行业中合作重要性的故事。 </p><p> We wanted to give a shout out to all the people that have contributed to this journey: Alexander Dalal, Ameet Kotian, Andrew Mason, Anju Bansal, Brian Ramos, Chris Sullivan, Daren Seagrave, Deepak Barge, Deepthi Sigireddi, Huiqing Zhou, Josh Varner, Leigh Johnson, Manuel Fontan, Manasi Limbachiya, Malcolm Akinje, Milena Talavera, Mike Demmer,  Morgan Jones, Neil Harkins, Paul O’Connor, Paul Tuckfield, Renan Rangel, Ricardo Lorenzo, Richard Bailey, Ryan Park, Sara Bee, Serry Park, Sugu Sougoumarane, V. Brennan and all the others who we probably forgot.</p><p>我们想向所有为这一旅程做出贡献的人们大声疾呼：亚历山大·达拉尔，阿米特·科蒂安，安德鲁·梅森，安朱·班萨尔，布莱恩·拉莫斯，克里斯·沙利文，达伦·希格拉夫，迪帕克·驳船，迪皮·西吉雷迪，周慧清，乔什·瓦纳 ，雷·约翰逊（Leigh Johnson），曼努埃尔·方丹（Manuel Fontan），玛纳西·林巴希亚（Manasi Limbachiya），马尔科姆·阿金耶（Malcolm Akinje），麦琳娜·塔拉维拉（Mikena Demmer），摩根·琼斯（Morgan Demmer），尼尔·哈金斯（Neil Harkins），保罗·奥康纳（Paul O'Connor），保罗·塔克菲尔德（Paul Tuckfield），雷南·兰格尔（Renan Rangel），里卡多·洛伦佐（Ricardo Lorenzo），理查德·贝利（Richard Bailey），瑞安·帕克（Ryan Park），莎拉·比尔（Sara Bee），Serry Park ，Sugu Sougoumarane，V。Brennan和我们可能忘记的所有其他人。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://slack.engineering/scaling-datastores-at-slack-with-vitess/">https://slack.engineering/scaling-datastores-at-slack-with-vitess/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/vitess/">#vitess</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>