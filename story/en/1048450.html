<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>每秒12个请求–现实的Python Web框架 12 requests per second – Realistic Python web frameworks</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">12 requests per second – Realistic Python web frameworks<br/>每秒12个请求–现实的Python Web框架 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-19 10:35:39</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/f41f962f6231a5b67d33cf1b8bcacb57.jpeg"><img src="http://img2.diglog.com/img/2021/2/f41f962f6231a5b67d33cf1b8bcacb57.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>If you take a look around the blogosphere at various benchmarks for Python web frameworks, you might start to feel pretty bad about your own setup. Or, alternatively, super-hyped about the possibilities.</p><p>如果您在Python Web框架的各种基准上查看博客圈，您可能会开始对自己的设置感到非常糟糕。或者，或者，超级估量关于可能性。</p><p> Consider, for instance, the incredible work of the guys at  magic stack, getting  100,000 requests per second from  uvloop in a single thread. This is on par with compiled language like Go&#39;s performance.</p><p> 例如，考虑魔术堆栈中的人员的难以置信的工作，从单个线程中从UVLoop获得每秒100,000个请求。这与编译语言相似，如Go＆＃39; s表现。</p><p> But that benchmark doesn&#39;t really cover a fully fleshed out web framework, right? We need a lot more functionality and structure from our frameworks than reading and writing bytes. What about fully fleshed-out web-frameworks in python?</p><p> 但那个基准并不真的覆盖一个完全充实的Web框架，对吗？我们需要我们框架的更多功能和结构，而不是读写字节。在Python中完全充实的网页框架怎么样？</p><p> One such framework is  Sanic, which again has been shown to have similar performance:  100,000 requests per-second. Or there&#39;s  Vibora. Not only does this claim to be a drop-in replacement for  Flask, but it also has its own templating engine. And it handles  350,000 requests per second!</p><p> 一个这样的框架是SANIC，它再次被证明具有类似的性能：每秒100,000个请求。或者＆＃39; s vibora。这一声称不仅是烧瓶的替代品，而且还有自己的模板发动机。它每秒处理350,000个请求！</p><p> Even more mind-blowing is  Japronto which claims an insane  1.2 million requests per-second in a single thread 🤯 trouncing the performance of other languages and frameworks:</p><p> 更令人兴奋的是日本吹嘘，其中索赔了一个疯狂的120万次在一个线程中的请求♪促进其他语言和框架的表现：</p><p>  Recently we&#39;ve been doing a lot of work improving the performance of our Python APIs. Currently we&#39;re running  Flask, and we initially had a single question:  how can we serve more requests from a single worker thread? But looking at these benchmarks had us asking more:</p><p>  最近我们一直在做很多工作，提高了我们Python API的表现。目前我们＆＃39;重新运行烧瓶，我们最初有一个问题：我们如何从单个工人线程提供更多请求？但是看着这些基准让我们询问更多：</p><p>  In other words, how much should we trust these benchmarks? And to what extent should they influence our choice of technology?</p><p>  换句话说，我们应该相信这些基准多少钱？他们应该在多大程度上影响我们的技术选择？ </p><p> In order to answer these questions, in this post, I benchmark a realistic Flask application along with it&#39;s  Sanic equivalent. I&#39;m going to guess that most readers come from a background with one of the more &#34;traditional&#34; Python frameworks ( Flask or  Django), and it&#39;s certainly more relevant to devs here at Suade Labs. For this reason, I run the Flask app in a number of different ways, to see what the best bang for our buck is: how performant can we make our application with (almost) zero changes to the code? Along the way we&#39;ll pick up some tips for the original question:  how can we serve more requests from a single worker thread?</p><p>为了回答这些问题，在这篇文章中，我将一个现实的烧瓶申请与它一起基准。我将猜测大多数读者来自一个＆＃34之一的背景;传统＆＃34; Python框架（烧瓶或Django），它＆＃39;在Suade Labs的Devs肯定更相关。出于这个原因，我以多种不同的方式运行烧瓶应用程序，看看我们的降价最好的爆炸是：我们如何使用（几乎）对代码进行零变化的应用程序？沿着我们＆＃39; ll为原始问题提取了一些提示：我们如何从单个工人线程提供更多请求？</p><p> Sidenote: if you&#39;re new to Python&#39;s web frameworks, or its asynchronous libraries, take a look at [1] from the addenda at the bottom of this post for a quick explainer. This post mostly assumes you know these things.</p><p> Sidenote：如果你＆＃39;重新到Python＆＃39;它的Web框架或其异步库，从此帖子底部的Addenda看看[1]以获得快速解释者。这篇文章主要假设您知道这些事情。</p><p>  First let&#39;s run some simple &#34;Hello, World!&#34; benchmarks on our system to get a meaningful baseline for comparison. For reference, the Flask benchmarks on  techempower give 25,000 requests per second.</p><p>  首先让＆＃39;跑了一些简单的＆＃34;你好，世界！＆＃34;我们系统的基准测试以获得有意义的基线进行比较。供参考，TechEmpower上的烧瓶基准将每秒提供25,000个请求。</p><p>  app = Flask(__name__)@app.route(&#34;/&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;])def hello(): if request.method == &#34;GET&#34;: return &#34;Hello, World!&#34; data = request.get_json(force=True) try: return &#34;Hello, {id}&#34;.format(**data) except KeyError: return &#34;Missing required parameter &#39;id&#39;&#34;, 400</p><p>  app = flask（__ name __）@ app.route（＆＃34; /＆＃34;方法= [＆＃34; get＆＃34; post＆＃34;]）def hello（）：如果请求.Method ==＆＃34; Get＆＃34 ;:返回＆＃34;你好，世界！＆＃34; data = request.get_json（force = true）尝试：返回＆＃34; hello，{id}​​＆＃34; .format（**数据）除keyerror之外：返回＆＃34;缺少必需参数＆＃39; id＆＃ 39;＆＃34 ;,400</p><p> I ran it under a variety of conditions. First &#34;raw&#34; via  python app.py, and then under  Gunicorn with a single  sync worker via  gunicorn -k sync app:app and finally Gunicorn with a single  gevent worker via  gunicorn -k gevent app:app. In theory Gunicorn should handle concurrency and dropped connections much better than the raw python, and using the gevent worker should allow us to do asynchronous IO without changing our code [2a]. We also ran these benchmarks under  PyPy, which in theory should speed up any CPU-bound code without making any changes (if you haven&#39;t heard of PyPy see [2b] in the addenda below for a quick explanation and some terminology).</p><p> 我在各种情况下跑了它。第一个＆＃34;生＆＃34;通过Python app.py，然后在Gunicorn下通过Gunicorn -k同步应用程序：应用程序和最后枪手通过Gunicorn -K Gevent应用程序：应用程序的枪手，并终于使用单个Gevent工作者在理论上，丘陵应该处理并发性和掉落的连接，比原始Python好得多，并且使用Gevent Worlt应该让我们在不改变代码[2a]的情况下进行异步IO。我们也在小明课程下运行了这些基准，从理论上应该加快任何CPU绑定的代码而不进行任何变化（如果您没有听到Pypy在下面的addenda中看到[2b]，以便快速解释和一些术语） 。</p><p>  app = Sanic(__name__)@app.route(&#34;/&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;])async def hello(request): if request.method == &#34;GET&#34;: return text(&#34;Hello, World!&#34;) data = request.json try: return text(&#34;Hello, {id}&#34;.format(**data)) except KeyError: raise InvalidUsage(&#34;Missing required parameter &#39;id&#39;&#34;)</p><p>  app = sanic（__ name __）@ app.route（＆＃34; /＆＃34;方法= [＆＃34; get＆＃34; post＆＃34; post＆＃34;]）async def hello（请求）：如果请求..ethod ==＆＃34; get＆＃34 ;:返回文本（＆＃34;您好，世界！＆＃34;）data = request.json try：返回文本（＆＃34; hello，{id} ＆＃34; .format（**数据））除了KeyError之外：提高Invalidusage（＆＃34;缺少所需参数＆＃39; id＆＃39;＆＃34;）</p><p>   Some technical details: I used Python 3.7 with the regular CPython interpreter and Python 3.6 with PyPy 7.3.3. At the time of writing, running 3.6 is the latest PyPy interpreter, and their Python 2.7 interpreter is faster in some edge cases, but as Python 2 is  officially dead, I don&#39;t believe it productive to benchmark. My system details are available in the addenda [3]. I used  wrk to actually execute the benchmarks.  I&#39;ll break the results down in two parts. First: Sanic dominates, with 23,000 requests a second, although running our Flask app under Guncorn + gevent and PyPy does a pretty good job at keeping up. Second: what&#39;s going on with the performance range for our Flask app?</p><p>   一些技术细节：我使用Python 3.7与常规CPython解释器和Python 3.6，具有Pypy 7.3.3。在撰写本文时，运行3.6是最新的Pypy解释器，他们的Python 2.7解释器在一些边缘案件中更快，但随着Python 2正式死亡，我不相信它为基准提高了它。我的系统细节可在附录[3]中提供。我用WRK实际执行基准。我＆＃39; ll在两部分中断结果。第一：Sanic占主导地位，每秒有23,000个请求，虽然在枪击+ Gevent和Pypy下运行我们的烧瓶应用程序，但在跟上持续很好的工作。第二：什么＆＃39;我们的烧瓶应用程序的性能范围？ </p><p> Under CPython, we see that using Gunicorn quadruples the number of Flask requests per second from 1,000 to 4,000 and using a gevent worker adds a mild (sub 10%) speed boost to this. The PyPy results are more impressive. In the raw test, it is churning through 3,000 requests a second; it received the same 4x speed boost from Gunicorn, getting us to 12,000 requests a second; finally with the addition of gevent, it cranks up to 17,000 requests a second, 17x more than the raw CPython version without changing a single line of code.</p><p>在CPython下，我们看到使用Gunicorn将每秒Flask请求的数量增加了三倍，从1000增至4,000，而使用gevent worker增加了适度（低于10％）的速度。 PyPy的结果更令人印象深刻。在原始测试中，它每秒处理3,000个请求。它获得了Gunicorn相同的4倍速度提升，使我们每秒达到12,000个请求；最后，加上gevent，它每秒处理多达17,000个请求，比原始CPython版本高出17倍，而无需更改任何代码行。</p><p> I was quite struck by the fact that gevent had such little effect on the CPython process - probably this is because the CPU is maxed out at this point. On the other hand, it seems that PyPy&#39;s better speed means it is still spending time waiting on system calls / IO, even under Gunicorn. Adding gevent to the mix means that it switches between concurrent connections, processing them as fast as the CPU will let it.</p><p> gevent对CPython进程的影响很小，这让我感到非常震惊-可能是因为此时CPU已达到极限。另一方面，PyPy的速度似乎更快，这意味着即使在Gunicorn的支持下，PyPy仍在花时间等待系统调用/ IO。在混合中添加gevent意味着它可以在并发连接之间切换，并以CPU允许的速度进行处理。</p><p> To get a real sense of this, I ran the benchmark whilst monitoring CPU usage. Here&#39;s a short test against the raw app under PyPy:</p><p> 为了真正理解这一点，我在监视CPU使用率的同时运行了基准测试。以下是针对PyPy下原始应用的简短测试：</p><p>  You can see that the program hops between CPU cores and rarely utilises 100% of a given core. On the other hand, here&#39;s part of a much longer test against the Gunicorn gevent worker under PyPy:</p><p>  您可以看到该程序在CPU内核之间跳转，很少使用给定内核的100％。另一方面，这是在PyPy下针对Gunicorn gevent工作者进行的更长测试的一部分：</p><p>  Now it&#39;s evident that there is no switching between CPU cores (the process has become &#34;sticky&#34;) and the individual core is being utilised to a far higher degree.</p><p>  现在，很明显，CPU内核之间没有切换（该过程变得“粘滞”），并且单个内核的使用程度更高。</p><p>   The benchmark above, while fun, is pretty meaningless for real-world applications. Let&#39;s add some more functionality to our app!  First, we&#39;ll allow users to actually store data in a database, which we&#39;ll retrieve via an ORM (in our case  SQLAlchemy, the de-facto stand-alone ORM in python). Second, we&#39;ll add input-validation to make sure our users get meaningful error messages, and that we&#39;re not accepting junk that crashes our app. Finally we&#39;ll add a response marshaller to automate the process of converting our database object to JSON.</p><p>   上面的基准测试虽然很有趣，但对于现实世界的应用程序却毫无意义。让我们为我们的应用添加更多功能！首先，我们将允许用户将数据实际存储在数据库中，并通过ORM（在我们的示例中为SQLAlchemy，实际上是python中的独立ORM）检索数据库。其次，我们将添加输入验证，以确保我们的用户收到有意义的错误消息，并且我们不接受使应用程序崩溃的垃圾邮件。最后，我们将添加一个响应编组器以自动化将数据库对象转换为JSON的过程。</p><p> We&#39;ll write a simple book store app, for a publishing house. We have a number of authors each writing zero or more books in several genres. For simplicity, each book has only a single author, but can have multiple genres - for example we could have a book which is in both the &#34;Existential Fiction&#34; and &#34;Beatnik Poetry&#34; categories. We&#39;re going to add 1 million authors to our database and roughly 10 million books. [4]</p><p> 我们将为出版社编写一个简单的书店应用。我们有许多作者，每本书写作几类或零本或更多本。为简单起见，每本书只有一位作者，但可以有多种体裁-例如，我们可以有一本书同时存在于＆＃34; Existential Fiction＆＃34;中。和＆＃34;贝特尼克诗歌＆＃34;类别。我们将向数据库中增加100万名作者和大约1000万本书。 [4] </p><p>  class Author(db.Model): id = db.Column(UUIDType, primary_key=True) name = db.Column(db.String, nullable=False) ... # snip!class Book(db.Model): author_id = db.Column( UUIDType, db.ForeignKey(&#34;author.id&#34;), nullable=False, index=True ) author = db.relationship(&#34;Author&#34;, backref=&#34;books&#34;) ... # snip!</p><p>类Author（db.Model）：id = db.Column（UUIDType，primary_key = True）名称= db.Column（db.String，nullable = False）...＃snip！class Book（db.Model）：author_id = db.Column（UUIDType，db.ForeignKey（＆＃34; author.id＆＃34;），nullable = False，index = True）author = db.relationship（＆＃34; Author＆＃34 ;, backref =＆＃34 ; books＆＃34;）...＃剪！</p><p> To marshal these, we use  Marshmallow, which is a popular Python marshalling library. Here&#39;s an example of the Marshmallow model for the Author overview:</p><p> 要封送这些文件，我们使用棉花糖，这是一个流行的Python封送库。这是作者概述的棉花糖模型的示例：</p><p> class Author(Schema): id = fields.Str(dump_only=True) name = fields.Str(required=True) country_code = EnumField(CountryCodes, required=True) email = fields.Str(required=True) phone = fields.Str(required=True) contact_address = fields.Str(required=True) contract_started = fields.DateTime(format=&#34;iso&#34;) contract_finished = fields.DateTime(format=&#34;iso&#34;) contract_value = fields.Integer()</p><p> 类Author（Schema）：id =字段.Str（dump_only = True）名称=字段.Str（required = True）country_code = EnumField（CountryCodes，required = True）email =字段.Str（required = True）phone =字段。 Str（required = True）contact_address =字段.Str（required = True）contract_started =字段.DateTime（format =＆＃34; iso＆＃34;）contract_finished =字段.DateTime（format =＆＃34; iso＆＃34;） contract_value = fields.Integer（）</p><p>  @bp.route(&#34;/author&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;])def author(): &#34;&#34;&#34;View all authors, or create a new one.&#34;&#34;&#34; if request.method == &#34;GET&#34;: args = validate_get(marshallers.LimitOffsetSchema()) limit = args[&#34;limit&#34;] offset = args[&#34;offset&#34;] authors = Author.query.limit(limit).offset(offset).all() return jsonify(marshallers.authors.dump(authors)) if request.method == &#34;POST&#34;: author = Author(**validate_post(marshallers.author)) db.session.add(author) db.session.commit() return jsonify({&#34;id&#34;: author.id})</p><p>  @ bp.route（＆＃34; / author＆＃34 ;, methods = [＆＃34; GET＆＃34 ;,＆＃34; POST＆＃34;]）def author（）：＆＃34;＆＃34; ＆＃34;查看所有作者，或创建一个新作者。＆＃34;＆＃34;＆＃34;如果request.method ==＆＃34; GET＆＃34 ;： args = validate_get（marshallers.LimitOffsetSchema（））limit = args [＆＃34; limit＆＃34;] offset = args [＆＃34; offset＆＃34; ] authors = Author.query.limit（limit）.offset（offset）.all（）如果request.method ==＆＃34; POST＆＃34;则返回jsonify（marshallers.authors.dump（authors））：author = Author （** validate_post（marshallers.author））db.session.add（作者）db.session.commit（）返回jsonify（{＆＃34; id＆＃34 ;: author.id}）</p><p> The full source code can be viewed in the  GitHub repo. Here, the thing to note is that  marshallers.foo is an instance of a  Marshmallow schema, which can be used both to validate a Foo input, for instance in a POST request, as well as to marshal Foo instances ready for returning as JSON.</p><p> 完整的源代码可以在GitHub存储库中查看。在这里，需要注意的是marshallers.foo是Marshmallow模式的实例，可用于验证Foo输入（例如在POST请求中）以及封送准备作为JSON返回的Foo实例。</p><p> In order to actually perform asynchronous database requests, some fancy footwork is required with patching libraries, which depends on which postgres connector you use. SQLAlchemy does not support this out of the box, and in fact its primary developer has a great post arguing that  an async ORM is not always a great idea. Juicy technical details in addenda [5], but beware that just using a Gunicorn gevent worker will not necessarily get you what you want.</p><p> 为了实际执行异步数据库请求，修补库需要花哨的步伐，这取决于您使用的postgres连接器。 SQLAlchemy不支持此功能，实际上，它的主要开发人员在一篇很棒的文章中指出，异步ORM并不总是一个好主意。附录[5]中有许多技术细节，但是请注意，仅使用Gunicorn gevent worker并不一定能为您提供所需的东西。</p><p> PyPy tends to suffer a performance hit when using C-extensions and libraries instead of pure python, conversely CPython should get a performance boost from the C-based libs. To take account of this I tested two different underlying database connectors: both  psycopg2 and a pure-python counterpart  pg8000, and two different classes of async gunicorn worker:  gevent and a pure-python counterpart  eventlet.</p><p> 当使用C扩展名和库而不是纯python时，PyPy往往会遭受性能下降，相反，CPython应该从基于C的库中获得性能提升。考虑到这一点，我测试了两个不同的基础数据库连接器：psycopg2和一个纯Python对应的pg8000，以及两个不同类的异步gunicorn worker：gevent和一个纯python对应的eventlet。 </p><p> What about the Sanic rewrite of our app? Well, as mentioned SQLAlchemy is not really async, and it definitely doesn&#39;t support python&#39;s  await syntax. So if we want non-blocking database requests we have three choices:</p><p>Sanic重写我们的应用程序怎么样？好吧，如上所述，SQLAlchemy并不是真正异步的，它绝对不支持python的await语法。因此，如果我们要非阻塞数据库请求，我们有三个选择：</p><p> choose a library like  databases which allows us to keep the models / SQLAlchemy core for queries, but loose a lot of the features</p><p> 选择一个像数据库这样的库，它使我们能够保留模型/ SQLAlchemy核心进行查询，但会失去很多功能</p><p> We&#39;ll get the best code from 1, but it will also involve the most thought and re-writing. It pulls in many other considerations: for instance, schema migrations, testing, how to deal with missing features (SQLAlchemy just does a lot of  advanced stuff that other ORMs don&#39;t do). The fastest application will probably come from 3, but also the most technical debt, pain and opacity.</p><p> 我们将从1中获得最好的代码，但它也将涉及最多的思考和重写。它引入了许多其他考虑因素：例如，架构迁移，测试，如何处理缺少的功能（SQLAlchemy只是做了许多其他ORM不会做的高级事情）。最快的应用程序可能来自3，但也有技术欠佳，痛苦和不透明性。</p><p> In the end I opted for 2 and almost immediately wished I&#39;d done 1. In part this was due to some incompatibilities between the various libraries. But it also made joins very tedious and hacky to marshal correctly. After this brief diversion, I switched to  Tortoise ORM which was really pleasant in comparison!</p><p> 最后，我选择了2，几乎立即希望我完成1。部分原因是各个库之间存在一些不兼容性。但是，这也使加入变得非常乏味和hacky，无法正确地进行封送。经过短暂的转移之后，我切换到了Tortoise ORM，相比之下，这真的很令人愉快！</p><p>  @bp.route(&#34;/author&#34;, methods=[&#34;GET&#34;, &#34;POST&#34;])async def author(request): &#34;&#34;&#34;View all authors, or create a new one.&#34;&#34;&#34; if request.method == &#34;GET&#34;: args = validate_get(request, marshallers.LimitOffsetSchema()) limit = args[&#34;limit&#34;] offset = args[&#34;offset&#34;] authors = await Author.all().prefetch_related( &#34;country_code&#34; ).limit(limit).offset(offset) return json(marshallers.authors.dump(authors)) if request.method == &#34;POST&#34;: author = Author(**validate_post(marshallers.author)) await author.save() return json({&#34;id&#34;: author.id})</p><p>  @ bp.route（＆＃34; / author＆＃34 ;, Methods = [＆＃34; GET＆＃34 ;,＆＃34; POST＆＃34;]）异步定义作者（请求）：＆＃34;＆＃ 34;＆＃34;查看所有作者，或创建一个新作者。＆＃34;＆＃34;＆＃34;如果request.method ==＆＃34; GET＆＃34 ;： args = validate_get（request，marshallers.LimitOffsetSchema（））limit = args [＆＃34; limit＆＃34;] offset = args [＆＃34; offset＆＃ 34;] authors = await Author.all（）。prefetch_related（＆＃34; country_code＆＃34;）.limit（limit）.offset（offset）如果请求request.method =，则返回json（marshallers.authors.dump（authors））。 =＆＃34; POST＆＃34 ;：作者= Author（** validate_post（marshallers.author））等待author.save（）返回json（{＆＃34; id＆＃34 ;: author.id}）</p><p> Notice in the above that I had to &#34;prefetch&#34; (i.e. join) the country code table.  This had to do with difficulty expressing that I wanted a foreign key constraint, but not a relationship/join in Tortoise ORM. There is undoubtably some voodoo I can do to fix this, but it&#39;s not super-obvious. The country code table just consists of the 300 or so ISO 3166 country codes, so is probably in memory and any overhead will be marginal.</p><p> 请注意，在上文中，我必须＆＃34; prefetch＆＃34; （即加入）国家/地区代码表。这与表达我想要外键约束而不是Tortoise ORM中的关系/联接很困难。毫无疑问，我可以采取一些伏都教来解决此问题，但这并不是很明显。国家/地区代码表仅由300个左右的ISO 3166国家/地区代码组成，因此可能在内存中，任何开销都是很小的。</p><p> Key takeaways: Switching frameworks requires you to evaluate and choose an entire ecosystem of libraries, along with their peculiarities. Sanic and Tortoise are really nice and have great ergonomics for working with  asyncio. Working without an ORM is tedious.</p><p> 关键要点：转换框架要求您评估和选择整个库生态系统及其特性。 Sanic和Tortoise真的很棒，并且在使用asyncio方面具有出色的人体工程学。没有ORM的工作很乏味。 </p><p>  Let&#39;s start with the  /author/&lt;author_id&gt; endpoint. Here we select a single author, by primary key, from the database - collect a summary of each of their books and package the whole lot up to return to the user.</p><p>让我们从/ author /＆lt; author_id＆gt;开始端点。在这里，我们通过主键从数据库中选择一位作者-收集他们每本书的摘要，并将全部书籍打包以返回给用户。</p><p> Since I wanted at least some business logic in our app, I added what I consider to be an interesting field to the  Author model and  AuthorDetail marshaller:</p><p> 由于我希望在我们的应用程序中至少包含一些业务逻辑，因此我在Author模型和AuthorDetail marshaller中添加了我认为是有趣的字段：</p><p>  This essentially says that, to return the author&#39;s genres, we have to pull out all of their books&#39;  genres, and then merge into a deduplicated and sorted list.</p><p>  从本质上讲，要返回作者的体裁，我们必须拿出他们所有的书。种类，然后合并为已删除重复数据和排序的列表。</p><p>  As expected, the pure python libraries performed a little better than their C-based counterparts under PyPy and a little worse under CPython. Because nothing outside of a micro-benchmark is entirely neat, this was not always the case, and in fact the difference was completely marginal, so I didn&#39;t include all of the results. See addenda [6] for full results.</p><p>  不出所料，纯python库在PyPy下的性能比其基于C的同类要好一些，而在CPython下则要差一些。因为微基准测试之外没有什么是完全整洁的，所以情况并非总是如此，实际上差异完全是微不足道的，因此我没有包括所有结果。有关完整结果，请参见附录[6]。</p><p> No matter what libraries or setup we use here, we&#39;re performing less requests than the worst &#34;Hello, World!&#34; example in the intro. What&#39;s more, it seems like the asynchronous PyPy worker does worse than the synchronous one with high concurrency - which sort of flips the original benchmark on its head! Which pretty conclusively answers the other questions we had: &#34;Hello, World!&#34; benchmarks are not realistic and bear little relation to our actual application.</p><p> 无论我们在这里使用什么库或设置，与最差的“ Hello，World！”相比，我们执行的请求都更少。简介中的示例。更有什者，异步PyPy worker似乎比具有高并发性的同步PyPy worker更糟-这在某种程度上颠覆了原始基准！哪一个最终可以回答我们遇到的其他问题：＆nbsp; Hello，World！＆＃34;基准并不现实，与我们的实际应用关系不大。</p><p> Another conclusion we can draw is clear: if the database is fast, use PyPy to make the Python app fast too. Whatever interpreter you choose, the difference between asynchronous and synchronous workers is not really too big: certainly we could pick the best performing in each case, but it may have been noise [7]. Sanic performs a little less than twice as well as CPython + Flask, which is impressive, but probably not worth the effort of rewriting the app if we can get this for free under PyPy.</p><p> 我们可以得出的另一个结论很明确：如果数据库是快速的，也可以使用PyPy来使Python应用程序快速。无论您选择哪种解释器，异步工作者和同步工作者之间的区别并不是太大：当然，在每种情况下我们都可以选择性能最好的，但是它可能是噪音[7]。 Sanic的性能不到CPython + Flask的两倍，这是令人印象深刻的，但如果我们可以在PyPy下免费获得它，则可能不值得重写应用程序。</p><p> The  /author overview endpoint gives pretty much the same results. But let&#39;s see what happens if we put a little more load on the database. To simulate a complex query we&#39;re going to hit  /author?limit=20&amp;offset=50000, which should give the database something other to do than looking up by primary key. There&#39;s also some python work to be done validating parameters and marshalling 20 authors. Here&#39;s the result:</p><p> / author概述端点给出的结果几乎相同。但是，让我们看看如果对数据库施加更多的负载会发生什么。为了模拟复杂的查询，我们将命中/ author？limit = 20＆amp; offset = 50000，这应该使数据库除通过主键查找外还可以做其他事情。还需要完成一些Python工作，以验证参数并编组20位作者。结果如下： </p><p>  This time it&#39;s clear that, along with PyPy, using asynchronous gunicorn workers, or an async framework like Sanic goes a long way to speeding up our app. This is the mantra of async: if you make long / irregular requests in your application, use asyncio, so that you can perform other work while waiting for a reply. At a certain point, our database hits maximum capacity and the number of requests per second stops increasing. We can take this to the extreme, by increasing the offset to 500,000:</p><p>这次很明显，与PyPy一起使用异步gunicorn工作者或Sanic之类的异步框架对于加速我们的应用程序大有帮助。这就是异步的口头禅：如果您在应用程序中发出长时间/不定期的请求，请使用异步，以便您可以在等待回复的同时执行其他工作。在某个时刻，我们的数据库达到最大容量，并且每秒的请求数量停止增加。通过将偏移量增加到500,000，我们可以将这一点发挥到极致：</p><p>  Both our sync workers are now hitting a blazing  12  requests per second 😅 Using async workers seems to help a lot, but oddly Sanic struggles here. I think the Sanic result was more to do with the extra join in my Tortoise ORM code I mentioned earlier. I expect it put a tiny bit of extra load on the database. It&#39;s a valuable lesson in switching frameworks: to maintain performance you also have to choose, evaluate and tune several libraries, not just the one.  For reference, during the async benchmarks, the database was hitting 1050% CPU usage, while the API was cruising along at 50%. If we want to serve more users, one thing is clear: we&#39;re going to need to upgrade our database! Let&#39;s hope we don&#39;t have any other applications using this database, because they&#39;re probably going to be in trouble!</p><p>  现在，我们两个同步工作人员都达到了每秒12个请求的速度。😅使用异步工作程序似乎有很大帮助，但奇怪的是Sanic在这里挣扎。我认为Sanic的结果更多与前面提到的Tortoise ORM代码中的额外联接有关。我希望它会给数据库带来一点额外的负载。这是交换框架方面的宝贵一课：要保持性能，您还必须选择，评估和调整多个库，而不仅仅是一个。作为参考，在异步基准测试期间，数据库的CPU使用率达到1050％，而API的使用率达到了50％。如果我们想为更多的用户提供服务，那就很清楚了：我们将需要升级数据库！希望我们没有其他任何使用此数据库的应用程序，因为它们可能会遇到麻烦！</p><p> Key takeaways: PyPy wins. Sanic is fast, but not  that fast. You should probably run your &#34;traditional&#34; app with an async worker.</p><p> 关键要点：PyPy获胜。 Sanic很快，但没有那么快。您可能应该运行传统的＆＃34;带有异步工作程序的应用程序。</p><p>  In reality most of the &#34;super-fast&#34; benchmarks mean very little except for a few niche use-cases. If you look at the code in detail, you&#39;ll see that they&#39;re either simple &#34;Hello, World!&#34; or echo servers and all of them spend most of their time calling hand-crafted C code with Python bindings.</p><p>  实际上，大多数＆＃34; super-fast＆＃34;除了一些利基用例之外，基准测试几乎没有意义。如果您仔细查看代码，您会发现它们要么很简单，要么是Hello，World !！或echo服务器，并且所有人都花费大量时间调用带有Python绑定的手工C代码。</p><p> That means that these tools are great if you want to build a proxy, or serve static content, possibly even for streaming. But as soon as you introduce any actual Python work into the code you&#39;ll see those numbers plunge. If you rely upon the speed of these frameworks, then it will be hard to maintain that level of performance without e.g.  cythonising all of your code. If you plan on writing almost no Python, then choosing these frameworks is the best option. But presumably, you&#39;re writing an application in Python because you need more than a simple &#34;Hello, World!&#34; and you&#39;d actually like to write quite a bit of Python, thank you very much!</p><p> 这意味着如果您想构建代理或提供静态内容（甚至可能用于流式传输），这些工具也非常有用。但是，一旦您将任何实际的Python工作引入代码中，您就会发现这些数字急剧下降。如果您依靠这些框架的速度，那么在没有诸如此类的情况下很难保持该性能水平。 cythonize您的所有代码。如果您打算几乎不编写Python，那么选择这些框架是最好的选择。但是想必您正在用Python编写应用程序，因为您需要的不仅仅是一个简单的＆nbsp; Hello，World！＆＃34;。并且您实际上想编写很多Python，非常感谢！</p><p> If your service is receiving 100,000 requests a second, it&#39;s likely that the specific Python framework you use is not going to be the bottleneck. Especially if your API is stateless and you can scale it via Kubernetes or similar. At that point, a good database, with decent schema design and good architecture are going to matter far more. Having said that, if you do want more processing power, use PyPy.</p><p> 如果您的服务每秒接收100,000个请求，则您使用的特定Python框架可能不会成为瓶颈。特别是如果您的API是无状态的，并且您可以通过Kubernetes或类似的方法进行扩展。到那时，拥有良好的数据库，良好的架构设计和良好的体系结构将变得至关重要。话虽如此，如果您确实想要更多的处理能力，请使用PyPy。</p><p> Having the ability to run with some asynchronous capability offers clear advantages if database or service requests are likely to be anything other than instantaneous. Even if requests are usually instantaneous, picking an asynchronous runner is a low-cost way to bullet proof your app against intermittent delays. Whilst async-first frameworks like Sanic give you this out of the box, you can just as easily use a different Gunicorn worker with your Flask or Django app.</p><p> 如果数据库或服务请求可能不是即时的，则具有以某种异步功能运行的能力提供了明显的优势。即使请求通常是瞬时的，选择异步运行程序也是一种使应用程序免受间歇性延迟影响的低成本方法。虽然像Sanic这样的异步优先框架为您提供了开箱即用的功能，但您可以轻松地在Flask或Django应用中使用其他Gunicorn worker。 </p><p> What we&#39;ve seen in the benchmarks is that schema design, database choice and architecture will be the bottlenecks. Going with one of the new fully async frameworks purely for speed will probably not be as effective as just using PyPy and an async Gunicorn worker. I also found it gave me a kind of decision paralysis, asking many more questions like:  if we can keep our latency low, is it more or less performant to use a synchronous Foo client written in C, or an async one written in pure Python?</p><p>我们在基准测试中看到的是架构设计，数据库选择和体系结构将成为瓶颈。仅出于速度考虑，使用一种新的完全异步框架可能不会像仅使用PyPy和异步Gunicorn工人那样有效。我还发现它给我带来了决策瘫痪，它提出了许多其他问题，例如：是否可以保持较低的延迟，使用C编写的同步Foo客户端还是纯Python编写的异步Foo客户端，或多或少地表现出性能？ ？</p><p> That doesn&#39;t mean that these frameworks aren&#39;t great pieces of engineering, or that they&#39;re not fun to write code in - they are! Actually I ended up loving the usability of Tortoise ORM when compared to kludging something together with SQLAlchemy core and databases, and I loved the explicitness of writing  await Foo.all() over an implicit query queue and connection pool.</p><p> 这并不意味着这些框架不是很好的工程，也不是说用它们编写代码并不是一件很有趣的事情！实际上，与将某些东西与SQLAlchemy核心和数据库混合在一起相比，我最终爱上了Tortoise ORM的可用性，并且我喜欢在隐式查询队列和连接池上编写await Foo.all（）的明确性。</p><p> For me, all of this emphasises the fact that unless you have some super-niche use-case in mind, it&#39;s actually a better idea to choose your framework based upon ergonomics and features, rather than speed. One framework I haven&#39;t mentioned that seems to have next-level ergonomics for industrial applications (request parsing, marshalling, automatic API documentation) is  FastAPI.</p><p> 对我而言，所有这些都强调了一个事实，除非您牢记一些超级小众用例，否则根据人体工程学和功能而非速度来选择框架实际上是一个更好的主意。我没有提到的一个框架似乎具有针对工业应用程序的下一级人体工程学（请求解析，编组，自动API文档）。</p><p> Right now I&#39;m satisfied that our combination of Flask, Gunicorn and gevent running under PyPy is pretty much the fastest we can go in all scenarios. We&#39;ll be actively exporing FastAPI in the near future, not for its benchmarks, but for its features.  Like working on interesting problems and digging deep in to tech? We&#39;re hiring:  https://suade.org/lead/</p><p> 现在，我感到满意的是，在PyPy下运行的Flask，Gunicorn和gevent的组合几乎可以在所有情况下实现最快的速度。我们将在不久的将来积极开发FastAPI，这不是因为它的基准，而是因为它的功能。喜欢研究有趣的问题并深入研究技术吗？我们正在招聘：https：//suade.org/lead/</p><p>  (1)  Most &#34;traditional&#34; Python web frameworks fall under a standard called  WSGI, where requests are handled in sequence: request comes in, is processed, reply is sent, next request comes in, etc. Most of the &#34;new-school&#34; Python frameworks use Python&#39;s  asyncio library and a different standard called  ASGI, which means that while waiting for IO (e.g. for bytes to arrive over the web) the application can switch to working on a d</p><p>  （1）大部分＆＃34;传统＆＃34; Python Web框架属于称为WSGI的标准，其中按顺序处理请求：请求进入，处理，发送答复，下一个请求进入，等等。大多数＆＃34; new-school＆＃34; Python框架使用Python的asyncio库和另一个称为ASGI的标准，这意味着在等待IO（例如，字节通过Web到达）时，应用程序可以切换到在d上工作</p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://suade.org/dev/12-requests-per-second-with-python/">https://suade.org/dev/12-requests-per-second-with-python/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/python/">#python</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/现实/">#现实</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/realistic/">#realistic</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据库/">#数据库</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>