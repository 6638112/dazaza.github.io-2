<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>一个语音检测器来控制他们One Voice Detector to Rule Them All</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">One Voice Detector to Rule Them All<br/>一个语音检测器来控制他们</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-21 01:25:36</div><div class="page_narrow text-break page_content"><p>In our work we are often surprised by the fact that most people know about Automatic Speech Recognition ( ASR), but know very little about Voice Activity Detection ( VAD). It is baffling, because  VAD is among the most important and fundamental algorithms in any production or data preparation pipelines related to speech – though it remains mostly “hidden” if it works properly.</p><p>在我们的工作中，我们经常惊讶于这样一个事实：大多数人都知道自动语音识别（ASR），但对语音活动检测（VAD）知之甚少。这令人费解，因为VAD是与语音相关的任何生产或数据准备管道中最重要、最基本的算法之一——尽管如果它工作正常，它基本上仍然是“隐藏”的。</p><p> Another problem arises if you try to find a high quality VAD with a permissible license. Typically academic solutions are poorly supported, slow, and may not support streaming. Google’s formidable WebRTC VAD is an established and well-known solution, but it has started to show its age. Despite its stellar performance (30ms chunks, &lt;&lt; 1ms CPU time per chunk) it often fails to properly distinguish speech from noise. Commercial solutions typically have strings attached and send some or another form of telemetry or are not “free” in other ways.</p><p>另一个问题是，如果你试图找到一个高质量的VAD与许可证。通常，学术解决方案支持率低、速度慢，可能不支持流媒体。谷歌强大的WebRTC VAD是一个成熟且众所周知的解决方案，但它已经开始显示出它的时代。尽管它的性能非常出色（30毫秒块，每个块的CPU时间<1毫秒），但它通常无法正确区分语音和噪音。商业解决方案通常会附加字符串，并发送某种或另一种形式的遥测数据，或者以其他方式不“免费”。</p><p> So we decided to fix this and publish (under a permissible license) our internal VAD satisfying the following criteria:</p><p>因此，我们决定解决这个问题，并发布（在许可证下）我们的内部VAD，满足以下标准：</p><p> One chunk takes ~ 1ms on a single CPU thread. ONNX may be up to 2-3x faster;</p><p>在单个CPU线程上，一个数据块需要约1ms的时间。ONNX可能快2-3倍；</p><p> In this article we will tell you about Voice Activity Detection in general, describe our approach to VAD metrics, and show how to use our VAD and test it on your own voice.</p><p>在本文中，我们将向您介绍一般的语音活动检测，描述我们的VAD度量方法，并展示如何使用我们的VAD并在您自己的语音上进行测试。</p><p>   Voice Activity Detection is the problem of  looking for voice activity – or in other words, someone speaking – in a continuous audio stream. It is an integral pre-processing step in most voice-related pipelines and an activation trigger for various production pipelines. Typically VAD should be from 1 to 3 orders of magnitude less compute intensive than Speech-to-Text and may live together somewhere with wake word detection in the chain of algorithms.</p><p>语音活动检测是在连续的音频流中寻找语音活动——或者换句话说，某人说话——的问题。它是大多数语音相关管道中不可或缺的预处理步骤，也是各种生产管道的激活触发器。通常，VAD的计算密集度应该比语音到文本低1到3个数量级，并且可能与算法链中的唤醒词检测一起存在。</p><p>   Basically, VAD should tell speech apart from noise and silence. The input is just a small audio chunk, and the output is a probability that this chunk contains speech given its history. Seems easy enough, just a binary classifier, you say? Well yeah, but as usual the devil is in the details.</p><p>基本上，VAD应该将语音与噪音和沉默区分开来。输入只是一个小的音频块，而输出则是根据其历史，该块包含语音的概率。看起来很简单，只是一个二进制分类器，你说呢？嗯，是的，但和往常一样，魔鬼在细节中。</p><p> If we abstract from the nuts and bolts for a bit, to be competitive a modern VAD should satisfy four main criteria:</p><p>如果我们从螺母和螺栓中抽象出一点，要想具有竞争力，现代VAD应该满足四个主要标准：</p><p> High quality. It may depend per application, but higher precision or recall may be preferable in different situations;</p><p>高质量。这可能取决于每个应用，但在不同的情况下，更高的精确度或召回率可能更可取；</p><p> Low user perceived latency, i.e. CPU latency + audio chunk size. Typically anything lower than 100 ms is good enough. Quite often speech chunks shorter than 100 ms are not really meaningful even to humans;</p><p>低用户感知延迟，即CPU延迟+音频块大小。通常，低于100毫秒的时间就足够了。通常，短于100毫秒的语音块甚至对人类来说都没有真正意义；</p><p> Good generalization, i.e. it should work reasonably well for all domains, audio sources, noise, quality and SNR levels and require only minor fiddling with hyper-parameters;</p><p>良好的泛化能力，即它应该适用于所有领域、音频源、噪声、质量和信噪比水平，并且只需要稍微修改超参数；</p><p> In practice a well trained VAD behaves somewhat like this (notice that some loud sounds are not speech):</p><p>在实践中，训练有素的VAD的行为有点像这样（请注意，有些响亮的声音不是语言）：</p><p>   Although there are many public VADs out there, not all of them are production ready.  A decent production ready VAD should strike a fine balance between low compute / latency and decent modern quality. We did not find a solution that would satisfy all of our criteria. Plus the majority of “open” solutions receive little to no frequent updates and mostly serve as demonstrations or research artifacts. This is why we decided to develop our own VAD.</p><p>尽管有很多公共VAD，但并不是所有VAD都能投入生产。良好的生产准备VAD应该在低计算/延迟和良好的现代质量之间取得良好的平衡。我们没有找到一个满足所有标准的解决方案。此外，大多数“开放”解决方案很少或没有频繁更新，主要用作演示或研究工件。这就是为什么我们决定开发自己的VAD。</p><p> The table below summarizes the advantages and limitations of most popular available VAD engines. All information was sourced from the documentation, public issues, code, colab notebooks and examples.</p><p>下表总结了最流行的VAD发动机的优点和局限性。所有信息来源于文档、公共问题、代码、colab笔记本和示例。</p><p> Even though we were not able to calculate the latency and quality metrics for all of the models shown in the table, сhunk size can indirectly indicate the delay and / or compute. Our main goal was to make a production-ready easy-to-use model that could be used by other people without installing tons of dependencies and that could be easily integrated for streaming tasks while maintaining decent quality.</p><p>尽管我们无法计算表中所示所有模型的延迟和质量指标，但C hunk size可以间接指示延迟和/或计算。我们的主要目标是制作一个生产就绪、易于使用的模型，该模型可以在不安装大量依赖项的情况下供其他人使用，并且可以轻松集成到流媒体任务中，同时保持良好的质量。</p><p>   Also it is notable that the solutions may differ in the amount of languages and data used for their training. VADs are much less data hungry than speech-to-text models, but a correlation between generalization, quality and amount of data used is still observed. We trained our VAD on approximately 13k hours of speech spanning 100 languages. Academic solutions typically lean towards using several small academic datasets. Commercial solutions usually do not disclose how they were trained (or we did not search well enough).</p><p>此外，值得注意的是，解决方案在用于培训的语言和数据量上可能有所不同。VAD比语音到文本模型对数据的需求要小得多，但仍然可以观察到泛化、质量和使用的数据量之间的相关性。我们对VAD进行了大约13k小时的语音培训，涵盖100种语言。学术解决方案通常倾向于使用几个小型学术数据集。商业解决方案通常不会透露他们是如何接受培训的（或者我们搜索得不够好）。</p><p>  A few days back we published a new totally reworked  Silero VAD. You can try it on your own voice via interactive demo with a video  here or via basic demo  here. We employ a multi-head attention (MHA) based neural network under the hood with the Short-time Fourier transform as features. This architecture was chosen due to the fact that  MHA-based networks have shown promising results in many applications ranging from natural language processing to computer vision and speech processing, but our experiments and recent papers show that you can achieve good results with any sort of fully feedforward network, you just need to do enough experiments (i.e. typical choices are MHA-only or transformer networks, convolutional neural networks or their hybrids) and optimize the architecture.</p><p>几天前，我们出版了一部全新的、经过彻底改造的Silero VAD。你可以通过这里的视频互动演示或通过这里的基本演示在自己的语音上进行尝试。我们采用了基于多头注意（MHA）的发动机罩下神经网络，并以短时傅立叶变换为特征。之所以选择这种体系结构，是因为基于MHA的网络在从自然语言处理到计算机视觉和语音处理的许多应用中都显示出了有希望的结果，但我们的实验和最近的论文表明，使用任何类型的全前馈网络都可以获得良好的结果，你只需要做足够的实验（例如，典型的选择是仅MHA或变压器网络、卷积神经网络或它们的混合），并优化架构。</p><p>  Supports 8 kHz and 16 kHz. The PyTorch model also accepts 32 kHz and 48 kHz and resamples audios from these sample rates to 16 kHz by slicing;</p><p>支持8千赫和16千赫。Pytork模型还接受32 kHz和48 kHz，并通过切片将这些采样率的音频重新采样到16 kHz；</p><p>   One chunk takes ~ 1ms on a single CPU thread. ONNX may be up to 2-3x faster;</p><p>在单个CPU线程上，一个数据块需要约1ms的时间。ONNX可能快2-3倍；</p><p>  import torchtorch.set_num_threads(1)model, utils = torch.hub.load(repo_or_dir=&#39;snakers4/silero-vad&#39;, model=&#39;silero_vad&#39;)(get_speech_timestamps, _, read_audio, _, _) = utilswav = read_audio(&#39;test.wav&#39;, sampling_rate=16000)speech_timestamps = get_speech_timestamps(wav, model, sampling_rate=16000, visualize_probs=True, return_seconds=True)</p><p>进口火炬手。设置线程数（1）模型，utils=torch。中心负荷（回购或定向=&#39；斯内克尔斯4/silero vad&#39；，型号=&#39；silero vad&#39；）（get#speech#timestamps，u，read_audio，，，）=utilswav=read#audio（&#39；test.wav&#39；，sampling#rate=16000）speech#timestamps=get#speech#audio#timestamps（wav，model，sampling#rate=16000，visualized#probs=True，return</p><p> For more usage examples (including streaming) and tutorials, please see the  repo, the links  section and  FAQ.</p><p>有关更多使用示例（包括流媒体）和教程，请参阅repo、链接部分和常见问题解答。</p><p>  Test dataset collection for a VAD with a 30 ms chunk is a challenge. Ideally, you would divide each audio in such chunks and manually annotate each chunk with 1 or 0. But in real life this may be prohibitively expensive and introduce a lot of errors and bias (people are notorious for being inaccurate and have problems with short speech chunks).</p><p>VAD数据集是一个测试数据集的块30。理想情况下，您可以将每个音频分割成这样的块，并手动为每个块添加1或0。但在现实生活中，这可能代价高昂，并且会带来很多错误和偏见（人们因不准确而臭名昭著，并且在短语块方面存在问题）。</p><p> Of course you can ask assessors to mark only the start and end timestamps, but in real life this becomes messy and problematic too, just take a look at the below chart:</p><p>当然，你可以要求评估人员只标记开始和结束时间戳，但在现实生活中，这也会变得混乱和有问题，只需看看下面的图表：</p><p>  It is easy to see that with real speech usually there are no clear well-defined boundaries, sometimes there are many short chunks separated by very brief pauses. Nevertheless most likely people will just mark “global” start and end.</p><p>很容易看出，在真实的演讲中，通常没有明确定义的界限，有时会有许多短语块被非常短暂的停顿隔开。然而，最有可能的是，人们只会标记“全球”的开始和结束。</p><p> We chose a much simpler and more concise testing methodology - annotate the whole utterance with 1 or 0 depending on whether it has any speech at all. The rules are as follows:</p><p>我们选择了一种更简单、更简洁的测试方法——根据是否有语音，用1或0注释整个话语。规则如下：</p><p>   A wide variety of domains and audio sources (calls, studio records, noisy audios with background noise or speech, etc);</p><p>广泛的领域和音频源（电话、录音室录音、带有背景噪音或语音的嘈杂音频等）；</p><p> At this moment an ambiguity arises. We have 7 second long audios, but the model classifies 30 ms long chunks! Each utterance contains hundreds of such chunks. In practice though, meaningful speech is usually longer than 250 ms. Of course there are exceptions, but they are rare.</p><p>此时此刻，一个模棱两可的问题出现了。我们有7秒长的音频，但模型将30毫秒长的音频块分类！每一句话都包含数百个这样的句子块。但在实践中，有意义的演讲通常长于250毫秒。当然也有例外，但很少有例外。</p><p> Ok, if we have 250ms of consecutive non-interrupted speech then we are golden. But what if we have 150ms of speech, slight silence and then 150ms more speech (see the above chart)? In this case we wait up to 250ms to allow speech to continue. The above chart shows the most important cases.</p><p>好吧，如果我们有250毫秒的连续无中断演讲，那么我们就是黄金。但是，如果我们有150毫秒的演讲，轻微的沉默，然后再多150毫秒的演讲（见上图），会怎么样？在这种情况下，我们最多等待250毫秒，以允许语音继续。上图显示了最重要的案例。</p><p> The VAD predicts a probability for each audio chunk to have speech or not. In the majority of cases a default 50% threshold works fine, but there are some exceptions and some minor  fine-tuning may be required per domain.</p><p>VAD预测每个音频块有无语音的概率。在大多数情况下，默认的50%阈值可以正常工作，但也有一些例外，每个域可能需要一些小的微调。</p><p>  Using the above algorithm, calculate whether there is speech in a give utterance for different thresholds ranging from 0 to 1;</p><p>使用上述算法，在0到1的不同阈值范围内计算给定话语中是否有语音；</p><p>        There is not really  much to it. One chunk takes around 1 ms with a PyTorch model regardless of the chunks size. If you  batch inputs across several audio streams, the throughput across several audios will become even more impressive. Surprisingly, people reported that the ONNX model is 30-60% faster, which we previously observed for small STT models. Torch freeze also provides around a 5-10% speed bump.</p><p>其实没什么大不了的。无论区块大小如何，PyTorch模型的一个区块大约需要1毫秒。如果您跨多个音频流批量输入，跨多个音频的吞吐量将更加可观。令人惊讶的是，人们报告说ONNX模型的速度快了30-60%，这是我们之前在小型STT模型中观察到的。火炬冻结也能提供大约5-10%的减速。</p><p>  Voice activity detection seems a more or less solved task due to its simplicity and abundance of data. As it usually happens, public academic solutions have issues and enterprise / commercial solutions have strings attached (or poor quality). We tried to fill the spot where we have proper quality, easy-to-use fast minimalistic models, no strings attached and decent generalization with 100+ languages.</p><p>由于语音活动检测的简单性和数据量的丰富性，它似乎或多或少是一项解决的任务。通常情况下，公共学术解决方案有问题，企业/商业解决方案有附加条件（或质量差）。我们试图填补这个空白，我们有适当的质量，易于使用的快速极简模型，没有附加任何字符串和体面的概括100+语言。</p><p> As for other VAD-related  tasks, there remain many unsolved, partially solved, poorly defined or less researched complementary tasks like music detection, audio event classification, and generalizable wake word detection. Integrating some of these tasks inside of our VAD model or solving some of the speaker diarization challenges without sacrificing the core values we brought to the table in our VAD would be a challenge for the future releases.</p><p>至于其他与VAD相关的任务，仍有许多未解决、部分解决、定义不清或研究较少的补充任务，如音乐检测、音频事件分类和广义唤醒词检测。将这些任务整合到我们的VAD模型中，或者在不牺牲我们在VAD中提出的核心价值观的情况下解决一些演讲者日记化挑战，对于未来的版本来说将是一个挑战。</p><p>     Author Bio  Alexander Veysov is a Data Scientist in Silero, a small company building NLP / Speech / CV enabled products, and author of Open STT - probably the largest public Russian spoken corpus (we are planning to add more languages). Silero has recently shipped its own Russian STT engine. Previously he worked in a then Moscow-based VC firm and Ponominalu.ru, a ticketing startup acquired by MTS (major Russian TelCo). He received his BA and MA in Economics in Moscow State University for International Relations (MGIMO). You can follow his channel in  telegram (@snakers41).</p><p>作者Bio Alexander Veysov是Silero的数据科学家，Silero是一家开发NLP/Speech/CV功能产品的小公司，也是Open STT的作者——这可能是最大的公共俄语口语语料库（我们计划添加更多语言）。Silero最近推出了自己的俄罗斯STT发动机。此前，他曾在一家当时位于莫斯科的风投公司和Ponominalu工作。ru，一家由MTS（俄罗斯主要电信公司）收购的票务初创公司。他在莫斯科国立国际关系大学（MGIMO）获得经济学学士和硕士学位。你可以通过电报追踪他的频道（@snakers41）。</p><p>   Alexander Veysov and Dimitrii Voronin, &#34;One Voice Detector to Rule Them All&#34;, The Gradient, 2022.</p><p>亚历山大·维索夫和迪米特里·沃罗宁&#34；一个语音检测器来控制所有的#34；，梯度，2022年。</p><p>  @article{veysov2020towardimagenetstt,   author = {Veysov, Alexander and Voronin, Dimitrii},   title = {One Voice Detector to Rule Them All},   journal = {The Gradient},   year = {2022},   howpublished = {\url{ https://thegradient.pub/one-voice-detector-to-rule-them-all/ } }, }</p><p>@文章{Veysov2020Towardimagnett，作者{Veysov，Alexander and Voronin，Dimitri}，标题{一个声音检测器来统治他们所有人}，期刊{The Gradient}，年份{2022}，howpublished={\url{https://thegradient.pub/one-voice-detector-to-rule-them-all/ } }, }</p><p>  If you enjoyed this piece and want to hear more,  subscribe to the Gradient and follow us on  Twitter.</p><p>如果你喜欢这篇文章，想听到更多，请订阅Gradient并在Twitter上关注我们。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/检测器/">#检测器</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/detector/">#detector</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/vad/">#vad</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>