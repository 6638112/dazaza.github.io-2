<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>使用pytorch和numpy？ 你犯了一个错误 Using PyTorch and NumPy? You're making a mistake</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Using PyTorch and NumPy? You're making a mistake<br/>使用pytorch和numpy？ 你犯了一个错误 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-04-11 14:43:13</div><div class="page_narrow text-break page_content"><p>Bugs in ML code are notoriously hard to fix - they don’t cause compile errors but silently regress accuracy. Once you have endured the pain and fixed one of these, the lesson is forever etched into your brain, right?  Wrong. Recently, an old foe made a comeback - a familiar bug bit me again! As before, the performance improved significantly after fixing it.</p><p>ML代码中的错误难以修复 - 它们不会导致编译错误，但默默地回归准确性。一旦你忍受了痛苦并固定其中一个，就会将课程蚀刻到你的大脑中？错误的。最近，一个古老的敌人卷土重来 - 一个熟悉的虫子再次咬我！如前所述，修复后性能显着提高。</p><p> The bug was subtle and easy to make. How many others has it done damage to? Curious, I downloaded over a hundred thousand repositories from GitHub that import PyTorch, and analysed their source code. I kept projects that define a custom dataset, use NumPy’s random number generator with multi-process data loading, and are more-or-less straightforward to analyse using abstract syntax trees. Out of these, over 95% of the repositories are plagued by this problem. It’s inside PyTorch’s official  tutorial, OpenAI’s  code, and NVIDIA’s  projects. Even Karpathy  admitted falling prey to it.</p><p> 这虫子很微妙，易于制作。它有多少伤害？好奇，我从导入pytorch的github下载了超过十万个存储库，并分析了他们的源代码。我保留了定义自定义数据集的项目，使用Numpy的随机数发生器具有多过程数据加载，并且使用抽象语法树来分析更加有或更短的简单。其中，超过95％的存储库受到这个问题的困扰。它位于Pytorch的官方教程中，Openai的代码和Nvidia的项目。甚至Karpataly也承认了牺牲者。</p><p>  The canonical way to load, pre-process and augment data in PyTorch is to subclass the  torch.utils.data.Dataset and overwrite its  __getitem__ method. To apply augmentations, such as random cropping and image flipping, the  __getitem__ method often makes use of NumPy to generate random numbers. The map-styled dataset is then passed to the  DataLoader to create batches. The training pipeline might be bottlenecked by data pre-processing, and therefore it makes sense to load data in parallel. This can be achieved by increasing the  num_workers parameter in the  DataLoader object.</p><p>  在Pytorch中加载，预处理和增强数据的规范方式是将retch.utils.data.dataset子类分类并覆盖其__getItem__方法。要应用增强，例如随机裁剪和图像翻转，__getItem__方法通常使用numpy来生成随机数。然后将Map样式数据集传递给DataLoader以创建批处理。培训管道可能是通过数据预处理的瓶颈，因此它有意义地加载数据。这可以通过增加DataLoader对象中的Num_Workers参数来实现。</p><p>   To make the issue concrete, here’s an example dataset which returns three-element random vectors. We use a batch size of two and four worker processes.</p><p>   要使问题进行具体信息，这是一个示例数据集，它返回三个元素随机向量。我们使用两个和四个工人流程的批量大小。</p><p> import  numpy  as  np from  torch.utils.data  import Dataset, DataLoader class  RandomDataset(Dataset):  def __getitem__(self, index):  return np .random .randint( 0,  1000,  3)  def __len__(self):  return  16 dataset  = RandomDataset()dataloader  = DataLoader(dataset, batch_size = 2, num_workers = 4) for batch  in dataloader:  print(batch)</p><p> 从Torch.utils.Data导入数据集，Dataloader类随机ataSet（DataSet）：def __getItem __（self，索引）：返回np .random .randint（0,000,3）def __len __（self）：返回16个数据集= Datraloader批处理的DramanDataset（）Dataloader = Dataloader = Dataloader（DataSet，Batch_size = 2，Num_Workers = 4）：打印（批量）</p><p>  tensor([[ 116,  760,  679],  # 1st batch, returned by process 0 [ 754,  897,  764]])tensor([[ 116,  760,  679],  # 2nd batch, returned by process 1 [ 754,  897,  764]])tensor([[ 116,  760,  679],  # 3rd batch, returned by process 2 [ 754,  897,  764]])tensor([[ 116,  760,  679],  # 4th batch, returned by process 3 [ 754,  897,  764]])tensor([[ 866,  919,  441],  # 5th batch, returned by process 0 [  20,  727,  680]])tensor([[ 866,  919,  441],  # 6th batch, returned by process 1 [  20,  727,  680]])tensor([[ 866,  919,  441],  # 7th batch, returned by process 2 [  20,  727,  680]])tensor([[ 866,  919,  441],  # 8th batch, returned by process 3 [  20,  727,  680]])</p><p>  张量（[[116,760,679]，＃第1批次，由过程0返回[754,897,764]）张量（[[116,760,679]，＃第2批次，通过过程返回[754， 897,764]]）张量（[116,760,679]，第3批次，由过程2返回[754,897,764]）张量（[[116,760,679]，＃第4批次返回通过方法3 [754,897,764]]）张量（[[866,919,441]，由过程0返回的＃5批次[20,727,680]）张量（[[866,919,441] ，＃6批次，由过程1 [20,727,680]]）张量（[[866,919,441]，＃第7批次，由过程2返回[20,727,680]）张量（[[第86,919,441]，第8批次，由过程3返回[20,727,680]]）</p><p>   PyTorch uses multiprocessing to load data in parallel. The worker processes are created using the  fork  start method. This means each worker process inherits all resources of the parent, including the state of NumPy’s random number generator.</p><p>   Pytorch使用多处理来并行加载数据。使用Fork Start方法创建工人进程。这意味着每个工作进程继承父的所有资源，包括Numpy的随机数生成器状态。 </p><p>  The  DataLoader constructor has an optional  worker_init_fn parameter. This function is called in each worker process at initialization before any data loading has happened. You can set the seed for NumPy in the  worker_init_fn, for example:</p><p>DataLoader构造函数具有可选的Worker_init_fn参数。在发生任何数据加载之前，在初始化之前调用此函数在发生任何数据加载之前。您可以在worker_init_fn中设置numpy的种子，例如：</p><p> def  worker_init_fn(worker_id): np .random .seed(np .random .get_state()[ 1][ 0]  + worker_id)dataset  = RandomDataset()dataloader  = DataLoader(dataset, batch_size = 2, num_workers = 4, worker_init_fn =worker_init_fn) for batch  in dataloader:  print(batch)</p><p> def worker_init_fn（worker_id）：np .random .seed（np .random .get_state（）[1] [0] + worker_id）dataset = somandeDataset（）dataloader = dataloader（数据集，batch_size = 2，num_wayers = 4，worker_init_fn = worker_init_fn ）对于DataLoader中的批处理：打印（批量）</p><p>  tensor([[ 282,  4,  785], [  35,  581,  521]])tensor([[ 684,  17,  95], [ 774,  794,  420]])tensor([[ 939,  988,  37], [ 983,  933,  821]])tensor([[ 832,  50,  453], [  37,  322,  981]])tensor([[ 180,  413,  50], [ 894,  318,  729]])tensor([[ 530,  594,  116], [ 636,  468,  264]])tensor([[ 142,  88,  429], [ 407,  499,  422]])tensor([[  69,  965,  760], [ 360,  872,  22]])</p><p>  张量（[[282,4,785]，[35,581,521]]张量（[684,17,95]，[774,794,420]]张量（[[939,988,37] ，[983,933,821]]）张量（[[832,50,453]，[37,222,981]）张量（[[180,413,50]，[894,318,729]]）张量（[[530,594,116]，[636,468,264]）张量（[[142,88,429]]张量（[[69,965,760] ，[360,872,22]]）</p><p>    epoch:  0tensor([[ 282,  4,  785], [  35,  581,  521]])tensor([[ 684,  17,  95], [ 774,  794,  420]])tensor([[ 939,  988,  37], [ 983,  933,  821]])tensor([[ 832,  50,  453], [  37,  322,  981]]) -------------------------epoch:  1tensor([[ 282,  4,  785], [  35,  581,  521]])tensor([[ 684,  17,  95], [ 774,  794,  420]])tensor([[ 939,  988,  37], [ 983,  933,  821]])tensor([[ 832,  50,  453], [  37,  322,  981]]) -------------------------epoch:  2tensor([[ 282,  4,  785], [  35,  581,  521]])tensor([[ 684,  17,  95], [ 774,  794,  420]])tensor([[ 939,  988,  37], [ 983,  933,  821]])tensor([[ 832,  50,  453], [  37,  322,  981]]) -------------------------</p><p>    时期：0tensor（[282,4,785]，[35,581,521]）张量（[684,17,95]，[774,794,420]]张量（[[939,988， 37]，[983,933,821]]）张量（[[832,50,453]，[37,322,981]]）------------------- -------时期：1强度（[282,4,785]，[35,581,521]）张量（[[684,17,95]，[774,794,420]）张量（[[939,988,37]，[983,933,821]]）张量（[[832,50,453]，[37,322,981]]）----------  - ------------ epoch：2tensor（[[282,4,785]，[35,581,521]）张量（[[684,17,95]，[774， 794,420]]）张量（[983,988,37]]）张量（[[832,50,453]，[37,322,981]]）---- ---------------------</p><p>  Iterating over the dataset three times produces the same random numbers at each epoch. This happens because all changes to random states are local to each worker. By default, the worker processes are killed at the end of each epoch, and all worker resources are lost. At the same time, the random state in the main process hasn’t changed, and it’s used to initialize each worker process again.</p><p>  在数据集上迭代三次在每个时代产生相同的随机数。这发生了，因为所有对随机状态的更改都是每个工人的本地。默认情况下，工人进程在每个时代末尾终止，并且所有工人资源都会丢失。同时，主进程中的随机状态尚未更改，它用于再次初始化每个工人进程。</p><p> Therefore you need to change the NumPy’s seed at every epoch, for example by  np.random.seed(initial_seed + epoch).</p><p> 因此，您需要在每个时代更改numpy的种子，例如通过np.random.seed（ini​​tial_seed + epoch）。</p><p> Moreover, you won’t have these issues if you sample random numbers using PyTorch (for example,  torch.randint) or Python’s built-in random number generator. PyTorch takes care of these by  setting the above seeds to  seed + worker_id automatically.</p><p> 此外，如果使用Pytorch（例如，Torch.Randint）或Python内置随机数发生器对随机数进行样本随机数，则不会有这些问题。通过将上述种子设置为Seed + Worker_ID，Pytorch根据自动设置上述种子来照顾这些。 </p><p>    A go-to tutorial for using a custom dataset in PyTorch is the one listed on their  website. The tutorial demonstrates how to use the  Dataset and  DataLoader classes on a face-landmarks dataset. It also mentions the importance of data augmentation, and provides an example of a random crop augmentation. This is implemented using NumPy’s random number generator.</p><p>在Pytorch中使用自定义数据集的Go-to tutorial是他们网站上列出的tut。本教程演示了如何在面部地标数据上使用DataSet和Dataloader类。它还提到了数据增强的重要性，并提供了随机作物增强的示例。这是使用numpy的随机数生成器实现的。</p><p>  Following the tip of speeding up data loading by increasing  num_workers, you get identical crops:</p><p>  通过增加Num_Workers加速数据加载的尖端，您可以获得相同的作物：</p><p>   In the paper  Implicit Generation and Modeling with Energy-Based Models, an energy-based model is used for generative modeling of images. The dataset’s  __getitem__  method reads images and labels from disk, corrupts the former, and returns all three:</p><p>   在涉及能量基模型的纸张隐含生成和建模中，用于图像的生成建模的基于能量的模型。 DataSet的__getItem__方法从磁盘读取图像和标签，损坏前者，并返回所有三个：</p><p> if FLAGS .datasource  ==  &#39;default&#39;: im_corrupt  = im  +  0.3  * np .random .randn(image_size, image_size) elif FLAGS .datasource  ==  &#39;random&#39;: im_corrupt  =  0.5  +  0.5  * np .random .randn(image_size, image_size) return im_corrupt, im, label</p><p> 如果标志.DataSource ==＆＃39;默认＆＃39 ;: im_corrupt = im + 0.3 * np .random .randn（image_size，image_size）elif标志.datasource ==＆＃39; rancor＆＃39 ;: im_corrupt = 0.5 + 0.5 * np .random .randn（image_size，image_size）return im_corrupt，im，label</p><p>    The official code for MelGAN, a model for generative audio synthesis published in the NeurIPS conference,  augments the loudness of audio files by sampling random scalars using NumPy.</p><p>    Melgan的官方代码，在Neurips会议上发布的生成音频合成模型，通过使用Numpy采样随机标量来增强音频文件的响度。</p><p> data, sampling_rate  = load(full_path, sr =self .sampling_rate)data  =  0.95  * normalize(data) if self .augment: amplitude  = np .random .uniform(low = 0.3, high = 1.0) data  = data  * amplitude</p><p> 数据，sampling_rate = load（full_path，sr = self .sampling_rate）数据= 0.95 *标准化（数据）如果自我。ugment：sollude = np .random .uniform（低= 0.3，high = 1.0）数据=数据*幅度</p><p>   The bug is easy to make. In some cases, it has minimal effect on final performance. In others, the identical augmentations can cause severe degradations.</p><p>   错误很容易制作。在某些情况下，它对最终性能产生最小的影响。在其他人中，相同的增强可能导致严重的降级。 </p><p> Based on the analysis of open-source PyTorch projects, I am afraid the issue is present in many codebases supporting real products. I hope that better awareness of the trap, and eventually,  better handling of it in PyTorch, makes these products a little bit better.</p><p>基于开源Pytorch项目的分析，担心问题存在于支持真实产品的许多码名中。 我希望更好地了解陷阱，最终在Pytorch中更好地处理它，使这些产品变得更好。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://tanelp.github.io/posts/a-bug-that-plagues-thousands-of-open-source-ml-projects/">https://tanelp.github.io/posts/a-bug-that-plagues-thousands-of-open-source-ml-projects/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/pytorch/">#pytorch</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>