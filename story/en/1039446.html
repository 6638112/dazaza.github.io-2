<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>如何可靠地扩展您的数据平台以实现大批量 How to Reliably Scale Your Data Platform for High Volumes</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">How to Reliably Scale Your Data Platform for High Volumes<br/>如何可靠地扩展您的数据平台以实现大批量 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-13 20:22:50</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/47b779723aa5f58270e89279b088be97.jpg"><img src="http://img2.diglog.com/img/2020/12/47b779723aa5f58270e89279b088be97.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Black Friday and Cyber Monday—or as we like to call it, BFCM—is one of the largest sales events of the year. It’s also one of the most important moments for Shopify and our merchants. To put it into perspective, this year our merchants across more than 175 countries sold a record breaking $5.1+ billion over the sales weekend.</p><p>黑色星期五和网络星期一（或者我们称之为BFCM）是一年中最大的销售活动之一。对于Shopify和我们的商家来说，这也是最重要的时刻之一。放眼来看，今年，我们在超过175个国家/地区的商家在周末的销售记录中打破了创纪录的5.1亿美元以上的记录。</p><p> That’s a lot of sales. That’s a lot of data, too.</p><p> 那是很多销售额。这也是很多数据。</p><p> This BFCM, the Shopify data platform saw an average throughput increase of 150 percent. Our mission as the Shopify Data Platform Engineering (DPE) team is to ensure that our merchants, partners, and internal teams have access to data quickly and reliably. It shouldn’t matter if a merchant made one sale per hour or a million; they need access to the most relevant and important information about their business, without interruption. While this is a must all year round, the stakes are raised during BFCM.</p><p> Shopify数据平台BFCM的平均吞吐量提高了150％。我们作为Shopify数据平台工程（DPE）团队的使命是确保我们的商人，合作伙伴和内部团队能够快速可靠地访问数据。商家每小时赚一百万还是一百万都没关系；他们需要不间断地访问有关其业务的最相关和重要的信息。虽然这是全年必须的，但在BFCM期间会增加赌注。</p><p> Creating a data platform that withstands the largest sales event of the year means our platform services need to be ready to handle the increase in load. In this post, we’ll outline the approach we took to reliably scale our data platform in preparation for this high-volume event.</p><p> 创建一个可以承受当年最大销售事件的数据平台，意味着我们的平台服务需要随时准备应对负载的增加。在本文中，我们将概述为可靠地扩展我们的数据平台而准备的方法，以准备这场盛会。</p><p>  Shopify’s data platform is an interdisciplinary mix of processes and systems that collect and transform data for use by our internal teams and merchants. It enables access to data through a familiar pipeline:</p><p>  Shopify的数据平台是流程和系统的跨学科组合，可收集和转换数据以供内部团队和商家使用。它允许通过熟悉的管道访问数据：</p><p> Ingesting data in any format, from any part of Shopify. “Raw” data (for example, pageviews, checkouts, and orders) is extracted from Shopify’s operational tables without any manipulation. Data is then conformed to an Apache Parquet format on disk.</p><p> 从Shopify的任何部分提取任何格式的数据。从Shopify的操作表中提取“原始”数据（例如，浏览量，结帐和订单），而无需进行任何操作。然后，数据将符合磁盘上的Apache Parquet格式。</p><p> Processing data, in either  batches or  streams, to form the foundations of business insights. Batches of data are “enriched” with models developed by data scientists, and processed within Apache Spark or  dbt.</p><p> 批量或流处理数据，以形成业务洞察力的基础。数据科学家开发的模型“丰富”了批数据，并在Apache Spark或dbt中对其进行了处理。 </p><p> Delivering  data to our merchants, partners, and internal teams so they can use it to make great decisions quickly. We rely on an internal collection of streaming and serving applications, and libraries that power the merchant-facing analytics in Shopify. They’re backed by BigTable, GCS, and CloudSQL.</p><p>将数据传递给我们的商人，合作伙伴和内部团队，以便他们可以使用它们快速做出重大决策。我们依靠内部的流和服务应用程序集合以及支持Shopify中面向商家的分析的库。它们得到了BigTable，GCS和CloudSQL的支持。</p><p> In an average month, the Shopify data platform processes about 880 billion MySQL records and 1.75 trillion Kafka messages.</p><p> 平均每个月，Shopify数据平台处理约8800亿条MySQL记录和1.75万亿条Kafka消息。</p><p>  As engineers, we want to conquer every challenge  right now. But that’s not always realistic or strategic, especially when not all data services require the same level of investment. At Shopify, a  tiered services taxonomy helps us prioritize our reliability and infrastructure budgets in a broadly declarative way. It’s based on the potential impact to our merchants and looks like this:</p><p>  作为工程师，我们现在想征服所有挑战。但这并不总是现实或具有战略意义的，特别是当并非所有数据服务都需要相同水平的投资时。在Shopify，分层服务分类法以广泛的声明方式帮助我们确定可靠性和基础架构预算的优先级。它基于对我们商家的潜在影响，如下所示：</p><p> This service is  critical  externally, for example. to a merchant’s ability to run their business</p><p> 例如，此服务在外部至关重要。商户经营业务的能力</p><p>   This service is an  experiment, in very early development, or is otherwise disposable. For example, an emoji generator</p><p>   此服务是一项实验，尚处于早期开发阶段，或者可以一次性使用。例如，表情符号生成器</p><p>  The highest tiers are top priority. Our ingestion services, called  Longboat and  Speedboat, and our merchant-facing query service  Reportify are examples of services in Tier 1.</p><p>  最高层是最高优先级。我们的提取服务（称为Longboat和Speedboat）以及面向商家的查询服务Reportify是方法1中的示例。</p><p>  As we’ve mentioned, each BFCM the Shopify data platform receives an unprecedented volume of data and queries. Our data platform engineers did some forecasting work this year and predicted nearly two times the traffic of 2019. The challenge for DPE is ensuring our data platform is prepared to handle that volume.</p><p>  如上所述，每个BFCM Shopify数据平台都会收到前所未有的数据和查询量。我们的数据平台工程师今年做了一些预测工作，预测的流量是2019年的近两倍。DPE面临的挑战是确保我们的数据平台已准备好处理该数量。 </p><p> When it comes to BFCM, the primary risk to a system’s reliability is directly proportional to its throughput requirements. We call it  throughput risk. It increases the closer you get to the front of the data pipeline, so the systems most impacted are our   ingestion and   processing systems.</p><p>对于BFCM，系统可靠性的主要风险与吞吐量要求成正比。我们称其为吞吐量风险。它使您更接近数据管道的前端，因此受影响最大的系统是我们的提取和处理系统。</p><p> With such a titillating forecast, the risk we faced was unprecedented throughput pressure on data services. In order to be BFCM ready, we had to prepare our platform for the tsunami of data coming our way.</p><p> 有了如此激动人心的预测，我们面临的风险是数据服务面临前所未有的吞吐量压力。为了做好BFCM的准备，我们必须为即将到来的数据海啸做好准备。</p><p>  We tasked our Reliability Engineering team with Tier 1 and Tier 2 service preparations for our ingestion and processing systems. Here’s the steps we took to prepare our systems most impacted by BFCM volume:</p><p>  我们为可靠性工程团队分配了摄取和处理系统的第1层和第2层服务准备工作。以下是我们准备受BFCM量影响最大的系统的步骤：</p><p>  A data ingestion service&#39;s main operational priority can be different from that of a batch processing or streaming service. We determine upfront what the service is optimizing for. For example, if we’re extracting messages from a limited-retention Kafka topic, we know that the ingestion system needs to ensure, above all else, that no messages are lost in the ether because they weren’t consumed fast enough. A batch processing service doesn’t have to worry about that, but it may need to prioritize the delivery of one dataset versus another.</p><p>  数据提取服务的主要操作优先级可以不同于批处理或流服务的优先级。我们先确定服务正在优化的内容。例如，如果我们要从保留期限有限的Kafka主题中提取消息，那么我们知道，提取系统首先要确保没有消息丢失，因为它们的消耗速度不够快。批处理服务不必担心，但是可能需要优先安排一个数据集与另一个数据集的交付。</p><p> In Longboat’s case, as a batch data ingestion service, its primary objective is to ensure that a raw dataset is available within the interval defined by its data freshness service level objective (SLO). That means Longboat is operating reliably so long as every dataset being extracted is no older than eight hours— the default  freshness SLO. For Reportify , our main query serving service, its primary objective is to get query results out as fast as possible; its reliability is measured against a  latency SLO.</p><p> 对于Longboat，作为批处理数据提取服务，其主要目标是确保原始数据集在其数据新鲜度服务水平目标（SLO）定义的间隔内可用。这意味着只要提取的每个数据集不超过八个小时（默认的新鲜度SLO），Longboat就能可靠运行。对于我们主要的查询服务服务Reportify来说，其主要目标是尽快获得查询结果。它的可靠性是根据延迟SLO来衡量的。</p><p>  With primary objectives confirmed, you need to identify what you can “turn up or down” to sustain those objectives.</p><p>  确认主要目标后，您需要确定可以“调高或调低”以维持这些目标的内容。</p><p> In Longboat’s case, extraction jobs are orchestrated with a batch scheduler, and so the first obvious lever is  job frequency. If you discover a raw production dataset is stale, it could mean that the extraction job simply needs to run more often. This is a service-specific lever.</p><p> 在Longboat的情况下，提取作业是使用批处理调度程序进行编排的，因此第一个显而易见的杠杆是作业频率。如果发现原始生产数据集是陈旧的，则可能意味着提取作业仅需要更频繁地运行。这是特定于服务的杠杆。 </p><p> Another service-specific lever is Longboat’s “overlap interval” configuration, which configures an extraction job to redundantly ingest some overlapping span of records in an effort to catch late-arriving data. It’s specified in a number of hours.</p><p>Longboat的另一项特定于服务的杠杆是Longboat的“重叠间隔”配置，该配置将提取作业配置为冗余地提取记录的某些重叠范围，以捕获较晚到达的数据。指定时间（以小时为单位）。</p><p> Memory and CPU are universal compute levers that we ensure we have control of. Longboat and Reportify run on Google Kubernetes Engine, so it’s possible to demand that jobs request more raw compute to get their expected amount of work done within their scheduled interval (ignoring total compute constraints for the sake of this discussion).</p><p> 内存和CPU是我们可以控制的通用计算杠杆。 Longboat和Reportify在Google Kubernetes Engine上运行，因此可以要求作业请求更多的原始计算，以在计划的时间间隔内完成预期的工作量（在此讨论中，忽略了总的计算约束）。</p><p>      Now that we have some known controls, we can use them to deliberately constrain the service’s resources. As an example, to simulate an unrelenting N-times throughput increase, we can turn the infrastructure knobs so that we have 1/N the amount of compute headroom, so we’re at N-times nominal load.</p><p>      现在我们有了一些已知的控件，我们可以使用它们来故意限制服务的资源。举例来说，为了模拟持续不间断的N倍吞吐量增长，我们可以旋转基础结构旋钮，以使我们有1 / N的计算余量，因此我们处于N倍的标称负载下。</p><p> For Longboat’s simulation, we manipulated its “overlap interval” configuration and tripled it. Every table suddenly looked like it had roughly three times more data to ingest within an unchanged job frequency; throughput was tripled.</p><p> 对于Longboat的模拟，我们操纵了其“重叠间隔”配置并将其增加了三倍。在不改变工作频率的情况下，每个表突然看起来要吸收大约三倍的数据。吞吐量增加了两倍。</p><p> For Reportify, we leveraged our  load testing tools to simulate some truly haunting throughput scenarios, issuing an increasingly extreme volume of queries, as seen here:</p><p> 对于Reportify，我们利用我们的负载测试工具来模拟一些真正令人困扰的吞吐量方案，发出越来越多的查询量，如下所示：</p><p>     If any of the answers to these questions leave us unsatisfied, the reliability roadmap writes itself: we need to engineer our way into satisfactory answers to those questions. That leads us to the next step.</p><p>     如果对这些问题的答案中有任何一个令我们不满意，那么可靠性路线图就这样写出来：我们需要将自己的方法设计成对这些问题的满意答案。这将我们引向下一步。</p><p>  A service’s reliability depends on the speed at which it can recover from interruption. Whether that recovery is performed by a machine or human doesn’t matter when your CTO is staring at a service’s reliability metrics! After deliberately constraining resources, the operations channel turns into a (controlled) hellscape and it&#39;s time to act as if it were a real production incident.</p><p>  服务的可靠性取决于它从中断中恢复的速度。当您的CTO盯着服务的可靠性指标时，恢复是由机器执行还是由人员执行并不重要！在故意限制资源之后，操作渠道变成了（受控）地狱，现在该该采取行动了，好像是真实的生产事件一样。 </p><p> Talking about mitigation strategy could be a blog post on its own, but here are the tenets we found most important:</p><p>谈论缓解策略可能是一篇博客文章，但以下是我们发现最重要的原则：</p><p> Every alert must be directly actionable. Just saying “the curtains are on fire!” without mentioning “put it out with the extinguisher!” amounts to noise.</p><p> 每个警报必须可以直接采取行动。只是说“窗帘着火了！”不用说“把它扔出去！”等于噪音。</p><p> Assume that mitigation instructions will be read by someone broken out of a deep sleep. Simple instructions are carried out the fastest.</p><p> 假设缓解睡眠的人将阅读缓解说明。简单的指令执行速度最快。</p><p> If there is   any  ambiguity or unexpected behavior during controlled load tests, you’ve identified new reliability risks. Your service is less reliable than you expected. For Tier 1 services, that means everything else drops and those risks should be addressed immediately.</p><p> 如果在受控负载测试期间有任何歧义或意外行为，则说明您发现了新的可靠性风险。您的服务不如预期的可靠。对于第1层服务，这意味着其他所有内容都会下降，这些风险应立即解决。</p><p>  Always over-communicate, even if acting alone. Other engineers will devote their brain power to your struggle.</p><p>  即使单独行动，也总是要沟通过度。其他工程师将全力以赴为您奋斗。</p><p>  Now that we know what can happen with an overburdened infrastructure, we can make an informed decision whether the service carries real throughput risk. If we absolutely hammered the service and it skipped along smiling without risking its primary objective, we can leave it alone (or even scale  down, which will have the CFO smiling too).</p><p>  既然我们知道过载的基础架构会发生什么，我们就可以做出明智的决定，即该服务是否承担实际的吞吐量风险。如果我们绝对敲定该服务，并且在不冒其主要目标的风险的情况下跳过了微笑，那么我们可以不理会它（甚至缩减规模，这也会使CFO感到微笑）。</p><p> If we don’t feel confident in our ability to recover, we’ve unearthed new risks. The service’s development team can use this information to plan resiliency projects, and we can collectively scale our infrastructure to minimize throughput risk in the interim.</p><p> 如果我们对自己的恢复能力不抱有信心，我们将发掘新的风险。该服务的开发团队可以使用此信息来计划弹性项目，并且我们可以共同扩展我们的基础架构，以最大程度地减少过渡期间的吞吐量风险。 </p><p> In general, to be prepared infrastructure-wise to cover our capacity, we perform  capacity planning. You can learn more about  Shopify’s BFCM capacity planning efforts on the blog.</p><p>通常，为了在基础架构方面做好准备以覆盖我们的能力，我们会进行能力规划。您可以在博客上详细了解Shopify的BFCM容量规划工作。</p><p>  Our mitigation strategy for Longboat and Reportify was healthy, needing gentle tweaks to our load-balancing maneuvers.</p><p>  我们对Longboat和Reportify的缓解策略是健康的，需要对我们的负载平衡操作进行微调。</p><p> We should scale up our clusters to handle the increased load, not only from shoppers, but also from some of our own fun stuff like the  BFCM Live Map.</p><p> 我们应该扩展集群以应对不断增加的负载，不仅来自购物者，还来自我们一些有趣的东西，例如BFCM Live Map。</p><p> We needed to tune our systems to make sure our merchants could track their online store’s performance in real-time through the Live View in the analytics section of their admin.</p><p> 我们需要调整系统，以确保商家可以通过其管理员的“分析”部分中的“实时视图”实时跟踪其在线商店的表现。</p><p> Some jobs could use some tuning, and some of their internal queries could use optimization.</p><p> 有些作业可以使用某些调优，而某些内部查询可以使用优化。</p><p> Most importantly, we refreshed our understanding of data service reliability. Ideally, it’s not any more exciting than that. Boring reliability studies are best.</p><p> 最重要的是，我们刷新了对数据服务可靠性的理解。理想情况下，没有比这更令人兴奋的了。无聊的可靠性研究是最好的。</p><p> We hope to perform these exercises more regularly in the future, so BFCM preparation isn’t particularly interesting. In this post we talked about throughput risk as one example, but there are other risks to data integrity, correctness, latency. We aim to get out in front of them too because data grows faster than engineering teams do. “Trillions of records every month” turns into “quadrillions” faster than you expect.</p><p> 我们希望以后能更定期地进行这些练习，因此准备BFCM并不是特别有趣。在本文中，我们以吞吐量风险为例进行了讨论，但是对数据完整性，正确性和延迟也存在其他风险。我们的目标是也能脱颖而出，因为数据的增长速度快于工程团队。 “每月成千上万的记录”变成“四百万”的速度比您预期的快。 </p><p>  After months of rigorous preparation systematically improving our indices, schemas, query engines, infrastructure, dashboards, playbooks, SLOs, incident handling and alerts, we can proudly say BFCM 2020 went off without a hitch!</p><p>经过几个月的严格准备，系统地改善了我们的索引，架构，查询引擎，基础架构，仪表板，剧本，SLO，事件处理和警报，我们可以自豪地说BFCM 2020顺利进行了！</p><p> During the big moment we traced down every spike, kept our eyes glued to utilization graphs, and turned knobs from time to time, just to keep the margins fat. There were only a handful of minor incidents that didn’t impact merchants, buyers or internal teams - mainly self healing cases thanks to the nature of our platform and our spare capacity.</p><p> 在重要时刻，我们追踪了每个峰值，使我们的眼睛盯着利用率图表，并不时转动旋钮，只是为了保持利润。只有极少数的小事件不会影响商人，买家或内部团队-主要是由于我们平台的性质和我们的备用能力而导致的自我修复案例。</p><p> This success doesn’t happen by accident, it happens because of diligent planning, experience, curiosity and—most importantly—teamwork.</p><p> 取得成功并非偶然，而是因为勤奋的计划，经验，好奇心以及最重要的是团队合作。</p><p> Arbab is a seven-year veteran at Shopify serving as Reliability Engineering lead. He&#39;s previously helped launch Shopify payments, some of the first Shopify public APIs, and Shopify&#39;s Retail offerings before joining the Data Platform. 99% of Shopifolk joined after him!</p><p> Arbab是Shopify七年的资深人士，担任可靠性工程负责人。在加入数据平台之前，他曾帮助启动Shopify付款，一些首批Shopify公共API和Shopify的零售产品。 99％的Shopifolk跟随他！</p><p> Bruno is a DPE TPM working with the Site Reliability Engineering team. He has a record of 100% successful BFCMs under his belt and plans to keep it that way.</p><p> Bruno是DPE TPM，与站点可靠性工程团队一起工作。他拥有100％成功完成BFCM的记录，并计划保持这种状态。</p><p>  Interested in helping us scale and tackle interesting problems? We’re planning to double our engineering team in 2021 by hiring 2,021 new technical roles. Learn more  here!</p><p>  有兴趣帮助我们扩展并解决有趣的问题吗？我们计划在2021年通过聘用2,021个新技术职位来使我们的工程团队增加一倍。在这里了解更多！</p><p>  Stories from the teams who build and scale Shopify, the leading cloud-based, multi-channel commerce platform powering over 1,000,000 businesses around the world.</p><p>  来自构建和扩展Shopify的团队的故事，Shopify是领先的基于云的多渠道商业平台，为全球超过1,000,000家企业提供支持。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://shopify.engineering/reliably-scale-data-platform">https://shopify.engineering/reliably-scale-data-platform</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/扩展/">#扩展</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/scale/">#scale</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/数据/">#数据</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>