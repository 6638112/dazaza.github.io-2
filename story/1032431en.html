<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>现代Kafka-API存储系统的每核线程缓存管理</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">现代Kafka-API存储系统的每核线程缓存管理</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-31 09:55:07</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/f84a708f828fff14f092a76e6ba324d4.jpg"><img src="http://img2.diglog.com/img/2020/10/f84a708f828fff14f092a76e6ba324d4.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Superscalar CPUs with wide GB/s memory, NVMe access times in the order of 10-100’s of microseconds, necessitates a new buffer management for low latency storage systems.</p><p>超标量CPU具有宽GB/s内存，NVMe访问时间在10-100微秒量级，这就需要为低延迟存储系统进行新的缓冲区管理。</p><p> As I have previously observed, software does not run on category theory, it runs on superscalar CPUs with wide, multi-channel GB/s memory units and NVMe SSD access times in the order of 10-100’s of microseconds. The reason some software written a decade ago - on a different hardware platform - feels slow is because it fails to exploit the advances in modern hardware.</p><p>正如我之前观察到的，软件不是基于类别理论运行的，它运行在超标量CPU上，具有宽的多通道Gb/s存储单元和10-100微秒量级的NVMe固态硬盘访问时间。十年前在不同的硬件平台上编写的一些软件感觉很慢的原因是它没有利用现代硬件的进步。</p><p> The new bottleneck in storage systems is the CPU. SSD devices are 100-1000x faster than spinning disks and are 10x cheaper today[1] than they were a decade ago, from $2,500 down to $200 per Terabyte. Networks have 100x higher throughput in public clouds from 1Gbps to 100Gbps.</p><p>存储系统中的新瓶颈是CPU。SSD设备比旋转磁盘快100-1000倍，今天的价格比十年前便宜10倍[1]，从每TB 2500美元降到200美元。从1Gbps到100Gbps，公共云中的网络吞吐量提高了100倍。</p><p> Although computers did, in fact, get faster, single-core speeds remain roughly the same. The reason being that CPU frequency has a cubic dependency on power consumption, and we’ve hit a wall. Instruction level parallelism, prefetching, speculative execution, branch prediction, deep hierarchy of data caches and instruction caches, etc, have contributed to programs  feeling faster when you interact with them, but in the datacenter, the material improvements have come from the rise in core count. While the instructions per clock are 3x higher than a decade ago, core count is up 20x.</p><p>事实上，虽然计算机确实变得更快了，但单核速度大致保持不变。原因是CPU频率与功耗之间存在立方依赖关系，因此我们遇到了障碍。指令级并行性、预取、推测性执行、分支预测、数据高速缓存和指令高速缓存的深层层次结构等，使程序在与它们交互时感觉速度更快，但在数据中心，实质性的改进来自于内核数量的增加。虽然每个时钟的指令数比十年前增加了3倍，但内核数量增加了20倍。</p><p> This is all to say that the rise of readily available, many-core systems necessitates a different approach for building infrastructure. Case in point[9]: in order to take full advantage of 96 vCPUs on a i3en.metal on AWS, you’ll need to find a way to exploit sustained CPU clock speed of 3.1 GHz, 60 TB of total NVMe instance storage, 768 GiB of memory and NVMe devices capable of delivering up to 2 million random IOPS at 4 KB block sizes. This kind of beast necessitates a new kind of storage engine and threading model that leverages these hardware advances.</p><p>这就是说，随手可得的多核心系统的兴起需要一种不同的基础设施建设方法。案例[9]：为了充分利用AWS上i3en.Metal上的96个vCPU，您需要找到一种方法来利用3.1 GHz的持续CPU时钟速度、60 TB的总NVMe实例存储、768 GiB的内存以及能够以4 KB块大小提供高达200万随机IOPS的NVMe设备。这种野兽需要一种新的存储引擎和线程模型来利用这些硬件的进步。</p><p> Redpanda - a Kafka-API compatible system for mission critical workloads[3] - addresses all of these issues. It uses a thread-per-core architecture with Structured Message Passing (SMP) to communicate between these pinned threads. Threading is a foundational decision for any application, whether you are using a thread-pool, pinned threads with a network of Single Producer Single Consumer SPSC[7] queues, or any other of the advanced Safe Memory Reclamation (SMR) techniques, threading is your ring-0, the true kernel of your application. It tells you what your sensitivity is for blocking - which for Redpanda is less than 500 microseconds - otherwise, Seastar’s[4] reactor will print a stack trace warning you of the blocking since it effectively injects latency on the network poller.</p><p>Redpanda--一个适用于任务关键型工作负载的Kafka-API兼容系统[3]--解决了所有这些问题。它使用具有结构化消息传递(SMP)的每核线程架构来在这些固定的线程之间通信。线程化是任何应用程序的基本决策，无论您是使用线程池、通过单生产者单消费者SPSC[7]队列网络固定的线程，还是任何其他高级安全内存回收(SMR)技术，线程都是您的环-0，是应用程序的真正内核。它会告诉你你对阻塞的敏感度是什么--对于小熊猫来说，这个敏感度小于500微秒-否则，Seastar的[4]反应堆将打印堆栈跟踪，警告你阻塞，因为它有效地增加了网络轮询器的延迟。</p><p> Once you have decided on your threading model, the next step is your memory model and ultimately, for storage engines, your buffer management. In this post, we’ll cover the perils of buffer management in a thread-per-core environment and describe  iobuf, our solution for a 0-copy memory management in the world of Seastar.</p><p>一旦您决定了您的线程模型，下一步就是您的内存模型，最终对于存储引擎而言，是您的缓冲区管理。在这篇文章中，我们将讨论每核线程环境中缓冲区管理的危险，并描述我们在Seastar世界中的零拷贝内存管理解决方案iobuf。</p><p>  As mentioned earlier, Redpanda uses a  single pinned thread per core architecture to do everything. Network polling, submitting async IO to the kernel, reaping events, triggering timers, scheduling compute tasks, etc. Structurally, it means nothing can block for longer than 500 microseconds, or you’ll be introducing latency in other parts of your stack. This is an incredibly strict programming paradigm, but this opinionated idea forces a truly asynchronous system, whether you like it or not as the programmer.</p><p>如前所述，redpanda在每个核心架构中使用单个固定线程来执行所有操作。网络轮询、向内核提交异步IO、获取事件、触发计时器、安排计算任务等等。从结构上讲，这意味着没有任何东西可以阻塞超过500微秒，否则您将在堆栈的其他部分引入延迟。这是一个令人难以置信的严格编程范例，但是这种固执己见的想法迫使一个真正的异步系统，无论您作为程序员是否喜欢它。</p><p>  The challenge in a TpC (thread-per-core) architecture[8] is that all communication between cores is explicit. This muscles the programmer into implementing algorithms that favor core-locality (d-cache, i-cache) over the straightforward multi-threaded implementations via mutexes. This imperative has to be co-designed with the asynchronicity of a  future&lt;&gt;-based implementation.</p><p>TPC(每核线程)体系结构[8]中的挑战是核心之间的所有通信都是显式的。这迫使程序员通过互斥锁实现更偏爱核心局部性(d-cache、i-cache)而不是直接的多线程实现的算法。这一要求必须与基于未来&lt；&gt；的实现的异步性共同设计。</p><p> For our Kafka-API implementation as shown in Figure 1, we explicitly trade memory usage to reduce latency and increase throughput by materializing key components. The metadata Cache is materialized on every core since every request has to know if the partition exists, and that that particular machine is, in fact, the leader of the partition. The Partition Router maintains a map of which logical core actually owns the underlying Kafka partition on the machine. Other things like Access Control Lists (ACLs) are deferred until the request reaches the destination core since they can get unwieldy in memory footprint. We have no hard and fast rule of what we materialize on every core vs. what is deferred for the destination core, and it’s often a function of memory (smaller data structures are good candidates for broadcast), computation (how much time is spent deciding) and frequency of access (very likely operations tend to get materialized on every core).</p><p>对于我们的Kafka-API实现(如图1所示)，我们显式地交换内存使用量，以通过具体化关键组件来减少延迟并提高吞吐量。元数据高速缓存在每个核心上被具体化，因为每个请求都必须知道该分区是否存在，并且该特定机器实际上是该分区的领导者。分区路由器维护机器上哪个逻辑核心实际拥有底层Kafka分区的映射。访问控制列表(Access Control List，ACL)等其他内容会推迟到请求到达目标内核，因为它们可能会占用大量内存。我们没有严格的规则来确定我们在每个核心上实现的内容与目标核心延迟实现的内容，而且通常取决于内存(较小的数据结构很适合用于广播)、计算(决定花费多少时间)和访问频率(很可能操作往往会在每个核心上实现)。</p><p> One question remaining is how, exactly, does memory management work in a TpC architecture? How does data actually travel from L-core-0 to L-core-66 safely using a network of SPSC queues within a fully asynchronous execution model where things can suspend at any point in time?</p><p>剩下的一个问题是，内存管理在TPC架构中究竟是如何工作的？在完全异步执行模型中，数据如何使用SPSC队列网络安全地从L-CORE-0传输到L-CORE-66呢？在完全异步执行模型中，可以在任何时间点挂起数据。</p><p>   To understand  iobuf, we need to understand the actual memory constraints of Seastar, our TpC framework. During program bootstrap, Seastar allocates the full memory of the computer and splits it evenly across all the cores. It consults the hardware to understand what memory belongs to each particular core, reducing inter-core traffic to main memory.</p><p>要理解iobuf，我们需要了解我们的TPC框架Seastar的实际内存限制。在程序引导过程中，Seastar会分配计算机的全部内存，并将其平均分配给所有内核。它会咨询硬件以了解哪个内存属于每个特定的内核，从而减少到主内存的内核间流量。</p><p>  As Figure 2 suggests, memory allocated on core-0,  must be deallocated on core-0. However, there is no way to guarantee that a Java or Go client connecting to Redpanda will actually communicate with the exact core that owns the data.</p><p>如图2所示，在core-0上分配的内存必须在core-0上解除分配。然而，无法保证连接到redpanda的Java或Go客户端实际上会与拥有数据的确切核心进行通信。</p><p> At its core, an iobuf is a ref-counted, fragmented-buffer-chain with deferred deletes that allows Redpanda to simply share a view of a remote core’s parsed messages as the fragments come in, without incurring a copy overhead.</p><p>在其核心，iobuf是一个具有延迟删除的引用计数的分段缓冲链，它允许redpanda在碎片传入时简单地共享远程核心的已解析消息的视图，而不会招致复制开销。</p><p>  The fragmented buffers abstraction is not new. The linux kernel has  sk_buff[5] and the freebsd kernel has an  mbuf[6] which are roughly similar. The additional extension of an iobuf is that it works in the TCP model leveraging Seastar’s network of SPSC queues to have proper deletes in addition to being able to share sub-views arbitrarily, tailored for a storage-like workload.</p><p>碎片化的缓冲区抽象并不是什么新鲜事。Linux内核的sk_buff[5]和FreeBSD内核的mbuf[6]大致相似。Iobuf的另一个扩展是它在TCP模型中工作，它利用Seastar的SPSC队列网络进行适当的删除，此外还能够任意共享子视图，这些子视图是为类似存储的工作负载量身定做的。</p><p> Removing the C++ templates, allocators, pooling, pointer caching, etc, one could think of an iobuf as being equivalent to:</p><p>删除C++模板、分配器、池、指针缓存等，可以认为iobuf等同于：</p><p> struct    {  void  * data ; size_t ref_count ; size_t capacity ; size_t size ; fragment * next ;  // list fragment * prev ; } struct    { fragment * head ; } ;</p><p>Struct{void*data；size_t ref_count；size_t acity；size_t size；Fragment*Next；//列出片段*prev；}struct{Fragment*Head；}；</p><p> The origins of iobuf are rooted in one of our central product tenets for building a Kafka® replacement for mission critical systems - giving users 10x lower tail latencies for most workloads. Aside from a thread-per-core architecture, the memory management would have been our second bottleneck if not designed from the ground up for latency. On long running storage systems, memory fragmentation is a real problem, and one that is eventually either met with a proper solution (iobuf), stalls or an OOM.</p><p>Iobuf的起源源于我们为任务关键型系统构建Kafka®替代品的核心产品原则之一-让用户将大多数工作负载的尾部延迟缩短为原来的1/10。除了每核线程架构之外，如果没有从头开始设计延迟，内存管理将成为我们的第二个瓶颈。在长时间运行的存储系统上，内存碎片是一个真正的问题，最终要么会遇到适当的解决方案(Iobuf)，要么会停滞，要么会出现OOM。</p><p> Like its predecessors skbuff and mbuff, iobuf allows us to optimize and train our memory allocator with predictable memory sizes. Here is our iobuf allocation table logic:</p><p>与它的前身skbuff和mbuff一样，iobuf允许我们使用可预测的内存大小优化和训练内存分配器。下面是我们的iobuf分配表逻辑：</p><p> struct    {  static  constexpr size_t max_chunk_size  =  128  *  1024 ;  static  constexpr size_t default_chunk_size  =  512 ;  // &gt;&gt;&gt; x=512  // &gt;&gt;&gt; while x &lt; int((1024*128)):  // ... print(x)  // ... x=int(((x*3)+1)/2)  // ... x=int(min(1024*128,x))  // print(1024*128)  static  constexpr std ::array &lt; uint32_t ,  15 &gt; alloc_table  =  // computed from a python script above  { { 512 ,  768 ,  1152 ,  1728 ,  2592 ,  3888 ,  5832 ,  8748 ,  13122 ,  19683 ,  29525 ,  44288 ,  66432 ,  99648 ,  131072 } } ;  static size_t  next_allocation_size (size_t data_size ) ; } ;</p><p>Struct{静态常量expr size_t max_chunk_size=128*1024；静态常量expr size_t default_chunk_size=512；//&gt；&gt；x=512//&gt；&gt；&gt；而x&lt；int((1024*128))：//...。打印(X)//...。X=int(x*3)+1)/2)//...。X=int(min(1024x128，x))//print(1024x128)静态常量expr std：：array&lt；uint32_t，15&gt；alloc_table=//上面的python脚本{{5127681152 17282592388858328748,13122,19683,29525,44288,66432,99648,131072}}；static size_t NEXT_ALLOCATION_SIZE(SIZE_T DATA_SIZE)；}；</p><p> Predictability, memory pooling, fixed sizes, size capping, fragmented traversal, etc, are all known techniques to reduce latency. Asking for contiguous and variably sized memory could cause the allocator to compact all of the arenas and reshuffle a lot of bytes for what could be a short-lived request, not only injecting latency on the request path, but for the entire system since we have exactly one thread performing all operations.</p><p>可预测性、内存池、固定大小、大小上限、分段遍历等都是减少延迟的已知技术。请求连续且大小可变的内存可能会导致分配器压缩所有区域，并为可能是短暂的请求重新洗牌大量字节，这不仅会给请求路径带来延迟，而且会给整个系统带来延迟，因为我们只有一个线程执行所有操作。</p><p> Hardware is the platform. When we ask the network layer to give us exactly 11225 bytes in contiguous memory, we are simply asking the allocator to linearize an empty buffer of that exact size and for the network layer to copy bytes as the fragments come from the hardware into the destination buffer. There is ultimately no free lunch when it comes to trying to squeeze every single ounce of performance of your hardware and often it requires re-architecting from zero.</p><p>硬件就是平台。当我们要求网络层为我们提供11225字节的连续内存时，我们只是要求分配器将该大小的空缓冲区线性化，并要求网络层将来自硬件的碎片复制到目的缓冲区中。当试图挤压硬件的每一盎司性能时，最终都不会有免费的午餐，而且通常需要从零开始重新设计。</p><p> If you made it this far, I encourage you to sign up for our  Community Slack (here!) and ask us questions directly or engage with us on twitter via  @vectorizedio or personally at  @emaxerrno</p><p>如果你走到这一步，我鼓励你注册我们的社区松弛(这里！)。并直接向我们提问或在Twitter上通过@Vector torizedio或亲自发送电子邮件至@emaxerrno与我们联系。</p><p>  Special thanks to our Sarah, Noah, Ben, David, Michal and our external reviewers Mark Papadakis and Travis Downs for reviewing earlier drafts of this post.</p><p>特别感谢我们的莎拉、诺亚、本、大卫、米哈尔以及我们的外部评论员马克·帕帕达基斯和特拉维斯·唐斯审阅了这篇文章的早期草稿。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://vectorized.io/tpc-buffers/">https://vectorized.io/tpc-buffers/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/线程/">#线程</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/缓存/">#缓存</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/kafka/">#kafka</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/core/">#core</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/内存/">#内存</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>