<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Nvidia开发了一种截然不同的压缩视频通话方式Nvidia developed a radically different way to compress video calls</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Nvidia developed a radically different way to compress video calls<br/>Nvidia开发了一种截然不同的压缩视频通话方式</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-22 00:40:26</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/6c12022d8618588f58be5893d40befb0.png"><img src="http://img2.diglog.com/img/2020/11/6c12022d8618588f58be5893d40befb0.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Last month, Nvidia  announced a new platform called Maxine that uses AI to enhance the performance and functionality of video conferencing software. The software uses a neural network to create a compact representation of a person&#39;s face. This compact representation can then be sent across the network, where a second neural network reconstructs the original image—possibly with helpful modifications.</p><p>上个月，Nvidia宣布了一个名为Maxine的新平台，该平台使用AI增强了视频会议软件的性能和功能。该软件使用神经网络来创建人脸的紧凑表示。然后，可以通过网络发送此紧凑表示，第二个神经网络可以重构原始图像-可能会进行有用的修改。</p><p> Nvidia says that its technique can reduce the bandwidth needs of video conferencing software by a factor of 10 compared to conventional compression techniques. It can also change how a person&#39;s face is displayed. For example, if someone appears to be facing off-center due to the position of her camera, the software can rotate her face to look straight instead. Software can also replace someone&#39;s real face with an animated avatar.</p><p> Nvidia表示，与传统的压缩技术相比，其技术可以将视频会议软件的带宽需求减少10倍。它还可以更改人脸的显示方式。例如，如果某人由于摄像机的位置而显得偏心，则该软件可以旋转她的脸以使其看起来笔直。该软件还可以用动画化身来代替某人的真实面孔。</p><p> Maxine is a software development kit, not a consumer product. Nvidia is hoping third-party software developers will use Maxine to improve their own video conferencing software. And the software comes with an important limitation: the device receiving a video stream needs an Nvidia GPU with tensor core technology. To support devices without an appropriate graphics card, Nvidia recommends that video frames be generated in the cloud—an approach that may or may not work well in practice.</p><p> Maxine是软件开发套件，而不是消费产品。 Nvidia希望第三方软件开发人员将使用Maxine改进自己的视频会议软件。而且该软件具有一个重要的限制：接收视频流的设备需要具有张量核心技术的Nvidia GPU。为了支持没有合适图形卡的设备，英伟达（Nvidia）建议在云中生成视频帧，这种方法在实践中可能会或可能不会很好地起作用。</p><p> But regardless of how Maxine fares in the marketplace, the concept seems likely to be important for video streaming services in the future. Before too long, most computing devices will be powerful enough to generate realtime video content using neural networks. Maxine and products like it could allow for higher-quality video streams with much lower bandwidth consumption.</p><p> 但是，不管Maxine在市场上的表现如何，这一概念对于将来的视频流服务而言似乎都很重要。不久之后，大多数计算设备将足够强大，可以使用神经网络生成实时视频内容。 Maxine及其类似产品可以以更低的带宽消耗实现更高质量的视频流。</p><p>    A GAN is a neural network—a complex mathematical function that takes numerical inputs and produces numerical outputs. For visual applications, the input to a neural network is typically a pixel-by-pixel representation of an image. One  famous neural network, for example, took images as inputs and output the estimated probability that the image fell into each of 1,000 categories like &#34;dalmatian&#34; and &#34;mushroom.&#34;</p><p>    GAN是一个神经网络，它是一个复杂的数学函数，需要数字输入并产生数字输出。对于视觉应用，神经网络的输入通常是图像的逐像素表示。例如，一个著名的神经网络将图像作为输入，并输出图像落入“达尔马提亚”和“蘑菇”等1,000个类别中的估计概率。</p><p> Neural networks have thousands—often millions—of tunable parameters. The network is trained by evaluating its performance against real-world data. The network is shown a real-world input (like a picture of a dog) whose correct classification is known to the training software (perhaps &#34;dalmatian&#34;). The training software then uses a technique called  back-propagation to optimize the network&#39;s parameters. Values that pushed the network toward the right answer are boosted, while those that contributed to a wrong answer get dialed back. After repeating this process on thousands—even millions—of examples, the network may become quite effective at the task it&#39;s being trained for.</p><p>神经网络具有数千个（通常是数百万个）可调参数。通过根据实际数据评估其性能来训练网络。网络显示了一个真实世界的输入（如狗的图片），其正确的分类对于训练软件（也许是“达尔马提亚狗”）是已知的。然后，培训软件使用一种称为反向传播的技术来优化网络参数。推动将网络推向正确答案的价值得到提升，而导致错误答案的价值则被重新拨回。在数以千计甚至数百万个示例中重复此过程之后，网络可能会在针对其进行训练的任务上变得非常有效。</p><p>  Training software needs to know the correct answer for each input. For this reason, classic machine-learning projects often required people to label thousands of examples by hand. But the training process can be greatly sped up if there&#39;s a way to automatically generate training data.</p><p>  培训软件需要知道每个输入的正确答案。因此，经典的机器学习项目通常需要人们手工标记成千上万个示例。但是，如果有一种自动生成训练数据的方法，则可以大大加快训练过程。</p><p> A generative adversarial network is a clever way to train a neural network without the need for human beings to label the training data. As the name implies, a GAN is actually two networks that &#34;compete&#34; against one another.</p><p> 生成对抗网络是一种训练神经网络的聪明方法，而无需人工标记训练数据。顾名思义，GAN实际上是两个相互“竞争”的网络。</p><p> The first network is a generator that takes random data as an input and tries to produce a realistic image. The second network is a discriminator that takes an image and tries to determine whether it&#39;s a real image or a forgery created by the first network.</p><p> 第一个网络是一个生成器，它使用随机数据作为输入并尝试生成逼真的图像。第二个网络是一个鉴别器，它获取图像并尝试确定它是真实图像还是由第一个网络创建的伪造品。</p><p>    The training software runs these two networks simultaneously, with each network&#39;s results being used to train the other:</p><p>    训练软件可同时运行这两个网络，每个网络的结果将用于训练另一个：</p><p> The discriminator&#39;s answers are used to train the generator. When the discriminator wrongly classifies a generator-created photo as genuine, that means the generator is doing a good job of creating realistic images—so parameters that led to that result are reinforced. On the other hand, if the discriminator classifies an image as a forgery, that&#39;s treated as a failure for the generator.</p><p>鉴别器的答案用于训练发电机。当鉴别者错误地将生成器创建的照片归类为真实照片时，这意味着生成器在创建逼真的图像方面做得很好，因此可以增强导致该结果的参数。另一方面，如果鉴别器将图像分类为伪造品，则将其视为生成器失败。</p><p> Meanwhile, training software shows the discriminator a random selection of images that are either real or created by the generator. If the discriminator guesses right, that&#39;s treated as a success, and the discriminator network&#39;s parameters are updated to reflect that.</p><p> 同时，训练软件向鉴别器显示生成器生成的真实或随机图像选择。如果鉴别器猜测正确，则视为成功，并且鉴别器网络的参数会更新以反映这一点。</p><p> At the start of training, both networks are bad at their jobs, but they improve over time. As the quality of the generator&#39;s images improve, the discriminator has to become more sophisticated to detect fakes. As the discriminator becomes more discriminating, the generative network gets trained to make photos that look more and more realistic.</p><p> 在培训开始时，这两个网络都不适合他们的工作，但是随着时间的推移它们会不断改善。随着生成器图像质量的提高，鉴别器必须变得更加复杂以检测伪造品。随着区分器变得越来越具有区分性，生成网络将受到训练以使照片看起来越来越逼真。</p><p> The results can be spectacular. A website called  ThisPersonDoesNotExist.com does exactly what it sounds like: it generates realistic photographs of human beings that don&#39;t exist.</p><p> 结果可能是惊人的。一个名为ThisPersonDoesNotExist.com的网站确实听起来很像：它生成不存在的人类的逼真的照片。</p><p>    The site is powered by a generative neural network called  StyleGAN that was developed by researchers at Nvidia. Over the last decade, as Nvidia&#39;s graphics cards have become one of the most popular ways to do neural network computations, Nvidia has invested heavily in academic research into neural network techniques.</p><p>    该站点由Nvidia研究人员开发的称为StyleGAN的生成神经网络提供支持。在过去的十年中，由于Nvidia的图形卡已成为进行神经网络计算的最流行方法之一，因此Nvidia投入了大量资金进行神经网络技术的学术研究。</p><p>   The earliest GANs just tried to produce random realistic-looking images within a broad category like human faces. These are known as unconditional GANs. More recently, researchers have developed conditional GANs—neural networks that take an image (or other input data) and then try to produce a corresponding output image.</p><p>最早的GAN试图在诸如人脸之类的广泛类别中生成随机逼真的图像。这些称为无条件GAN。最近，研究人员开发了条件GAN（神经网络），先获取图像（或其他输入数据），然后尝试生成相应的输出图像。</p><p> In some cases, the training algorithm provides the same input information to both the generator and the discriminator. In other cases, the generator&#39;s loss function—the measure of how well the network did for training purposes—combines the output of the discriminator with some other metric that judges how well the output fits the input data.</p><p> 在某些情况下，训练算法向生成器和鉴别器提供相同的输入信息。在其他情况下，生成器的损失函数（即网络出于训练目的的良好程度的度量）将鉴别器的输出与其他一些判断输出的输入数据拟合程度的度量结合起来。</p><p> This approach has a wide range of applications. Researchers have used conditional GANs to  generate works of art from textual descriptions, to  generate photographs from sketches, to  generate maps from satellite images, to  predict how people will look when they&#39;re older, and a lot more.</p><p> 这种方法具有广泛的应用范围。研究人员已使用条件GAN从文字描述生成艺术品，从草图生成照片，从卫星图像生成地图，预测人们年纪大后的样子等等。</p><p> This brings us back to Nvidia Maxine. Nvidia hasn&#39;t provided full details on how the technology works, but it did point us to a  2019 paper that described some of the underlying algorithms powering Maxine.</p><p> 这使我们回到了Nvidia Maxine。 Nvidia尚未提供有关该技术工作原理的完整细节，但确实向我们指出了2019年的一篇论文，该论文描述了支持Maxine的一些基本算法。</p><p> The paper describes a conditional GAN that takes as input a video of one person&#39;s face talking and a few photos of a second person&#39;s face. The generator creates a video of the second person making the same motions as the person in the original video.</p><p> 该论文描述了一个条件GAN，它以一个人脸说话的视频和第二人脸的几张照片为输入。生成器创建第二个人的视频，该视频的动作与原始视频中的人物相同。</p><p>  Nvidia&#39;s new video conferencing software uses a slight modification of this technique. Instead of taking a video as input, Maxine takes a set of keypoints extracted from the source video—data points specifying the location and shape of the subject&#39;s eyes, mouth, nose, eyebrows, and other facial features. This data can be represented far more compactly than an ordinary video, which means it can be transmitted across the network with minimal bandwidth used. The network also sends a high-resolution video frame so that the recipient knows what the subject looks like. The receiver&#39;s computer then uses a conditional GAN to reconstruct the subject&#39;s face.</p><p>Nvidia的新视频会议软件对该技术进行了少许修改。 Maxine不用从视频中输入内容，而是从源视频中提取了一组关键点-数据点指定了对象的眼睛，嘴巴，鼻子，眉毛和其他面部特征的位置和形状。与普通视频相比，该数据可以更紧凑地表示，这意味着它可以在网络上以最小的带宽传输。网络还发送高分辨率视频帧，以便接收者知道对象的外观。接收者的计算机然后使用条件GAN重建对象的脸部。</p><p>    A key feature of the network Nvidia researchers described in 2019 is that it wasn&#39;t specific to one face. A single network could be trained to generate videos of different people based on the photos provided as inputs. The practical benefit for Maxine is that there&#39;s no need to train a new network for each user. Instead, Nvidia can provide a pre-trained generator network that can draw anyone&#39;s face. Using a pre-trained network requires far less computing power than training a new network from scratch.</p><p>    Nvidia研究人员在2019年描述的网络的一项关键功能是，它并非仅针对一张脸。可以训练单个网络，以根据提供的照片作为输入来生成不同人的视频。 Maxine的实际好处是无需为每个用户训练新的网络。取而代之的是，Nvidia可以提供可以吸引任何人的面孔的预训练发电机网络。与从头开始训练新网络相比，使用经过预先训练的网络所需的计算能力要低得多。</p><p> Nvidia&#39;s approach makes it easy to manipulate output video in a number of useful ways. For example, a common problem with videoconferencing technology is for the camera to be off-center from the screen, causing a person to appear to be looking to the side. Nvidia&#39;s neural network can fix this by rotating the keypoints of a user&#39;s face so that they are centered. Nvidia isn&#39;t the first company to do this. Apple has been  working on its own version of this feature for FaceTime. But it&#39;s possible that Nvidia&#39;s GAN-based approach will be more powerful, allowing modifications to the entire face rather than just the eyes.</p><p> 英伟达的方法可以轻松地以多种有用的方式来处理输出视频。例如，视频会议技术的一个常见问题是摄像头偏离屏幕中心，从而使人看起来好像在向侧面看。 Nvidia的神经网络可以通过旋转用户面部的关键点使其居中来解决此问题。英伟达并不是第一个这样做的公司。苹果公司一直在为FaceTime开发自己的此功能版本。但是，Nvidia基于GAN的方法可能会更强大，从而可以修饰整个面部而不只是眼睛。</p><p> Nvidia Maxine can also replace a subject&#39;s real head with an animated character who performs the same actions. Again, this isn&#39;t new—Snapchat popularized the concept a few years ago, and it has become common on video chat apps. But Nvidia&#39;s GAN-based approach could enable more realistic images that work in a wider range of head positions.</p><p> Nvidia Maxine还可以用执行相同动作的动画角色代替对象的真实头部。再说一次，这并不是什么新鲜事-几年前Snapchat推广了这一概念，并且在视频聊天应用程序中已经很普遍。但是Nvidia的基于GAN的方法可以使更真实的图像在更广泛的头部位置上工作。</p><p>   Maxine isn&#39;t a consumer product. Rather it&#39;s a software development kit for building video conferencing software. Nvidia is providing developers with a number of different capabilities and letting them decide how to put them together into a usable product.</p><p>   Maxine不是消费产品。而是一个用于开发视频会议软件的软件开发套件。 Nvidia为开发人员提供了许多不同的功能，并让他们决定如何将它们组合成可用的产品。</p><p> And at least the initial version of Maxine will come with an important limitation: it requires a recent Nvidia GPU on the receiving end of the video stream. Maxine is built atop tensor cores, compute units in newer Nvidia graphics cards that are optimized for machine-learning operations. This poses a challenge for a video-conferencing product, since customers are going to expect support for a wide variety of hardware.</p><p>至少Maxine的初始版本会有一个重要的限制：它需要在视频流的接收端使用最新的Nvidia GPU。 Maxine建立在张量核心之上，而张量核心则是在最新的Nvidia图形卡中针对计算机学习操作进行了优化的计算单元。这对视频会议产品构成了挑战，因为客户将期望获得对各种硬件的支持。</p><p> When I asked an Nvidia rep about this, he argued that developers could run Maxine on a cloud server equipped with the necessary Nvidia hardware, then stream the rendered video to client devices. This approach allows developers to capture some but not all of Maxine&#39;s benefits. Developers can use Maxine to re-orient a user&#39;s face to improve eye contact, replace a user&#39;s background, and perform effects like turning a subject&#39;s face into an animated character. Using Maxine this way can also save bandwidth on a user&#39;s video uplink, since Maxine&#39;s keypoint extraction technology doesn&#39;t require an Nvidia GPU.</p><p> 当我向Nvidia代表询问此事时，他认为开发人员可以在配备必要Nvidia硬件的云服务器上运行Maxine，然后将渲染的视频流传输到客户端设备。这种方法使开发人员可以获得Maxine的部分好处，但并非全部。开发人员可以使用Maxine重新调整用户面部的方向，以改善眼神交流，替换用户背景，并执行诸如将主体的面部变成动画角色的效果。由于Maxine的关键点提取技术不需要Nvidia GPU，因此以这种方式使用Maxine还可以节省用户视频上行链路的带宽。</p><p> Still, Maxine&#39;s strongest selling point is probably its dramatically smaller bandwidth requirements. And the full bandwidth savings can only be realized if video generation occurs on client devices. That would require Maxine to support devices without Nvidia GPUs.</p><p> 尽管如此，Maxine最强的卖点可能是其对带宽的要求大大降低。而且只有在客户端设备上生成视频时，才能实现全部带宽节省。这将要求Maxine支持不具有Nvidia GPU的设备。</p><p> When I asked Nvidia whether it planned to add support for non-Nvidia GPUs, it declined to comment on future product plans.</p><p> 当我问Nvidia是否计划增加对非Nvidia GPU的支持时，它拒绝评论未来的产品计划。</p><p> Right now, Maxine is in the &#34;early access&#34; stage of development. Nvidia is offering access to a select group of early developers who are helping Nvidia refine Maxine&#39;s APIs. At some point in the future—again, Nvidia wouldn&#39;t say when—Nvidia will open the platform to software developers generally.</p><p> 目前，Maxine处于开发的“早期访问”阶段。 Nvidia提供了一些早期开发人员的访问权限，这些人员正在帮助Nvidia完善Maxine的API。在将来的某个时候（Nvidia不会再说什么时候），Nvidia将向软件开发人员总体开放该平台。</p><p> And of course, Nvidia is unlikely to maintain a monopoly on this approach to video conferencing. As far as I can tell, other major tech companies have not yet announced plans to use GANs to improve video conferencing. But Google, Apple, and Qualcomm have all been working to  build more powerful chips to support machine learning on smartphones. It&#39;s a safe bet that engineers at these companies are exploring the possibility of Maxine-like video compression using neural networks. Apple may be particularly well-positioned to develop software like this given the tight integration of its hardware and software.</p><p>当然，英伟达不太可能在这种视频会议方法上保持垄断。据我所知，其他主要科技公司尚未宣布使用GAN改善视频会议的计划。但是，谷歌，苹果和高通都在致力于开发更强大的芯片来支持智能手机上的机器学习。可以肯定的是，这些公司的工程师正在探索使用神经网络进行类似Maxine的视频压缩的可能性。鉴于其硬件和软件的紧密集成，苹果在开发此类软件方面处于特别有利的位置。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://arstechnica.com/gadgets/2020/11/nvidia-used-neural-networks-to-improve-video-calling-bandwidth-by-10x/">https://arstechnica.com/gadgets/2020/11/nvidia-used-neural-networks-to-improve-video-calling-bandwidth-by-10x/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/nvidia/">#nvidia</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/开发/">#开发</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/压缩/">#压缩</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/developed/">#developed</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>