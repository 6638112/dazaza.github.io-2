<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>飞的Prometheus指标 Fly’s Prometheus Metrics</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Fly’s Prometheus Metrics<br/>飞的Prometheus指标 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-05-14 05:42:01</div><div class="page_narrow text-break page_content"><p>Fly.io transforms container images into fleets of micro-VMs running around the world on our hardware. It’s bananas easy to try it out; if you’ve got a Docker container,  it can be running on Fly in single-digit minutes.</p><p>Fly.io将集装箱图像转换为在我们的硬件上运行的Micro-VM车队。这是香蕉易于尝试的;如果您有一个Docker容器，它可以在单位数分钟内运行。</p><p> We should talk a bit about metrics and measurement and stuff, because they’re how we all know what’s going on.</p><p> 我们应该谈论指标和测量和东西，因为他们是如何知道发生了什么的。</p><p>  There’s two reasons we’ve written this post. The first is just that we think this stuff is interesting, and that the world can always use another detailed case study. The second, though, is that the work we’re going to describe is now part of our public API, and you can piggyback your own application metrics on top of ours. Our hope is, the more we describe this stuff, the more likely you are to get value out of it.</p><p>  我们写了这篇文章有两个原因。第一个只是我们认为这些东西很有趣，而世界总是可以使用另一个详细的案例研究。但是，第二个是我们将描述的工作现在是我们公共API的一部分，您可以在我们的顶部搭载您自己的应用指标。我们的希望是，我们更越是描述这种东西，你就越有可能得到它的价值。</p><p>   Here’s where we’ll start: an HTTP request arrives for an app running on Fly. It lands on one of our “edge nodes”, at  fly-proxy, our Rust request router. The proxy’s job is to deliver the request to the closest unloaded “worker node”, which hosts  Firecracker VMs running user applications.</p><p>   这是我们开始的地方：HTTP请求到达在飞行上运行的应用程序。它在我们的Rust Request Router中占据了我们的“边缘节点”之一。代理的作业是将请求提供给最接近的卸载“工作节点”，该卸载的“Worker节点”将托管运行用户应用程序的FireCracker VM。</p><p>  Somewhere in  fly-proxy’s memory there’s a counter, for the number of incoming bytes the proxy has handled. While routing the incoming request, the proxy bumps the counter.</p><p>  在Fly-Proxy的内存中的某个地方有一个计数器，对于代理已经处理的传入字节数。在路由传入请求时，代理撞击计数器。</p><p>  Bumping a counter is cheap. So there’s counters, and things that act them, all over the edge node. The operating system counts packets processed, TCP SYN segments seen, bytes processed per interface. CPU load. Free memory. Naturally, the proxy counts a bunch of things: a histogram of the times taken to complete TLS handshakes, number of in-flight connections, and so on. Some counts are global, others per-user.</p><p>  碰到柜台便宜。所以有柜台，以及所有在边缘节点上行动的东西。操作系统计算已处理的数据包，每个接口处理的TCP SYN段，字节处理。 CPU负载。免费内存。当然，代理计算一堆事物：时间拍摄的直方图，以完成TLS握手，飞行次数的数量等。有些计数是全球性的，其他人的用户。</p><p>  The proxy directs the request over a  WireGuard interface (more counters) to the worker node, which also runs  fly-proxy (still more). The worker’s proxy finds the tap interface associated with the application and routes the request to its address (you guessed it). Inside the VM, where the user’s app is running, the guest OS counts a series of arriving Ethernet frames, a newly opened TCP connection, memory used, CPU cycles burned. Your app running in that VM might count a new HTTP connection.</p><p>  该代理将该请求指向Wireguard接口（更多计数器）到工人节点，也运行Fly-Proxy（仍然更多）。 Worker的代理查找与应用程序关联的TAP接口，并将请求路由到其地址（您猜测）。在VM内部，在用户的应用程序运行的情况下，客户操作系统计算一系列到达以太网帧，新打开的TCP连接，使用内存，CPU循环刻录。您的应用程序在该VM中运行可能会计算新的HTTP连接。 </p><p>  We count everything we can think to count, because computers like counting stuff, so it’s cheap. You’d do the same thing even if you were building on a  potato-powered MSP430 microcontroller.</p><p>我们计算我们可以考虑的一切，因为电脑喜欢计数的东西，所以它很便宜。即使您在拍摄的MSP430微控制器上建造了同样的事情。</p><p>   To do anything with this information we need to collect it. There was once in our industry a lively debate about how to accomplish this, but Google settled it. Services expose HTTP endpoints that dump the stats; if your service can’t do that, it arranges to have something else do it for them.</p><p>   为我们需要收集的信息做任何事情。我们的行业有一次热烈的辩论关于如何完成这一点，但谷歌确定了它。服务公开转储统计数据的HTTP端点;如果您的服务无法做到，它会安排为他们提供别的东西。</p><p>  Google called these web pages  /varz, and filled them with a line-by-line human-readable format (apocryphally: intended for human readers) that Google’s own tools learned to parse. In the real world, we’ve kept the text format, and call the pages  /metrics. Here’s what it looks like:</p><p>  Google称为这些网页/ varz，并以逐行的人类可读格式（apocryphally：适用于人类读者）填充它们，该工具学会了解析。在现实世界中，我们保留了文本格式，并调用页面/指标。这是它的样子：</p><p>  fly_proxy_service_egress_http_responses_count{status=“200”,app_id=“3146”,app_name=“debug”,alloc=“f3e39e04”,service=“app-3146-tcp-8080”,org_id=“17”} 1586</p><p>  fly_proxy_service_egress_http_responses_count {status =“200”，app_id =“3146”，app_name =“debug”，alloc =“f3e39e04”，service =“app-3146-tcp-8080”，org_id =“17”} 1586</p><p> If you’re an Advent of Code kind of person and you haven’t already written a parser for this format, you’re probably starting to feel a twitch in your left eyelid. Go ahead and write the parser; it’ll take you 15 minutes and the world can’t have too many implementations of this exposition format. There&#39;s a lesson about virality among programmers buried in here somewhere.</p><p> 如果你是一个人的代码种类，并且你尚未为这种格式写一个解析器，那么你可能开始感受到左眼睑的抽搐。继续写解析器;它会带你15分钟，世界不能有太多的这种博览会的实现。在这里埋藏的程序员之间的课程有一个教训。</p><p>  To collect metrics, you scrape all the  /metrics pages you know about, on an interval, from a collector with some kind of indexed storage. Give that system a query interface and now it’s it’s a time-series database (a TSDB). We can now generate graphs and alerts.</p><p>  要收集指标，您将在与带有某种索引存储的收集器的收集器中刮擦您了解的所有/指标页面。为该系统提供查询界面，现在它是它是时序数据库（TSDB）。我们现在可以生成图形和警报。</p><p>  What we’re describing is, obviously,  Prometheus, which is an  escaped implementation of Borgmon, Google’s cluster monitoring system. Google SREs have a  complicated relationship with Borgmon, which was once said to have been disclosed in an “industry-disabling paper”. A lot of people hate it.</p><p>  我们描述的是，显然是ProMetheus，这是谷歌的群集监控系统的逃脱实施。谷歌SRE与Borgmon有复杂的关系，曾经说过在“行业禁用文件”中披露。很多人都讨厌它。 </p><p>  None of us have ever worked for Google, let alone as SREs. So we’re going out on a limb: this is a good design; it’s effective, so easy to implement that every serious programming environment supports it, and neatly separates concerns: things that generate metrics just need to keep counters and barf them out into an HTTP response, which makes it easy to generate lots of metrics.</p><p>我们曾经曾经为谷歌工作过，而是独自一人作为SRES。所以我们走出肢体：这是一个很好的设计;它很有效，因此可以轻松实现每个严肃的编程环境支持它，并且整齐地分开了担忧：生成指标的事情只需要将计数器和将其放在HTTP响应中，这使得很容易产生大量指标。</p><p>   The division of labor between components in this design is profoundly helpful on the database side as well. What makes general-purpose database servers hard to optimize is that they’re general-purpose: it’s hard to predict workloads. TSDBs don’t have this problem. Every Prometheus store shares a bunch of exploitable characteristics:</p><p>   在这种设计中的组件之间的劳动分工也在数据库方面非常有帮助。难以优化的通用数据库服务器是什么，它们是通用目的：很难预测工作负载。 TSDB没有这个问题。每个普罗米修斯商店都有一堆可利用的特征：</p><p>    Knowing this stuff ahead of time means you can optimize. You’ll use something like an  LSM tree to address the bias towards writes, jamming updates into a write-ahead log, indexing them in memory, gradually flushing them to immutable disk chunks. You can  compress; timestamps recorded on a relatively consistent interval are  especially amenable to compression. You can downsample older data. You can use a  column store instead of a row store.</p><p>    提前了解这个东西意味着你可以优化。您将使用类似LSM树的内容来解决写入的偏见，请将更新更新到注册日志中，将它们索引在内存中，逐渐将它们刷新到不可变磁盘块。你可以压缩;记录在相对一致的间隔上的时间戳尤其适用于压缩。您可以缩小旧数据。您可以使用列存储而不是行存储。</p><p>  And, of course, you can  scale this stuff out in a bunch of different ways; you can have Prometheus servers scrape and aggregate other Prometheus servers, or spill old chunks of data out to S3, or shard and hash incoming data into to a pool of servers.</p><p>  而且，当然，您可以以一堆不同的方式扩展出这个东西;您可以让ProMetheus服务器刮擦并汇总其他Prometheus服务器，或将旧块的数据泄漏到S3，或碎片和哈布和散列传入数据进入服务器池。</p><p>  Point being, keeping an indefinite number of time-series metrics available for queries is a tractable problem. Even a single server will probably hold up longer than you think. And when you you need to, you can scale it out to keep up.</p><p>  要点，保留可用于查询的无限数量的时间序列度量是一个易解问题。即使是单个服务器也可能会比您想象的更长。当您需要时，您可以扩展到跟上。</p><p>  Which is what we’ve done at Fly. And, since we’ve built this, there’s no sense in keeping it to ourselves; if you’re running an app on Fly, you can hand us your metrics too. Let’s describe how that works.</p><p>  这是我们在飞行中所做的事情。而且，由于我们已经建立了这一点，因此对自己保持了没有意义;如果您正在飞行上运行应用程序，您也可以向我们提供指标。让我们描述一下工作原理。</p><p>   A typical Fly.io POP will run some small number of lightweight edge nodes, a larger number of beefier worker nodes, and, in some cases, an infra host or two. All of these nodes — within and between POPs — are connected with a WireGuard mesh.</p><p>   典型的fly.io pop将运行一些少量的轻量级边缘节点，更多的牛群工作者节点，以及在某些情况下，一个红外主机或两个。所有这些节点 - 弹出窗口内部和之间的所有节点都与Wioguard网格连接。 </p><p>  Our metrics stack — which is built around Prometheus-style metrics — features the following cast of characters:</p><p>我们的指标堆栈 - 围绕Prometheus风格指标构建 - 具有以下特征的字符：</p><p>  Victoria Metrics (“Vicky”, for the rest of this post), in a clustered configuration, is our metrics database. We run a cluster of fairly big Vicky hosts.</p><p>  维多利亚指标（“Vicky”，在此帖子的其余部分），在群集配置中，是我们的指标数据库。我们经营一群相当大的Vicky主机。</p><p> Telegraf, which is to metrics sort of what Logstash is to logs: a swiss-army knife tool that adapts arbitrary inputs to arbitrary output formats. We run Telegraf agents on our nodes to scrape local Prometheus sources, and Vicky scrapes Telegraf. Telegraf simplifies the networking for our metrics; it means Vicky (and our  iptables rules) only need to know about one Prometheus endpoint per node.</p><p> TeleGraf是指算法的标志来标记为日志：一个瑞士军刀工具，适应任意输出格式的任意输入。我们在节点上运行TeleGraf Agents来刮掉本地Prometheus来源，而Vicky Scrapes Telegraf。 TeleGraf简化了我们的指标的网络;它意味着Vicky（和我们的iptables规则）只需要了解每个节点的一个prometheus端点。</p><p> Vector, which is like a Rust version of Telegraf — we use Vector extensively for our log pipeline (a whole other blog post), and also as a remote-writer for some Prometheus metrics. Our Vector is likely to eat our Telegraf this year.</p><p> 向量，这就像铁路飞行的锈版 - 我们为我们的日志管道（整个其他博客文章）广泛使用矢量，也是一些Prometheus指标的遥控器。我们的载体可能会在今年吃我们的Telegraf。</p><p> Prometheus exporters, to which Telegraf is wired to scrape, expose metrics for our services (and, in some cases, push stats directly to Vicky with the Prometheus  remote_write API).</p><p> Prometheus出口商，即可将Telegraf连接到刮刮，揭露我们的服务指标（以及在某些情况下，将统计数据直接推向Vicky与Prometheus Remote_Write API）。</p><p> Consul, which manages the configuration for all these things, so each host’s Telegraf agent can find local metrics sources, and Vicky can find new hosts, &amp;c.</p><p> 领域，管理所有这些事情的配置，所以每个主机的Telegraf代理都可以找到当地的指标来源，而Vicky可以找到新的主机，＆amp; c。</p><p>  Steve here  For metrics nerds, the only interesting thing about this stack is Vicky. The story there is probably boring. Like everyone else, we started with a simple Prometheus server. That worked until it didn’t. We spent some time scaling it with  Thanos, and Thanos was a lot, as far as ops hassle goes. We’d dabbled with Vicky just as a long-term storage engine for vanilla Prometheus, with  promxy set up to deduplicate metrics.</p><p>  史蒂夫在这里为指标书呆子，这个堆栈的唯一有趣的事情是vicky。故事可能很无聊。像其他人一样，我们开始使用一个简单的Prometheus服务器。这效果并没有。我们花了一些时间用thans缩放它，而且，就Ops Hassle来了，Thanos很多。我们只是作为Vanilla Prometheus的长期存储引擎删除了Vicky，Promxy设置为重复数据删除指标。 </p><p>  Vicky grew into a more ambitious offering, and added its own Prometheus scraper; we adopted it and scaled it as far as we reasonably could in a single-node configuration. Scaling requirements ultimately pushed us into a clustered deployment; we run an HA cluster (fronted by  haproxy). Current Vicky has a  really straightforward multi-tenant API — it’s easy to namespace metrics for customers — and it chugs along for us without too much minding.</p><p>Vicky成长为更雄心勃勃的产品，并添加了自己的普通粉刮刀;我们采用它并缩放了它，据我们合理地在单节点配置中缩放。缩放要求最终将我们推入集群部署;我们运行HA Cluster（由Haproxy前面）。目前的vicky有一个非常简单的多租户API  - 这很容易为客户命名空间度量 - 而且它越过我们而不是太多的心灵。</p><p> Assume, in the abstract, an arbitrarily-scaling central Vicky cluster. Here’s a simplified view of an edge node:</p><p> 在摘要中假设一个任意缩放的中央vicky集群。以下是边缘节点的简化视图：</p><p>    So far so simple. Edge nodes take traffic from the Internet and route it to worker nodes. Our edge  fly-proxy exports Prometheus stats, and so does a standard Prometheus  node-exporter , to provide system stats (along with a couple other exporters that you can sort of mentally bucket into the  node-exporter).</p><p>    到目前为止这么简单。边缘节点从Internet获取流量并将其路由到工作节点。我们的边缘Fly-Proxy导出Prometheus统计数据，也是一个标准的Prometheus Node-Exporter，提供系统统计数据（以及几个其他出口商，您可以在节点出口商中排出精神桶）。</p><p>  Jerome here    The proxy hosts further logic for transforming internal metrics into user-relevant metrics. We built our own  metrics::Recorder that sends metrics through default prometheus Recorder, but also tees some of them off for users, rewriting labels in the process.</p><p>  Jerome在这里，代理将内部指标转换为用户相关度量的进一步逻辑。我们构建了自己的指标::录像机，通过默认的Prometheus Recorder发送指标，但也将其中一些用于用户，在过程中重写标签。</p><p>       A worker node hosts user app instances. We use HashiCorp Nomad to orchestrate Firecrackers; one of the three biggest components of our system is driver we wrote for Nomad that manages Firecrackers. Naturally, Nomad exports a bunch of metrics to Telegraf.</p><p>       Worker节点主持用户应用程序实例。我们使用HashiCorp游牧到编排鞭炮;我们系统的三个最大组成部分之一是我们为管理爆竹的游牧民族写的驱动程序。当然，Nomad将一堆指标导出到Telegraf。</p><p>      In each Firecracker instance, we run our  custom init, which launches the user’s application. Our Nomad driver, our init, and Firecracker conspire to establish a  vsock — a host Unix domain socket that presents as a synthetic Virtio device in the guest — that allows init to communicate with the host; we bundle  node-exporter-type JSON stats over the  vsock for Nomad to collect and relay to Vicky.</p><p>      在每个闪光灯实例中，我们运行我们的自定义init，它启动用户的应用程序。我们的NoMad驱动程序，我们的init和firecracker consire建立一个vsock  - 一个主机Unix域套接字，它在guest虚拟机中作为一个合成的virtio设备 - 允许init与主机通信;我们在vsock捆绑节点出口型JSON统计游牧收集并传递给玉萍。</p><p>  The Firecracker picture includes one of the more important details in our system. These VMs are all IP-addressable. If you like (and you should!), you can expose a Prometheus exporter in your app, and then tell us about it in your  fly.toml; it might look like this:</p><p>  FireCracker图片包括我们系统中更重要的细节之一。这些VM都是IP可寻址的。如果您喜欢（而且您应该！），您可以在您的应用中公开一个Prometheus出口商，然后在您的Fly.Toml中告诉我们它;它可能是这样的： </p><p>   When you deploy, our system notices those directives, and will arrange for your metrics to end up in our Vicky cluster.</p><p>当您部署时，我们的系统将注意到这些指令，并将为您的指标安排在我们的Vicky集群中最终。</p><p>       When it comes to automated monitoring, there are two big philosophies, “checks” and “metrics”. “Checks” is what many of us grew up with: you write a script that probes the system and makes sure it’s responding the way you expect it to. Simple.</p><p>       谈到自动化监控时，有两个大哲学，“检查”和“指标”。 “检查”是我们中许多人的增长：你编写了一个探测系统的脚本，并确保它响应您期望的方式。简单的。</p><p>  “Metrics” is subtler. Instead of running scripts, you export metrics. Metrics are so much cheaper than checks that you end up tracking lots more things. You write rules over metrics to detect anomalies, like “more 502 responses than expected”, or “sudden drop in requests processed”, or “95th percentile latency just spiked”. Because you’ve got a historical record of metrics, you don’t even need to know in advance what rules you need. You just write them as you think of them.</p><p>  “指标”是subtler。导出指标而不是运行脚本。指标比检查您最终追踪更多的东西。您将规则写在指标上以检测异常，例如“比预期的502次响应”，或“加工要求的突然下降”，或“刚刚飙升的第95百分位延迟”。因为你有一个历史记录的指标，你甚至不需要提前知道你需要什么规则。你只是在想到他们时写下它们。</p><p>  That’s “ white box monitoring”. Use metrics to open up your systems as much as you can, then build monitoring on top, so you can ramp up the monitoring over time without constantly redeploying new versions of your applications.</p><p>  这是“白盒监测”。使用指标可以尽可能多地打开系统，然后在顶部构建监控，因此您可以随着时间的推移而增加监控，而无需不断重新部署新版本的应用程序。</p><p>  What’s more, you’re not just getting alerting; the same data flow gives you trending and planning and pretty graphs to put on LCD status boards on your wall.</p><p>  更重要的是，你不仅仅是得到警报;相同的数据流为您提供趋势和规划和漂亮的图表，以便在墙壁上放置LCD状态板。</p><p>  Once again, the emerging standard for how to accomplish this appears to be Prometheus. It’s easy to add Prometheus exporters to your own applications, and you should do so, whether or not you run your app on Fly. Something will eventually collect those metrics and put them to good use!</p><p>  再次，新兴标准如何实现这一点似乎是普罗米修斯。它很容易向您自己的应用程序添加Prometheus出口商，您应该这样做，无论您是否在飞行中运行您的应用程序。最终会收集这些指标并将它们良好使用！</p><p>   So that’s the metrics infrastructure we built, and the observability ideas we shoplifted to do it.</p><p>   这是我们构建的指标基础设施，以及我们购物的可观察性思想要做。 </p><p>  The neat thing about all of this is that Prometheus metrics are part of our public API. You can take them out for a spin with the free-plan Grafana Cloud right now; sign up with your Github account and you can add our API as a Prometheus data source. The URL will be  https://api.fly.io/prometheus/$(your-org) — if you don’t know your org,  personal works — and you’ll authenticate with a customer  Authorization header  Bearer $(flyctl auth token).</p><p>关于所有这些的整洁的是，Prometheus指标是我们公众API的一部分。你现在可以用自由计划的免费计划旋转;使用GitHub帐户注册，您可以将我们的API添加为Prometheus数据源。 URL将是https://api.fly.io/prometheus/$$（Your-org） - 如果您不知道您的ORG，个人作品 - 以及您将通过客户授权标题持票人进行身份验证（Flyctl Auth令牌）。</p><p>  Add a dashboard (the plus sign on the left menu), add an empty panel, click “Metrics”, and you’ll get a dropdown of all the current available metrics. For instance:</p><p>  添加仪表板（左侧菜单上的加号），添加一个空面板，单击“度量标准”，然后您将获得所有当前可用度量的下拉列表。例如：</p><p>   You can display any metric as a table to see the available labels; you can generally count on “app”, “region”, “host” (an identifier for the worker node your app is running on) and “instance”. Poke around and see what’s there!</p><p>   您可以将任何度量标准显示为表以查看可用标签;您通常可以计算“app”，“区域”，“主机”（工作者节点的标识符，您的应用程序正在运行）和“实例”。捅他们，看看有什么！</p><p>  Remember, again, that if you&#39;re exporting your own Prometheus metrics and you&#39;ve told us about it in  fly.toml, those metrics will show up in Grafana as well; we&#39;ll index and store and handle queries on them for you, with the networking and access control stuff taken care of automatically.</p><p>  再次记得，如果你＆＃39;重新出口您自己的Prometheus指标和您＆＃39;在Fly.Toml中告诉我们，这些指标也将在格拉多纳出现;我们＆＃39; ll索引和存储并为您处理查询，网络和访问控制器自动处理。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://fly.io/blog/measuring-fly/">https://fly.io/blog/measuring-fly/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/指标/">#指标</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>