<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>避免顶级Nginx配置错误Avoiding the top Nginx configuration mistakes</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Avoiding the top Nginx configuration mistakes<br/>避免顶级Nginx配置错误</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-25 16:00:41</div><div class="page_narrow text-break page_content"><p>When we help NGINX users who are having problems, we often see the same configuration mistakes we’ve seen over and over in other users’ configurations – sometimes even in configurations written by fellow NGINX engineers! In this blog we look at 10 of the most common errors, explaining what’s wrong and how to fix it.</p><p>当我们帮助有问题的NGINX用户时，我们经常会看到我们在其他用户的配置中反复看到的配置错误——有时甚至是在NGINX工程师同事编写的配置中！在这篇博客中，我们将看到10个最常见的错误，解释什么是错误以及如何修复。</p><p>   The   worker_connections directive sets the maximum number of simultaneous connections that a NGINX worker process can have open (the default is 512). All types of connections (for example, connections with proxied servers) count against the maximum, not just client connections. But it’s important to keep in mind that ultimately there is another limit on the number of simultaneous connections per worker: the operating system limit on the maximum number of file descriptors (FDs) allocated to each process. In modern UNIX distributions, the default limit is 1024.</p><p>worker_connections指令设置NGINX工作进程可以打开的最大同时连接数（默认值为512）。所有类型的连接（例如，与代理服务器的连接）都会计入最大值，而不仅仅是客户端连接。但重要的是要记住，最终每个工作进程同时连接的数量还有另一个限制：操作系统对分配给每个进程的最大文件描述符（FD）数量的限制。在现代UNIX发行版中，默认限制为1024。</p><p> For all but the smallest NGINX deployments, a limit of 512 connections per worker is probably too small. Indeed, the default  nginx.conf file we distribute with NGINX Open Source binaries and NGINX Plus increases it to 1024.</p><p>对于除最小的NGINX部署之外的所有部署，每个工作线程512个连接的限制可能太小了。实际上，默认的nginx。我们使用NGINX开源二进制文件分发conf文件，NGINX Plus将其增加到1024。</p><p> The common configuration mistake is not increasing the limit on FDs to at least twice the value of  worker_connections. The fix is to set that value with the   worker_rlimit_nofile directive in the main configuration context.</p><p>常见的配置错误是没有将FD的限制增加到至少两倍于worker_连接的值。修复方法是在主配置上下文中使用workerrlimit_nofile指令设置该值。</p><p> Here’s why more FDs are needed: each connection from an NGINX worker process to a client or upstream server consumes an FD. When NGINX acts as a web server, it uses one FD for the client connection and one FD per served file, for a minimum of two FDs per client (but most web pages are built from many files). When it acts as a proxy server, NGINX uses one FD each for the connection to the client and upstream server, and potentially a third FD for the file used to store the server’s response temporarily. As a caching server, NGINX behaves like a web server for cached responses and like a proxy server if the cache is empty or expired.</p><p>以下是需要更多FD的原因：从NGINX工作进程到客户端或上游服务器的每个连接都会消耗一个FD。当NGINX充当web服务器时，它使用一个FD作为客户端连接，每个服务文件使用一个FD，每个客户端至少使用两个FD（但大多数web页面是由许多文件构建的）。当它充当代理服务器时，NGINX使用一个FD分别连接到客户端和上游服务器，并可能使用第三个FD临时存储服务器响应的文件。作为缓存服务器，对于缓存响应，NGINX的行为类似于web服务器，如果缓存为空或过期，NGINX的行为类似于代理服务器。</p><p> NGINX also uses an FD per log file and a couple FDs to communicate with master process, but usually these numbers are small compared to the number of FDs used for connections and files.</p><p>NGINX还使用每个日志文件一个FD和一对FD与主进程通信，但通常这些数字与用于连接和文件的FD数量相比很小。</p><p>  The  init script or  systemd service manifest variables if you start NGINX as a service</p><p>如果将NGINX作为服务启动，则init脚本或systemd服务清单变量</p><p> However, the method to use depends on how you start NGINX, whereas  worker_rlimit_nofile works no matter how you start NGINX.</p><p>然而，使用的方法取决于您如何启动NGINX，而worker_rlimit_nofile无论您如何启动NGINX都可以工作。</p><p> There is also a system‑wide limit on the number of FDs, which you can set with the OS’s   sysctl  fs.file-max command. It is usually large enough, but it is worth verifying that the maximum number of file descriptors all NGINX worker processes might use (  worker_rlimit_nofile  *  worker_connections) is significantly less than  fs.file‑max. If NGINX somehow uses all available FDs (for example, during a DoS attack), it becomes impossible even to log in to the machine to fix the issue.</p><p>FD的数量也有一个系统范围的限制，您可以使用操作系统的sysctl fs进行设置。文件最大值命令。它通常足够大，但值得验证的是，所有NGINX工作进程可能使用的最大文件描述符数量（worker_rlimit_nofile*worker_connections）明显少于fs。file‑max。如果NGINX以某种方式使用了所有可用的FD（例如，在DoS攻击期间），则即使登录到计算机也无法修复该问题。</p><p>  The common mistake is thinking that the   error_log  off directive disables logging. In fact, unlike the   access_log directive,   error_log does not take an  off parameter. If you include the   error_log  off directive in the configuration, NGINX creates an error log file named  off in the default directory for NGINX configuration files (usually  /etc/nginx).</p><p>常见的错误是认为error_log off指令禁用了日志记录。事实上，与access_log指令不同，error_log不带off参数。如果在配置中包含error_log off指令，NGINX会在NGINX配置文件（通常为/etc/NGINX）的默认目录中创建一个名为off的错误日志文件。</p><p> We don’t recommend disabling the error log, because it is a vital source of information when debugging any problems with NGINX. However, if storage is so limited that it might be possible to log enough data to exhaust the available disk space, it might make sense to disable error logging. Include this directive in the main configuration context:</p><p>我们不建议禁用错误日志，因为在调试NGINX的任何问题时，它是一个重要的信息源。但是，如果存储空间非常有限，可能需要记录足够的数据以耗尽可用的磁盘空间，那么禁用错误记录可能是有意义的。在主配置上下文中包含此指令：</p><p>  Note that this directive doesn’t apply until NGINX reads and validates the configuration. So each time NGINX starts up or the configuration is reloaded, it might log to the default error log location (usually  /var/log/nginx/error.log) until the configuration is validated. To change the log directory, include the   -e  &lt; error_log_location&gt; parameter on the   nginx command.</p><p>请注意，在NGINX读取并验证配置之前，此指令不适用。因此，每次NGINX启动或重新加载配置时，它可能会记录到默认的错误日志位置（通常为/var/log/NGINX/error.log），直到配置被验证。要更改日志目录，请包括-e&lt；错误_日志_位置&gt；nginx命令上的参数。</p><p>  By default, NGINX opens a new connection to an upstream (backend) server for every new incoming request. This is safe but inefficient, because NGINX and the server must exchange three packets to establish a connection and three or four to terminate it.</p><p>默认情况下，NGINX会为每个新的传入请求打开到上游（后端）服务器的新连接。这是安全的，但效率低下，因为NGINX和服务器必须交换三个数据包才能建立连接，交换三到四个数据包才能终止连接。</p><p> At high traffic volumes, opening a new connection for every request can exhaust system resources and make it impossible to open connections at all. Here’s why: for each connection  the 4-tuple of source address, source port, destination address, and destination port must be unique. For connections from NGINX to an upstream server, three of the elements (the first, third, and fourth) are fixed, leaving only the source port as a variable. When a connection is closed, the Linux socket sits in the  TIME‑WAIT state for two minutes, which at high traffic volumes increases the possibility of exhausting the pool of available source ports. If that happens, NGINX cannot open new connections to upstream servers.</p><p>在高流量的情况下，为每个请求打开一个新连接可能会耗尽系统资源，根本无法打开连接。原因如下：对于每个连接，源地址、源端口、目标地址和目标端口的4元组必须是唯一的。对于从NGINX到上游服务器的连接，其中三个元素（第一个、第三个和第四个）是固定的，只保留源端口作为变量。当连接关闭时，Linux套接字将处于TIME‑WAIT（等待）状态两分钟，这在高通信量时会增加耗尽可用源端口池的可能性。如果出现这种情况，NGINX将无法打开到上游服务器的新连接。</p><p> The fix is to enable  keepalive connections between NGINX and upstream servers – instead of being closed when a request completes, the connection stays open to be used for additional requests. This both reduces the possibility of running out of source ports and  improves performance.</p><p>解决方案是在NGINX和上游服务器之间启用keepalive连接，而不是在请求完成时关闭，而是保持连接打开以用于其他请求。这既降低了源端口耗尽的可能性，又提高了性能。</p><p>  Include the   keepalive directive in every   upstream{} block, to set the number of idle keepalive connections to upstream servers preserved in the cache of each worker process.</p><p>在每个上游{}块中包含keepalive指令，以设置到每个工作进程缓存中保留的上游服务器的空闲keepalive连接数。</p><p> Note that the  keepalive directive does not limit the total number of connections to upstream servers that an NGINX worker process can open – this is a common misconception. So the parameter to  keepalive does not need to be as large as you might think.</p><p>请注意，keepalive指令没有限制NGINX工作进程可以打开的到上游服务器的连接总数——这是一个常见的误解。因此keepalive的参数不需要像您想象的那么大。</p><p> We recommend setting the parameter to twice the number of servers listed in the  upstream{} block. This is large enough for NGINX to maintain keepalive connections with all the servers, but small enough that upstream servers can process new incoming connections as well.</p><p>我们建议将参数设置为上游{}块中列出的服务器数量的两倍。这足够大，NGINX可以与所有服务器保持保持连接，但足够小，上游服务器也可以处理新的传入连接。</p><p> Note also that when you specify a load‑balancing algorithm in the  upstream{} block – with the   hash,   ip_hash,   least_conn,   least_time, or   random directive – the directive must appear above the  keepalive directive. This is one of the rare exceptions to the general rule that the order of directives in the NGINX configuration doesn’t matter.</p><p>还请注意，当您在上游{}块中指定负载平衡算法时（使用哈希、ip_哈希、最小连接、最小时间或随机指令），该指令必须出现在keepalive指令的上方。这是NGINX配置中指令顺序无关紧要的一般规则的罕见例外之一。</p><p> In the   location{} block that forwards requests to an upstream group, include the following directives along with the   proxy_pass directive:</p><p>在将请求转发到上游组的location{}块中，包括以下指令以及proxy_pass指令：</p><p>  By default NGINX uses HTTP/1.0 for connections to upstream servers and accordingly adds the   Connection:  close header to the requests that it forwards to the servers. The result is that each connection gets closed when the request completes, despite the presence of the  keepalive directive in the  upstream{} block.</p><p>默认情况下，NGINX使用HTTP/1.0连接到上游服务器，并相应地将Connection:close头添加到它转发给服务器的请求中。结果是，当请求完成时，每个连接都会关闭，尽管上游{}块中存在keepalive指令。</p><p> The   proxy_http_version directive tells NGINX to use HTTP/1.1 instead, and the   proxy_set_header directive removes the  close value from the  Connection header.</p><p>proxy_http_version指令告诉NGINX改用http/1.1，proxy_set_header指令从连接头中删除close值。</p><p>  NGINX directives are inherited downwards, or “outside‑in”: a  child context – one nested within another context (its  parent) – inherits the settings of directives included at the parent level. For example, all   server{} and   location{} blocks in the  http{} context inherit the value of directives included at the  http level, and a directive in a  server{} block is inherited by all the child  location{} blocks in it. However, when the same directive is included in both a parent context and its child context, the values are not added together – instead, the value in the child context overrides the parent value.</p><p>NGINX指令向下继承，或“外部-内部”：子上下文（嵌套在另一个上下文（其父上下文）中）继承父级包含的指令设置。例如，http{}上下文中的所有服务器{}和位置{}块继承http级别包含的指令的值，而服务器{}块中的指令由其中的所有子位置{}块继承。但是，当同一指令同时包含在父上下文及其子上下文中时，这些值不会一起添加——相反，子上下文中的值会覆盖父值。</p><p> The mistake is to forget this “override rule” for  array directives, which can be included not only in multiple contexts but also multiple times within a given context. Examples include   proxy_set_header and   add_header – having “add” in the name of second makes it particularly easy to forget about the override rule.</p><p>错误在于忘记了数组指令的“覆盖规则”，它不仅可以包含在多个上下文中，而且可以在给定上下文中多次包含。例如proxy_set_header和add_header–在second的名称中加上“add”会让人特别容易忘记覆盖规则。</p><p>  http { add_header X-HTTP-LEVEL-HEADER 1; add_header X-ANOTHER-HTTP-LEVEL-HEADER 1; server { listen 8080; location / { return 200 &#34;OK&#34;; }  } server { listen 8081; add_header X-SERVER-LEVEL-HEADER 1; location / { return 200 &#34;OK&#34;; } location /test { add_header X-LOCATION-LEVEL-HEADER 1; return 200 &#34;OK&#34;; } location /correct { add_header X-HTTP-LEVEL-HEADER 1; add_header X-ANOTHER-HTTP-LEVEL-HEADER 1; add_header X-SERVER-LEVEL-HEADER 1; add_header X-LOCATION-LEVEL-HEADER 1; return 200 &#34;OK&#34;; }  }}</p><p>http{</p><p> For the server listening on port 8080, there are no  add_header directives in either the  server{} or  location{} blocks. So inheritance is straightforward and we see the two headers defined in the  http{} context:</p><p>对于侦听端口8080的服务器，在服务器{}或位置{}块中都没有add_头指令。所以继承很简单，我们可以看到http{}上下文中定义的两个头：</p><p> %  curl -is localhost:8080HTTP/1.1 200 OKServer: nginx/1.21.5Date: Mon, 21 Feb 2022 10:12:15 GMTContent-Type: text/plainContent-Length: 2Connection: keep-alive X-HTTP-LEVEL-HEADER: 1X-ANOTHER-HTTP-LEVEL-HEADER: 1OK</p><p>%curl-is localhost:8080</p><p> For the server listening on port 8081, there is an  add_header directive in the  server{} block but not in its child   location  / block. The header defined in the  server{} block overrides the two headers defined in the  http{} context:</p><p>对于侦听端口8081的服务器，在服务器{}块中有一个add_头指令，但在其子位置/块中没有。在服务器{}块中定义的头覆盖在http{}上下文中定义的两个头：</p><p> %  curl -is localhost:8081HTTP/1.1 200 OKServer: nginx/1.21.5Date: Mon, 21 Feb 2022 10:12:20 GMTContent-Type: text/plainContent-Length: 2Connection: keep-alive X-SERVER-LEVEL-HEADER: 1OK</p><p>%curl-is localhost:8081</p><p> In the child   location  /test block, there is an  add_header directive and it overrides both the header from its parent  server{} block and the two headers from the  http{} context:</p><p>在子位置/测试块中，有一个add_header指令，它覆盖来自其父服务器{}块的头和来自http{}上下文的两个头：</p><p> %  curl -is localhost:8081/testHTTP/1.1 200 OKServer: nginx/1.21.5Date: Mon, 21 Feb 2022 10:12:25 GMTContent-Type: text/plainContent-Length: 2Connection: keep-alive X-LOCATION-LEVEL-HEADER: 1OK</p><p>%curl-is localhost:8081/test</p><p> If we want a  location{} block to preserve the headers defined in its parent contexts along with any headers defined locally, we must redefine the parent headers within the  location{} block. That’s what we’ve done in the   location  /correct block:</p><p>如果我们想让位置{}块保留在其父上下文中定义的头以及本地定义的任何头，我们必须在位置{}块中重新定义父头。这就是我们在位置/正确区块中所做的：</p><p> %  curl -is localhost:8081/correctHTTP/1.1 200 OKServer: nginx/1.21.5Date: Mon, 21 Feb 2022 10:12:30 GMTContent-Type: text/plainContent-Length: 2Connection: keep-alive X-HTTP-LEVEL-HEADER: 1X-ANOTHER-HTTP-LEVEL-HEADER: 1X-SERVER-LEVEL-HEADER: 1X-LOCATION-LEVEL-HEADER: 1OK</p><p>%curl-localhost:8081/正确吗</p><p>  Proxy buffering is enabled by default in NGINX (the   proxy_buffering directive is set to  on). Proxy buffering means that NGINX stores the response from a server in internal buffers as it comes in, and doesn’t start sending data to the client until the entire response is buffered. Buffering helps to optimize performance with slow clients – because NGINX buffers the response for as long as it takes for the client to retrieve all of it, the proxied server can return its response as quickly as possible and return to being available to serve other requests.</p><p>在NGINX中默认启用代理缓冲（Proxy_buffering指令设置为on）。代理缓冲意味着NGINX将来自服务器的响应存储在内部缓冲区中，直到整个响应被缓冲后才开始向客户端发送数据。缓冲有助于优化低速客户端的性能——因为NGINX会在客户端检索所有响应所需的时间内对响应进行缓冲，所以代理服务器可以尽快返回其响应，并恢复到可用于服务其他请求的状态。</p><p> When proxy buffering is disabled, NGINX buffers only the first part of a server’s response before starting to send it to the client, in a buffer that by default is one memory page in  size (4 KB  or 8 KB depending on the operating system). This is usually just enough space for the response header. NGINX then sends the response to the client synchronously as it receives it, forcing the server to sit idle as it waits until NGINX can accept the next response segment.</p><p>当禁用代理缓冲时，NGINX只缓冲服务器响应的第一部分，然后再开始将其发送到客户端，在默认情况下，缓冲区的大小为一个内存页（4KB或8KB，取决于操作系统）。这通常刚好足够响应头的空间。然后，NGINX在接收到响应时同步向客户端发送响应，迫使服务器在等待NGINX接受下一个响应段时处于空闲状态。</p><p> So we’re surprised by how often we see   proxy_buffering  off in configurations. Perhaps it is intended to reduce the latency experienced by clients, but the effect is negligible while the side effects are numerous: with proxy buffering disabled, rate limiting and caching don’t work even if configured, performance suffers, and so on.</p><p>因此，我们惊讶地发现，在配置中，代理_缓冲的频率是如此之高。也许这是为了减少客户端所经历的延迟，但其影响可以忽略，而副作用却很多：禁用代理缓冲时，即使配置了，速率限制和缓存也无法工作，性能受到影响，等等。</p><p> There are only a small number of use cases where disabling proxy buffering might make sense (such as long polling), so we strongly discourage changing the default. For more information, see the  NGINX Plus Admin Guide.</p><p>只有少数情况下禁用代理缓冲可能有意义（例如长轮询），因此我们强烈反对更改默认设置。有关更多信息，请参阅《NGINX Plus管理指南》。</p><p>  The   if directive is tricky to use, especially in   location{} blocks. It often doesn’t do what you expect and can even cause segfaults. In fact, it’s so tricky that there’s an article titled  If is Evil in the NGINX Wiki, and we direct you there for a detailed discussion of the problems and how to avoid them.</p><p>if指令很难使用，尤其是在位置{}块中。它通常不会达到你期望的效果，甚至可能导致故障。事实上，非常棘手的是，NGINX Wiki上有一篇题为“如果是邪恶的”的文章，我们会指导您详细讨论这些问题以及如何避免它们。</p><p> In general, the only directives you can always use safely within an  if{} block are   return and   rewrite. The following example uses  if to detect requests that include the  X‑Test header (but this can be any condition you want to test for). NGINX returns  the  430  (Request  Header  Fields  Too  Large) error, intercepts it at the named location  @error_430 and proxies the request to the upstream group named  b.</p><p>一般来说，在if{}块中，唯一可以始终安全使用的指令是return和rewrite。下面的示例使用if来检测包含X-Test头的请求（但这可以是您想要测试的任何条件）。NGINX返回430（请求头字段太大）错误，在指定位置@error_430截取它，并将请求代理给名为b的上游组。</p><p> location / { error_page 430 = @error_430; if ($http_x_test) { return 430;  } proxy_pass http://a;}location @error_430 { proxy_pass b;}</p><p>地点/{</p><p> For this and many other uses of  if, it’s often possible to avoid the directive altogether. In the following example, when the request includes the  X‑Test header the   map{} block sets the  $upstream_name variable to  b and the request is proxied to the upstream group with that name.</p><p>对于if的这一用途和许多其他用途，通常可以完全避免该指令。在下面的示例中，当请求包含X‑Test头时，map{}块将$upstream_name变量设置为b，并将请求代理给具有该名称的上游组。</p><p> map $http_x_test $upstream_name { default &#34;b&#34;; &#34;&#34; &#34;a&#34;;}# ...location / { proxy_pass http://$upstream_name;}</p><p>映射$http_x_test$上游_name{</p><p>  It is quite common to configure multiple virtual servers to proxy requests to the same upstream group (in other words, to include the identical   proxy_pass directive in multiple   server{} blocks). The mistake in this situation is to include a   health_check directive in every  server{} block. This just creates more load on the upstream servers without yielding any additional information.</p><p>将多个虚拟服务器配置为将请求代理到同一上游组是很常见的（换句话说，在多个服务器{}块中包含相同的proxy_pass指令）。这种情况下的错误是在每个服务器{}块中包含一个health_check指令。这只会在上游服务器上产生更多负载，而不会产生任何附加信息。</p><p> At the risk of being obvious, the fix is to define just one health check per   upstream{} block. Here we define the health check for the upstream group named  b in a special named location, complete with appropriate timeouts and header settings.</p><p>冒着显而易见的风险，解决方法是只为每个上游{}块定义一个健康检查。在这里，我们在一个特殊的命名位置为名为b的上游组定义健康检查，并完成适当的超时和标头设置。</p><p> location / { proxy_set_header Host $host; proxy_set_header &#34;Connection&#34; &#34;&#34;; proxy_http_version 1.1; proxy_pass http://b;}location @health_check { health_check; proxy_connect_timeout 2s; proxy_read_timeout 3s; proxy_set_header Host example.com; proxy_pass http://b;}</p><p>地点/{</p><p> In complex configurations, it can further simplify management to group all health‑check locations in a single virtual server along with the  NGINX Plus API and dashboard, as in this example.</p><p>在复杂的配置中，它可以进一步简化管理，将所有健康检查位置与NGINX Plus API和仪表板一起分组到单个虚拟服务器中，如本例所示。</p><p> server {	listen 8080; 	location / {	 # … 	} 	location @health_check_b {	 health_check;	 proxy_connect_timeout 2s;	 proxy_read_timeout 3s;	 proxy_set_header Host example.com;	 proxy_pass http://b;	} 	location @health_check_c {	 health_check;	 proxy_connect_timeout 2s;	 proxy_read_timeout 3s;	 proxy_set_header Host api.example.com;	 proxy_pass http://c;	} 	location /api {	 api write=on;	 # directives limiting access to the API (see &#39;Mistake 8&#39; below)	} 	location = /dashboard.html {	 root /usr/share/nginx/html;	}}</p><p>服务器{</p><p> For more information about health checks for HTTP, TCP, UDP, and gRPC servers, see the  NGINX Plus Admin Guide.</p><p>有关HTTP、TCP、UDP和gRPC服务器运行状况检查的更多信息，请参阅《NGINX Plus管理指南》。</p><p>  Basic metrics about NGINX operation are available from the  Stub Status module. For NGINX Plus, you can also gather a much more extensive set of metrics with the   NGINX Plus API. Enable metrics collection by including the   stub_status or   api directive, respectively, in a   server{} or   location{} block, which becomes the URL you then access to view the metrics. (For the  NGINX Plus API, you also need to configure shared memory zones for the NGINX entities – virtual servers, upstream groups, caches, and so on – for which you want to collect metrics; see the instructions in the  NGINX Plus Admin Guide.)</p><p>有关NGINX操作的基本指标可从存根状态模块获得。对于NGINX Plus，您还可以使用NGINX Plus API收集更广泛的指标集。通过在服务器{}或位置{}块中分别包含stub_status或api指令来启用度量集合，该块将成为您随后访问以查看度量的URL。（对于NGINX Plus API，您还需要为NGINX实体（虚拟服务器、上游组、缓存等）配置共享内存区域，以便收集指标；请参阅《NGINX Plus管理指南》中的说明。）</p><p> Some of the metrics are sensitive information that can be used to attack your website or the apps proxied by NGINX, and the mistake we sometimes see in user configurations is failure to restrict access to the corresponding URL. Here we look at some of the ways you can secure the metrics. We’ll use  stub_status in the first examples.</p><p>其中一些指标是敏感信息，可用于攻击您的网站或NGINX代理的应用程序，我们有时在用户配置中看到的错误是未能限制对相应URL的访问。在这里，我们将介绍一些确保指标安全的方法。在第一个示例中，我们将使用stub_status。</p><p>    To password‑protect the metrics with  HTTP Basic Authentication, include the   auth_basic and  auth_basic_user_file directives. The file (here,  .htpasswd) lists the usernames and passwords of clients who can log in to see the metrics:</p><p>要使用HTTP Basic身份验证对指标进行密码保护，请包括auth_Basic和auth_Basic_user_file指令。文件（此处，.htpasswd）列出了可以登录以查看指标的客户端的用户名和密码：</p><p> server { listen 80; server_name example.com; location = /basic_status { auth_basic “closed site”; auth_basic_user_file conf.d/.htpasswd; stub_status; }}</p><p>服务器{</p><p>  If you don’t want authorized users to have to log in, and you know the IP addresses from which they will access the metrics, another option is the   allow directive. You can specify individual IPv4 and IPv6 addresses and CIDR ranges. The   deny  all directive prevents access from any other addresses.</p><p>如果您不希望授权用户必须登录，并且您知道他们将从中访问指标的IP地址，另一个选项是allow指令。您可以指定单个IPv4和IPv6地址以及CIDR范围。deny all指令禁止从任何其他地址访问。</p><p> server { listen 80; server_name example.com; location = /basic_status { allow 192.168.1.0/24; allow 10.1.1.0/16; allow 2001:0db8::/32; allow 96.1.2.23/32; deny all; stub_status; }}</p><p>服务器{</p><p>  What if we want to combine both methods? We can allow clients to access the metrics from specific addresses without a password and still require login for clients coming from different addresses. For this we use the   satisfy  any directive. It tells NGINX to allow access to clients who either log in with HTTP Basic auth credentials or are using a preapproved IP address. For extra security, you can set  satisfy to  all to require even people who come from specific addresses to log in.</p><p>如果我们想把这两种方法结合起来呢？我们可以允许客户在没有密码的情况下从特定地址访问指标，并且仍然需要来自不同地址的客户登录。为此，我们使用“满足任何指令”。它告诉NGINX允许使用HTTP基本身份验证凭据登录或使用预先批准的IP地址的客户端访问。为了获得额外的安全性，您可以将“满足所有”设置为要求来自特定地址的人登录。</p><p> server { listen 80; server_name monitor.example.com; location = /basic_status { satisfy any; auth_basic “closed site”; auth_basic_user_file conf.d/.htpasswd; allow 192.168.1.0/24; allow 10.1.1.0/16; allow 2001:0db8::/32; allow 96.1.2.23/32; deny all; stub_status; }}</p><p>服务器{</p><p> With NGINX Plus, you use the same techniques to limit access to the  NGINX Plus API endpoint ( http://monitor.example.com:8080/api/ in the following example) as well as the live activity monitoring dashboard at  http://monitor.example.com/dashboard.html.</p><p>使用NGINX Plus，您可以使用相同的技术来限制对NGINX Plus API端点的访问（http://monitor.example.com:8080/api/在下面的示例中）以及http://monitor.example.com/dashboard.html.</p><p> This configuration permits access without a password only to clients coming from the 96.1.2.23/32 network or localhost. Because the directives are defined at the  server{} level, the same restrictions apply to both the API and the dashboard. As a side note, the  write=on parameter to  api means these clients can also use the API to make configuration changes.</p><p>此配置仅允许来自96.1.2.23/32网络或本地主机的客户端无需密码即可访问。因为指令是在服务器{}级别定义的，所以相同的限制适用于API和仪表板。作为旁注，api的write=on参数意味着这些客户端也可以使用api进行配置更改。</p><p> For more information about configuring the API and dashboard, see the  NGINX Plus Admin Guide.</p><p>有关配置API和仪表板的更多信息，请参阅《NGINX Plus管理指南》。</p><p> server { listen 8080; server_name monitor.example.com;  satisfy any; auth_basic “closed site”; auth_basic_user_file conf.d/.htpasswd; allow 127.0.0.1/32; allow 96.1.2.23/32; deny all; location = /api/ {  api write=on; } location = /dashboard.html { root /usr/share/nginx/html; }}</p><p>服务器{</p><p>  The   ip_hash algorithm load balances traffic across the servers in an   upstream{} block, based on a hash of the client IP address. The hashing key is the first three octets of an IPv4 address or the entire IPv6 address. The method establishes session persistence, which means that requests from a client are always passed to the same server except when the server is unavailable.</p><p>ip_散列算法基于客户端ip地址的散列，在上游{}块中跨服务器负载平衡流量。哈希键是IPv4地址或整个IPv6地址的前三个八位字节。该方法建立会话持久性，这意味着来自客户端的请求总是传递到同一服务器，除非该服务器不可用。</p><p> Suppose that we have deployed NGINX as a reverse proxy in a virtual private network configured for high availability. We put various firewalls, routers, Layer 4 load balancers, and gateways in front of NGINX to accept traffic from different sources (the internal network, partner networks, the Internet, and so on) and pass it to NGINX for reverse proxying to upstream servers. Here’s the initial NGINX configuration:</p><p>假设我们在一个为高可用性配置的虚拟专用网络中将NGINX部署为反向代理。我们将各种防火墙、路由器、第4层负载平衡器和网关放在NGINX前面，以接受来自不同来源（内部网络、合作伙伴网络、互联网等）的流量，并将其传递给NGINX，以便反向代理到上游服务器。以下是NGINX的初始配置：</p><p> http { upstream { ip_hash; server 10.10.20.105:8080; server 10.10.20.106:8080; server 10.10.20.108:8080; }  server {# …}}</p><p>http{</p><p> But it turns out there’s a problem: all of the “intercepting” devices are on the same 10.10.0.0/24 network, so to NGINX it looks like all traffic comes from addresses in that CIDR range. Remember that the  ip_hash algorithm hashes the first three octets of an IPv4 address. In our deployment, the first three octets are the same – 10.10.0 – for every client, so the hash is the same for all of them and there’s no basis for distributing traffic to different servers.</p><p>但事实证明存在一个问题：所有的“拦截”设备都在同一个10.10.0.0/24网络上，所以对NGINX来说，所有流量似乎都来自该CIDR范围内的地址。请记住，ip_哈希算法对IPv4地址的前三个八位字节进行哈希运算。在我们的部署中，每个客户端的前三个八位字节都是相同的，即10.10.0，因此所有客户端的哈希都是相同的，并且没有将流量分配到不同服务器的依据。</p><p> The fix is to use the   hash </p><p>解决方法是使用散列</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/顶级/">#顶级</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/top/">#top</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/nginx/">#nginx</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>