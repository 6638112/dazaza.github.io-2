<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>研究人员提出了更快，更有效的反向传播替代方案 Researchers propose faster, more efficient alternatives to backpropagation</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Researchers propose faster, more efficient alternatives to backpropagation<br/>研究人员提出了更快，更有效的反向传播替代方案 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-12-21 01:51:37</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/12/d3de6ff49fbb2c53e0088f2170afee4e.jpg"><img src="http://img2.diglog.com/img/2020/12/d3de6ff49fbb2c53e0088f2170afee4e.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>In the 1960s, academics including Virginia Polytechnic Institute professor Henry J. Kelley, Stanford University’s Arthur E. Bryson, and Stuart Dreyfus at the University of California, Berkeley arrived at the theory of backpropagation. It’s an algorithm which would later become widely used to train neural networks, the computing systems vaguely inspired by the biological neural networks that constitute animal brains. Backpropagation rose to prominence in the 2010s in light of the emergence of cheap, powerful computing systems, leading to gains in speech recognition, computer vision, and natural language processing.</p><p>在1960年代，弗吉尼亚理工学院教授Henry J. Kelley，斯坦福大学的Arthur E. Bryson和加州大学伯克利分校的Stuart Dreyfus等学者提出了反向传播理论。它是一种算法，后来被广泛用于训练神经网络，这种计算系统受到构成动物大脑的生物神经网络的模糊启发。由于廉价，功能强大的计算系统的出现，反向传播在2010年代变得更加突出，从而导致语音识别，计算机视觉和自然语言处理的发展。</p><p> Backpropagation generally works well, but it’s constrained in that it optimizes AI models for a fixed rather than a moving target. Once the models learn to make predictions from a dataset, they run the risk of forgetting what they learned when given new training data — a phenomenon known as “catastrophic forgetting.” That’s why researchers are investigating techniques that move beyond backpropagation toward forms of continuous learning, which don’t entail retraining on their entire history of experiences. Experts believe this more humanlike way of learning, which confers the ability to learn new information without forgetting, could lead to significant advances in the field of AI and machine learning.</p><p> 反向传播通常效果很好，但由于它可以针对固定目标而不是移动目标优化AI模型，因此受到限制。一旦模型学会根据数据集做出预测，他们就有冒着在获得新训练数据时忘记所学知识的风险，这种现象被称为“灾难性遗忘”。这就是为什么研究人员正在研究从反向传播转向持续学习形式的技术，而这些技术并不需要对他们的整个经历进行再培训。专家认为，这种更人性化的学习方式赋予了学习新信息而不会忘记的能力，可能会导致AI和机器学习领域的重大进步。</p><p>  In early December, dozens of alternatives to traditional backpropagation were proposed during a workshop at the NeurIPS 2020 conference, which took place virtually. Some leveraged hardware like photonic circuits to further bolster the efficiency of backpropagation, while others adopted a more modular, flexible approach to training.</p><p>  在12月初，在实际上举行的NeurIPS 2020会议的研讨会上，提出了许多传统反向传播的替代方法。一些利用诸如光子电路之类的硬件来进一步提高反向传播的效率，而另一些则采用了更加模块化，灵活的训练方法。</p><p>  The simplest form of backpropagation involves computing the gradient — the optimization algorithm that’s used when training a machine learning model — of a loss function with respect to the weights of a model. (A loss function is a method of evaluating how well a specific algorithm models a given dataset.) Neural networks are made up of interconnected neurons through which data moves and weights control the signal between two neurons, deciding how much influence data fed into the network will have on the outputs that emerge from it.</p><p>  反向传播的最简单形式是计算损失函数相对于模型权重的梯度（训练机器学习模型时使用的优​​化算法）。 （损失函数是一种评估特定算法对给定数据集建模的良好程度的方法。）神经网络由相互连接的神经元组成，数据通过该神经元移动并由权重控制两个神经元之间的信号，从而决定将多少数据馈送到网络中将具有从中产生的输出。</p><p> Backpropagation is efficient, making it feasible to train multilayer networks containing many neurons while updating the weights to minimize loss. As alluded to earlier, it works by computing the gradient of the loss function with respect to each weight through what’s known as the chain rule, computing the gradient one layer at a time and iterating backward from the last layer to avoid redundant calculations.</p><p> 反向传播是有效的，使训练包含许多神经元的多层网络，同时更新权重以使损失最小化成为可能。如前所述，它的工作原理是通过链式规则计算损失函数相对于每个权重的梯度，一次计算一次梯度，然后从最后一层开始迭代，以避免重复计算。</p><p> But for all its advantages, backpropagation is severely limited in what it can accomplish up to a certain point. For example, as mathematician Anthony Repetto  points out, backpropagation makes it impossible to recognize a “constellation” of a dataset’s features. When a computer vision system trained using backpropagation classifies an object in an image — for example, “horse” — it can’t communicate which features in the image led it to that conclusion. (It’s lost this information.) Backpropagation also updates the network layers sequentially, making it difficult to parallelize the training process and leading to longer training times.</p><p> 但是，尽管具有所有优点，但是反向传播在可以达到一定程度的方面受到了严重限制。例如，正如数学家Anthony Repetto指出的那样，反向传播使得无法识别数据集特征的“星座”。当使用反向传播训练的计算机视觉系统将图像中的对象（例如“马”）分类时，它无法传达图像中的哪些特征导致了该结论。 （丢失了这些信息。）反向传播还会顺序更新网络层，从而难以并行化训练过程并导致更长的训练时间。</p><p> Another disadvantage of backpropagation is its tendency to become stuck in the local minima of the loss function. Mathematically, the goal in training a model is converging on the global minimum, the point in the loss function where the model has optimized its ability to make predictions. But there often exist approximations of the global minimum — points close to optimal, but not exact — that backpropagation finds instead. This isn’t always a problem, but it could lead to incorrect predictions on the part of the model.</p><p> 反向传播的另一个缺点是它倾向于陷入损失函数的局部最小值中。从数学上讲，训练模型的目标是收敛于全局最小值，即损失函数中模型已优化其预测能力的点。但是，通常存在全局最小值的近似值（接近最佳点，但不是精确点），而是反向传播找到的。这并不总是一个问题，但是它可能导致模型方面的错误预测。 </p><p>  It was once thought that the weights used for propagating backward through a network had to be the same as the weights used for propagating forward. But a recently discovered method called direct feedback alignment shows that random weights work equally well, because the network effectively learns how to make them useful. This opens the door to parallelizing the backwards pass, potentially reducing training time and power consumption by an order of magnitude.</p><p>曾经认为，用于通过网络向后传播的权重必须与用于向前传播的权重相同。但是最近发现的一种称为直接反馈对齐的方法表明，随机权重同样有效，因为网络有效地学习了如何使它们有用。这为后退通行并行化打开了大门，有可能将训练时间和功耗降低一个数量级。</p><p> Indeed, in a  paper submitted to the NeurIPS workshop anonymously, the coauthors propose “slot machine” networks where each “reel” — i.e., connection between neurons — contains a fixed set of random values. The algorithm “spins” the reels to seek “winning” combinations or selections of random weight values that minimize the given loss. The results show that allocating just a few random values to each connection, like eight values per connection, improves performance over trained baseline models.</p><p> 确实，在匿名提交给NeurIPS研讨会的论文中，合著者提出了“老虎机”网络，其中每个“卷轴”（即神经元之间的连接）都包含一组固定的随机值。该算法“旋转”转盘以寻求“获胜”组合或选择的随机权重值，以最大程度地减少给定损失。结果表明，仅为每个连接分配几个随机值，例如每个连接八个值，可以提高经过训练的基线模型的性能。</p><p> In another paper accepted to the workshop, researchers at LightOn, a startup developing photonic computing hardware,  claim that feedback alignment can successfully train a range of state-of-the-art machine learning architectures, with performance close to fine-tuned backpropagation. While the researchers acknowledge that their experiments required “substantial” cloud resources, they say the work provides “new perspectives” that might “favor the application of neural networks in fields previously inaccessible because of computational limits.”</p><p> 在接受该研讨会的另一篇论文中，开发光子计算硬件的初创公司LightOn的研究人员声称，反馈对准可以成功地训练一系列最先进的机器学习架构，其性能接近于微调的反向传播。尽管研究人员承认他们的实验需要“大量”的云资源，但他们说这项工作提供了“新观点”，可能“有利于神经网络在以前由于计算限制而无法访问的领域中的应用”。</p><p> But alignment isn’t a perfect solution. While it successfully trains models like  Transformers, it notoriously fails to train convolutional networks, a dominant form of computer vision model. Moreover, unlike backpropagation, feedback alignment hasn’t enjoyed decades of work on topics like adversarial attacks, interpretability, and fairness. The effects of scaled-up alignment remain understudied.</p><p> 但是对齐并不是完美的解决方案。尽管它成功地训练了诸如“变形金刚”之类的模型，但是众所周知，它未能训练卷积网络，而卷积网络是计算机视觉模型的主要形式。此外，与反向传播不同，反馈调整在对抗性攻击，可解释性和公平性等主题上的研究已有数十年之久。规模扩大对齐的效果仍未得到研究。</p><p>  Perhaps the most radical alternative to backpropagation proposed so far involves new hardware custom-built for feedback alignment. In a  study submitted to the workshop by another team at LightOn, the coauthors describe a photonic accelerator that’s ostensibly able to compute random projections with trillions of different variables. They claim that their hardware — a photonic coprocessor — is architecture-agnostic and potentially a step toward building scalable systems that don’t rely on backpropagation.</p><p>  迄今为止，提出的反向传播的最根本的替代方法可能是为反馈对齐量身定制的新硬件。在由LightOn的另一个小组提交给研讨会的研究中，合著者描述了一种光子加速器，该光子加速器表面上能够计算具有数万亿个不同变量的随机投影。他们声称自己的硬件（光子协处理器）与架构无关，并且可能朝着构建不依赖于反向传播的可扩展系统迈出了一步。</p><p> Photonic integrated circuits, which are the foundation of LightOn’s chip, promise a host of advantages over their electronic counterparts. They require only a limited amount of energy because light produces less heat than electricity does, and they’re less susceptible to changes in ambient temperature, electromagnetic fields, and other noise. Latency in  photonic designs is improved up to 10,000 times compared with silicon equivalents at power consumption levels “orders of magnitude” lower, and moreover, certain model workloads have been measured running 100 times faster compared with state-of-the-art electronic chips.</p><p> 光子集成电路是LightOn芯片的基础，与电子同类产品相比，具有许多优势。它们只需要有限的能量，因为光产生的热量比电少，并且不易受到环境温度，电磁场和其他噪声变化的影响。与硅等效产品相比，光子设计中的延迟提高了10,000倍，而功耗水平却降低了“几个数量级”，此外，某些模型的工作负载运行速度比最新的电子芯片快了100倍。</p><p> But it’s worth noting that LightOn’s hardware isn’t immune to the limitations of optical processing. Speedy photonic circuits require speedy memory, and then there’s the matter of packaging every component — including lasers, modulators, and optical combiners — on a tiny chip wafer. Plus, questions remain about what kinds of nonlinear operations, the basic building blocks of models that enable them to make predictions, can be executed in the optical domain.</p><p> 但是，值得注意的是，LightOn的硬件无法不受光学处理的限制。快速的光子电路需要快速的存储，然后是将每个组件（包括激光器，调制器和光学组合器）封装在一个微小的芯片晶圆上的问题。此外，关于在光学领域可以执行哪种类型的非线性运算（使它们能够进行预测的模型的基本构建模块）的问题仍然存在。 </p><p>  Another, not necessarily mutually exclusive answer to the backpropagation problem involves splitting neural networks into smaller, more manageable pieces. In an anonymously coauthored  study, researchers propose divvying up models into subnetworks called neighborhoods that are then trained independently, which comes with the benefits of parallelism and speedy training.</p><p>反向传播问题的另一个，不一定是互斥的答案涉及将神经网络拆分为更小，更易于管理的部分。在一项匿名合着的研究中，研究人员建议将模型划分为称为邻域的子网，然后对其进行独立训练，这具有并行性和快速训练的优势。</p><p> For their part, researchers at the University of Maryland’s Department of Computer Science  pretrained subnetworks independently before training the entire network. They also employed an attention mechanism between the subnetworks to help identify the most important modality (visual, acoustic, or textual) during ambiguous scenarios, which boosted performance. In this context, “attention” refers to a method that identifies which parts of an input sequence — for example, words — are relevant to each output.</p><p> 马里兰大学计算机科学系的研究人员则在训练整个网络之前，对子网进行了独立的预训练。他们还利用子网之间的注意力机制来帮助在模棱两可的场景中识别最重要的模式（视觉，听觉或文本），从而提高性能。在本文中，“注意力”是指一种识别输入序列的哪些部分（例如单词）与每个输出相关的方法。</p><p> The University of Maryland researchers say that their approach enables a simple network to achieve performance similar to a complicated architecture. Moreover, they say that it results in significantly reduced training time with tasks like sentiment analysis, emotion recognition, and speaker trait recognition.</p><p> 马里兰大学的研究人员说，他们的方法可以使简单的网络实现与复杂架构相似的性能。而且，他们说，这样做可以大大减少训练时间，例如情感分析，情感识别和说话人特质识别。</p><p>  In 2017, Geoffrey Hinton, a researcher at the University of Toronto and Google’s AI research division and a  winner of the Association for Computing Machinery’s Turing Award, told  Axios in an interview that he was “deeply suspicious” of deep learning. “My view is throw it all away and start again,” he said. “I don’t think that’s how the brain works.”</p><p>  2017年，多伦多大学和Google的AI研究部门研究员，计算机学会图灵奖得主Geoffrey Hinton在接受采访时对Axios表示，他对深度学习“深表怀疑”。他说：“我的观点是将其全部抛弃，然后重新开始。” “我不认为大脑是这样运作的。”</p><p> Hinton was referring to the fact that with backpropagation, a model must be “told” when it makes an error, meaning it’s “supervised” in the sense that it doesn’t learn to classify patterns on its own. He and others believe that unsupervised or self-supervised learning, where models look for patterns in a dataset without preexisting labels, are a necessary step toward more powerful AI techniques.</p><p> Hinton提到这样一个事实：通过反向传播，模型在出错时必须“告知”，这意味着它是“受监督的”，因为它无法学会自己对模式进行分类。他和其他人认为，无监督学习或自我监督学习是朝着更强大的AI技术迈出的必经之路，其中模型在没有预先存在标签的情况下在数据集中寻找模式。</p><p> But this aside, backpropagation’s fundamental limitations continue to motivate the research community to seek out replacements. It’s early days, but if these early attempts pan out, the efficiency gains could broaden the accessibility of AI and machine learning among both practitioners and the enterprise.</p><p> 除此之外，反向传播的基本局限性继续激励研究团体寻找替代品。现在还处于初期，但是如果这些早期尝试成功了，效率的提高可能会扩大从业者和企业之间对AI和机器学习的访问范围。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://venturebeat.com/2020/12/16/at-neurips-2020-researchers-proposed-faster-more-efficient-alternatives-to-backpropagation/">https://venturebeat.com/2020/12/16/at-neurips-2020-researchers-proposed-faster-more-efficient-alternatives-to-backpropagation/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/人员/">#人员</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/propose/">#propose</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/训练/">#训练</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>