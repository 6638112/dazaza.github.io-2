<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么mmap比系统调用快(2019年)</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">为什么mmap比系统调用快(2019年)</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-10-21 06:20:41</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/10/7ddb46ee61d1815a1e2d289720055449.png"><img src="http://img2.diglog.com/img/2020/10/7ddb46ee61d1815a1e2d289720055449.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>When I ask my colleagues why mmap is faster than system calls, the answer is inevitably “system call overhead”: the cost of crossing the boundary between the user space and the kernel. It turns out that this overhead is more nuanced than I used to think, so let’s look under the hood to understand the performance differences.</p><p>当我问我的同事为什么mmap比系统调用快时，他们的回答不可避免地是“系统调用开销”：跨越用户空间和内核之间边界的开销。事实证明，这种开销比我过去认为的更细微，所以让我们来看看幕后的情况，以了解性能差异。</p><p>  System calls. A system call is a spe cial function that lets you cross protection domains. When a program executes in user mode (an unprivileged protection domain) it is not allowed to do things that are permitted for the code executing in the kernel mode (a privileged protection domain). For example, a program running in user space typically cannot read files without help from the kernel. When a user program asks a service from an operating system, the system protects itself from malicious or buggy programs via  system calls. A system call executes a special hardware instruction, often called “trap”, that transfers control into the kernel. Then the kernel can decide whether it will honour the request.</p><p>系统调用。系统调用是一种特殊的函数，它允许您跨越保护域。当程序在用户模式(非特权保护域)下执行时，不允许执行在内核模式(特权保护域)下执行的代码所允许的事情。例如，在用户空间中运行的程序通常在没有内核帮助的情况下无法读取文件。当用户程序从操作系统请求服务时，系统会通过系统调用保护自己不受恶意程序或错误程序的影响。系统调用执行特殊的硬件指令，通常称为“陷阱”，将控制转移到内核。然后内核可以决定是否接受该请求。</p><p> While this protection is super useful, it has a cost. When we cross from user space into the kernel, we have to save the hardware registers, because the kernel might need to use them. Further, since it is unsafe to directly dereference user-level pointers (what if they are null — that’ll crash the kernel!) the data referred to by these pointers must be copied into the kernel.</p><p>虽然这种保护非常有用，但也是有代价的。当我们从用户空间进入内核时，我们必须保存硬件寄存器，因为内核可能需要使用它们。此外，由于直接取消引用用户级指针是不安全的(如果它们为空怎么办-这会使内核崩溃！)。必须将这些指针引用的数据复制到内核中。</p><p> When we return from the system call, we have to repeat the sequence in the reverse order: copy out any data that the user requested (because we can’t just give user programs pointers into kernel memory), restore the registers and jump to user mode.</p><p>当我们从系统调用返回时，我们必须以相反的顺序重复该序列：复制出用户请求的任何数据(因为我们不能只给用户程序指向内核内存的指针)，恢复寄存器并跳转到用户模式。</p><p> Page faults. The operating system and the hardware together translate the addresses that are written down in your program’s executable (these are called  virtual addresses) to the addresses in the actual physical memory ( physical addresses). It would be pretty inconvenient for the compiler to generate physical addresses directly, because it doesn’t know on what machine you might run your program, how much memory it has and what other programs might be using physical memory at the time your program runs. Hence the need for this virtual-to-physical address translation. The translations, or mappings, are set up in your program’s page table. When your program begins to run, none of these mappings are set up. So when your program tries to access a virtual address, it generates a  page fault, which signals the kernel to go set up the mapping. The kernel is notified that it needs to handle a page fault via a trap, so in this way it is a bit similar to a system call. The difference is that the system call is explicit and the page fault is implicit.</p><p>页面错误。操作系统和硬件一起将写在程序可执行文件中的地址(称为虚拟地址)转换为实际物理内存中的地址(物理地址)。编译器直接生成物理地址非常不方便，因为它不知道您可能在哪台机器上运行您的程序，它有多少内存，以及在您的程序运行时，哪些其他程序可能正在使用物理内存。因此需要这种虚拟到物理地址转换。翻译或映射设置在程序的页表中。当您的程序开始运行时，不会设置任何这些映射。因此，当您的程序试图访问虚拟地址时，它会生成一个页面错误，通知内核去设置映射。内核被通知需要通过陷阱处理页面错误，因此在这种情况下，它有点类似于系统调用。不同之处在于系统调用是显式的，而页面错误是隐式的。</p><p> Buffer cache. Buffer cache is a part of kernel memory that is used to keep recently accessed chunks of files (these chunks are called blocks or pages). When a user program requests to read a file, the page from the file is (usually) first put into the buffer cache. Then the data is copied from the buffer cache out to the user-supplied buffer during the return from the system call.</p><p>缓冲区缓存。缓冲区缓存是内核内存的一部分，用于保存最近访问的文件块(这些块称为块或页)。当用户程序请求读取文件时，(通常)首先将文件中的页面放入缓冲区缓存。然后，在从系统调用返回期间，将数据从缓冲区高速缓存复制到用户提供的缓冲区。</p><p> Mmap. Mmap stands for  memory-mapped files. It is a way to read and write files without invoking system calls. The operating system reserves a chunk of a program virtual addresses to “map” directly to a chunk in a file. So if the program reads the data from that part of the address space, it will obtain the data that resides in the corresponding part of the file. If that part of the file happens to reside in the buffer cache, the virtual addresses of the mapped chunk will simply be mapped to the physical addresses of the corresponding buffer cache pages upon the first access, and no system calls or other traps will be invoked later on. If the file data is not in the buffer cache, accessing the mapped area will generate a page fault, prompting the kernel to go fetch the corresponding data from disk.</p><p>Mmap。Mmap代表内存映射文件。它是一种在不调用系统调用的情况下读写文件的方法。操作系统保留程序虚拟地址的块以直接“映射”到文件中的块。因此，如果程序从地址空间的该部分读取数据，它将获得驻留在文件的相应部分中的数据。如果文件的该部分恰好驻留在缓冲区高速缓存中，则在第一次访问时，映射区块的虚拟地址将简单地映射到相应缓冲区高速缓存页的物理地址，并且稍后不会调用任何系统调用或其他陷阱。如果文件数据不在缓冲区缓存中，访问映射区域将生成页面错误，提示内核从磁盘获取相应的数据。</p><p>  Let us begin by formulating the hypothesis. Why do we expect mmap to be faster? There are two obvious reasons. First, it requires no explicit crossing of protection domains, though there is still  implicit crossing when we have page faults. That said, if a given range in the file is accessed more than once, chances are we won’t incur page faults after the first access. That, however, did not occur in my experiments, so I did expect to hit a page fault every time I read a new block of the file.</p><p>让我们从阐述假设开始。为什么我们期望mmap更快呢？有两个显而易见的原因。首先，它不需要显式地跨越保护域，尽管当我们有页面错误时仍然存在隐式跨越。也就是说，如果文件中的给定范围被多次访问，那么在第一次访问之后，我们很可能不会出现页面错误。然而，在我的实验中没有出现这种情况，所以我确实预料到每次读取文件的新块时都会遇到页面错误。</p><p> Second, if the application is written such that it can access the data directly in the mapped region, we don’t have to perform a memory copy. In my experiments, though, I was interested in measuring the scenario where the application has the separate target buffer for the data it reads. So even though the file is mmapped, the application will still copy the data from the mapped area into the target buffer.</p><p>其次，如果编写的应用程序可以直接访问映射区域中的数据，我们就不必执行内存复制。不过，在我的实验中，我感兴趣的是测量应用程序为其读取的数据具有单独的目标缓冲区的场景。因此，即使文件被mmmap，应用程序仍然会将数据从映射区域复制到目标缓冲区。</p><p> Therefore, in my experimental environment, I expected mmap to be slightly faster than system calls, because I thought the code for handling page faults would be a bit more streamlined than that for system calls.</p><p>因此，在我的实验环境中，我预计mmap会比系统调用稍微快一些，因为我认为处理页面错误的代码会比系统调用的代码更精简一些。</p><p>  I set up my experiment in the following way. I create a 4GB file and then read it either   sequentially or   randomly using a block size of   4KB,   8KB or   16KB. I read the file using either a read   system call or   mmap. In the case of mmap, the data is copied from the mapped area into a separate “destination” buffer. I run these tests using either a   cold buffer cache, meaning that the file is not cached there, or a   warm buffer cache, meaning that the file is there in kernel memory. The storage medium is an SSD that you might expect to find in a typical server. All reads are performed using a single thread. The source code of my benchmark  here.</p><p>我用下面的方式设置了我的实验。我创建了一个4 GB的文件，然后使用4KB、8KB或16KB的块大小顺序或随机读取它。我使用read系统调用或mmap读取文件。在mmap的情况下，数据从映射区域复制到单独的“目标”缓冲区。我使用冷缓冲区缓存(意味着文件不在那里缓存)或热缓冲区缓存(意味着文件在内核内存中)来运行这些测试。存储介质是您可能期望在典型服务器中找到的SSD。所有读取都使用单个线程执行。这里是我的基准测试的源代码。</p><p>  The following charts show the throughput of the read benchmark for the  sequential/warm,  sequential/cold,  random/warm and  random/cold runs.</p><p>下图显示了顺序/暖、顺序/冷、随机/暖和随机/冷运行的读取基准的吞吐量。</p><p>     Barring few exceptions, mmap is  2–6 times faster than system calls. Let’s analyze what happens in the  warm experiments, since there mmap provides a more consistent improvement.</p><p>除了少数例外，mmap比系统调用快2-6倍。让我们分析一下热实验中发生了什么，因为mmap提供了更一致的改进。</p><p>  The following figure shows the CPU profile collected during the  sequential/warm syscall experiment with 16KB block size. During this experiment the CPU utilization is 100%, so the CPU profile tells us the whole story.</p><p>下图显示了在16KB块大小的顺序/暖SysCall实验期间收集的CPU配置文件。在此实验期间，CPU利用率为100%，因此CPU配置文件可以告诉我们全部情况。</p><p>  We see that ~60% of the time is spent in copy_user_enhanced_fast_string — a function that copies data out to user space. About 15% is spent on other work that occurs on crossing the system call boundary (functions do_syscall_64, entry_SYSCALL_64 and syscall_return_via_sysret), and about 6% in functions that find the data in the buffer cache (find_get_entry and generic_file_buffered_read).</p><p>我们看到大约60%的时间花在COPY_USER_ENHANDIZED_FAST_STRING上-这是一个将数据复制到用户空间的函数。大约15%用于跨越系统调用边界的其他工作(函数do_syscall_64、entry_syscall_64和syscall_return_via_sysret)，大约6%用于在缓冲区缓存中查找数据的函数(find_get_entry和Generic_file_Buffered_read)。</p><p> Now let’s look at what happens during the mmap test with the same parameters:</p><p>现在，让我们看一下在使用相同参数的mmap测试期间发生了什么：</p><p>  This profile is vastly different. About 60% of the time is spent in __memmove_avx_unaligned_erms, and a bunch of time in various functions that set up page mappings.</p><p>这份个人资料有很大的不同。大约60%的时间花在__memmove_avx_unaligned_erms上，还有大量时间花在设置页面映射的各种函数上。</p><p> We will come back to __memmove_avx_unaligned_erms in a moment, but for the time being let’s try to figure out exactly how much time is spent mapping pages. I have a neat trick up my sleeve to do that. On Linux, the mmap system call can accept a MAP_POPULATE flag. What this flag does is it  forces mmap to pre-populate all the page mappings during the actual system call, so none of the page-mapping work would be done when my test actually runs. So I changed my test to invoke mmap with MAP_POPULATE and learned that the experiment completes about 36% faster. (I only measure the timing of the main loop, and not that of the mmap system call). Therefore, I assume that in the above profile all those mapping functions take up about 36%.</p><p>我们稍后将返回到__memmove_avx_unaligned_erms，但现在让我们试着计算一下映射页面所花费的确切时间。我有一个巧妙的窍门来做那件事。在Linux上，mmap系统调用可以接受MAP_PUPULATE标志。此标志的作用是强制mmap在实际系统调用期间预先填充所有页面映射，因此在测试实际运行时不会执行任何页面映射工作。因此，我将测试更改为使用MAP_PUPULATE调用mmap，并了解到实验完成速度提高了大约36%。(我只测量主循环的计时，而不测量mmap系统调用的计时)。因此，我假设在上面的配置文件中，所有这些映射函数约占36%。</p><p> Let’s summarize what we have so far. Using the CPU profile we were able to explain about 82% of the execution time for the syscall experiment (60% spent on copyout, 15% on other domain crossing operations, and 8% on actually reading the file from the buffer cache), and about 96% for the mmap experiment: 60% was spent in user-level memory copy and about 36% in mapping pages.</p><p>让我们总结一下到目前为止我们所掌握的内容。使用CPU配置文件，我们能够解释syscall实验大约82%的执行时间(60%用于复制，15%用于其他跨域操作，8%用于从缓冲区缓存实际读取文件)，mmap实验大约96%：60%用于用户级内存复制，大约36%用于映射页面。</p><p> I also have to tell you that if I tweak the experiment to run for a very long time, accessing the same file in a loop over and over again, the mmap experiment is completely dominated by the memory copy:</p><p>我还必须告诉您，如果我调整实验运行很长时间，一遍又一遍地在循环中访问同一个文件，mmap实验完全由内存副本控制：</p><p>  The profile of the syscall experiment, if I run it much longer, stays largely the same:</p><p>如果我运行更长时间，Syscall实验的概况基本保持不变：</p><p>  Here is where things get very interesting. A huge portion of time — at least 60% — is spent in copying data. However, the functions used for syscall and mmap are very different, and not only in the name.</p><p>这就是事情变得非常有趣的地方。有很大一部分时间-至少60%-花费在复制数据上。但是，syscall和mmap使用的函数非常不同，不仅在名称上如此。</p><p> __memmove_avx_unaligned_erms, called in the mmap experiment, is implemented using Advanced Vector Extensions (AVX) (here is the source code of the  functions that it relies on). The  implementation of copy_user_enhanced_fast_string, on the other hand, is much more modest. That, in my opinion, is the huge reason why mmap is faster.   Using wide vector instructions for data copying effectively utilizes the memory bandwidth, and combined with CPU pre-fetching makes mmap really really fast.</p><p>__memmove_AVX_UNALIGNED_ERMS在mmap实验中称为__memmove_avx_unaligned_erms，它是使用高级矢量扩展(AVX)实现的(这里是它所依赖的函数的源代码)。另一方面，Copy_User_Enhanced_FAST_String的实现要简单得多。在我看来，这是mmap速度更快的重要原因。使用宽向量指令进行数据复制有效地利用了内存带宽，并与CPU预取相结合，使mmap变得非常快。</p><p> Why can’t the kernel implementation use AVX? Well, if it did, then it would have to save and restore those registers on each system call, and that would make domain crossing even more expensive. So this was a  conscious decision in the Linux kernel.</p><p>为什么内核实现不能使用AVX？如果它这样做了，那么它将不得不在每次系统调用时保存和恢复这些寄存器，这将使跨域变得更加昂贵。所以在Linux内核中这是一个有意识的决定。</p><p> In the meantime, converting your application to use mmap rather than system calls could make it run faster. That said, mmap is not always convenient to program with, but that’s a subject for another post…</p><p>同时，将应用程序转换为使用mmap而不是系统调用可以使其运行得更快。也就是说，使用mmap编程并不总是很方便，但这是另一篇…文章的主题</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://medium.com/@sasha_f/why-mmap-is-faster-than-system-calls-24718e75ab37">https://medium.com/@sasha_f/why-mmap-is-faster-than-system-calls-24718e75ab37</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/系统/">#系统</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/faster/">#faster</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/mmap/">#mmap</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>