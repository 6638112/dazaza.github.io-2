<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Linux负载平均：解决神秘 Linux Load Averages: Solving the Mystery</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Linux Load Averages: Solving the Mystery<br/>Linux负载平均：解决神秘 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-11 17:31:56</div><div class="page_narrow text-break page_content"><p>Load averages are an industry-critical metric – my company spends millions auto-scaling cloud instances based on them and other metrics – but on Linux there&#39;s some mystery around them. Linux load averages track not just runnable tasks, but also tasks in the uninterruptible sleep state. Why? I&#39;ve never seen an explanation. In this post I&#39;ll solve this mystery, and summarize load averages as a reference for everyone trying to interpret them.</p><p>负载平均值是一个行业关键的公制 - 我的公司基于它们和其他指标花费数百万自动缩放的云实例 - 但是在Linux上有一些幽灵。 Linux加载平均轨道不仅仅是可运行的任务，还可以在不间断睡眠状态下任务。为什么？我从未见过解释。在这篇文章中，我＆＃39; ll解决了这个神秘，并总结了负载平均值作为每个尝试解释它们的参考。</p><p> Linux load averages are &#34;system load averages&#34; that show the running thread (task) demand on the system as an average number of running plus waiting threads. This measures demand, which can be greater than what the system is currently processing. Most tools show three averages, for 1, 5, and 15 minutes:</p><p> Linux负载平均值是＆＃34;系统负载平均值＆＃34;将系统上的运行线程（任务）的需求显示为平均运行加等待线程的平均数量。这种措施需求，这可以大于系统目前正在处理的需求。大多数工具显示三个平均值，1,5和15分钟：</p><p> $ uptime 16:48:24 up 4:11, 1 user, load average:  25.25, 23.40, 23.46top - 16:48:42 up 4:12, 1 user, load average:  25.25, 23.14, 23.37$ cat /proc/loadavg  25.72 23.19 23.35 42/3411 43603</p><p> $正常运行时间16:48:24 up 4:11，1个用户，装载平均：25.25,23.40,23.46top  -  16:48:42起4:12，1个用户，载重平均：25.25,23.14,23.37 $ cat / proc / loadavg 25.72 23.19 23.35 42/3411 43603</p><p>  If the 1 minute average is higher than the 5 or 15 minute averages, then load is increasing.</p><p>  如果1分钟平均值高于5或15分钟平均值，则负载越来越大。</p><p> If the 1 minute average is lower than the 5 or 15 minute averages, then load is decreasing.</p><p> 如果1分钟平均值低于5或15分钟平均值，则负载降低。</p><p> If they are higher than your CPU count, then you might have a performance problem (it depends).</p><p> 如果它们高于CPU计数，那么您可能有性能问题（它取决于）。</p><p> As a set of three, you can tell if load is increasing or decreasing, which is useful. They can be also useful when a single value of demand is desired, such as for a cloud auto scaling rule. But to understand them in more detail is difficult without the aid of other metrics. A single value of 23 - 25, by itself, doesn&#39;t mean anything, but might mean something if the CPU count is known, and if it&#39;s known to be a CPU-bound workload.</p><p> 作为一组三个，您可以判断负载是否增加或减少，这是有用的。当需要单一的需求时，它们也可以是有用的，例如用于云自动缩放规则。但是在没有其他指标的帮助下，更难以了解他们更加困难。单一值为23  -  25，本身，不均意味着什么，但如果已知CPU计数，并且如果它被称为CPU绑定工作负载，则可能意味着什么。 </p><p> Instead of trying to debug load averages, I usually switch to other metrics. I&#39;ll discuss these in the &#34;Better Metrics&#34; section near the end.</p><p>我通常会切换到其他度量标准，而不是尝试调试加载平均值。我＆＃39; ll在＆＃34中讨论这些;更好的指标＆＃34;截面附近。</p><p>  The original load averages show only CPU demand: the number of processes running plus those waiting to run. There&#39;s a nice description of this in  RFC 546 titled &#34;TENEX Load Averages&#34;, August 1973:</p><p>  原始负载平均仅显示CPU需求：运行加上等待运行的进程数。在RFC 546中标题为＆＃34; Tenex Load平均值和第34次;，1973年8月：</p><p> [1] The TENEX load average is a measure of CPU demand. The load average is an average of the number of runnable processes over a given time period. For example, an hourly load average of 10 would mean that (for a single CPU system) at any time during that hour one could expect to see 1 process running and 9 others ready to run (i.e., not blocked for I/O) waiting for the CPU.</p><p> [1] Tenex负载平均值是CPU需求的衡量标准。负载平均值是给定时间段内可抵押过程的数量的平均值。例如，每小时的负载平均值为10将是（对于单个CPU系统），在那个时刻的任何时间都可以期望看到1个进程运行，9个准备运行的其他（即，没有阻止I / O阻塞）等待对于CPU。</p><p> The version of this on  ietf.org links to a PDF scan of a hand drawn load average graph from July 1973, showing that this has been monitored for decades:</p><p> IETF.ORG上的此版本与1973年7月的手绘加载平均图的PDF扫描链接，表明这已被监控数十年：</p><p>  Nowadays, the source code to old operating systems can also be found online. Here&#39;s an except of DEC macro assembly from  TENEX (early 1970&#39;s) SCHED.MAC:</p><p>  如今，也可以在线找到旧操作系统的源代码。在这里＆＃39;除了德克斯（1970年初＆＃39; s）schedmac：</p><p> NRJAVS==3 ;NUMBER OF LOAD AVERAGES WE MAINTAINGS RJAV,NRJAVS ;EXPONENTIAL AVERAGES OF NUMBER OF ACTIVE PROCESSES[...];UPDATE RUNNABLE JOB AVERAGESDORJAV: MOVEI 2,^D5000 MOVEM 2,RJATIM ;SET TIME OF NEXT UPDATE MOVE 4,RJTSUM ;CURRENT INTEGRAL OF NBPROC+NGPROC SUBM 4,RJAVS1 ;DIFFERENCE FROM LAST UPDATE EXCH 4,RJAVS1 FSC 4,233 ;FLOAT IT FDVR 4,[5000.0] ;AVERAGE OVER LAST 5000 MS[...];TABLE OF EXP(-T/C) FOR T = 5 SEC.EXPFF: EXP 0.920043902 ;C = 1 MIN EXP 0.983471344 ;C = 5 MIN EXP 0.994459811 ;C = 15 MIN</p><p> nrjavs == 3;负载次数我们维护rjav，nrjavs;活动过程数量的指数平均值;更新runnable作业veragesdorjav：movi 2，^ d5000 movem 2，rjatim;下次更新的设置时间4 ，rjtsum; nbproc + ngproc提交4，rjavs1的当前积分;与上次更新Exch 4的差异，rjavs1 fsc 4,233;漂浮它fdvr 4，[5000.0];平均持续5000 ms; exp（ - T / C）对于T = 5秒。Expff：Exp 0.920043902; C = 1 min Exp 0.983471344; C = 5 min Exp 0.994459811; C = 15分钟</p><p>  #define EXP_1 1884 /* 1/exp(5sec/1min) as fixed-point */#define EXP_5 2014 /* 1/exp(5sec/5min) */#define EXP_15 2037 /* 1/exp(5sec/15min) */</p><p>  #define exp_1 1884 / * 1 / Exp（5sec / 1min）作为定点* /＃定义EXP_5 2014 / * 1 / EXP（5SEC / 5MIN）* /＃定义EXP_15 2037 / * 1 / EXP（5SEC / 15min） * / </p><p>  There have been similar load average metrics in older systems, including  Multics, which had an exponential scheduling queue average.</p><p>旧系统中存在类似的负载平均度量，包括多个数据，其具有指数调度队列平均值。</p><p>  These three numbers are the 1, 5, and 15 minute load averages. Except they aren&#39;t really averages, and they aren&#39;t 1, 5, and 15 minutes. As can be seen in the source above, 1, 5, and 15 minutes are constants used in an equation, which calculate exponentially-damped moving sums of a five second average. The resulting 1, 5, and 15 minute load averages reflect load well beyond 1, 5, and 15 minutes.</p><p>  这三个数字是1,5和15分钟的负载平均值。除了他们aren＆＃39; t真正的平均值，他们aren＆＃39; t 1,5和15分钟。如上所述，在上述源中可以看出，1,5和15分钟是在等式中使用的常数，其计算五秒平均值的指数衰减的移动和。由此产生的1,5和15分钟的载荷平均值反射超过1,5和15分钟的负荷。</p><p> If you take an idle system, then begin a single-threaded CPU-bound workload (one thread in a loop), what would the one minute load average be after 60 seconds? If it was a plain average, it would be 1.0. Here is that experiment, graphed:</p><p> 如果您拍摄了空闲系统，则开始一个线程的CPU绑定工作负载（一个循环中的一个线程），60秒后，一分钟的载荷平均会有多少？如果是平均值，那将是1.0。以下是实验，绘制：</p><p>  The so-called &#34;one minute average&#34; only reaches about 0.62 by the one minute mark. For more on the equation and similar experiments, Dr. Neil Gunther has written an article on load averages:  How It Works, plus there are many Linux source block comments in  loadavg.c.</p><p>  所谓的＆＃34;一分钟平均值＆＃34;只有一个分钟标记才达到约0.62。有关更多关于方程和类似的实验，尼尔·冈特尔博士已经写了一篇关于加载平均值的文章：如何运作，加上Loadavg.c中有许多Linux源块注释。</p><p>  When load averages first appeared in Linux, they reflected CPU demand, as with other operating systems. But later on Linux changed them to include not only runnable tasks, but also tasks in the uninterruptible state (TASK_UNINTERRUPTIBLE or nr_uninterruptible). This state is used by code paths that want to avoid interruptions by signals, which includes tasks blocked on disk I/O and some locks. You may have seen this state before: it shows up as the &#34;D&#34; state in the output  ps and  top. The ps(1) man page calls it &#34;uninterruptible sleep (usually IO)&#34;.</p><p>  当加载平均在Linux中出现时，它们反映了CPU需求，如其他操作系统。但后来在Linux上改变它们不仅包括可运行的任务，还包括不间断状态的任务（Task_UnintRuptible或NR_UNINTRUPTIBLE）。该状态用于避免信号中断的代码路径使用，该信号包括在磁盘I / O和一些锁上阻止的任务。你可能之前可能看到这个状态：它显示为＆＃34; D＆＃34;输出PS和顶部的状态。 PS（1）手册页称它＆＃34;不间断睡眠（通常是IO）＆＃34;</p><p> Adding the uninterruptible state means that Linux load averages can increase due to a disk (or NFS) I/O workload, not just CPU demand. For everyone familiar with other operating systems and their CPU load averages, including this state is at first deeply confusing.</p><p> 添加不间断状态意味着由于磁盘（或NFS）I / O工作负载，Linux负载平均可能会增加，而不仅仅是CPU需求。对于熟悉其他操作系统及其CPU负载平均值的每个人，包括此状态乍一看都很困惑。</p><p>  There are countless articles on load averages, many of which point out the Linux nr_uninterruptible gotcha. But I&#39;ve seen none that explain or even hazard a guess as to why it&#39;s included. My own guess would have been that it&#39;s meant to reflect demand in a more general sense, rather than just CPU demand.</p><p>  负载平均值有无数的文章，其中许多指出了Linux NR_UNINTRUPTIBLE GOTCHA。但是，我看到没有那么解释甚至危害它为什么猜测它＆＃39;包括为什么。我自己的猜测是它的意思是反映更一般意义的需求，而不是CPU需求。 </p><p>  Understanding why something changed in Linux is easy: you read the git commit history on the file in question and read the change description. I checked the history on  loadavg.c, but the change that added the uninterruptible state predates that file, which was created with code from an earlier file. I checked the other file, but that trail ran cold as well: the code itself has hopped around different files. Hoping to take a shortcut, I dumped &#34;git log -p&#34; for the entire Linux github repository, which was 4 Gbytes of text, and began reading it backwards to see when the code first appeared. This, too, was a dead end. The oldest change in the entire Linux repo dates back to 2005, when Linus imported Linux 2.6.12-rc2, and this change predates that.</p><p>了解为什么Linux中改变的事情很容易：您在问题上读取了在文件中的Git提交历史记录，并阅读了更改说明。我检查了loadavg.c上的历史记录，但添加了不间断状态的更改会在较早文件中使用代码创建的文件。我检查了另一个文件，但这也是冷的：代码本身跳过不同的文件。希望拍摄快捷方式，我倾倒＆＃34; git log -p＆＃34;对于整个Linux Github存储库，这是4 GB的文本，并开始向后读取它，看看代码何时出现。这也是一个死胡同。当Linus导入Linux 2.6.12-RC2时，整个Linux Repo的最旧的更改返回到2005年，此更改会达到。</p><p> There are historical Linux repos ( here and  here), but this change description is missing from those as well. Trying to discover, at least, when this change occurred, I searched tarballs on  kernel.org and found that it had changed by 0.99.15, and not by 0.99.13 – however, the tarball for 0.99.14 was missing. I found it elsewhere, and confirmed that the change was in Linux 0.99 patchlevel 14, Nov 1993. I was hoping that the release description for 0.99.14 by Linus would explain the change, but  that too, was a dead end:</p><p> 有历史Linux Repos（这里和此处），但也缺少此更改描述。至少试图发现这种变化发生时，我在kernel.org上搜索了tarballs，发现它已经改变了0.99.15，而不是0.99.13  - 但是，0.99.14的Tarball缺失。我在其他地方找到了它，并确认了改变在Linux 0.99 PatchLevel 1993年11月14日。我希望Linus的0.99.14释放描述将解释变化，但也是一个死胡同：</p><p> &#34;Changes to the last official release (p13) are too numerous to mention (or even to remember)...&#34; – Linus</p><p> ＆＃34;上次官方发布的变更（p13）太多了，以提及（甚至要记住）...＆＃34; -  Linus.</p><p>  Based on the date, I looked up the  kernel mailing list archives to find the actual patch, but the oldest email available is from June 1995, when the sysadmin writes:</p><p>  基于日期，我查询内核邮件列表档案，以查找实际补丁，但最古老的电子邮件是从1995年6月开始的，当时Sysadmin写道：</p><p> &#34;While working on a system to make these mailing archives scale moreeffecitvely I accidently destroyed the current set of archives (ahwhoops).&#34;</p><p> ＆＃34;在一个系统上工作，使这些邮寄档案规模MoreeffecityIvely我意外地摧毁了当前的档案（Ahwhoops）。＆＃34;</p><p> My search was starting to feel cursed. Thankfully, I found some older linux-devel mailing list archives, rescued from server backups, often stored as tarballs of digests. I searched over 6,000 digests containing over 98,000 emails, 30,000 of which were from 1993. But it was somehow missing from all of them. It really looked as if the original patch description might be lost forever, and the &#34;why&#34; would remain a mystery.</p><p> 我的搜索开始被诅咒。值得庆幸的是，我发现了一些旧的Linux-devel邮件列表存档，从服务器备份中救出，通常存储为摘要的tarball。我搜索了超过6,000多家摘要，其中包含超过98,000封电子邮件，其中30,000人来自1993年。但是，在某些人中失踪了。它真的看起来好像原始补丁描述可能永远丢失，而＃34;为什么＆＃34;仍然是一个谜。</p><p>  Fortunately, I did finally find the change, in a compressed mailbox file from 1993 on  oldlinux.org. Here it is:</p><p>  幸运的是，我终于在1993年在Oldlinux.org中找到了一个压缩的邮箱文件中的更改。这里是： </p><p> From: Matthias Urlichs &lt;urlichs@smurf.sub.org&gt;Subject: Load average broken ?Date: Fri, 29 Oct 1993 11:37:23 +0200 The kernel only counts &#34;runnable&#34; processes when computing the load average.I don&#39;t like that; the problem is that processes which are swapping orwaiting on &#34;fast&#34;, i.e. noninterruptible, I/O, also consume resources.It seems somewhat nonintuitive that the load average goes down when youreplace your fast swap disk with a slow swap disk...Anyway, the following patch seems to make the load average much moreconsistent WRT the subjective speed of the system. And, most important, theload is still zero when nobody is doing anything. ;-)--- kernel/sched.c.orig Fri Oct 29 10:31:11 1993+++ kernel/sched.c Fri Oct 29 10:32:51 1993@@ -414,7 +414,9 @@ unsigned long nr = 0; for(p = &amp;LAST_TASK; p &gt; &amp;FIRST_TASK; --p)- if (*p &amp;&amp; (*p)-&gt;state == TASK_RUNNING)+ if (*p &amp;&amp; ((*p)-&gt;state == TASK_RUNNING) ||+ (*p)-&gt;state == TASK_UNINTERRUPTIBLE) ||+ (*p)-&gt;state == TASK_SWAPPING)) nr += FIXED_1; return nr; }--Matthias Urlichs \ XLink-POP N|rnberg | EMail: urlichs@smurf.sub.orgSchleiermacherstra_e 12 \ Unix+Linux+Mac | Phone: ...please use email.90491 N|rnberg (Germany) \ Consulting+Networking+Programming+etc&#39;ing 42</p><p>来自：Matthias Urlichs＆lt; urlichs@smurf.sub.org& gt;主题：负载平均破碎？日期：星期五，1993年10月1993年10月11:37:23 +0200只有核心＆＃34; runnable＆＃34;计算加载均值时的过程。我不喜欢那样;问题是在＆＃34上交换或＃34的进程;快速＆＃34;即不间断，I / O，也消耗资源。当您使用缓慢的交换磁盘时，负载平均值下降似乎有些不合适磁盘......无论如何，以下补丁似乎使负载平均更加孤立的WRT系统的主观速度。而且，最重要的是，当没有人在做任何事情时，更为重要的是零。 ;  - ）--- kernel / sched.c.orig fri fri 29 10:31:11 1993 +++ kernel / sched.c fri fri 2019年10月10:32:51 1993 @@ -414,7 +414,9 @ @ unsigned long nr = 0; for（p =＆amp; last_task; p＆gt;＆amp; first_task;  -  p） -  if（* p＆amp;＆amp;（* p） - ＆gt;状态== task_running）+如果（* p＆amp;＆amp;（ （* p） - ＆gt; state == task_running）|| +（* p） - ＆gt; state == task_uninteruptible）|| +（* p） - ＆gt; state == task_swappe））nr + = fixed_1;返回nr; }  -  Matthias Urlichs \ XLink-Pop n | rnberg |电子邮件：urlichs@smurf.sub.orgschleiermacherstra_e 12 \ Unix + Linux + Mac |电话：...请使用电子邮件.90491 n | rnberg（德国）\咨询+网络+编程+等等＆＃39; ing 42</p><p>  This confirms that the load averages were deliberately changed to reflect demand for other system resources, not just CPUs. Linux changed from &#34;CPU load averages&#34; to what one might call &#34;system load averages&#34;.</p><p>  这证实了负载平均值被故意改变以反映对其他系统资源的需求，而不仅仅是CPU。 Linux从＆＃34改变; CPU负载平均值＆＃34;到一个人可能会呼叫＆＃34;系统负载平均值＆＃34;</p><p> His example of using a slower swap disk makes sense: by degrading the system&#39;s performance, the demand on the system (measured as running + queued) should increase. However, load averages decreased because they only tracked the CPU running states and not the swapping states. Matthias thought this was nonintuitive, which it is, so he fixed it.</p><p> 他使用较慢的交换磁盘的示例是有意义的：通过降级系统和＃39; S的性能，对系统的需求（测量为+排队）应该增加。但是，负载平均值减少，因为它们仅跟踪CPU运行状态而不是交换状态。 Matthias认为这是不必要的，所以他修好了它。</p><p>  But don&#39;t Linux load averages sometimes go too high, more than can be explained by disk I/O? Yes, although my guess is that this is due to a new code path using TASK_UNINTERRUPTIBLE that didn&#39;t exist in 1993. In Linux 0.99.14, there were 13 codepaths that directly set TASK_UNINTERRUPTIBLE or TASK_SWAPPING (the swapping state was later removed from Linux). Nowadays, in Linux 4.12, there are nearly 400 codepaths that set TASK_UNINTERRUPTIBLE, including some lock primitives. It&#39;s possible that one of these codepaths should not be included in the load averages. Next time I have load averages that seem too high, I&#39;ll see if that is the case, and if it can be fixed.</p><p>  但是Don＆＃39; T Linux负载平均值有时会过高，超过磁盘I / O解释的更多？虽然我的猜测是，这是由于使用Task_uninteruptible的新代码路径，它在1993年中存在。在Linux 0.99.14中，有13个CodePath直接设置Task_uninteruptible或Task_swapping（稍后删除交换状态）来自Linux）。如今，在Linux 4.12中，有近400个代码夫人设置了Task_UnintRuptibly，包括一些锁定基元。它可能包含其中一个CodePaths不应包含在负载平均值中。下次我有加载平均值，似乎太高，i＆＃39; ll请参阅是否是这种情况，如果它可以修复。</p><p> I emailed Matthias (for the first time) to ask what he thought about his load average change almost 24 years later. He responded in one hour (as I mentioned on  Twitter), and wrote:</p><p> 我通过电子邮件发送了Matthias（第一次），询问他几乎他近24年后的负载平均变化的想法。他在一小时内回复（正如我在Twitter上提到的），并写道：</p><p> &#34;The point of &#34;load average&#34; is to arrive at a number relating how busythe system is from a human point of view. TASK_UNINTERRUPTIBLE means(meant?) that the process is waiting for something like a disk readwhich contributes to system load. A heavily disk-bound system might beextremely sluggish but only have a TASK_RUNNING average of 0.1, whichdoesn&#39;t help anybody.&#34;</p><p> ＆＃34;＆＃34;加载平均值＆＃34;是到达有关系统来自人类角度的繁忙的数字。 Task_Un中断性意味着（含义？）该过程正在等待像磁盘读取的内容有助于系统加载。一个庞大的磁盘束缚系统可能是不贬低的，但只有0.1的Task_running平均值，其中NOTOONNN和＃39; T帮助任何人。＆＃34;</p><p> (Getting a response so quickly, or even a response at all, really made my day. Thanks!)</p><p> （如此迅速地获得回复，甚至一切都是回应，真的让我的一天。谢谢！） </p><p> So Matthias still thinks it makes sense, at least given what TASK_UNINTERRUPTIBLE used to mean.</p><p>所以Matthias仍然认为它是有道理的，至少给出了什么Task_uninterruptible常用。</p><p> But TASK_UNITERRUPTIBLE matches more things today. Should we change load averages to be just CPU and disk demand? Scheduler maintainer Peter Zijstra has already sent me one clever option to explore for doing this: include  task_struct-&gt;in_iowait in load averages instead of TASK_UNINTERRUPTIBLE, so that it more closely matches disk I/O. It begs another question, however, which is what do we really want? Do we want to measure demand on the system in terms of threads, or just demand for physical resources? If it&#39;s the former, then waiting on uninterruptible locks should be included as those threads are demand on the system. They aren&#39;t idle. So perhaps Linux load averages already work the way we want them to.</p><p> 但是Task_Unitible今天匹配更多的东西。我们应该改变负载平均值只是CPU和磁盘需求吗？ Scheduler维护者Peter Zijstra已经向我发送了一个聪明的选项来探索这样做：包括task_struct-＆gt; in_iowait在加载平均值而不是task_uninteruptible，因此它更紧密地匹配磁盘I / O.然而，它乞求另一个问题，这是我们真正想要的？我们是否希望在线程方面测量系统的需求，或仅对物理资源的需求？如果它是前者＆＃39;那么等待不间断锁的等待应该包括在系统上的那些线程。他们不闲着。所以也许Linux负载平均已经正常工作。</p><p> To better understand uninterruptible code paths, I&#39;d like a way to measure them in action. Then we can examine different examples, quantify time spent in them, and see if it all makes sense.</p><p> 更好地了解不间断的代码路径，我＆＃39; d喜欢在行动中测量它们的方法。然后我们可以检查不同的例子，量化在其中所花费的时间，并查看它是否都是有意义的。</p><p>  The following is an  Off-CPU flame graph from a production server, spanning 60 seconds and showing kernel stacks only, where I&#39;m filtering to only include those in the TASK_UNINTERRUPTIBLE state ( SVG). It provides many examples of uninterruptible code paths:</p><p>  以下是从生产服务器，跨越60秒并仅显示内核堆栈的关注点火焰图，其中i＆＃39; m过滤仅包括任务_un中间状态（svg）中的内核堆栈。它提供了许多不间断代码路径的示例：</p><p>  If you are new to off-CPU flame graphs: you can click on frames to zoom in, examining the full stacks which appear as a tower of frames. The x-axis size is proportional to the time spent blocked off-CPU, and the sort order (left to right) has no real meaning. The color is blue for off-CPU stacks (I use warm colors for on-CPU stacks), and the saturation has random variance to differentiate frames.</p><p>  如果您是off-CPU火焰图的新增功能：您可以单击框架以放大，检查显示为帧塔的完整堆栈。 x轴大小与堵塞关CPU的时间成比例，并且排序顺序（左到右）没有真正的含义。用于关注CPU堆栈的颜色为蓝色（我使用On-CPU堆栈的暖色），饱和度具有随机方差来区分帧。</p><p> I generated this using my offcputime tool from  bcc (this tool needs eBPF features from Linux 4.8+), and my  flame graph software:</p><p> 我使用来自BCC的我的offcputime工具生成了这一点（此工具需要来自Linux 4.8+的EBPF功能），以及我的Flame图软件：</p><p> #  ./bcc/tools/offcputime.py -K --state 2 -f 60 &gt; out.stacks#  awk &#39;{ print $1, $2 / 1000 }&#39; out.stacks | ./FlameGraph/flamegraph.pl --color=io --countname=ms &gt; out.offcpu.svgb&gt;</p><p> ＃./bcc/tools/offcputime.py -k --state 2 -f 60＆gt; out.stacks＃awk＆＃39; {打印$ 1，$ 2/1000}＆＃39; out.stacks | ./flamegraph/flamegraph.pl --color = io --countname = ms＆gt; out.offcpu.svgb＆gt; </p><p> I&#39;m using awk to change the output from microseconds to milliseconds. The offcputime &#34;--state 2&#34; matches on TASK_UNINTERRUPTIBLE (see sched.h), and is an option I just added for this post. Facebook&#39;s Josef Bacik first did this with his  kernelscope tool, which also uses bcc and flame graphs. In my examples, I&#39;m just showing the kernel stacks, but offcputime.py supports showing the user stacks as well.</p><p>使用awk将输出从微秒更改为milliSeconds的i＆＃39; offcputime＆＃34;  - 州2＆＃34;匹配Task_unintRuptible（请参阅sched.h），是我刚刚为此帖子添加的选项。 Facebook＆＃39; S Josef Bacik首先用他的Kernelscope工具做到了这个，这也使用了BCC和火焰图。在我的例子中，i＆＃39; m只是显示内核堆栈，但offcputime.py支持显示用户堆栈。</p><p> As for the flame graph above: it shows that only 926 ms out of 60 seconds were spent in uninterruptible sleep. That&#39;s only adding 0.015 to our load averages. It&#39;s time in some cgroup paths, but this server is not doing much disk I/O.</p><p> 至于上面的火焰图：它表明，在不间断的睡眠中仅花了60秒的926毫秒。 ＆＃39;只添加0.015到我们的负载平均值。它＆＃39;在一些cgroup路径中的时间，但这台服务器没有做太多磁盘I / O.</p><p>   The wide tower on the right is showing  systemd-journal in proc_pid_cmdline_read() (reading /proc/PID/cmdline), getting blocked, and contributing 0.07 to the load average. And there is a wider page fault tower on the left, that also ends up in rwsem_down_read_failed() (adding 0.23 to the load average). I&#39;ve highlighted those functions in magenta using the flame graph search feature. Here&#39;s an excerpt from rwsem_down_read_failed():</p><p>   右侧的宽塔在proc_pid_cmdline_read（）中显示systemd-journal_read（）（读取/ proc / pid / cmdline），被阻止，并为负载平均贡献0.07。左侧有一个更广泛的页面错误塔，即在RWSEM_DOWN_READ_FAILED（）中也已最终（增加0.23到负载平均值）。我突出了洋红色中的那些函数使用火焰图搜索功能。这里＆＃39; rwsem_down_read_failed（）的摘录：</p><p> /* wait to be given the lock */ while (true) { set_task_state(tsk, TASK_UNINTERRUPTIBLE); if (!waiter.task) break; schedule(); }</p><p> / *等待锁定* / while（true）{set_task_state（tsk，task_unintruptible）;如果（！waiteer.task）休息;日程（）; }</p><p> This is lock acquisition code that&#39;s using TASK_UNINTERRUPTIBLE. Linux has uninterruptible and interruptible versions of mutex acquire functions (eg, mutex_lock() vs mutex_lock_interruptible(), and down() and down_interruptible() for semaphores). The interruptible versions allow the task to be interrupted by a signal, and then wake up to process it before the lock is acquired. Time in uninterruptible lock sleeps usually don&#39;t add much to the load averages, but in this case they are adding 0.30. If this was much higher, it would be worth analyzing to see if lock contention could be reduced (eg, I&#39;d start digging on systemd-journal and proc_pid_cmdline_read()!), which should improve performance and lower the load average.</p><p> 这是锁采集代码，＆＃39; s使用task_uninteruptible。 Linux具有不间断和可中断版本的互斥锁获取功能（例如，mutex_lock（）vs mutex_lock_interruptible（），down（）和down_interruptible（）用于信号量）。中断版本允许任务由信号中断，然后在获取锁之前唤醒以处理它。不间断锁定的时间通常不会增加负载平均值，但在这种情况下，他们正在增加0.30。如果这要高得多，值得分析，看看是否可以减少锁争用（例如，i＆＃39; d在systemd-journal和proc_pid_cmdline_read（）！）上开始挖掘，这应该提高性能并降低负载平均值。</p><p> Does it make sense for these code paths to be included in the load average? Yes, I&#39;d say so. Those threads are in the middle of doing work, and happen to block on a lock. They aren&#39;t idle. They are demand on the system, albeit for software resources rather than hardware resources.</p><p> 这些代码路径是否包含在负载平均值中是有意义的吗？是的，我＆＃39; d这么说。那些线程在进行工作的中间，并发生在锁上。他们不闲着。它们是对系统的需求，尽管用于软件资源而不是硬件资源。</p><p>  Can the Linux load average value be fully decomposed into components? Here&#39;s an example: on an idle 8 CPU system, I launched  tar to archive some uncached files. It spends several minutes mostly blocked on disk reads. Here are the stats, collected from three different terminal windows:</p><p>  Linux加载平均值是否可以完全分解成组件？这里的一个例子：在空闲8个CPU系统上，我推出了归档了一些未加工的文件。它花费了几分钟大部分磁盘读取。以下是从三个不同的终端窗口收集的统计数据： </p><p> terma$  pidstat -p `pgrep -x tar` 60Linux 4.9.0-rc5-virtual (bgregg-xenial-bpf-i-0b7296777a2585be1) 08/01/2017 _x86_64_ (8 CPU)10:15:51 PM UID PID %usr %system %guest %CPU CPU Command10:16:51 PM 0 18468 2.85 29.77 0.00 32.62 3 tartermb$  iostat -x 60[...]avg-cpu: %user %nice %system %iowait %steal %idle 0.54 0.00 4.03 8.24 0.09 87.10Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilxvdap1 0.00 0.05 30.83 0.18 638.33 0.93 41.22 0.06 1.84 1.83 3.64 0.39 1.21xvdb 958.18 1333.83 2045.30 499.38 60965.27 63721.67 98.00 3.97 1.56 0.31 6.67 0.24 60.47xvdc 957.63 1333.78 2054.55 499.38 61018.87 63722.13 97.69 4.21 1.65 0.33 7.08 0.24 61.65md0 0.00 0.00 4383.73 1991.63 121984.13 127443.80 78.25 0.00 0.00 0.00 0.00 0.00 0.00termc$  uptime 22:15:50 up 154 days, 23:20, 5 users, load average: 1.25, 1.19, 1.05[...]termc$  uptime 22:17:14 up 154 days, 23:21, 5 users, load average: 1.19, 1.17, 1.06</p><p>terma $ pidstat -p`pgrep-x tar` 60linux 4.9.0-rc5-virtual（bgregg-xenial-bpf-i-0b7296777a2585be1）08/01/2017_x86_64_（8 CPU）10:15:51 PM UID PID％USR ％System％Guest％CPU CPU Command10：16：51 PM 0 18468 2.85 29.77 0.85 29.77 0.00 32.62 3 tartermb $ iostat-x 60 [...] avg-cpu：％用户％nice％system％iowait％窃听％空闲0.54 0.03 8.24 0.09 87.10DEVICE：RRQM / S WRQM / SR / SW / S RKB / S WKB / S AVGRQ-SZ-SZ-SZ AVGQU-SZ AWAIT R_AWAIT W_AWAIT SVCTM％utilXVDAP1 0.05 0.05 30.83 0.18 638.33 0.93 41.22 0.0.06 1.84 1.83 3.64 0.06 1.21xvdb 958.18 1333.83 2045.30 499.38 60965.27 63721.67 98.00 3.97 1.56 0.31 6.67 0.24 60.47xvdc 957.63 1333.78 2054.55 499.38 61018.87 63722.13 97.69 4.21 1.65 0.33 7.08 0.24 0.00 61.65md0 0.00 4383.73 1991.63 121984.13 127443.80 78.25 0.00 0.00 0.00 0.00 0.00 0.00termc $运行时间22时15分五十零秒了154天， 23:20，5个用户，加载平均：1.25,1.19,1.05 [...] Termc $正常运行时间22:17:14增长154天，23:21,5用户，装载平均：1.19,1.17,1.06</p><p>    0.67 is from tar&#39;s uninterruptible disk reads, inferred (offcpu flame graph has this at 0.69, I suspect as it began collecting a little later and spans a slightly different time range)</p><p>    0.67来自焦油＆＃39; S不间断磁盘读取，推断（offcpu火焰图在0.69时，我怀疑它开始稍后收集并跨越略微不同的时间范围）</p><p> 0.04 is from other CPU consumers (iostat user + system, minus tar&#39;s CPU from pidstat)</p><p> 0.04来自其他CPU消费者（iostat用户+系统，减去PIDStat的CPU）</p><p> 0.11 is from kernel workers uninterruptible disk I/O time, flushing disk writes (offcpu flame graph, the two towers on the left)</p><p> 0.11来自内核工作者不间断磁盘I / O时间，刷新磁盘写入（offcpu火焰图，左侧的两个塔）</p><p> That adds up to 1.15. I&#39;m still missing 0.04, some of which may be rounding and measurement interval offset errors, but a lot may be due to the load average being an exponentially-damped moving sum, whereas the other averages I&#39;m using (pidstat, iostat) are normal averages. Prior to 1.19, the one minute average was 1.25, so some of that will still be dragging us high. How much? From my earlier graphs, at the one minute mark, 62% of the metric was from that minute, and the rest was older. So 0.62 x 1.15 + 0.38 x 1.25 = 1.18. That&#39;s pretty close to the 1.19 reported.</p><p> 增加到1.15。 i＆＃39; m仍然缺少0.04，其中一些可能是舍入和测量间隔偏移误差，但很多可能是由于负载平均值是指数衰减的移动和，而另一个平均值i＆＃39; m使用（ PIDSTAT，IOSTAT）是正常平均值。在1.19之前，一分钟平均为1.25，因此其中一些仍将拖延我们。多少？从我之前的图表中，在一个分钟的标记，62％的公制来自那一分钟，其余的年龄较大。所以0.62 x 1.15 + 0.38 x 1.25 = 1.18。那个＆＃39;非常接近1.19报道。</p><p> This is a system where one thread (tar) plus a little more (some time in kernel worker threads) are doing work, and Linux reports the load average as 1.19, which makes sense. If it was measuring &#34;CPU load averages&#34;, the system would have reported 0.37 (inferred from mpstat&#39;s summary), which is correct for CPU resources only, but hides the fact that there is demand for over one thread&#39;s worth of work.</p><p> 这是一个线程（焦油）加一倍（在内核工作线程中的一段时间）正在进行工作的系统，Linux将负载平均报告为1.19，这是有意义的。如果它是测量＆＃34; CPU负载平均值＆＃34;，系统将报告0.37（从MPSTAT＆＃39; s摘要推断出来，这对于CPU资源来说是正确的，但隐藏了有需要的事实一个螺纹＆＃39;值得的工作。</p><p> I hope this example shows that the numbers really do mean something deliberate (CPU + uninterruptible), and you can decompose them and figure it out.</p><p> 我希望这个例子表明，数字确实意味着刻意的东西（CPU +不间断），你可以分解它们并弄清楚。 </p><p>  I grew up with OSes where load averages meant CPU load averages, so the Linux version has always bothered me. Perhaps the real problem all along is that the words &#34;load averages&#34; are about as ambiguous as &#34;I/O&#34;. Which type of I/O? Disk I/O? File system I/O? Network I/O? ... Likewise, which load averages? CPU load averages? System load averages? Clarifying it this way lets me make sense of it like this:</p><p>我与iSS一起长大，其中加载平均值意味着CPU负载平均值，因此Linux版本一直困扰我。也许这一切都是真正的问题是单词＆＃34;负载平均值＆＃34;关于＆＃34; I / O＆＃34;哪种类型的I / O？磁盘I / O？文件系统I / O？网络I / O？ ......同样，加载平均值？ CPU负载平均值？系统加载平均值？以这种方式澄清它让我喜欢这样的意识：</p><p> On Linux, load averages are (or try to be) &#34; system load averages&#34;, for the system as a whole, measuring the number of threads that are working and waiting to work (CPU, disk, uninterruptible locks). Put differently, it measures the number of threads that aren&#39;t completely idle. Advantage: includes demand for different resources.</p><p> 在Linux上，负载平均值是（或尝试是）＆＃34;系统加载平均值＆＃34;为系统整体，测量工作和等待工作的线程数（CPU，磁盘，不间断锁定）。换句话说，它测量areN的线程数量且完全闲置。优势：包括对不同资源的需求。</p><p> On other OSes, load averages are &#34; CPU load averages&#34;, measuring the number of CPU running + CPU runnable threads. Advantage: can be easier to understand and reason about (for CPUs only).</p><p> 在其他操作系统上，负载平均值是＆＃34; CPU负载平均值＆＃34 ;,测量运行的CPU + CPU RUNNABLE线程的数量。优点：可以更容易理解和理由（仅限CPU）。</p><p> Note that there&#39;s another possible type: &#34; physical resource load averages&#34;, which would include load for physical resources only (CPU + disk).</p><p> 请注意，另一种可能类型：＆＃34;物理资源负载平均值＆＃34;，其中仅包括物理资源（CPU +磁盘）的负载。</p><p> Perhaps one day we&#39;ll add additional load averages to Linux, and let the user choose what they want to use: a separate &#34;CPU load averages&#34;, &#34;disk load averages&#34;, &#34;network load averages&#34;, etc. Or just use different metrics altogether.</p><p> 也许有一天我们＆＃39; ll为Linux添加额外的负载平均值，让用户选择他们想要使用的内容：单独的＆＃34; CPU负载平均值＆＃34;＆＃34;磁盘负荷平均值＆＃34;， ＆＃34;网络负载平均值＆＃34;等等或只是使用不同的指标。</p><p>   Some people have found values that seem to work for their systems and workloads: they know that when load goes over X, application latency is high and customers start complaining. But there aren&#39;t really rules for this.</p><p>   有些人发现似乎为他们的系统和工作负载工作的价值观：他们知道当加载到x时，应用程序延迟很高，客户开始抱怨。但是，没有真正的规定这个问题。</p><p> With CPU load averages, one could divide the value by the CPU count, then say that if that ratio is over 1.0 you are running at saturation, which may cause performanc</p><p> 使用CPU负载平均值，可以通过CPU计数划分值，然后说如果该比率超过1.0，则在饱和度下运行，这可能会导致性能 </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html">http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/load/">#load</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/cpu/">#cpu</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>