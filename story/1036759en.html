<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Io_uring和eBPF将彻底改变Linux中的编程Io_uring and eBPF Will Revolutionize Programming in Linux</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Io_uring and eBPF Will Revolutionize Programming in Linux<br/>Io_uring和eBPF将彻底改变Linux中的编程</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-27 03:10:08</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/2e6d00f1bda1d820b86ea36f05bc4842.png"><img src="http://img2.diglog.com/img/2020/11/2e6d00f1bda1d820b86ea36f05bc4842.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>Things will never be the same again after the dust settles. And yes, I’m talking about Linux.</p><p>尘埃落定后，事情将再也不会相同。是的，我说的是Linux。</p><p> As I write this, most of the world is in lockdown due to COVID-19. It’s hard to say how things will look when this is over (it will be over, right?), but one thing is for sure: the world is no longer the same. It’s a weird feeling: it’s as if we ended 2019 in one planet and started 2020 in another.</p><p> 在我撰写本文时，由于COVID-19，世界上大多数地区处于锁定状态。很难说结束后的样子（结束了，对吧？），但是有一点可以肯定：世界不再一样。感觉很奇怪：好像我们在一个星球上结束了2019年，在另一个星球上开始了2020年。</p><p> While we all worry about jobs, the economy and our healthcare systems, one other thing that has changed dramatically may have escaped your attention: the Linux kernel.</p><p> 尽管我们都担心工作，经济和医疗保健系统，但是发生了巨大变化的另一件事可能已经引起您的注意：Linux内核。</p><p> That’s because every now and then something shows up that replaces evolution with revolution. The black swan. Joyful things like the introduction of the automobile, which forever changed the landscape of cities around the world. Sometimes it’s less joyful things, like 9/11 or our current nemesis, COVID-19.</p><p> 那是因为不时出现了一些东西，用革命代替了进化。黑天鹅。诸如引入汽车之类的快乐事情，永远改变了世界各地城市的景观。有时候，它会变得不那么快乐，例如9/11或我们目前的宿敌COVID-19。</p><p> I’ll put what happened to Linux in the joyful bucket. But it’s a sure revolution, one that most people haven’t noticed yet. That’s because of two new, exciting interfaces: eBPF (or BPF for short) and io_uring, the latter added to Linux in 2019 and still in very active development. Those interfaces may look evolutionary, but they are revolutionary in the sense that they will — we bet — completely change the way applications work with and think about the Linux Kernel.</p><p> 我将把发生在Linux上的事情放在欢乐的水桶中。但这是一场确定无疑的革命，大多数人还没有注意到。这是因为有两个激动人心的新界面：eBPF（简称BPF）和io_uring，后者在2019年添加到Linux中，并且仍处于积极开发中。这些接口可能看起来是进化的，但是就它们（我们敢打赌）的意义而言，它们是革命性的，它们将完全改变应用程序的工作方式并考虑Linux内核。</p><p> In this article, we will explore what makes these interfaces special and so powerfully transformational, and dig deeper into our experience at ScyllaDB with io_uring.</p><p> 在本文中，我们将探讨如何使这些接口变得如此特殊且具有强大的转换能力，并通过io_uring深入研究我们在ScyllaDB的经验。</p><p>  In the old days of the Linux you grew to know and love, the kernel offered the following system calls to deal with file descriptors, be they storage files or sockets:</p><p>  在您逐渐了解和喜爱的Linux的早期，内核提供了以下系统调用来处理文件描述符，无论它们是存储文件还是套接字：</p><p>  Those system calls are what we call blocking system calls. When your code calls them it will sleep and be taken out of the processor until the operation is completed. Maybe the data is in a file that resides in the Linux page cache, in which case it will actually return immediately, or maybe it needs to be fetched over the network in a TCP connection or read from an HDD.</p><p>这些系统调用就是我们所谓的阻止系统调用。当您的代码调用它们时，它将进入休眠状态，并从处理器中取出，直到操作完成。也许数据位于Linux页面缓存中的文件中，在这种情况下，它实际上会立即返回，或者可能需要通过TCP连接通过网络或从HDD读取数据。</p><p> Every modern programmer knows what is wrong with this: As devices continue to get faster and programs more complex, blocking becomes undesirable for all but the simplest things. New system calls, like  select() and  poll() and their more modern counterpart,  epoll() came into play: once called, they will return a list of file descriptors that are ready. In other words, reading from or writing to them wouldn’t block. The application can now be sure that blocking will not occur.</p><p> 每个现代程序员都知道这有什么问题：随着设备继续变得越来越快，程序越来越复杂，除了最简单的事情以外，阻塞对于所有其他人来说都是不可取的。新的系统调用，例如select（）和poll（）及其更现代的对等形式epoll（）起作用：一旦被调用，它们将返回准备好的文件描述符列表。换句话说，对他们的阅读或书写不会受到阻碍。应用程序现在可以确保不会发生阻塞。</p><p>  It’s beyond our scope to explain why, but this readiness mechanism really works only for network sockets and pipes — to the point that  epoll() doesn’t even accept storage files. For storage I/O, classically the blocking problem has been solved with   thread pools: the main thread of execution dispatches the actual I/O to helper threads that will block and carry the operation on the main thread’s behalf.</p><p>  我们无法解释原因，但是这种准备机制实际上仅适用于网络套接字和管道，以至于epoll（）甚至不接受存储文件。对于存储I / O，传统上，阻塞问题是通过线程池解决的：执行的主线程将实际的I / O分派给帮助线程，这些帮助线程将代表主线程阻塞并执行操作。</p><p>  As time passed, Linux grew even more flexible and powerful: it turns out database software  may not want to use the Linux page cache. It then became possible to open a file and specify that we want direct access to the device. Direct access, commonly referred to as Direct I/O, or the  O_DIRECT flag, required the application to manage its own caches — which databases may want to do anyway, but also allow for zero-copy I/O as the application buffers can be sent to and populate from the storage device directly.</p><p>  随着时间的流逝，Linux变得更加灵活和强大：事实证明，数据库软件可能不想使用Linux页面缓存。然后就可以打开一个文件，并指定我们要直接访问该设备。直接访问（通常称为直接I / O或O_DIRECT标志）要求应用程序管理自己的缓存-数据库可能仍要执行此操作，但由于应用程序缓冲区可能是零拷贝的I / O，因此可以直接发送到存储设备并从中填充。</p><p> As storage devices got faster, context switches to helper threads became even less desirable. Some devices in the market today, like the  Intel Optane series have latencies in the single-digit microsecond range — the same order of magnitude of a context switch. Think of it this way: every context switch is a missed opportunity to dispatch I/O.</p><p> 随着存储设备变得越来越快，上下文切换到辅助线程变得越来越不可取。当今市场上的某些设备（例如Intel Optane系列）的延迟都在单位微秒范围内（与上下文切换的幅度相同）。这样想：每个上下文切换都是错过分配I / O的机会。</p><p> With Linux 2.6, the kernel gained an  Asynchronous I/O ( linux-aio for short) interface. Asynchronous I/O in Linux is simple at the surface: you can submit I/O with the io_submit system call, and at a later time you can call io_getevents and receive back events that are ready. Recently,  Linux even gained the ability to add  epoll() to the mix: now you could not only submit storage I/O work, but also submit your intention to know whether a socket (or pipe) is readable or writable.</p><p> 使用Linux 2.6，内核获得了异步I / O（简称linux-aio）接口。 Linux上的异步I / O从表面上看很简单：您可以使用io_submit系统调用来提交I / O，稍后再调用io_getevents并接收准备就绪的事件。最近，Linux甚至可以将epoll（）添加到组合中：现在，您不仅可以提交存储I / O工作，还可以提交意图以了解套接字（或管道）是可读还是可写的。</p><p> Linux-aio was a potential game-changer. It allows programmers to make their code fully asynchronous. But due to the way it evolved, it fell short of these expectations. To try and understand why, let’s hear from Mr. Torvalds himself in his usual upbeat mood, in response to someone trying to extend the interface  to support opening files asynchronously:</p><p> Linux-aio是潜在的游戏规则改变者。它允许程序员使其代码完全异步。但是由于它的发展方式，它没有达到这些期望。为了尝试理解原因，让我们听听Torvalds先生本人的乐观态度，以回应有人试图扩展界面以支持异步打开文件的情况：</p><p> AIO is a horrible ad-hoc design, with the main excuse being “other, less gifted people, made that design, and we are implementing it for compatibility because database people — who seldom have any shred of taste — actually use it”.</p><p>AIO是一种糟糕的临时设计，其主要借口是“其他人，没有那么天赋的人进行了这种设计，而我们为了实现兼容性而实施它是因为很少有人喜欢的数据库人实际上会使用它”。</p><p> First, as database people ourselves, we’d like to take this opportunity to apologize to Linus for our lack of taste. But also expand on why he is right. Linux AIO is indeed rigged with problems and limitations:</p><p> 首先，作为数据库人员，我们想借此机会向Linus道歉，因为我们缺乏品味。但也要解释他为什么是对的。 Linux AIO确实存在许多问题和局限性：</p><p> The interface is not designed to be extensible. Although it is possible — we did extend it — every new addition is complex.</p><p> 该接口并非设计为可扩展的。尽管有可能（我们确实对其进行了扩展），但每个新添加的内容都是复杂的。</p><p> Although the interface is technically non-blocking,  there are many reasons that can lead it to blocking, often in ways that are impossible to predict.</p><p> 尽管从技术上说该接口是非阻塞的，但是有很多原因会导致其阻塞，通常是无法预测的。</p><p> We can clearly see the evolutionary aspect of this: interfaces grew organically, with new interfaces being added to operate in conjunction with the new ones. The problem of blocking sockets was dealt with with an interface to test for readiness. Storage I/O gained an asynchronous interface tailored-fit to work with the kind of applications that really needed it at the moment and nothing else. That was the nature of things. Until…  io_uring came along.</p><p> 我们可以清楚地看到它的进化方面：接口有机地增长，添加了新接口以与新接口一起运行。解决套接字阻塞的问题是通过一个接口来测试准备情况。存储I / O获得了量身定制的异步接口，可以与当前真正需要它的那种应用程序一起工作，而没有其他要求。那就是事物的本质。直到…io_uring出现。</p><p>  io_uring is the brainchild of Jens Axboe, a seasoned kernel developer who has been involved in the Linux I/O stack for a while.  Mailing list archaeology tells us that this work started with a simple motivation: as devices get extremely fast, interrupt-driven work is no longer as efficient as polling for completions — a common theme that underlies the architecture of performance-oriented I/O systems.</p><p>  io_uring是Jens Axboe的创意，Jens Axboe是一位经验丰富的内核开发人员，已经参与Linux I / O堆栈已有一段时间了。邮件列表考古学告诉我们，这项工作的动机很简单：随着设备变得非常快，中断驱动的工作不再像轮询完成那样高效-这是面向性能的I / O系统架构的一个常见主题。</p><p> But as the work evolved, it grew into a radically different interface, conceived from the ground up to allow fully asynchronous operation. It’s a basic theory of operation is close to linux-aio: there is an interface to push work into the kernel, and another interface to retrieve completed work.</p><p> 但是随着工作的发展，它发展成为一个完全不同的接口，它是从头开始构思的，以允许完全异步的操作。它的基本操作原理接近linux-aio：有一个将工作推送到内核的接口，另一个是检索已完成工作的接口。</p><p>  By design, the interfaces are designed to be truly asynchronous. With the right set of flags, it will never initiate any work in the system call context itself and will just queue work. This guarantees that the application will never block.</p><p>通过设计，接口被设计为真正异步的。使用正确的标志集，它将永远不会在系统调用上下文本身中启动任何工作，而只会将工作排队。这样可以保证应用程序永远不会阻塞。</p><p> It works with any kind of I/O: it doesn’t matter if they are cached files, direct-access files, or even blocking sockets. That is right: because of its async-by-design nature, there is no need for poll+read/write to deal with sockets. One submits a blocking read, and once it is ready it will show up in the completion ring.</p><p> 它可以与任何类型的I / O配合使用：它们是否是缓存文件，直接访问文件甚至阻止套接字都没关系。没错：由于其按设计异步的性质，因此不需要轮询+读/写来处理套接字。一个提交阻塞读取，一旦准备就绪，它将显示在完成环中。</p><p> It is flexible and extensible: new opcodes are being added at a rate that leads us to believe that indeed soon it will grow to re-implement every single Linux system call.</p><p> 它具有灵活性和可扩展性：新操作码的添加速度使我们相信，不久以后它将可以重新实现每个Linux系统调用。</p><p> The  io_uring interface works through two main data structures: the submission queue entry (sqe) and the completion queue entry (cqe). Instances of those structures live in a shared memory single-producer-single-consumer ring buffer between the kernel and the application.</p><p> io_uring接口通过两个主要数据结构工作：提交队列条目（sqe）和完成队列条目（cqe）。这些结构的实例位于内核和应用程序之间的共享内存单生产者单消费者环形缓冲区中。</p><p>  The application asynchronously adds sqes to the queue (potentially many) and then tells the kernel that there is work to do. The kernel does its thing, and when work is ready it posts the results in the cqe ring. This also has the added advantage that system calls are now batched. Remember  Meltdown? At the time I wrote about how little it affected our Scylla NoSQL database, since we would batch our I/O system calls through  aio. Except now we can batch much more than just the storage I/O system calls, and this power is also available to any application.</p><p>  该应用程序异步将sqes添加到队列（可能很多），然后告诉内核有工作要做。内核完成其工作，并在工作准备就绪时将结果发布到cqe环中。这还具有额外的优势，即系统调用现在已批处理。还记得熔毁吗？当时，我写了有关它对Scylla NoSQL数据库影响不大的文章，因为我们将通过aio批处理I / O系统调用。除了现在，我们可以批处理的不仅仅是存储I / O系统调用，而且此功能还可以用于任何应用程序。</p><p> The application, whenever it wants to check whether work is ready or not, just looks at the cqe ring buffer and consumes entries if they are ready. There is no need to go to the kernel to consume those entries.</p><p> 无论何时要检查工作是否准备就绪，应用程序都只会查看cqe环形缓冲区，并在准备就绪时使用条目。无需转到内核使用这些条目。</p><p> Here are some of the operations that  io_uring supports:  read,  write,  send,  recv,  accept,  openat,  stat, and even way more specialized ones like  fallocate.</p><p> 以下是io_uring支持的一些操作：读取，写入，发送，接收，openat，stat以及更专业的方法（例如fallocate）。</p><p> This is not an evolutionary step. Although  io_uring is slightly similar to  aio, its extensibility and architecture are disruptive: it brings the power of asynchronous operations to anyone, instead of confining it to specialized database applications.</p><p>这不是进化步骤。尽管io_uring与aio稍微相似，但是它的可扩展性和体系结构具有破坏性：它将异步操作的功能带给任何人，而不是将其限制在特定的数据库应用程序中。</p><p> Our CTO, Avi Kivity,  made the case for async at the Core C++ 2019 event. The bottom line is this; in modern multicore, multi-CPU devices, the CPU itself is now basically a network, the intercommunication between all the CPUs is another network, and calls to disk I/O are effectively another. There are good reasons why network programming is done asynchronously, and you should consider that for your own application development too.</p><p> 我们的首席技术官Avi Kivity在Core C ++ 2019活动中提出了异步的理由。底线是这个;在现代的多核，多CPU设备中，CPU本身现在基本上是一个网络，所有CPU之间的相互通信是另一个网络，而对磁盘I / O的调用实际上是另一个网络。网络编程异步完成是有充分的理由的，对于您自己的应用程序开发，也应该考虑到这一点。</p><p> It fundamentally changes the way Linux applications are to be designed: Instead of a flow of code that issues syscalls when needed, that have to think about whether or not a file is ready, they naturally become an event-loop that constantly add things to a shared buffer, deals with the previous entries that completed, rinse, repeat.</p><p> 它从根本上改变了Linux应用程序的设计方式：代替了在需要时发出系统调用的代码流，而不得不考虑文件是否准备就绪，它们自然地变成了一个事件循环，不断地向文件添加内容。共享缓冲区，处理之前完成的条目，冲洗和重复。</p><p> So, what does that look like? The code block below is an example on how to dispatch an entire array of reads to multiple file descriptors at once down the  io_uring interface:</p><p> 那么，那是什么样的呢？下面的代码块是一个示例，说明如何在io_uring接口下一次将读取的整个数组分配给多个文件描述符：</p><p>  At a later time, in an event-loop manner, we can check which reads are ready and process them. The best part of it is that due to its shared-memory interface, no system calls are needed to consume those events. The user just has to be careful to tell the  io_uring interface that the events were consumed.</p><p>  稍后，我们可以以事件循环的方式检查哪些读取准备就绪并进行处理。最好的部分是，由于其共享内存接口，不需要系统调用即可消耗这些事件。用户只需要小心地告诉io_uring界面事件已被使用。</p><p>  This simplified example works for reads only, but it is easy to see how we can batch all kinds of operations together through this unified interface. A queue pattern also goes very well with it: you can just queue operations at one end, dispatch, and consume what’s ready at the other end.</p><p>  这个简化的示例仅适用于只读，但是很容易看出我们如何通过该统一接口将各种操作一起批处理。队列模式也可以很好地与之配合使用：您可以只在一端对操作进行排队，在另一端分派和使用已准备好的东西。</p><p>  Aside from the consistency and extensibility of the interface,  io_uring offers a plethora of advanced features for specialized use cases. Here are some of them:</p><p>  除了界面的一致性和可扩展性之外，io_uring还为专用用例提供了许多高级功能。这里是其中的一些：</p><p> File registration: every time an operation is issued for a file descriptor, the kernel has to spend cycles mapping the file descriptor to its internal representation. For repeated operations over the same file,  io_uring allows you to pre-register those files and save on the lookup.</p><p>文件注册：每次对文件描述符发出操作时，内核都必须花费一些周期将文件描述符映射到其内部表示形式。要对同一文件进行重复操作，可以使用io_uring预注册这些文件并保存在查找中。</p><p> Buffer registration: analogous to file registration, the kernel has to map and unmap memory areas for Direct I/O.  io_uring allows those areas to be pre-registered if the buffers can be reused.</p><p> 缓冲区注册：类似于文件注册，内核必须映射和取消映射直接I / O的内存区域。 io_uring如果可以重用缓冲区，则可以预先注册这些区域。</p><p> Poll ring: for very fast devices, the cost of processing interrupts is substantial.  io_uring allows the user to turn off those interrupts and consume all available events through polling.</p><p> 轮询环：对于非常快的设备，处理中断的成本很高。 io_uring允许用户关闭这些中断并通过轮询消耗所有可用事件。</p><p> Linked operations: allows the user to send two operations that are dependent on each other. They are dispatched at the same time, but the second operation only starts when the first one returns.</p><p> 链接的操作：允许用户发送相互依赖的两个操作。它们是在同一时间分派的，但是第二个操作仅在第一个操作返回时才开始。</p><p> And as with other areas of the interface, new features are also being added quickly.</p><p> 与界面的其他区域一样，新功能也正在迅速添加。</p><p>  As we said, the  io_uring interface is largely driven by the needs of modern hardware. So we would expect some performance gains. Are they here?</p><p>  正如我们所说，io_uring接口在很大程度上受现代硬件需求的驱动。因此，我们期望性能有所提高。他们在这里吗？</p><p> For users of  linux-aio, like ScyllaDB, the gains are expected to be few, focused in some particular workloads and come mostly from the advanced features like buffer and file registration and the poll ring. This is because  io_uring and  linux-aio are not that different as we hope to have made clear in this article:  io_uring is first and foremost bringing all the nice features of  linux-aio to the masses.</p><p> 对于像ScyllaDB这样的linux-aio用户，预计收益很少，主要集中在某些特定的工作负载上，并且主要来自缓冲区和文件注册以及轮询环等高级功能。这是因为io_uring和linux-aio并没有什么不同，正如我们希望在本文中明确指出的那样：io_uring首先是将linux-aio的所有出色功能带给了大众。</p><p> We have used the well-known  fio utility to evaluate 4 different interfaces: synchronous reads,  posix-aio (which is implemented as a thread pool),  linux-aio and  io_uring. In the first test, we want all reads to hit the storage, and not use the operating system page cache at all. We then ran the tests with the Direct I/O flags, which should be the bread and butter for  linux-aio. The test is conducted on NVMe storage that should be able to read at 3.5M IOPS. We used 8 CPUs to run 72  fio jobs, each issuing random reads across four files with an  iodepth of 8. This makes sure that the CPUs run at saturation for all backends and will be the limiting factor in the benchmark. This allows us to see the behavior of each interface at saturation. Note that with enough CPUs all interfaces would be able to at some point achieve the full disk bandwidth. Such a test wouldn’t tell us much.</p><p>我们使用了著名的fio实用程序来评估4个不同的接口：同步读取，posix-aio（作为线程池实现），linux-aio和io_uring。在第一个测试中，我们希望所有读取都命中存储，而不使用操作系统页面缓存。然后，我们使用Direct I / O标志运行测试，这应该是linux-aio的基础。该测试是在应能够以3.5M IOPS读取的NVMe存储上进行的。我们使用8个CPU运行72个作业，每个作业在iodepth为8的四个文件中发出随机读取。这确保了CPU在所有后端均处于饱和状态，并且将成为基准测试的限制因素。这使我们可以看到每个接口处于饱和状态。请注意，如果有足够的CPU，则所有接口都将能够在某个时候达到完整的磁盘带宽。这样的测试不会告诉我们太多。</p><p>  Table 1: performance comparison of 1kB random reads at 100% CPU utilization using Direct I/O, where data is never cached: synchronous reads,  posix-aio  (uses a thread pool),  linux-aio , and the basic  io_uring  as well as  io_uring  using its advanced features.</p><p>  表1：使用直接I / O在CPU利用率为100％时1kB随机读取的性能比较，其中永不缓存数据：同步读取，posix-aio（使用线程池），linux-aio和基本的io_uring以及io_uring使用其高级功能。</p><p> We can see that as we expect,  io_uring is a bit faster than  linux-aio, but nothing revolutionary. Using advanced features like buffer and file registration (io_uring enhanced) gives us an extra boost, which is nice, but nothing that justifies changing your entire application, unless you are a database trying to squeeze out every operation the hardware can give. Both  io_uring and  linux-aio are around twice as fast as the synchronous read interface, which in turn is twice as fast as the thread pool approach employed by  posix-aio, which is surprisingly at first.</p><p> 我们可以看到，正如我们期望的那样，io_uring比linux-aio快一点，但是没有什么革命性的。使用缓冲区和文件注册之类的高级功能（增强了io_uring的功能）为我们带来了额外的提升，这很好，但是没有理由证明更改整个应用程序是合理的，除非您是一个数据库，试图挤出硬件可以进行的所有操作。 io_uring和linux-aio的速度大约是同步读取接口的两倍，而同步读取接口的速度是posix-aio所采用的线程池方法的两倍，而后者最初是令人惊讶的。</p><p> The reason why  posix-aio is the slowest is easy to understand if we look at the context switches column at Table 1: every event in which the system call would block, implies one additional context switch. And in this test, all reads will block. The situation is just worse for  posix-aio. Now not only there is the context switch between the kernel and the application for blocking, the various threads in the application have to go in and out the CPU.</p><p> 如果我们看一下表1中的上下文切换列，则posix-aio最慢的原因很容易理解，即系统调用将阻塞的每个事件都意味着一个附加的上下文切换。并且在此测试中，所有读取都将被阻止。对于posix-aio来说情况更糟。现在，不仅在内核和应用程序之间存在上下文切换以进行阻塞，而且应用程序中的各个线程也必须进出CPU。</p><p> But the real power of  io_uring can be understood when we look at the other side of the scale. In a second test, we preloaded all the memory with the data in the files and proceeded to issue the same random reads. Everything is equal to the previous test, except we now use buffered I/O and expect the synchronous interface to never block — all results are coming from the operating system page cache, and none from storage.</p><p> 但是，当我们从量表的另一端看时，io_uring的真正力量可以理解。在第二项测试中，我们将文件中的数据预加载到所有内存中，并进行相同的随机读取。一切都与之前的测试相同，除了我们现在使用缓冲的I / O并期望同步接口永远不会阻塞-所有结果都来自操作系统页面缓存，而没有结果来自存储。</p><p>  Table 2: comparison between the various backends. Test issues 1kB random reads using buffered I/O files with preloaded files and a hot cache. The test is run at 100% CPU.</p><p>  表2：各种后端之间的比较。测试问题使用带有预加载文件和热缓存的缓冲I / O文件进行1kB随机读取。该测试在100％CPU上运行。</p><p> We don’t expect a lot of difference between synchronous reads and  io_uring interface in this case because no reads will block. And that’s indeed what we see. Note, however, that in real life applications that do more than just read all the time there will be a difference, since  io_uring supports batching many operations in the same system call.</p><p> 在这种情况下，我们预计同步读取和io_uring接口之间不会有太多差异，因为不会阻塞任何读取。这确实是我们所看到的。但是请注意，在现实生活中，不仅要始终读取所有内容，还会有区别，因为io_uring支持在同一系统调用中分批处理许多操作。</p><p> The other two interfaces, however, suffer a big penalty: the large number of context switches in the  posix-aio interface due to its thread pool completely destroys the benchmark performance at saturation.  Linux-aio, which is not designed for buffered I/O, at all, actually becomes a synchronous interface when used with buffered I/O files. So now we pay the price of the asynchronous interface — having to split the operation in a dispatch and consume phase, without realizing any of the benefits.</p><p>但是，其他两个接口会遭受较大的损失：posix-aio接口中的大量上下文切换由于其线程池而完全破坏了饱和时的基准性能。根本不是为缓冲I / O设计的Linux-aio在与缓冲I / O文件一起使用时实际上变成了同步接口。因此，现在我们付出了异步接口的代价—必须在分派和使用阶段拆分操作，而没有意识到任何好处。</p><p> Real applications will be somewhere in the middle: some blocking, some non-blocking operations. Except now there is no longer the need to worry about what will happen. The  io_uring interface performs well in any circumstance. It doesn’t impose a penalty when the operations would not block, is fully asynchronous when the operations would block, and does not rely on threads and expensive context switches to achieve its asynchronous behavior. And what’s even better: although our example focused on random reads,  io_uring will work for a large list of opcodes. It can open and close files, set timers, transfer data to and from network sockets. All using the same interface.</p><p> 真正的应用程序将处于中间位置：一些阻塞，一些非阻塞操作。除了现在，不再需要担心会发生什么。 io_uring接口在任何情况下都可以正常运行。当操作不会阻塞时，它不会造成任何损失；当操作会阻塞时，它是完全异步的，并且不依赖线程和昂贵的上下文切换来实现其异步行为。更好的是：尽管我们的示例着重于随机读取，但io_uring将适用于大量操作码。它可以打开和关闭文件，设置计时器，在网络套接字之间来回传输数据。全部使用相同的界面。</p><p>  Because Scylla scales up to 100% of server capacity before scaling out, it relies exclusively on Direct I/O and we have been using  linux-aio since the start.</p><p>  由于Scylla在扩展之前最多可扩展到服务器容量的100％，因此它完全依赖于Direct I / O，并且从一开始就一直使用linux-aio。</p><p> In our journey towards  io_uring, we have initially seen results as high as 50% better in some workloads. At closer inspection, that made clear that this is because our implementation of  linux-aio was not as good as it could be. This, in my view, highlights one usually underappreciated aspect of performance: how easy it is to achieve it. As we fixed our  linux-aio implementation according to the deficiencies  io_uring shed light into, the performance difference all but disappeared. But that took effort, to fix an interface we have been using for many years. For  io_uring, achieving that was trivial.</p><p> 在进行io_uring的过程中，我们最初看到在某些工作负载下，结果可提高50％。经过仔细检查，可以清楚地看出这是因为我们对linux-aio的实现不尽人意。在我看来，这突出了绩效通常一个未被重视的方面：实现这一目标有多容易。当我们根据发现的缺陷修复了linux-aio实现时，性能差异几乎消失了。但这花费了很多精力，以修复我们已经使用多年的界面。对于io_uring来说，做到这一点是微不足道的。</p><p> However, aside from that,  io_uring can be used for much more than just file I/O (as already mentioned many times throughout this article). And it comes with specialized high performance interfaces like buffer registration, file registration, and a poll interface with no interrupts.</p><p> 但是，除此之外，io_uring不仅可以用于文件I / O，它还可以用于更多的事情（如本文中多次提到的那样）。它带有专用的高性能接口，例如缓冲区注册，文件注册和无中断的轮询接口。</p><p> When  io_uring’s advanced features are used, we do see a performance difference: we observed a 5% speedup when reading 512-byte payloads from a single CPU in an Intel Optane device, which is consistent with the  fio results in Tables 1 and 2. While that doesn’t sound like a lot, that’s very valuable for databases trying to make the most out of the hardware.</p><p> 使用io_uring的高级功能时，我们确实看到了性能差异：观察到从Intel Optane设备中的单个CPU读取512字节有效负载时的速度提高了5％，这与表1和表2的结果一致。这听起来并不多，对于试图充分利用硬件的数据库而言，这非常有价值。</p><p> linux-aio: Throughput : 330 MB/s Lat average : 1549 usec Lat quantile= 0.5 : 1547 usec Lat quantile= 0.95 : 1694 usec Lat quantile= 0.99 : 1703 usec Lat quantile=0.999 : 1950 usec Lat max : 2177 usec</p><p> linux-aio： 吞吐率：330 MB / s 平均平均得分：1549 纬度分位数= 0.5：1547微秒 纬度分位数= 0.95：1694微秒 纬度分位数= 0.99：1703微秒 纬度分位数= 0.999：1950微秒 最大纬度：2177微秒</p><p> io_uring, with buffer and file registration and poll: Throughput : 346 MB/s Lat average : 1470 usec Lat quantile= 0.5 : 1468 usec Lat quantile= 0.95 : 1558 usec Lat quantile= 0.99 : 1613 usec Lat quantile=0.999 : 1674 usec Lat max : 1829 usec</p><p>io_uring，使用缓冲区和文件注册以及轮询： 吞吐量：346 MB / s 时空平均值：1470微秒 纬度分位数= 0.5：1468微秒 纬度分位数= 0.95：1558微秒 纬度分位数= 0.99：1613 usc 纬度分位数= 0.999：1674 USEC 最大纬度：1829年</p><p> Reading 512-byte buffers from an Intel Optane device from a single CPU. Parallelism of 1000 in-flight requests. There is very little difference between  linux-aio io_uring for the basic interface. But when advanced features are used, a 5% difference is seen.</p><p> 从单个CPU从Intel Optane设备读取512字节缓冲区。并行处理1000个进行中的请求。基本接口的linux-aio io_uring之间几乎没有什么区别。但是当使用高级功能时，可以看到5％的差异。</p><p> The  io_uring interface is advancing rapidly. For many of its features to come, it plans to rely on another earth-shattering new addition to the Linux Kernel: eBPF.</p><p> io_uring界面正在迅速发展。对于它的许多功能，它计划依赖Linux内核中另一项惊天动地的新功能：eBPF。</p><p>  eBPF stands for extended Berkeley Packet Filter. Remember  iptables? As the name implies, the original BPF allows the user to specify rules that will be applied to network packets as they flow through the network. This has been part of Linux for years.</p><p>  eBPF代表扩展的伯克利包过滤器。还记得iptables吗？顾名思义，原始BPF允许用户指定规则，这些规则将在网络数据包流经网络时应用于网络数据包。这已经是Linux的一部分了。</p><p> But when BPF got extended,   it allowed users to add code that is executed by the kernel in a safe manner in various points of its execution, not only in the network code.</p><p> 但是，当BPF扩展时，它允许用户以安全的方式在内核的各个执行点中添加内核执行的代码，而不仅仅是在网络代码中。</p><p> I will suggest the reader to pause here and read this sentence again, to fully capture its implications: You can execute arbitrary code in the Linux kernel now. To do essentially whatever you want.</p><p> 我建议读者在这里停下来再读一遍，以充分理解其含义：您现在可以在Linux内核中执行任意代码。基本上可以做任何您想做的事。</p><p> eBPF programs have types, which determine what they will attach to. In other words, which events will trigger their execution. The old-style packet filtering use case is still there. It’s a program of the  BPF_PROG_TYPE_SOCKET_FILTER type.</p><p> eBPF程序具有类型，这些类型确定它们将附加到的内容。换句话说，哪些事件将触发它们的执行。旧的数据包过滤用例仍然存在。这是BPF_PROG_TYPE_SOCKET_FILTER类型的程序。</p><p> But over the past decade or so, Linux has been accumulating an enormous infrastructure for performance analysis, that adds tracepoints and probe points almost everywhere in the kernel. You can attach a tracepoint, for example, to a syscall — any syscall — entry or return points. And through the  BPF_PROG_TYPE_KPROBE and  BPF_PROG_TYPE_TRACEPOINT types, you can attach bpf programs essentially anywhere.</p><p>但是在过去的十年左右的时间里，Linux一直在积累用于性能分析的庞大基础结构，该结构几乎在内核的所有位置都添加了跟踪点和探测点。例如，您可以将跟踪点附加到syscall（任何syscall）入口或返回点。通过BPF_PROG_TYPE_KPROBE和BPF_PROG_TYPE_TRACEPOINT类型，您可以在任何地方附加bpf程序。</p><p> The most obvious use case for this is performance analysis and monitoring. A lot of those tools are being maintained through the  bcc p</p><p> 最明显的用例是性能分析和监视。许多这些工具都通过密件抄送维护</p><p>......</p><p>......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/">https://www.scylladb.com/2020/05/05/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/编程/">#编程</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/uring/">#uring</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>