<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>带SHARK的Apple M1 MAX GPU上的PyTorch–比TensorFlow Metal更快PyTorch on Apple M1 MAX GPUs with SHARK – faster than TensorFlow-Metal</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">PyTorch on Apple M1 MAX GPUs with SHARK – faster than TensorFlow-Metal<br/>带SHARK的Apple M1 MAX GPU上的PyTorch–比TensorFlow Metal更快</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2022-02-24 21:21:58</div><div class="page_narrow text-break page_content"><p>In this blog we demonstrate  PyTorch Training and Inference on the Apple M1Max GPU with SHARK with only a few lines of additional code and outperforming Apple’s  Tensorflow-metal plugin.  Though Apple has released GPU support for Tensorflow via the now deprecated  tensorflow-macos plugin and the newer  tensorflow-metal plugin the most popular machine learning framework  PyTorch lacks GPU support on Apple Silicon. Until now.</p><p>在这个博客中，我们用SHARK演示了PyTorch在Apple M1Max GPU上的训练和推理，只需几行额外的代码，性能优于Apple的Tensorflow metal插件。尽管苹果已经通过现在不受欢迎的Tensorflow macos插件和更新的Tensorflow metal插件发布了对Tensorflow的GPU支持，但最流行的机器学习框架PyTorch在苹果硅上缺乏GPU支持。直到现在。</p><p> SHARK is built on  MLIR and  IREE and can target various hardware seamlessly. Since SHARK generates kernels on the fly for each workload you can port to new architectures like the M1Max without the vendor provided handwritten / hand tuned library.</p><p>SHARK基于MLIR和IREE构建，可以无缝瞄准各种硬件。由于SHARK为每个工作负载动态生成内核，因此您可以移植到新的体系结构，如M1Max，而无需供应商提供的手写/手动调整库。</p><p> Nod.ai has added AMD GPU support to be able to retarget the code generation for AMD MI100/MI200 class devices and move machine learning workloads from Nvidia V100/A100 to AMD MI100 seamlessly. In the past  we demonstrated better codegen than Intel MKL and Apache/OctoML TVM on Intel Alderlake CPUs and  outperforming Nvidia’s cuDNN/cuBLAS/CUTLASS used by ML frameworks such as Onnxruntime, Pytorch/Torchscript and Tensorflow/XLA. Today we demonstrate SHARK targeting  Apple’s 32 Core GPU in the M1Max with PyTorch Models for BERT Inference and Training. So if you love PyTorch and want to use those 32 GPU cores in your new Apple Silicon Macbook Pro read on.</p><p>点头ai增加了AMD GPU支持，能够重新定位AMD MI100/MI200类设备的代码生成，并将机器学习工作负载从Nvidia V100/A100无缝转移到AMD MI100。在过去，我们在Intel Alderlake CPU上展示了比Intel MKL和Apache/OctoML TVM更好的codegen，并优于Nvidia的cuDNN/cuBLAS/CUTLASS，后者被Onnxruntime、Pytorch/Torchscript和Tensorflow/XLA等ML框架使用。今天，我们将展示SHARK在M1Max中瞄准苹果的32核GPU，并使用PyTorch模型进行推理和训练。所以，如果你喜欢PyTorch，想在新的Apple Silicon Macbook Pro中使用32个GPU内核，请继续阅读。</p><p> For our experiment we will utilize a 14″ MacBook Pro with the Apple M1 Max with 64GB RAM. We have also run the same benchmarks on a 16″ MacBook Pro and notice the same performance and both don’t thermally throttle during our benchmarks.</p><p>在我们的实验中，我们将使用14英寸的MacBook Pro和64GB内存的Apple M1 Max。我们也在16英寸MacBook Pro上运行了相同的基准测试，并注意到了相同的性能，而且在我们的基准测试期间，两者都不会发生热节流。</p><p>     (base) anush@MacBook-Pro examples % ./shark-bench --module_file=minilm_jan6_m1max.vmfb --entry_function=predict --function_input=1x128xi32 --function_input=1x128xi32 --function_input=1x128xi32 --benchmark_repetitions=102022-02-20T10:17:55-08:00Running ./shark-benchRun on (10 X 24.1214 MHz CPU s)CPU Caches: L1 Data 64 KiB (x10) L1 Instruction 128 KiB (x10) L2 Unified 4096 KiB (x5)Load Average: 2.14, 1.91, 1.81-----------------------------------------------------------------------------------Benchmark Time CPU Iterations-----------------------------------------------------------------------------------BM_predict/process_time/real_time 11.7 ms 1.39 ms 58BM_predict/process_time/real_time 11.5 ms 1.34 ms 58BM_predict/process_time/real_time 11.7 ms 1.43 ms 58BM_predict/process_time/real_time 11.6 ms 1.30 ms 58BM_predict/process_time/real_time 11.5 ms 1.33 ms 58BM_predict/process_time/real_time 11.7 ms 1.46 ms 58BM_predict/process_time/real_time 11.6 ms 1.31 ms 58BM_predict/process_time/real_time 11.5 ms 1.33 ms 58BM_predict/process_time/real_time 11.7 ms 1.46 ms 58BM_predict/process_time/real_time 11.6 ms 1.30 ms 58 BM_predict/process_time/real_time_mean 11.6 ms  1.36 ms 10BM_predict/process_time/real_time_median 11.6 ms 1.34 ms 10BM_predict/process_time/real_time_stddev 0.074 ms 0.063 ms 10BM_predict/process_time/real_time_cv 0.64 % 4.65 % 10(base) anush@MacBook-Pro examples %</p><p>（基础）anush@MacBook-支持范例%/鲨鱼长凳——模块文件=minilm_jan6_m1max。vmfb——输入函数=预测——函数输入=1x128xi32——函数输入=1x128xi32——函数输入=1x128xi32——基准测试重复次数=102022-02-20T10:17:55-08:00运行/shark benchRun on（10 X 24.1214 MHz CPU）CPU缓存：L1数据64千字节（x10）L1指令128千字节（x10）L2统一4096千字节（x5）平均负载：2.14,1.91，1.81----------------------------------------------------------------基准时间CPU迭代----------------------------------------------------------------------BM_预测/处理时间/实时11.7毫秒1.39毫秒58BM_预测/处理时间/实时11.5毫秒1.34毫秒58BM_预测/处理时间/实时11.7毫秒1.43毫秒58BM_预测/处理时间/实时11.6毫秒1.30毫秒58BM_预测/处理时间/实时11.5毫秒1.33毫秒58BM_预测/处理时间/实时11.7毫秒1.46毫秒58BM_预测/处理时间/实时11.6毫秒1.31毫秒58BM_预测/处理时间/实时11.5毫秒1.33毫秒58BM_预测/处理时间/实时11.7毫秒1.46毫秒58BM_预测/过程时间/实时11.6ms 1.30ms 58BM_预测/过程时间/实时平均11.6ms 1.36ms 10BM_预测/过程时间/实时中位数11.6ms 1.34ms 10BM_预测/过程时间/实时标准差0.074ms 0.063ms 10BM_预测/过程时间/实时cv 0.64%4.65%10（基数）anush@MacBook-专业范例%</p><p> Arguments: Namespace(models=[&#39;microsoft/MiniLM-L12-H384-uncased&#39;], model_source=&#39;pt&#39;, model_class=None, engines=[&#39;tensorflow&#39;], cache_dir=&#39;./cache_models&#39;, onnx_dir=&#39;./onnx_models&#39;, use_gpu=True, precision=&lt;Precision.FLOAT32: &#39;fp32&#39;&gt;, verbose=False, overwrite=False, optimize_onnx=False, validate_onnx=False, fusion_csv=&#39;fusion.csv&#39;, detail_csv=&#39;detail.csv&#39;, result_csv=&#39;result.csv&#39;, input_counts=[1], test_times=1000, batch_sizes=[1], sequence_lengths=[128], disable_ort_io_binding=False, num_threads=[10])Metal device set to: Apple M1 MaxsystemMemory: 64.00 GBmaxCacheSize: 24.00 GBAll model checkpoint layers were used when initializing TFBertModel.All the layers of TFBertModel were initialized from the model checkpoint at microsoft/MiniLM-L12-H384-uncased.If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training. Run Tensorflow on microsoft/MiniLM-L12-H384-uncased with input shape [1, 128]{&#39;engine&#39;: &#39;tensorflow&#39;, &#39;version&#39;: &#39;2.8.0&#39;, &#39;device&#39;: &#39;cuda&#39;, &#39;optimizer&#39;: &#39;&#39;, &#39;precision&#39;: &lt;Precision.FLOAT32: &#39;fp32&#39;&gt;, &#39;io_binding&#39;: &#39;&#39;, &#39;model_name&#39;: &#39;microsoft/MiniLM-L12-H384-uncased&#39;, &#39;inputs&#39;: 1, &#39;threads&#39;: 10, &#39;batch_size&#39;: 1, &#39;sequence_length&#39;: 128, &#39;datetime&#39;: &#39;2022-02-21 07:48:04.965312&#39;, &#39;test_times&#39;: 1000, &#39;latency_variance&#39;: &#39;0.00&#39;, &#39;latency_90_percentile&#39;: &#39;19.25&#39;, &#39;latency_95_percentile&#39;: &#39;22.10&#39;, &#39;latency_99_percentile&#39;: &#39;23.14&#39;, &#39; average_latency_ms&#39;: &#39;16.99&#39;, &#39;QPS&#39;: &#39;58.85&#39;}</p><p>参数：名称空间（models=[&#39；microsoft/MiniLM-L12-H384-uncased&#39；]，模型#源=&#39；第39页；，model#class=None，引擎=[&#39；tensorflow&#39；]，cache_dir=&#39/cache#u模型&#39；，onnx_dir=&#39/onnx#U型号&#39；，使用gpu=True，精度=&lt；精确浮动32:&#39；fp32和#39&gt；，verbose=False，overwrite=False，optimize_onnx=False，validate_onnx=False，fusion_csv=&#39；融合csv和#39；，细节_csv=#39；细节csv和#39；，结果_csv=&#39；后果csv和#39；，输入计数=[1]，测试次数=1000，批量大小=[1]，序列长度=[128]，禁用端口io绑定=假，线程数=[10]）金属设备设置为：Apple M1 MaxsystemMemory:64.00 GBmaxCacheSize:24.00 GBAll model检查点层在初始化TFModel时使用。TFBertModel的所有层都是从位于microsoft/MiniLM-L12-H384-uncased的模型检查点初始化的。如果您的任务与检查点模型训练的任务类似，则无需进一步训练即可使用TFBertModel进行预测。在microsoft/MiniLM-L12-H384-uncased上使用输入形状[1128]运行Tensorflow39；39；39；39；39；39；39；39；39；39；39；39；39；39；39；版本和39；39；39；39；39；39；39；39；39；版本和39；39；39；39；39；39；39；39；39；39；39；39；39；发动机和发动机和39；39；39；发动机和39；39；发动机和39；39；39；39；39；39；39；39；39；39；39；39；39；39；39；39；39；版本和39；版本和39；39；39；版本和39；39；39；39；版本和39；39；39；39；39；版本和39；39；39；39；39；版本和39；39；39；版本和39；39；39；版本和；39；39；39；39；39；39；39；39；39；版本和与与与39；39；39；39；39；39；39；39；版本和和和和和；#39；fp32和#39；&#39；io#U绑定和#39；&#39；&#39；&#模型#名称和#39；&#39；microsoft/MiniLM-L12-H384-uncated和#39；&#输入和#39；：1和#39；线程和#10, &#39;批量大小&#39；：1, &#39;序列长度&#39；：128, &#39;日期时间&#39；：&#39;2022-02-21 07:48:04.965312&#39;, &#39;测试次数&#39；：1000, &#39;延迟变化&#39；：&#39;0.00&#39;, &#39;潜伏期90%和39%&#39;19.25&#39;, &#39;潜伏期95%和39%&#39;22.10&#39;, &#39;潜伏期99_39；：&#39;23.14&#39;, &#39; 平均延迟(毫秒#39；：&#39;16.99&#39;, &#39;QPS&#39；：&#39;58.85&#39;}</p><p>           In our tests we noticed the Tensorflow-metal plugin doesn’t seem to offload the backwards graph onto the GPU efficiently. Since Tensorflow-metal is a binary only release we have no way to debug it. The same  Tensorflow implementation works well to offload onto CUDA GPUs.</p><p>在我们的测试中，我们注意到Tensorflow金属插件似乎无法有效地将反向图形卸载到GPU上。由于Tensorflow metal是一个仅二进制版本，我们无法调试它。同样的Tensorflow实现可以很好地卸载到CUDA GPU上。</p><p> (base) anush@MacBook-Pro examples % ./shark-bench --module_file=bert_training_feb17.vmfb --function_input=1x512xi32 --function_input=1x512xi32 --function_input=1x512xi32 --function_input=1xi32 --entry_function=learn --benchmark_repetitions=102022-02-20T23:04:22-08:00Running ./shark-benchRun on (10 X 24.2416 MHz CPU s)CPU Caches: L1 Data 64 KiB (x10) L1 Instruction 128 KiB (x10) L2 Unified 4096 KiB (x5)Load Average: 2.03, 2.53, 2.31---------------------------------------------------------------------------------Benchmark Time CPU Iterations---------------------------------------------------------------------------------BM_learn/process_time/real_time 104 ms 16.2 ms 5BM_learn/process_time/real_time 104 ms 16.1 ms 5BM_learn/process_time/real_time 105 ms 15.3 ms 5BM_learn/process_time/real_time 104 ms 16.0 ms 5BM_learn/process_time/real_time 103 ms 15.2 ms 5BM_learn/process_time/real_time 105 ms 16.9 ms 5BM_learn/process_time/real_time 104 ms 15.5 ms 5BM_learn/process_time/real_time 104 ms 14.9 ms 5BM_learn/process_time/real_time 105 ms 17.4 ms 5BM_learn/process_time/real_time 105 ms 15.7 ms 5 BM_learn/process_time/real_time_mean 104 ms  15.9 ms 10BM_learn/process_time/real_time_median 104 ms 15.8 ms 10BM_learn/process_time/real_time_stddev 0.830 ms 0.774 ms 10BM_learn/process_time/real_time_cv 0.80 % 4.86 % 10(base) anush@MacBook-Pro examples %</p><p>（基础）anush@MacBook-支持范例%/shark bench——模块文件=bert_training_feb17。vmfb——函数输入=1x512xi32——函数输入=1x512xi32——函数输入=1x512xi32——函数输入=1xi32——输入函数=学习——基准测试重复次数=102022-02-20T23:04:22-08:00运行/（10 X 24.2416 MHz CPU）CPU缓存上的shark benchRun：L1数据64千字节（x10）L1指令128千字节（x10）L2统一4096千字节（x5）平均负载：2.03,2.53，2.31----------------------------------------------------------------基准时间CPU迭代----------------------------------------------------------------------BM_学习/处理时间/实时104毫秒16.2毫秒5BM_学习/处理时间/实时104毫秒16.1毫秒5BM_学习/处理时间/实时105毫秒15.3毫秒5BM_学习/过程时间/实时104毫秒16.0毫秒5BM_学习/过程时间/实时103毫秒15.2毫秒5BM_学习/过程时间/实时105毫秒16.9毫秒5BM_学习/过程时间/实时104毫秒15.5毫秒5BM_学习/过程时间/实时104毫秒14.9毫秒5BM学习/过程时间/实时105毫秒17.4毫秒5BM_学习/过程时间/实时105毫秒15.7毫秒5BM_学习/过程时间/实时平均104毫秒15.9毫秒10BM_学习/过程时间/实时平均104毫秒15.8毫秒10BM_学习/过程时间/实时标准差0.830毫秒0.774毫秒10BM_学习/过程时间/实时标准差0.80%4.86%10（基数）anush@MacBook-专业范例%</p><p> (base) anush@MacBook-Pro nlp_models % python ./bert_small_tf_run.pyMetal device set to: Apple M1 MaxsystemMemory: 64.00 GBmaxCacheSize: 24.00 GB2022-02-21 07:58:36.593857: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.2022-02-21 07:58:36.594010: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)1 Physical GPUs, 1 Logical GPUModel: &#34;bert_classifier&#34;__________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_word_ids (InputLayer) [(None, None)] 0 [] input_mask (InputLayer) [(None, None)] 0 [] input_type_ids (InputLayer) [(None, None)] 0 [] bert_encoder_1 (BertEncoder) [(None, None, 768), 15250176 [&#39;input_word_ids[0][0]&#39;, (None, 768)] &#39;input_mask[0][0]&#39;, &#39;input_type_ids[0][0]&#39;] dropout_1 (Dropout) (None, 768) 0 [&#39;bert_encoder_1[0][1]&#39;] sentence_prediction (Classific (None, 5) 3845 [&#39;dropout_1[0][0]&#39;] ationHead) ==================================================================================================Total params: 15,254,021Trainable params: 15,254,021Non-trainable params: 0__________________________________________________________________________________________________...time: 1.7728650569915771 time/iter: 0.19698500633239746</p><p>（基础）anush@MacBook-Pro nlp_模型%python/bert_small_tf_run。pyMetal设备设置为：Apple M1 MaxsystemMemory:64.00 GBmaxCacheSize:24.00 GB2022-02-21 07:58:36.593857:I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory。cc:305]无法识别平台GPU ID 0的NUMA节点，默认为0。您的内核可能没有使用NUMA支持构建。2022-02-21 07:58:36.594010:I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory。cc:271]创建了TensorFlow设备（/job:localhost/replica:0/task:0/device:GPU:0，内存为0 MB）&gt；物理可插拔设备（设备：0，名称：METAL，pci总线id:&lt；未定义&gt；）1个物理GPU，1个逻辑GPU模型：&#34；目前，34日，34日，34日，34日，3日，3日，3日，3日，34日，3日，3日，34日，4日，3日，3日，3日，3日，3日，3日，34日，34日，3日，34日，3日，3日，34日，34日，3日，3月月，UUUU厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄厄_____===========================================================================================================================礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼礼元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元15250176[&#39；输入单词ID[0][0]&#39；，（无768人）&#39人；输入_掩码[0][0]&#39&#39;输入_type_id[0][0]&#39；]辍学1（辍学）（无，768）0[&#39；bert#u编码器#1[0][1]&#39；]句子预测（分类（无，5）3845[&#39；辍学者#1[0][0]&#39；]1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1月1_。。。时间：1.7728650569915771时间/iter:0.19698500633239746</p><p>       Power measurements need to be done in a very controlled and instrumented environment. However this is a good approximation for running the same BERT training model on an A100 and M1MAX.</p><p>功率测量需要在非常受控和仪表化的环境中进行。然而，对于在A100和M1MAX上运行相同的伯特训练模型来说，这是一个很好的近似值。</p><p> The A100 draws 131W peak during the training run, the M1MAX GPU draws a maximum of 15.4 W. The A100 runs an iteration at 7ms vs 104ms (with SHARK) and 196ms (with TF-Metal). Since TF-Metal doesn’t offload the Training graph well onto the GPU we remove it from our perf/watt comparisons.</p><p>A100在训练过程中消耗131W的峰值，M1MAX GPU消耗的最大功率为15.4W。A100在7ms与104ms（使用SHARK）和196ms（使用TF Metal）之间进行迭代。由于TF Metal不能很好地将训练图卸载到GPU上，我们将其从性能/功率比较中删除。</p><p> Mon Feb 21 00:56:33 2022 +-----------------------------------------------------------------------------+| NVIDIA-SMI 470.57.02 Driver Version: 470.57.02 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA A100-SXM... Off | 00000000:00:04.0 Off | 0 || N/A 36C P0  131W / 350W | 39341MiB / 40536MiB |  53% Default || | | Disabled |+-------------------------------+----------------------+----------------------+</p><p>2022年2月21日星期一00:56:33+---------------------------------------------------------------+|NVIDIA-SMI 470.57.02驱动程序版本：470.57.02 CUDA版本：11.4 | | CUDA版本：11.4 | GPU名称持久化-M |总线Id显示。A |挥发性解甲。ECC | | |风扇温度性能压水堆：使用率/上限|内存使用率| GPU Util Compute M.| | | | | | |====================================================================================================================================================================================================================================================================================。。。Off |00000000:00:04.0 Off | 0 | | | | | | | N/A 36C P0131W/350W | 39341MiB/40536MiB | 53%默认| | | | | | |禁用|+-------------------------------+----------------------+----------------------+</p><p> samples/ModelCompiler/nlp_models$ python bert_small_tf_run.py..Model: &#34;bert_classifier&#34;__________________________________________________________________________________________________Layer (type) Output Shape Param # Connected to==================================================================================================input_word_ids (InputLayer) [(None, None)] 0__________________________________________________________________________________________________input_mask (InputLayer) [(None, None)] 0__________________________________________________________________________________________________input_type_ids (InputLayer) [(None, None)] 0__________________________________________________________________________________________________bert_encoder_1 (BertEncoder) [(None, None, 768), 15250176 input_word_ids[0][0] input_mask[0][0] input_type_ids[0][0]__________________________________________________________________________________________________dropout_1 (Dropout) (None, 768) 0 bert_encoder_1[0][1]__________________________________________________________________________________________________sentence_prediction (Classifica (None, 5) 3845 dropout_1[0][0]==================================================================================================Total params: 15,254,021Trainable params: 15,254,021Non-trainable params: 0__________________________________________________________________________________________________.. time: 6.907362699508667 time/iter: 0.006977134039907744</p><p>samples/ModelCompiler/nlp_models$python bert_small_tf_run。皮耶。。型号：&#34；bert#u分类器&#34__________________________________________________________________________________________________====================================================================================================================================================================================礼礼礼礼礼礼礼元（类型））输出形状（类型）输出（类型）输出形状（类型）层（类型）输出（类型）形状）层（类型）输出（类型）形状）形形形形参数参数参数（类型）层（类型）元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元元UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢卢_________________________________1.0[0 0[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8月月UU\UUUUUUU\\UUUUUUUUUUUU\\UUUUUUUUUUUUU\\\UUUUUUUUUUUUUUUUUU\\\\UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU（无，无，768）无）0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 8 8 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8__________________;句子预测（Classifica（None，5）3845辍学者_1[0][0]================================================================================================================================================================================================================================================================================================================================================================================================================UUUUUUU（UUUUUU（据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据据UUUUUU月月月月月月月月月月月月月月月月（UUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUUU。。时间：6.907362699508667时间/iter:0.006977134039907744</p><p> Package Power: 2.48W (avg: 2.34W peak: 37.78W) throttle: no  CPU: 1.37W (avg: 1.03W peak: 32.32W)   GPU: 0.00W (avg: 0.00W peak: 15.41W)</p><p>封装功率：2.48W（平均功率：2.34W峰值：37.78W）油门：无CPU：1.37W（平均功率：1.03W峰值：32.32W）GPU：0.00W（平均功率：0.00W峰值：15.41W）</p><p>  The SHARK Runtime is available as pip package and requires a few lines of code changes in your Pytorch/Python file. Here is an example of running  Resnet50 and  BERT from Python using SHARK and we plan to add more Python examples there.  For the experiments in the post we will use a benchmark binary built with  Google Benchmark support to run a controlled experiment though calling from Python adds a few microseconds. We test a  BERT MiniLM Model (used in  HuggingFace’s Infinity demos) and a  BERT Training model. You can also try  bert-base-uncased, Resnet50, Mobilenetv3 etc and all the models are automatically offloaded to the GPU via torch-mlir.</p><p>SHARK运行时以pip包的形式提供，需要在Pytorch/Python文件中更改几行代码。下面是一个使用SHARK从Python运行Resnet50和BERT的示例，我们计划在这里添加更多Python示例。对于本文中的实验，我们将使用一个使用谷歌基准测试支持构建的基准二进制文件来运行一个受控实验，尽管从Python调用会增加几微秒。我们测试了一个BERT MiniLM模型（用于HuggingFace的无限演示）和一个BERT训练模型。您还可以尝试bert base uncased、Resnet50、Mobilenetv3等，所有型号都会通过torch mlir自动卸载到GPU。</p><p> The Torch-MLIR lowering for the BERT training graph is being integrated in this  staging branch.  All the code the recreate the tests are  here and  here. However you will need to install PyTorch  torchvision from source since torchvision  doesn’t have support for M1 yet. You will also need to build SHARK from the  apple-m1-max-support branch from the  SHARK repository.</p><p>BERT训练图的火炬MLIR降低正在该阶段分支中集成。所有重新创建测试的代码都在这里和这里。但是，由于torchvision还不支持M1，您需要从源代码处安装PyTorch torchvision。您还需要从SHARK存储库的apple-m1-max-support分支构建SHARK。</p><p> (base) anush@MacBook-Pro examples % pip list | grep tensorflowtensorflow-estimator 2.6.0tensorflow-macos 2.8.0tensorflow-metal 0.3.0(base) anush@MacBook-Pro examples % pip list | grep onnx onnx 1.10.1onnxconverter-common 1.8.1(base) anush@MacBook-Pro examples % pip list | grep tensortensorboard 2.8.0tensorboard-data-server 0.6.0tensorboard-plugin-wit 1.8.0tensorflow-estimator 2.6.0tensorflow-macos 2.8.0tensorflow-metal 0.3.0(base) anush@MacBook-Pro examples % pip list | grep torch torch 1.10.0torchvision 0.9.0a0(base) anush@MacBook-Pro examples % pip list | grep onnxy(base) anush@MacBook-Pro examples % pip list | grep onnx onnx 1.10.1onnxconverter-common 1.8.1(base) anush@MacBook-Pro examples %</p><p>（基础）anush@MacBook-Pro示例%pip列表| grep TensorFlow TensorFlow估计器2.6.0传感器流量-macos 2.8.0传感器流量-metal 0.3.0（基本）anush@MacBook-专业示例%pip list | grep onnx onnx 1.10.1非通用1.8.1（基本）anush@MacBook-专业示例%pip list | grep tensortensortensorboard 2.8.0tensorboard-data-server 0.6.0tensorboard-plugin-wit 1.8.0tensorflow-estimator 2.6.0tensorflow-macos 2.8.0tensorflow-metal 0.3.0（基本）anush@MacBook-专业示例%pip列表| grep火炬1.10.0或视觉0.9.0a0（基本）anush@MacBook-专业示例%pip列表| grep onnxy（基本）anush@MacBook-专业示例%pip list | grep onnx onnx 1.10.1非通用1.8.1（基本）anush@MacBook-专业范例%</p><p> We use  asitop to monitor the Power Usage, GPU Usage, Core throttling etc which can be installed via pip.</p><p>我们使用asitop监控电源使用情况、GPU使用情况、内核节流等，这些都可以通过pip安装。</p><p> Apple’s CoreML has the ability to target not just the CPU or GPU but also the Apple Neural Engine though only for inference. We did try to get CoreML to work for the inference comparison but ran into model conversion issues  here and excluded it from the tests. The latest coremltools seems to require an older Tensorflow version 2.5.0 which is no longer the default when you install tensorflow with conda.</p><p>苹果的CoreML不仅能够针对CPU或GPU，还能够针对苹果的神经引擎，尽管只是用于推理。我们确实试图让CoreML进行推理比较，但在这里遇到了模型转换问题，并将其排除在测试之外。最新的coremltools似乎需要旧的Tensorflow版本2.5.0，这不再是使用conda安装Tensorflow时的默认版本。</p><p> The OSX Window Server  crashes when all GPUs are used to the maximum. We have filed a Feedback issue with Apple on this bug and hope it will be resolved soon but when recreating results we recommend ssh into the MacBook Pro and don’t turn on the display.</p><p>当所有GPU被最大限度地使用时，OSX窗口服务器崩溃。我们已经就这个错误向苹果公司提出了反馈问题，希望它能很快得到解决，但在重新创建结果时，我们建议在MacBook Pro中使用ssh，不要打开显示屏。</p><p> All the code in these repositories are very much work in progress and shown as technical previews which requires some level of polish before they are ready for non-technical users to be able to use. We plan to continue making it user friendly and add eager mode support to Torch-MLIR so PyTorch support on M1Max GPUs works out the box for seamless development to deployment. If putting all the open source pieces is not your thing or if you have a business case for deploying PyTorch with GPU support on M1 devices today and professional solution please sign up  here and our solutions team will reach out to help.</p><p>这些存储库中的所有代码都是正在进行的工作，并以技术预览的形式显示，这需要一定程度的润色，然后才能供非技术用户使用。我们计划继续使其用户友好，并为Torch MLIR添加渴望模式支持，因此M1Max GPU上的PyTorch支持为无缝开发和部署提供了条件。如果你不想把所有的开源部分都放在自己的位置上，或者如果你有在M1设备上部署支持GPU的PyTorch和专业解决方案的商业案例，请在这里注册，我们的解决方案团队将伸出援助之手。</p><p>     Using SHARK Runtime, we demonstrate high performance PyTorch models on Apple M1Max GPUs. It outperforms Tensorflow-Metal by 1.5x for inferencing and 2x in training BERT models. In the near future we plan to enhance end user experience and add “eager” mode support so it is seamless from development to deployment on any hardware.</p><p>使用SHARK Runtime，我们在Apple M1Max GPU上演示了高性能的PyTorch模型。它比Tensorflow Metal的推理能力强1.5倍，训练BERT模型能力强2倍。在不久的将来，我们计划增强最终用户体验，并添加“渴望”模式支持，以便在任何硬件上实现从开发到部署的无缝连接。</p><p> If you would like access to the commercial version of SHARK Runtime sign up for access  here and if you have trouble recreating results please open an  issue.</p><p>如果您想访问SHARK Runtime的商业版，请在此处注册访问，如果您在重新创建结果时遇到问题，请打开一个问题。</p><p>  SHARK is built on open source packages  Torch-MLIR,   LLVM/MLIR and  Google IREE and we are thank for all developers and community for their support. We specifically want to call out Ben Vanik, Lei Zhang, Stella Laurenzo and Thomas Raoux for their help, support and guidance on the MLIR/IREE GPU codegen paths.</p><p>SHARK建立在开源软件包Torch MLIR、LLVM/MLIR和Google iRE之上，我们感谢所有开发者和社区的支持。我们特别希望呼吁本·瓦尼克、张磊、斯特拉·劳伦佐和托马斯·拉乌克斯在MLIR/IREE GPU代码生成路径上提供帮助、支持和指导。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/gpu/">#gpu</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1072795.html"><img src="http://img2.diglog.com/img/2022/2/thumb_67e83f3c7a76a3f338b277f9e33f520f.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1072795.html">苹果将通过让未知的标签更容易找到来打击航空标签跟踪</a></div><span class="my_story_list_date">2022-2-13 3:53</span></div><div class="col-sm"><div><a target="_blank" href="/story/1072794.html"><img src="http://img2.diglog.com/img/2022/2/thumb_9def2843fff3e285b7f9f9b0431edf43.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1072794.html">苹果修复了软件更新中Mac电池耗尽、WebKit漏洞</a></div><span class="my_story_list_date">2022-2-13 3:51</span></div><div class="col-sm"><div><a target="_blank" href="/story/1072738.html"><img src="http://img2.diglog.com/img/2022/1/thumb_f1c654ea53b78f1e4a6ef41536f304cf.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1072738.html">Apple允许韩国第三方应用付款选项</a></div><span class="my_story_list_date">2022-1-11 15:58</span></div><div class="col-sm"><div><a target="_blank" href="/story/1072730.html"><img src="http://img2.diglog.com/img/2022/1/thumb_a25414c340ca8beb7ea56829ba2f6456.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1072730.html">作为Uber和Didi的99款与驾驶员短缺和巴西的高燃料价格，由Rio City政府开发的一个应用程序，乘坐骑手</a></div><span class="my_story_list_date">2022-1-11 13:34</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>