<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>许多模型在Python中的工作流程：第一部分 Many models workflows in Python: Part I</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Many models workflows in Python: Part I<br/>许多模型在Python中的工作流程：第一部分 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-03-30 00:12:34</div><div class="page_narrow text-break page_content"><p>This summer I worked on my first substantial research project in Python. I’ve used Python for a number of small projects before, but this was the first time that it was important for me to have an efficient workflow for working with many models at once. In R, I spend almost all of my time using a ‘many models’ workflow that leverages list-columns in  tibbles and a hefty amount of  tidyverse manipulation. It’s taken me a while to find a similar workflow in Python that I like, and so I’m documenting it here for myself and other parties who might benefit.</p><p>今年夏天，我在Python的第一个实质性研究项目上工作。我以前使用过Python进行了多个小项目，但这是第一次对我来说很重要，以便一次与许多模特一起工作。在R中，我几乎所有的时间都使用了一个“许多模型”工作流程，它利用划分的列表列和巨大量的整个流行操作。它让我有一段时间在Python中找到一个类似的工作流，所以我正在为自己和可能受益的其他方记录在这里。</p><p> This post demonstrates how to organize models into dataframes for exploratory data analysis, and discusses why you might want to do this. In a future blog post I will show how to extend the basic workflow I present here to handle sample splitting, custom estimators, and parallel processing.</p><p> 此帖子演示如何将模型组织到DataFrame以进行探索性数据分析，并讨论您可能想要执行此操作的原因。在未来的博客文章中，我将展示如何扩展我在此处展示的基本工作流程以处理样本拆分，自定义估算器和并行处理。</p><p> The many models workflow is an extension of the ‘split-apply-combine’ workflow, a largely functional approach to data manipulation implemented in  dplyr and  pandas. The essential ideas of ‘split-apply-combine’ are articulated nicely in Hadley Wickham’s short and easy to read  paper, which I strongly encourage you to read. I assume you are comfortable with this general idea, which largely facilitates natural ways to compute descriptive statistics from data, and have some experience applying these concepts in either  dplyr,  pandas, or SQL.</p><p> 许多型号的工作流程是“拆分 - 应用组合”工作流程的扩展，在D次和Pandas中实现的数据操作的主要功能方法。 “拆分 - 申请结合”的基本思想在哈德利威克姆的短语中呈现出来，易于阅读纸张，我强烈鼓励您阅读。我假设您对这一普遍的想法感到满意，这在很大程度上促进了从数据中计算描述性统计数据的自然方式，并且在DOPLER，PANDAS或SQL中有一些体验应用这些概念。</p><p> Several years ago, this idea evolved: what if, instead of computing descriptive statistics, we wanted to compute more complicated estimands, while leveraging the power of grouped operations? Hadley’s solution, which has proven very fruitful and served as the underlying idea for  tidymodels,  tidyverts, and several other modeling frameworks, is to put model objects themselves into dataframes. Hadley  presented on this idea, and also wrote about it in the  many models chapter of  R for Data Science and it has turned out to be quite fruitful.</p><p> 几年前，这个想法进化了：如果，而不是计算描述性统计，我们想要计算更复杂的估算量，而利用分组操作的力量？ Hadley的解决方案已被证明是非常富有成效的，并作为TidyModels，Tidyverts和其他几个建模框架的潜在想法是将模型对象本身放入DataFrame中。 Hadley介绍了这个想法，也在r的r的许多模型章节中撰写了关于数据科学的章节，它已成为相当富有成效的。</p><p> The simple answer is that it keeps information about your models from drifting apart, as it tends to do otherwise.</p><p> 简单的答案是它会使您的模型与漂移分开的信息，因为它往往否则。</p><p>  I want to compare models across a range of hyperparameter values. Often there are several distinct hyperparameters to consider at once  1.</p><p>  我想比较一系列封立的超参数值。经常有几个不同的超参数一次考虑1。</p><p> I want to look at many different properties of the model. As a contrived example, I might want to look at AIC, BIC,  \(R^2\) and RMSE for all my models  2.</p><p> 我想看看模型的许多不同的属性。作为一个创意的例子，我可能希望查看所有模型2的AIC，BIC，\（R ^ 2 \）和RMSE。 </p><p> I want to quick create plots to compare these models, and am probably using a plotting libraries expects data in data frames</p><p>我想快速创建图表来比较这些模型，并且我可能是使用绘图库期望数据帧中的数据</p><p> Anyway it turns out that dataframes handle this use case super well, provided we have some helpers. The overall workflow will look like this:</p><p> 无论如何，它就证明，只要我们有一些帮助者，Dataframes处理这个用例。整体工作流程将如下所示：</p><p> Specifying models to fit: We organize a dataframe so that each row of the dataframe corresponds to one model we want to fit. For example, a single row of the dataframe might correspond to a single set of hyperparameter values, or a subset of the data.</p><p> 指定模型以适合：我们组织DataFrame，以便数据帧的每一行对应于我们想要适合的一个型号。例如，DataFrame的单行可以对应于单个的超参数值或数据的子集。</p><p> Iterative model fitting: We create a new column in the dataframes that holds fit models.</p><p> 迭代模型拟合：我们在包含适合模型的DataFrame中创建一个新列。</p><p> Iterative estimate extraction: We extract information we want from the fits into new data frame columns, and then manipulate this data with our favorite dataframe or plotting libraries.</p><p> 迭代估计提取：我们从适合中提取我们想要的信息进入新的数据帧列，然后使用我们喜欢的DataFrame或绘图库操纵此数据。</p><p> Note that steps (2) and (3) require iteration over many models. In functional languages,  map-style operations are a natural (and easily parallelizable!) way to do this; in Python we can use list-comprehensions.</p><p> 请注意，步骤（2）和（3）需要在许多模型上进行迭代。在功能语言中，地图式操作是一种自然的（且易于平行化！）方式来做这件事;在Python中，我们可以使用列表 - 全局。</p><p> Another innovation in this workflow came from standardizing step (3), where we extract information from the models into a new column of the dataframe. A big issue that we can run into here is that when we extract information from the model object, it can have an inconvenient type that is hard to put in a dataframe column. This may seem esoteric but it turns out to matter a lot more than you’d expect.</p><p> 此工作流程中的另一个创新来自标准化步骤（3），其中我们将模型中的信息从模型中提取到DataFrame的新列。我们可以遇到的一个大问题是，当我们从模型对象中提取信息时，它可能会产生不方便的类型，很难放入DataFrame列中。这似乎是疏远的，但它比你预期的更多更重要。 </p><p> David Robinson in his  broom package proposed a solution that is increasingly the standard within the R community  3. The idea is to create special getter methods for model objects that always return information in consistently formatted data frames. For each model object, we get a data frame with information. Since there are many model objects, we end up with a column of dataframes, which we then flatten.</p><p>他的扫帚包中的David Robinson提出了一个解决方案，该解决方案是R社区中的标准。这个想法是为模型对象创建特殊的吸气方法，以始终以一致格式的数据帧返回信息。对于每个模型对象，我们使用信息获取数据帧。由于有许多模型对象，我们最终结束了一列DataFrame，我们然后达到平坦。</p><p> There has been a lot of pushback around the idea of a column of dataframes with the Python community, largely on the basis that it is not a performant data structure  4. This misses the point. The compute time in these workflows comes from model fitting, and afterwards we just want to keep track of things  5.</p><p> 围绕Python社区的一列Dataframe的想法有很多次要，主要是在这不是表演数据结构4的基础上。这是错过了这一点。这些工作流程中的计算时间来自模型配件，之后我们只想跟踪事物5。</p><p> Anyway, there’s a lot more to say about the workflow conceptually, but for now, it is time to show some code and see how things work in practice. For this post, I am going to recreate the analysis that Hadley does in his linked presentation above, which makes use of the  gapminder data. The gapminder data consists of life expectancies for 142 counties, reported every 5 years from 1952 to 2007.</p><p> 无论如何，还有很多关于概念上工作流程的更多信息，但现在，现在是时候展示了一些代码，看看在实践中如何运作。对于这篇文章来说，我将重新创建哈德利在他的联系呈现中的分析，这是利用GapMinder数据。 GapMinder数据由1952年至2007年的每5年报告每5年的人寿期预期。</p><p> import matplotlib.pyplot as pltimport pandas as pdimport seaborn as snsimport statsmodels.formula.api as smf# you can download the gapminder csv file from my google drive# https://drive.google.com/file/d/1pAgIKdsZPwYteQ2rfq14WrUtS23Tg4Lu/gapminder = pd.read_csv(&#39;../data/gapminder.csv&#39;)gapminder</p><p> 导入matplotlib.pyplot作为pltimport熊猫作为pdimport seabors作为snsimport statsmodels.formula.api作为smf＃您可以从我的google drive＃https://drive.google.com/file/d/1pagikdszpwytq2rfq14wruts23tg4lu / gapminder = pd.read_csv（＆＃39; ../ data / gapminder.csv＆＃39;）gapminder</p><p> ## country continent year lifeExp pop gdpPercap## 0 Afghanistan Asia 1952 28.801 8425333 779.445314## 1 Afghanistan Asia 1957 30.332 9240934 820.853030## 2 Afghanistan Asia 1962 31.997 10267083 853.100710## 3 Afghanistan Asia 1967 34.020 11537966 836.197138## 4 Afghanistan Asia 1972 36.088 13079460 739.981106## ... ... ... ... ... ... ...## 1699 Zimbabwe Africa 1987 62.351 9216418 706.157306## 1700 Zimbabwe Africa 1992 60.377 10704340 693.420786## 1701 Zimbabwe Africa 1997 46.809 11404948 792.449960## 1702 Zimbabwe Africa 2002 39.989 11926563 672.038623## 1703 Zimbabwe Africa 2007 43.487 12311143 469.709298## ## [1704 rows x 6 columns]</p><p> ##乡村大陆流行GDPPERCAP ## 0 AFGHANISTAN亚洲1952年28.801 8425333 779.445314 ## 1阿富汗亚洲1962 31.997 10267083 853.100710＃3 AFGHANISTAN ASIA 1967 34.020 11537966 836.100710303/297193030303030303030303030303030303030303030197 36.088 13079460 739.981106 ## ...... ...... ...... ...... ## 1699津巴布韦非洲1987 62.351 9216418 706.157306/17306/57306 ## 1700津巴布韦非洲1992 60.377 10704340 693.420786 ## 1701津巴布韦非洲1997年46.809 11404948 792.449960 ## 1702津巴布韦非洲2002 39.989 11926563 672.038623 ## 1703津巴布韦非洲2007 43.487 12311143 469.709298 #### [1704行x 6列]</p><p> To get a feel for the data, lets plot the life expectancy of each country over time, facetting by continent  6.</p><p> 为了让数据感受，让我们随着时间的推移绘制每个国家的预期寿命，由大陆的大陆平静。</p><p> Following Hadley’s presentation, suppose we would like to summarize the trend for each country by fitting a linear regression to the data from each country. So we have a correspondence 1 model ~ data from 1 country, and want to set up our data frame so that each row corresponds to data from a single country.</p><p> 在Hadley的演示之后，假设我们希望通过对来自每个国家的数据进行线性回归来总结每个国家的趋势。因此，我们有一个通信1〜来自1个国家的模型〜数据，并希望设置数据框架，以便每行对应来自单个国家/地区的数据。 </p><p> groupby() plus a list-comprehension handles this nicely, leveraging the fact that  gapminder.groupby(&#39;country&#39;) is an iterable. In R, you could also use  group_by() for this step, or additionally  nest() or  rowwise(), two  tidyverse specification abstractions.</p><p>groupby（）加上一个列表 - 理解良好地处理，利用GapMinder.groupby（＆＃39;国家/地区＆＃39;）是一个迭代的事实。在r中，您还可以为此步骤使用Group_by（），或者另外嵌套（）或rowwork（），两个tidyverse规范抽象。</p><p> models = pd.DataFrame({ # this works because grouped dataframes in pandas are iterable # and because you can pretty much treat series objects like # they are lists &#39;data&#39;: [data for _, data in gapminder.groupby(&#39;country&#39;)],})models.index = [country for country, _ in gapminder.groupby(&#39;country&#39;)]# the downside of putting weird stuff into pandas dataframes is that# the dataframes print poorlymodels</p><p> 模型= pd.dataframe（{＃这是作品，因为Pandas中的分组数据标记是可拍卖的＃，因为你可以很多享受＃，它们是名单＆＃39;数据＆＃39 ;: [数据用于绘制的数据。 groupby（＆＃39; country＆＃39;）]，}）models.index = [国家/地区，_在GapMinder.groupby（＆＃39; Country＆＃39;）]＃将奇怪的东西放入Pandas DataFrames的缺点是＃dataframes打印不良修道框</p><p> ## data## Afghanistan country continent year lifeExp ...## Albania country continent year lifeExp pop ...## Algeria country continent year lifeExp pop...## Angola country continent year lifeExp pop ...## Argentina country continent year lifeExp p...## ... ...## Vietnam country continent year lifeExp p...## West Bank and Gaza country continent year life...## Yemen, Rep. country continent year lifeExp ...## Zambia country continent year lifeExp po...## Zimbabwe country continent year lifeExp ...## ## [142 rows x 1 columns]</p><p> ## Data ##阿富汗国家大陆生命XP ... ##阿尔巴尼亚国家大陆年度Lifeexp Pop ... ##阿尔及利亚国家大陆年度Lifeexp Pop ... ##安哥拉乡村大陆Lifeexp Pop ... ##阿根廷国家大陆生命年份P ... ## ...... ##越南国家大陆年度Lifeexp P ... ##西岸和加沙乡村大陆生活... ##也门，代表。乡村大陆Lifeexp。 .. ##赞比亚国家大陆年度Lifeexp Po ... ##津巴布韦国家大陆年度Lifeexp ... ## ## [142行x 1列]</p><p> Now we need to do the actual model fitting. My preferred approach is to use list-comprehensions.</p><p> 现在我们需要做实际的模型配件。我的首选方法是使用列表 - 全局。</p><p> def country_model(df): return smf.ols(&#39;lifeExp ~ year&#39;, data=df).fit()models[&#39;fit&#39;] = [ country_model(data) for _, data in gapminder.groupby(&#39;country&#39;)]</p><p> def country_model（df）：return smf.ols（＆＃39; lifeexp〜年＆＃39; data = df）.fit（）模型[＆＃39; fit＆＃39;] = [country_model（数据）for _， gapminder.groupby（＆＃39; country＆＃39;）中的数据]</p><p> One compelling advantage of this (effectively) functional approach to iteration over list-columns of models is that most of these computations are embarrassingly parallel, and map()-like operations are often very easy to parallelize.</p><p> 这种（有效地）在模型列表列中迭代的（有效地）功能方法的一个令人兴奋的优势是，大多数这些计算都是令人尴尬的平行，而映射（） - 类似的操作通常很容易并行化。</p><p> An alternative approach here is to use  DataFrame.apply(). However, I have found the  Series.apply() and  DataFrame.apply() methods to be hard to reason about when used together with list-columns, and so I recommend avoiding them.</p><p> 这里的另一种方法是使用dataframe.apply（）。但是，我已经找到了series.Apply（）和dataframe.apply（）方法难以推理，当与列表列一起使用时，我建议避免它们。 </p><p> Now that we’ve fit all of our models, we can extract information from them. Here I’ll define some helper functions very much in the spirit of  broom. When you don’t own the model classes you’re using, you pretty much have to write extractor functions to do this; see Michael Chow’s excellent  blog post showing how to handle this in an elegant way.</p><p>现在我们符合我们所有的模型，我们可以从他们那里提取信息。在这里，我将在扫帚的精神中非常多么辅助功能。当您不拥有您使用的模型类时，您几乎必须写入提取器功能来执行此操作;查看Michael Chow的优秀博客文章，显示如何以优雅的方式处理此操作。</p><p> Even if you do own the model objects you’re using, I recommend extractor functions over class methods. This is because, during EDA, you typically fit some expensive models once, and then repeatedly investigate them–you can modify an extractor function and use it right away, but if you modify a method for a model class, you’ll have to refit all the model objects. This leads to slow iteration.</p><p> 即使您自己使用了您使用的模型对象，我建议使用类方法提取函数。这是因为，在EDA期间，您通常适用于一旦昂贵的模型，然后重复调查它们 - 您可以修改提取器功能并立即使用它，但如果修改模型类的方法，则必须重新修改所有模型对象。这导致缓慢迭代。</p><p> # ripped directly from michael chow&#39;s blog post!!# go read his stuff it&#39;s very cool!def tidy(fit): from statsmodels.iolib.summary import summary_params_frame tidied = summary_params_frame(fit).reset_index() rename_cols = { &#39;index&#39;: &#39;term&#39;, &#39;coef&#39;: &#39;estimate&#39;, &#39;std err&#39;: &#39;std_err&#39;, &#39;t&#39;: &#39;statistic&#39;, &#39;P&gt;|t|&#39;: &#39;p_value&#39;, &#39;Conf. Int. Low&#39;: &#39;conf_int_low&#39;, &#39;Conf. Int. Upp.&#39;: &#39;conf_int_high&#39; } return tidied.rename(columns = rename_cols)def glance(fit): return pd.DataFrame({ &#39;aic&#39;: fit.aic, &#39;bic&#39;: fit.bic, &#39;ess&#39;: fit.ess, # explained sum of squares &#39;centered_tss&#39;: fit.centered_tss, &#39;fvalue&#39;: fit.fvalue, &#39;f_pvalue&#39;: fit.f_pvalue, &#39;nobs&#39;: fit.nobs, &#39;rsquared&#39;: fit.rsquared, &#39;rsquared_adj&#39;: fit.rsquared_adj }, index=[0])# note that augment() takes 2 inputs, whereas tidy() and glance() take 1def augment(fit, data): df = data.copy() if len(df) != fit.nobs: raise ValueError(&#34;`data` does not have same number of observations as in training data.&#34;) df[&#39;fitted&#39;] = fit.fittedvalues df[&#39;resid&#39;] = fit.resid return df</p><p> ＃直接从Michael Chow＆＃39;博客帖子！ ）Rename_Cols = {＆＃39;索引＆＃39 ;:＆＃39;术语＆＃39;＆＃39; cof; cof＆＃39 ;:＆＃39;估计＆＃39 ;,＆＃39; std err＆＃39 ;: ＆＃39; std_err＆＃39;＆＃39; t＆＃39 ;:＆＃39;统计＆＃39 ;,＆＃39; p＆gt; |＆＃39; p_value＆＃39;， ＆＃39; conf。 int。低价＆＃39 ;:＆＃39; conf_int_low＆＃39;和＃39; conf。 int。 UPP。＆＃39 ;:＆＃39; conf_int_high＆＃39;返回tidied.rename（列= Rename_Cols）Def Glance（适合）：返回PD.DATAFRAME（{＆＃39; AIC＆＃39 ;: fit.AIC，＆＃39; BIC＆＃39 ;: fit.bic，＆＃ 39; ess＆＃39 ;: fit.ess，＃解释的正方形和＃39; centered_tss＆＃39 ;: fit.centred_tss，＆＃39; fvalue＆＃39 ;: fit.fvalue，＆＃39; f_pvalue＆＃39; f_pvalue＆＃39; ：fit.f_pvalue，＆＃39; nobs＆＃39 ;: fit.nobs，＆＃39; rsquared＆＃39 ;: fit.rsquared，＆＃39; rsquared_adj＆＃39 ;: fit.rsquared_adj}，index = [0] ）＃注意，Augment（）需要2个输入，而Tidy（）和Glance（）取1def增强（适合，数据）：df = data.copy（）如果​​len（df）！= fit.nobs：提高valueerror（＆ ＃34;`数据`没有与训练数据相同的观察。＆＃34;）df [＆＃39;拟合＆＃39;] = fit.fittedvalues df [＆＃39; resid＆＃39;] = fit.resid返回df.</p><p> We sanity check the helper functions by seeing if they work on a single model object before working with the entire list-column of models.</p><p> 我们通过在使用型号的整个列表列之前，通过查看它们在单个模型对象上工作，查看辅助功能检查辅助功能。</p><p>  ## term estimate std_err ... p_value conf_int_low conf_int_high## 0 Intercept -507.534272 40.484162 ... 1.934055e-07 -597.738606 -417.329937## 1 year 0.275329 0.020451 ... 9.835213e-08 0.229761 0.320896## ## [2 rows x 7 columns]</p><p>  ##术语estimess std_err ... p_value conf_int_low conf_int_high ## 0拦截-507.534272 40.484162 ... 1.9340552-07 -597.738606 -597.738606 -417.329937 ## 1年0.275329 0.020451 ... 9.8352132 08 0.222961 0.320896 ##### [2行x 7列]</p><p>  ## aic bic ess ... nobs rsquared rsquared_adj## 0 40.69387 41.663683 271.006011 ... 12.0 0.947712 0.942483## ## [1 rows x 9 columns]</p><p>  ## AIC BIC ESS ... NOBS RSQUARED RSQUARED_ADJ ## 0 40.69387 41.663683 221.006011 ... 12.0 0.947712 0.942483 ## ## [1行x 9列]</p><p> augment() actually takes two inputs, where one input is the model object, and the other is the training data used to fit that model object.</p><p> Augment（）实际上需要两个输入，其中一个输入是模型对象，另一个输入是用于适合该模型对象的训练数据。 </p><p>  ## country continent year ... gdpPercap fitted resid## 0 Afghanistan Asia 1952 ... 779.445314 29.907295 -1.106295## 1 Afghanistan Asia 1957 ... 820.853030 31.283938 -0.951938## 2 Afghanistan Asia 1962 ... 853.100710 32.660582 -0.663582## 3 Afghanistan Asia 1967 ... 836.197138 34.037225 -0.017225## 4 Afghanistan Asia 1972 ... 739.981106 35.413868 0.674132## 5 Afghanistan Asia 1977 ... 786.113360 36.790512 1.647488## 6 Afghanistan Asia 1982 ... 978.011439 38.167155 1.686845## 7 Afghanistan Asia 1987 ... 852.395945 39.543798 1.278202## 8 Afghanistan Asia 1992 ... 649.341395 40.920442 0.753558## 9 Afghanistan Asia 1997 ... 635.341351 42.297085 -0.534085## 10 Afghanistan Asia 2002 ... 726.734055 43.673728 -1.544728## 11 Afghanistan Asia 2007 ... 974.580338 45.050372 -1.222372## ## [12 rows x 8 columns]</p><p>##国大陆一年... gdpPercap装渣油## 0亚洲阿富汗1952年...... 779.445314 29.907295 -1.106295 ## 1阿富汗1957年亚洲... 820.853030 31.283938 -0.951938 ## 2阿富汗1962年亚洲... 853.100710 32.660582 -0.663582 ## 3 AFGHANISTAN亚洲1967 ... 836.197138 34.017225 -0.017225 -0.017225 -0.017225 ## 4 AFGHANISTAN ASIA 1972 ... 739.981106 35.413868 0.674132＃6 AFGHANISTAN ASIA 1977 ... 786.113360 36.790512 1.6490512 1.6490512 1.6490512 1.6490512 1.647488＃ ＃7阿富汗亚洲1987 ... 852.395945 39.543798 1.278202＃69.543798 1.278202 ## 8 AFGHANISTAN ASIONA ... 649.341395 40.920442 0.753558＃6/920442 0.753558 ## 9阿富汗亚洲1997 ... 635.341351 42.297085 -0.534085 ## 10阿富汗亚洲2002 ... 726.734055 43.673728 -1.544728＃ ＃11阿富汗亚洲2007 ... 974.580338 45.050372 -1.222372 #### [12行x 8列]</p><p> Now we are ready to iterate over the list-column of models. Again, we leverage list-comprehensions. For  tidy() and  glance() these comprehension are straightforward, but for  augment(), which will consume elements from two columns at once, we will need to do something a little fancier. In R we could use  purrr::map2() or  purrr::pmap(), but the Pythonic idiom here is to use  zip() together with tuple unpacking.</p><p> 现在我们已准备好迭代模型列表。再次，我们利用名单 - 全面了解。对于Tidy（）和Glance（）这些理解是简单的，但对于Augment（），这将每次消耗两列的元素，我们需要做一些小鸽友。在r，我们可以使用purrr :: map2（）或purrr :: pmap（），但这里的pythonic idiom是用zip（）与元组unpacking一起使用。</p><p> models[&#39;tidied&#39;] = [tidy(fit) for fit in models.fit]models[&#39;glanced&#39;] = [glance(fit) for fit in models.fit]models[&#39;augmented&#39;] = [ augment(fit, data) for fit, data in zip(models.fit, models.data)]</p><p> 模型[＆＃39;整天＆＃39;] = [整洁（适合）适用于型号.Fit]型号[＆＃39;镜孔闪烁＆＃39;] = [GLANCE（FIT）适合型号.FIT]模型[ ＆＃39;增强＆＃39;] = [增强（适合，数据）for fit，zip中的数据（model.fit，models.data）]</p><p> Note for R users: In R the calls to  tidy(), etc, would typically live inside a  mutate() call. The  pandas equivalent is  assign, but  pandas doesn’t leverage non-standard evaluation and I typically don’t use  assign() unless I really want to leverage method chaining for some reason. Normally I save method chaining for data manipulation once I have a flat dataframe, and otherwise I operate entirely via list comprehensions.</p><p> r用户注意：在r tidy（）等的调用通常会在变异（）呼叫中。 Pandas等效物分配，但Pandas不利用非标准评估，我通常不使用Assign（），除非我真的希望利用方法链接，因为某种原因。通常，一旦我有一个平面的DataFrame，我将保存用于数据操作的方法，否则我完全通过列表的全面操作。</p><p>   ## data ... augmented## Afghanistan country continent year lifeExp ... ... country continent year ... gdpPerc...## Albania country continent year lifeExp pop ... ... country continent year lifeExp pop ...## Algeria country continent year lifeExp pop... ... country continent year ... gdpPercap ...## Angola country continent year lifeExp pop ... ... country continent year lifeExp pop ...## Argentina country continent year lifeExp p... ... country continent year ... gdpPerc...## ... ... ... ...## Vietnam country continent year lifeExp p... ... country continent year ... gdpPerca...## West Bank and Gaza country continent year life... ... country continent year ... ...## Yemen, Rep. country continent year lifeExp ... ... country continent year ... gdpP...## Zambia country continent year lifeExp po... ... country continent year ... gdpPercap...## Zimbabwe country continent year lifeExp ... ... country continent year ... gdpPerca...## ## [142 rows x 5 columns]</p><p>   ##数据...增强##阿富汗国家大陆生命赛......国家大陆年...... Gdpperc ...... Gdpperc ... ##阿尔巴尼亚国家大陆年度Lifeexp Pop ......国家大陆年份Lifeexp Pop。 .. ##阿尔及利亚国家大陆Lifeexp Pop ......国家大陆年...... Gdppercap ... ##安哥拉乡村大陆Lifeexp Pop ......国家大陆年份Lifeexp Pop ... ##阿根廷乡村大陆生命克斯......国家大陆... gdpperc ... ## ...... ...... ...... ##越南国家大陆生命克斯......乡村大陆...... Gdpperca ... ##西岸和加沙乡村大陆生活......国家大陆年...... ##也门，代表。乡村大陆生命XP ...... 。国家大陆...... GDPP ...... ##赞比亚国家大陆生命XP宝......国家大陆...... Gdppercap ... ##津巴布韦国家大陆生命XP ......国家大陆一年... gdpperca ... ## ## [142行x 5列]</p><p> Our final step before we can recreate Hadley’s plots is to flatten or “unnest” the dataframe.  pandas has no unnest method, but the following has served me well so far to unnest a single column. This will not play well with dataframes with MultiIndexes, which I recommend avoiding.</p><p> 我们在我们可以重新创建哈德利的情节之前的最后一步是削弱或“不最愚蠢的”数据污染。 Pandas没有不最铭文的方法，但以下为我提供了很好的是，到目前为止是一个单一列。使用多线索的Dataframes不会很好地玩，我建议避免。</p><p> def unnest(df, value_col): lst = df[value_col].tolist() unnested = pd.concat(lst, keys=df.index) unnested.index = unnested.index.droplevel(-1) return df.join(unnested).drop(columns=value_col)</p><p> def Unnest（df，value_col）：lst = df [value_col] .tolist（）取消标准= pd.concat（lst，keys = df.index）Unnested.index = Unnested.Index.Droplevel（-1）返回df.join（未使用）.drop（列= value_col） </p><p>  glance_results = unnest(models, &#39;glanced&#39;)# equivalentlyglance_results = ( models .pipe(unnest, &#39;glanced&#39;) # little bit of extra cleanup .reset_index() .rename(columns={&#39;index&#39;: &#39;country&#39;}))glance_results</p><p>Plance_results = Unnest（型号，＆＃39;镜头＆＃39;）＃等同viglance_results =（模型.pipe（unnest，＆＃39;镜头吻＆＃39;）＃小写额外清理.reset_index（）.rename（列= { ＆＃39;索引＆＃39 ;:＆＃39;国家＆＃39;}））glance_results</p><p> ## country ... rsquared_adj## 0 Afghanistan ... 0.942483## 1 Albania ... 0.901636## 2 Algeria ... 0.983629## 3 Angola ... 0.876596## 4 Argentina ... 0.995125## .. ... ... ...## 137 Vietnam ... 0.988353## 138 West Bank and Gaza ... 0.967529## 139 Yemen, Rep. ... 0.979290## 140 Zambia ... -0.034180## 141 Zimbabwe ... -0.038145## ## [142 rows x 14 columns]</p><p> ##国家...... rsquared_adj ## 0阿富汗... 0.942483 ## 1阿尔巴尼亚... 0.901636 ## 2阿尔及利亚... 0.983629 ##安哥拉... 0.876596 ## 4阿根廷... 0.995125 ##。 。... ... ... ## 137越南... 0.988353 ## 138西岸和加沙... 0.967529 ## 139也门，代表... 0.979290 ## 140赞比亚......  -  0.034180＃ ＃141 zimbabwe ... -0.038145 ## ## [142行x 14列]</p><p> Now we could ask a question like “what countries seem to have the most linear trends in life expectancy?” and use R-squared as a measure of this.</p><p> 现在我们可以问一个问题，比如“似乎有哪些国家在预期寿命中具有最线性趋势？”并使用R形被平方作为衡量标准。</p><p>   Okay so this plot is awful but I don’t have the patience at the moment to make it better. We could also look at residuals for individual fits to inspect them for any patterns that might indicate systematic error.</p><p>   好的，这剧情是可怕的，但目前我没有耐心，以使其更好。我们也可以查看个人适合的残差，以检查它们是否有可能表明系统错误的任何模式。</p><p> augment_results = unnest(models, &#39;augmented&#39;)p = ( sns.FacetGrid( data=augment_results, col=&#39;continent&#39;, col_wrap=2, hue=&#39;continent&#39;, height=5, aspect=16/9) .map(plt.plot, &#39;year&#39;, &#39;resid&#39;))plt.show()</p><p> Augment_Results = Unnest（模型，＆＃39;增强＆＃39;）p =（sns.facetgrid（data = augment_results，col =＆＃39;大陆＆＃39;，col_wrap = 2，hue =＆＃39;大陆＆＃39 ;，高度= 5，方面= 16/9）.map（plt.plot，＆＃39;年＆＃39 ;,＆＃39; resid＆＃39;））plt.show（）</p><p>  It would be nice to add smooths by continent as Hadley does but again I don’t have the patience or masochistic urge to figure out how to do that. In any case, there are some clear trends in the residuals, especially for Africa, which suggest that some further modeling is a good idea.</p><p>  在哈德利曾经再次，我没有耐心或受虐狂的冲动，这将是由大陆添加平滑的难以弄清楚如何做到这一点。无论如何，残差有一些明显的趋势，特别是对于非洲，这表明一些进一步的建模是一个好主意。</p><p> So that’s the basic idea behind the many models workflow. Note that we’ve been working at a fairly low-level of abstraction. This means you have a lot of control over what happens (good for research and EDA), but have to write a lot of code. If you’re just fitting prediction models and the only thing you want to do is compare risk estimates, you can save time and effort by using  sklearn’s  GridSearchCV object.</p><p> 所以这是许多模型工作流程背后的基本想法。请注意，我们一直在采用相当低的抽象。这意味着您对会发生的事情有很大的控制（适合研究和EDA），但必须编写大量代码。如果您只是拟合预测模型，并且您唯一想要做的是比较风险估计，您可以使用Sklearn的GridSearchCV对象节省时间和精力。 </p><p> One final note: in Hadley’s gapminder example we iterate over disjoint data sets. In practice I do this extremely rarely. Much more often I find myself iterating over  (train, test) pairs, or hyperparameters, or both at once. This hyperparameter optimization over many CV-folds workflow is more complex than the simple example here, but still fits nicely into the many models workflow I’ve described here. I’ll demonstrate how to do that in a followup post.</p><p>最后一个注意：在Hadley的GapMinder示例中，我们迭代不相交的数据集。在实践中，我很少这样做。更常见的是，我发现自己迭代（火车，测试）对或近似参数，或者一次。这种超级参数优化在许多CV折叠工作流程比这里的简单示例更复杂，但仍然适合我这里描述的许多模型工作流程。我将演示如何在后续帖子中做到这一点。</p><p> It often makes sense to store hyperparameters in dicts in Python. This means you can’t easily store model objects in dicts, because  model_store[hyperparameters] = my_model_object gets all fussy because the hyperparameter dictionary isn’t hashable. ↩</p><p> 在Python中存储HyperParameters常常是有意义的。这意味着您无法轻易存储DICT中的模型对象，因为Model_Store [HyperParameters] = My_model_Object获取所有挑剔，因为HyperParameter字典不是Hashable。 ↩</p><p> One natural approach here would be to have a list of model objects, a list of AIC values, a list of BIC values, etc. Now you run into an indexing issue where you have to figure out which index corresponds to a given model and keeping track of a bunch of maps like this. A natural solution is to say force all the indexes to match up – everything with index 0 should correspond to the first model. Congratulations, you’ve just invented a data frame. ↩</p><p> 这里的一种自然方法是有一个模型对象的列表，AIC值列表，BIC值列表等。现在您遇到了一个索引问题，在其中您必须弄清楚哪个索引对应于给定的模型和保留追踪像这样的一堆地图。自然解决方案是指符合匹配的所有索引 - 具有索引0的所有索引应对应于第一模型。恭喜，你刚刚发明了一个数据框架。 ↩</p><p>  Another big selling point of dataframes is vectorized operations on the columns. However, you can’t vectorize operations on model objects, and this leads to code that has to perform vectorize explicitly rather than letting dataframe libraries handle it implicitly. There is a definitely a learning curve for new users here but I’m pretty convinced it’s worth it. ↩</p><p>  DataFrame的另一个大卖点是列上的矢量化操作。但是，您无法将操作与模型对象的操作保持一致，这导致必须显式执行Vectorize的代码，而不是让DataFrame库隐式地处理它。这里的新用户肯定有一个学习曲线，但我很确信它是值得的。 ↩</p><p> Another complaint that you might have as a Pythonista is that you should construct a custom model object to handle all this model tracking. I’m actually pretty curious about this and would love to hear ideas. My suspition is that the big challenge is coming up with a sufficiently general framework to accommodate widely varying use cases. ↩</p><p> 您可能具有Pythonista的另一个抱怨是您应该构建自定义模型对象以处理所有此模型跟踪。我真的很好奇这一点，很想听到想法。我的吊销是，大量挑战正在推出足够一般的框架，以适应各种不同的用例。 ↩</p><p> Aside: I have found plotting in Python to be a largely unpleasant experience. At this point, I’ve pretty much settled on  seaborn as my go-to plotting library, and I use  sns.FacetGrid for almost everything, even when I don’t need facetting, because enough of my  ggplot2 intuition carries over that I can mostly get things done. ↩</p><p> 旁边：我发现在Python中策划是一个很大程度上不愉快的经历。在这一点上，我几乎在海运时定居为我的去绘制图书馆，我几乎所有的东西都使用sns.facetgrid，即使我不需要刻面，因为我的ggplot2直觉就可以了我可以大多数情况下完成了事情。 ↩ </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.alexpghayes.com/blog/many-models-workflows-in-python-part-i/">https://www.alexpghayes.com/blog/many-models-workflows-in-python-part-i/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/python/">#python</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/workflows/">#workflows</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>