<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>研究人员希望通过将通过无监督学习(如GPT-3)训练的语言模型与有标签的计算机视觉数据集相结合，将常识注入人工智能</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">研究人员希望通过将通过无监督学习(如GPT-3)训练的语言模型与有标签的计算机视觉数据集相结合，将常识注入人工智能</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-09 02:49:50</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/3a91d4ef2bcd8400e164a6b52c940020.jpeg"><img src="http://img2.diglog.com/img/2020/11/3a91d4ef2bcd8400e164a6b52c940020.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>You’ve probably heard us say this countless times: GPT-3, the gargantuan AI that spews uncannily human-like language, is a marvel. It’s also largely  a mirage. You can tell with a simple trick: Ask it  the color of sheep, and it will  suggest “black” as often as “white”—reflecting the phrase “black sheep” in our vernacular.</p><p>你可能已经听过我们无数次这样说：GPT-3，这个能发出不可思议的类似人类语言的巨型人工智能，是一个奇迹。这在很大程度上也是一种海市蜃楼。你可以用一个简单的技巧来分辨：问它绵羊的颜色，它会像暗示“白”一样频繁地暗示“黑”--这反映了我们俗语中的“害群之马”。</p><p>  That’s the problem with language models: because they’re only trained on text, they lack common sense. Now researchers from the University of North Carolina, Chapel Hill, have designed a new technique to change that. They call it “vokenization,” and it gives language models like GPT-3 the ability to “see.”</p><p>这就是语言模型的问题所在：因为它们只接受文本方面的训练，缺乏常识。现在，北卡罗来纳大学教堂山分校的研究人员设计了一种新技术来改变这一状况。他们称之为“声学化”，它赋予了像GPT-3这样的语言模型“看得见”的能力。</p><p>  It’s not the first time people have sought to combine language models with computer vision. This is actually a rapidly growing area of AI research. The idea is that both types of AI have different strengths. Language models like GPT-3 are trained through unsupervised learning, which requires no manual data labeling, making them easy to scale. Image models like object recognition systems, by contrast, learn more directly from reality. In other words, their understanding doesn’t rely on the kind of abstraction of the world that text provides. They can “see” from pictures of sheep that they are in fact white.</p><p>这并不是人们第一次试图将语言模型与计算机视觉相结合。这实际上是人工智能研究的一个快速增长的领域。我们的想法是，这两种类型的人工智能都有不同的优势。像GPT-3这样的语言模型是通过无监督学习来训练的，这不需要人工标记数据，使得它们很容易扩展。相比之下，像物体识别系统这样的图像模型更直接地从现实中学习。换句话说，他们的理解并不依赖于文本所提供的对世界的抽象。他们可以从绵羊的照片中“看出”它们实际上是白色的。</p><p>  AI models that can parse both language and visual input also have very practical uses. If we want to build robotic assistants, for example, they need computer vision to navigate the world and language to communicate about it to humans.</p><p>同时可以解析语言和视觉输入的人工智能模型也有非常实用的用途。例如，如果我们想要制造机器人助手，它们需要计算机视觉来导航世界，需要语言来与人类交流。</p><p>  But combining both types of AI is easier said than done. It isn’t as simple as stapling together an existing language model with an existing object recognition system. It requires training a new model from scratch with a data set that includes text and images, otherwise known as a visual-language data set.</p><p>但将这两种类型的人工智能结合起来说起来容易做起来难。它不像将现有的语言模型与现有的对象识别系统装订在一起那么简单。它需要用包含文本和图像的数据集(也称为视觉语言数据集)从头开始训练一个新模型。</p><p>  The most common approach for curating such a data set is to compile a collection of images with descriptive captions. A picture like the one below, for example, would be captioned “An orange cat sits in the suitcase ready to be packed.” This differs from typical image data sets, which would label the same picture with only one noun, like “cat.” A visual-language data set can therefore teach an AI model not just how to recognize objects but how they relate to and act on one other, using verbs and prepositions.</p><p>管理这样的数据集最常见的方法是汇编一个带有描述性说明的图像集合。例如，下面这张图片的标题是“一只橙色的猫坐在行李箱里，准备收拾行李。”这与典型的图像数据集不同，典型的图像数据集只用一个名词来标记同一张图片，比如“猫”。因此，视觉语言数据集不仅可以教会人工智能模型如何识别对象，还可以教会它们如何使用动词和介词相互关联和作用。</p><p>    But you can see why this data curation process would take forever. This is why the visual-language data sets that exist are so puny. A popular text-only data set like English Wikipedia (which indeed includes nearly all the English-language Wikipedia entries) might contain nearly 3 billion words. A visual-language data set like Microsoft Common Objects in Context, or MS COCO, contains only 7 million. It’s simply not enough data to train an AI model for anything useful.</p><p>但是你可以明白为什么这个数据管理过程会永远持续下去。这就是为什么现有的视觉语言数据集如此微不足道。像英文维基百科(实际上几乎包括所有英文维基百科条目)这样的纯文本数据集可能包含近30亿个单词。像Microsoft Common Objects in Context或MS Coco这样的可视化语言数据集只有700万个。这些数据根本不足以训练人工智能模型以获得任何有用的东西。</p><p>  “Vokenization” gets around this problem, using unsupervised learning methods to scale the tiny amount of data in MS COCO to the size of English Wikipedia. The resultant visual-language model outperforms state-of-the-art models in some of the hardest tests used to evaluate AI language comprehension today.</p><p>“Vokenization”绕过了这个问题，它使用无监督的学习方法，将MS Coco中的极少量数据缩放到英文维基百科的大小。由此产生的视觉语言模型在当今用于评估人工智能语言理解能力的一些最难的测试中表现优于最先进的模型。</p><p>  “You don’t beat state of the art on these tests by just trying a little bit,” says Thomas Wolf, the cofounder and chief science officer of the natural-language processing startup Hugging Face, who was not part of the research. “This is not a toy test. This is why this is super exciting.”</p><p>自然语言处理初创公司Huking Face的联合创始人兼首席科学官托马斯·沃尔夫(Thomas Wolf)说：“你不能仅仅试一试就在这些测试中击败最先进的技术。”沃尔夫没有参与这项研究。他说：“这不是玩具测试。这就是为什么这非常令人兴奋。“。</p><p>    Let’s first sort out some terminology. What on earth is a “voken”?</p><p>让我们先整理一下一些术语。“沃肯”到底是什么？</p><p>  In AI speak, the words that are used to train language models are known as tokens. So the UNC researchers decided to call the image associated with each token in their visual-language model a voken.  Vokenizer is what they call the algorithm that finds vokens for each token, and  vokenization is what they call the whole process.</p><p>在人工智能中，用于训练语言模型的单词被称为记号。因此，北卡罗来纳大学的研究人员决定将他们视觉语言模型中与每个符号相关的图像称为沃肯(Voken)。Vokenizer是他们所说的为每个令牌寻找语音的算法，而Vokenizer是他们所说的整个过程。</p><p>  The point of this isn’t just to show how much AI researchers love making up words. (They really do.) It also helps break down the basic idea behind vokenization. Instead of starting with an image data set and manually writing sentences to serve as captions—a very slow process—the UNC researchers started with a language data set and used unsupervised learning to match each word with a relevant image (more on this later). This is a highly scalable process.</p><p>这样做的意义不仅仅是为了表明人工智能研究人员有多喜欢编造单词。(确实如此。)。它还有助于打破声情化背后的基本理念。北卡罗来纳大学的研究人员没有从图像数据集开始，然后手动编写句子作为字幕-这是一个非常缓慢的过程-而是从语言数据集开始，使用无监督学习将每个单词与相关图像进行匹配(稍后将详细介绍)。这是一个高度可扩展的过程。</p><p>  The unsupervised learning technique, here, is ultimately the contribution of the paper. How do you actually find a relevant image for each word?</p><p>在这里，无监督学习技术是本文的最终贡献。你实际上是如何找到每个单词的相关图片的？</p><p>    Let’s go back for a moment to GPT-3. GPT-3 is part of a family of language models known as transformers, which represented a major breakthrough in applying unsupervised learning to natural-language processing when the first one was introduced in 2017. Transformers learn the patterns of human language by observing how words are used in context and then creating a mathematical representation of each word, known as a “word embedding,” based on that context. The embedding for the word “cat” might show, for example, that it is frequently used around the words “meow” and “orange” but less often around the words “bark” or “blue.”</p><p>让我们回到GPT-3。GPT-3是被称为转换器的语言模型家族的一部分，当2017年第一个模型被引入时，这代表着将无监督学习应用于自然语言处理方面的重大突破。变形金刚通过观察单词在上下文中的使用情况，然后根据上下文创建每个单词的数学表示，称为“单词嵌入”，从而学习人类语言的模式。例如，“猫”这个词的嵌入可能表明，它经常在“喵”和“橙”的周围使用，但在“树皮”或“蓝色”的周围使用的频率较低。</p><p>  This is how transformers approximate the meanings of words, and how GPT-3 can write such human-like sentences. It relies in part on these embeddings to tell it how to assemble words into sentences, and sentences into paragraphs.</p><p>这就是转换器如何近似单词的意思，以及GPT-3如何写出类似人类的句子。它在一定程度上依赖于这些嵌入来告诉它如何将单词组合成句子，以及如何将句子组合成段落。</p><p>  There’s a parallel technique that can also be used for images. Instead of scanning text for word usage patterns, it scans images for visual patterns. It tabulates how often a cat, say, appears on a bed versus on a tree, and creates a “cat” embedding with this contextual information.</p><p>还有一种类似的技术也可以用于图像。它不是扫描文本中的单词使用模式，而是扫描图像中的视觉模式。它将猫在床上出现的频率与在树上出现的频率制成表格，并创建一个嵌入上下文信息的“猫”。</p><p>  The insight of the UNC researchers was that they should use both embedding techniques on MS COCO. They converted the images into visual embeddings and the captions into word embeddings. What’s really neat about these embeddings is that they can then be graphed in a three-dimensional space, and you can literally see how they are related to one another. Visual embeddings that are closely related to word embeddings will appear closer in the graph. In other words, the visual cat embedding should (in theory) overlap with the text-based cat embedding. Pretty cool.</p><p>北卡罗来纳大学的研究人员的见解是，他们应该在MS Coco上同时使用这两种嵌入技术。他们将图片转换为视觉嵌入，并将字幕转换为文字嵌入。这些嵌入的真正巧妙之处在于，它们可以在三维空间中绘制出来，你可以从字面上看到它们是如何相互关联的。与单词嵌入密切相关的视觉嵌入将在图表中显示得更近。换句话说，可视化的猫嵌入(理论上)应该与基于文本的猫嵌入重叠。挺酷的。</p><p>  You can see where this is going. Once the embeddings are all graphed and compared and related to one another, it’s easy to start matching images (vokens) with words (tokens). And remember, because the images and words are matched based on their embeddings, they’re also matched based on context. This is useful when one word can have totally different meanings. The technique successfully handles that by finding different vokens for each instance of the word.</p><p>你可以看到事情的发展方向。一旦所有嵌入都绘制成图表并进行比较并相互关联，就很容易开始将图像(声符)与单词(标记)进行匹配。请记住，因为图像和单词是基于它们的嵌入进行匹配的，所以它们也是基于上下文进行匹配的。当一个单词可能有完全不同的含义时，这很有用。这项技术通过为单词的每个实例找到不同的发音，成功地处理了这一问题。</p><p>  The token is the word “contact” in both examples. But in the first sentence, context suggests that the word refers to contact information, so the voken is the contact icon. In the second sentence, the context suggests the word refers to touch, so the voken shows a cat being stroked.</p><p>在这两个例子中，令牌是单词“Contact”。但在第一句话中，上下文暗示这个词指的是联系信息，所以Voken是联系图标。在第二句话中，上下文暗示这个词指的是触摸，所以Voken表示一只猫被抚摸。</p><p>  The researchers used the visual and word embeddings they created with MS COCO to train their vokenizer algorithm. Once trained, the vokenizer was then able to find vokens for the tokens in English Wikipedia. It’s not perfect. The algorithm only found vokens for roughly 40% of the tokens. But that’s still 40% of a data set with nearly 3 billion words.</p><p>研究人员使用他们用MS Coco创建的视觉和文字嵌入来训练他们的发音器算法。一旦接受了训练，发声者就能在英文维基百科中找到代币的发音符号。这并不完美。该算法仅为大约40%的令牌找到了声符。但这仍然是近30亿字数据集的40%。</p><p>  With this new data set, the researchers retrained a language model known as BERT, an open-source transformer developed by Google that predates GPT-3. They then tested the new and improved BERT on six different language comprehension tests, including SQuAD, the Stanford Question Answering Dataset, which asks models to answer reading comprehension questions about a series of articles, and SWAG, which tries to trip up models with subtleties of the English language to probe whether it’s merely mimicking and memorizing. The improved BERT performed better on all of them, which Wolf says is nothing to sneeze at.</p><p>有了这个新的数据集，研究人员重新训练了一种名为BERT的语言模型，这是一种由谷歌开发的开源转换器，早于GPT-3。然后，他们在六种不同的语言理解测试中测试了新的、改进后的伯特，包括斯坦福大学问答数据集TEAND，它要求模特回答关于一系列文章的阅读理解问题，以及SWIG，它试图用英语的微妙之处绊倒模型，以探索它是否只是在模仿和记忆。改进后的伯特在所有这些人身上都表现得更好，沃尔夫说这是不容忽视的。</p><p>  The researchers, Hao Tan, a PhD student, and Mohit Bansal, his advisor, will be presenting their new vokenization technique in two weeks at the Conference on Empirical Methods in Natural Language Processing. While the work is still early, Wolf sees their work as an important conceptual breakthrough in getting unsupervised learning to work for visual-language models. It was a similar spark that helped dramatically advance natural-language processing back in the day.</p><p>研究人员，博士生谭浩和他的导师莫希特·班萨尔将在两周后的自然语言处理经验方法会议上展示他们的新语音技术。虽然这项工作还处于早期阶段，但沃尔夫认为，他们的工作在让无监督学习为视觉语言模型工作方面，是一个重要的概念突破。这是一个类似的火花，在当时帮助戏剧性地推进了自然语言处理。</p><p>  “In NLP, we had this huge breakthrough over two years ago, and then suddenly NLP was a field where a lot of things were happening and it kind of got ahead of all the other AI fields,” he says. “But we have this problem of connecting text with other things. So it’s like this robot that is only able to talk but cannot see, cannot hear.”</p><p>他说：“在NLP，我们在两年多前取得了巨大突破，然后突然NLP成为一个发生了很多事情的领域，它在某种程度上领先于所有其他人工智能领域。”“但我们有一个问题，那就是把文字和其他东西联系起来。所以这就像是这个机器人只会说话，但看不见，听不见。“。</p><p>  “This paper is one example where they managed to connect it to another modality and it works  better,” he says. “You could imagine that maybe some of these techniques could be reused when you want to leverage this really powerful language model in a robot. Maybe you use the same thing to connect the robot’s senses to text.”</p><p>他说：“这篇论文是他们成功地将其与另一种医疗方式联系起来的一个例子，而且效果更好。”“你可以想象，当你想要在机器人中利用这个非常强大的语言模型时，这些技术中的一些可能会被重复使用。”也许你会用同样的东西把机器人的感官和文字联系起来。“</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.technologyreview.com/2020/11/06/1011726/ai-natural-language-processing-computer-vision/">https://www.technologyreview.com/2020/11/06/1011726/ai-natural-language-processing-computer-vision/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/人工智能/">#人工智能</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/注入/">#注入</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/vision/">#vision</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/模型/">#模型</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1033876.html"><img src="http://img2.diglog.com/img/2020/11/thumb_4ac1289a01a77e1a3ffa18ee96ca70db.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1033876.html">一种新的人工智能程序可以听到你咳嗽，并识别你是否感染了冠状病毒。研究人员希望把它变成一款应用程序。</a></div><span class="my_story_list_date">2020-11-9 0:34</span></div><div class="col-sm"><div><a target="_blank" href="/story/1033807.html"><img src="http://img2.diglog.com/img/2020/11/thumb_7e3abe61c819a969dfcdc0d97a1fdfc9.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1033807.html">UnAnimous.ai和Expert.ai等人工智能公司表示，他们正确预测了关键州的选举结果，而传统民调再次达不到要求</a></div><span class="my_story_list_date">2020-11-8 13:4</span></div><div class="col-sm"><div><a target="_blank" href="/story/1033607.html"><img src="http://img2.diglog.com/img/2020/11/thumb_f5c9099dc638b16febf872f523ed0dfa.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1033607.html">人工智能专家路线图</a></div><span class="my_story_list_date">2020-11-7 11:12</span></div><div class="col-sm"><div><a target="_blank" href="/story/1033550.html"><img src="http://img2.diglog.com/img/2020/11/thumb_efe429ccc9761c4a7932f8bb1c4efc1c.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1033550.html">
Provizio为其使用传感器和人工智能的汽车安全平台完成了620万美元的种子融资</a></div><span class="my_story_list_date">2020-11-7 6:19</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/美国/">#美国</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/linux/">#linux</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/程序/">#程序</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>