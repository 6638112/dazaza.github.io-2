<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习无法解决自然语言理解问题</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">机器学习无法解决自然语言理解问题</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-08-10 05:37:46</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/8/a9b475e4112bf3722d3f8fee417c14b5.png"><img src="http://img2.diglog.com/img/2021/8/a9b475e4112bf3722d3f8fee417c14b5.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>在 1990 年代初期，一场统计革命以一场风暴取代了人工智能 (AI)——这场革命在 2000 年代达到高潮，神经网络以其现代深度学习 (DL) 的转世而凯旋归来。这一经验主义转向席卷了人工智能的所有子领域，尽管这项技术最具争议的应用是自然语言处理 (NLP)——一个人工智能的子领域已被证明比任何人工智能先驱者想象的要困难得多。数据驱动的经验方法在 NLP 中的广泛使用具有以下起源：在三年的霸权之后，符号和逻辑方法无法产生可扩展的 NLP 系统，导致了所谓的 NLP 经验方法（EMNLP）的兴起——我在这里使用的一个短语统称为数据驱动、基于语料库、统计和机器学习 (ML) 方法。这种向经验主义转变背后的动机非常简单：直到我们对语言如何运作以及语言如何与我们在日常口语中谈论的世界的知识有一些了解之前，经验和数据驱动的方法可能有助于构建一些实用的文本处理应用程序。正如 EMNLP 的先驱之一肯尼斯·丘奇 (Kenneth Church) 所解释的那样，NLP 的数据驱动和统计方法的倡导者对解决简单的语言任务很感兴趣——动机从来不是暗示这就是语言的工作方式，而是“它是做一些简单的事情总比什么都不做要好”。当天的呼声是：“我们去摘一些唾手可得的果实”。然而，在一篇题为“A Pendulum Swung Too Far”的必读文章中，Church (2007) 认为这种转变的动机被严重误解了。正如 McShane (2017) 还指出的那样，后代误解了这一经验趋势，其动机是通过假设这种可能近似正确 (PAC) 范式将扩展到完全自然语言理解 (NLU) 来寻找简单任务的实用解决方案。正如她所说：“这些信念如何在 NLP 社区中获得准公理地位是一个引人入胜的问题，Church 的一项观察部分回答了这一问题：近代和当代的 NLP 人员在语言学和历史方面接受的教育不够广泛NLP，因此，甚至没有动力去触及那个表面。”在我们看来，这种被误导的趋势导致了一种不幸的情况：坚持使用需要大量计算能力的“大型语言模型”（LLM）来构建 NLP 系统，试图逼近我们称之为无限对象的尝试是徒劳的。尝试记忆大量数据的自然语言。在我们看来，这种伪科学的方法不仅浪费时间和资源，而且还通过诱使一代年轻科学家认为语言只是数据而腐蚀了他们——这条道路只会导致失望，更糟的是, 阻碍自然语言理解 (NLU) 的任何真正进步。相反，我们认为是时候重新考虑我们的 NLU 工作方法了，因为我们相信 NLU 的“大数据”方法不仅在心理上、认知上，甚至在计算上都是不可信的，而且正如我们将在这里展示的，这种盲目的数据驱动的 NLU 方法在理论上和技术上也存在缺陷。虽然 NLP（自然语言处理）和 NLU（自然语言理解）经常互换使用，但两者之间存在实质性差异，突出这种差异至关重要。事实上，认识到语言理解和单纯的语言处理之间的技术差异会让我们意识到数据驱动和机器学习方法虽然可能适用于某些 NLP 任务，但它们甚至与 NLU 无关。考虑最常见的“下游 NLP”任务：上述所有任务都与作为所有机器学习方法基础的可能近似正确 (PAC) 范式一致。具体来说，评估某些 NLP 系统关于上述任务的输出是主观的：没有客观标准来判断一个总结是否优于另一个；或者某个系统提取的（关键）主题/短语是否比另一个系统提取的更好，等等。但是，语言理解不允许任何自由度。对话语或问题的完全理解需要理解说话者试图传达的唯一一个想法。要了解此过程的复杂性，请考虑以下自然语言查询（针对某个数据库/知识图）：我们是否有一位在冷战期间驻扎在东欧国家的退休 BBC 记者？在某些数据库中，上述查询只有一个正确答案。因此，将上述内容转换为正式的 SQL（或 SPARQL）查询非常具有挑战性，因为我们不会出错。理解这个问题背后的“确切”思想包括：</p><p>正确解释“退休的 BBC 记者”——即，作为所有在 BBC 工作且现已退休的记者的集合。通过保留所有在某些“东欧国家”工作的“退休 BBC 记者”，进一步过滤上述设置。除了地域限制，还有时间限制，即那些“退休的 BBC 记者”的工作时间必须是“冷战期间”。以上意味着将介词短语“在冷战期间”附加到“基于”而不是“一个东欧国家”（如果“冷战期间”被替换为“具有成员资格”，请考虑不同的介词短语附加Warsaw Pact”）进行正确的量词范围界定：我们寻找的不是在“某个”东欧国家工作的“a”（单个）记者，而是在任何东欧国家工作的任何记者 以上都没有挑战语义理解函数可以“大致”或“可能”正确——但绝对正确。换句话说，我们必须从对上述问题的多种可能解释中得出唯一的一种含义，即根据我们对世界的常识知识，是某些说话者打算提出的问题背后的一个想法。总而言之，对普通口语的真正理解与单纯的文本（或语言）处理是完全不同的问题，在文本（或语言）处理中，我们可以接受近似正确的结果——结果也有一些可接受的概率是正确的。通过这个简短的描述，应该清楚为什么 NLP 不同于 NLU 以及为什么 NLU 对机器来说很困难。但是 NLU 的困难根源究竟是什么？让我们首先描述我们所谓的“缺失文本现象”（MTP），我们认为这是自然语言理解中所有挑战的核心。语言交流如下图所示：说话者将思想编码为某种自然语言中的语言表达，然后听者将该语言表达解码为（希望）说话者打算传达的思想！</p><p>NLU 中的“U”就是“解码”过程——也就是说，理解语言话语背后的思想正是解码过程中发生的事情。此外，在这个“解码”过程中没有近似值或任何自由度——也就是说，从一个话语的众多可能含义中，只有一个说话者打算传达的思想和“理解”。解码消息的过程必须得到一个而且只有一个想法，这正是 NLU 困难的原因。让我们详细说明。在这种复杂的交流中，优化或有效交流有两种可能的选择：(i) 说话者可以压缩（和最小化）在思想编码中发送的信息量，并希望听者能做一些额外的工作解码（解压缩）过程；或者 (ii) 演讲者将努力工作并发送传达思想所需的所有信息，这将使听众无所事事（有关此过程的完整说明，请参阅本文）。这个过程的自然演变似乎导致了正确的平衡，即说话者和听者的总体工作都得到了同样的优化。这种优化导致说话者编码所需的尽可能少的信息，同时忽略了可以安全地假设为听众可用信息的所有其他信息。我们（所有人！）倾向于遗漏的信息通常是我们可以安全地假设对说话者和听者都可用的信息，而这正是我们通常所说的常见背景知识的信息。要了解这个过程的复杂性，请考虑下面黄色框中的（未优化的）通信，以及我们通常所说的等效但小得多的文本（绿色）。绿框中较短的信息，即我们通常所说的方式，传达的思想与较长的信息相同。通常我们不会明确说明所有其他内容，正是因为我们都知道：也就是说，为了有效沟通，我们不会说我们可以假设我们都知道的内容！这也正是我们都倾向于遗漏相同信息的原因——因为我们都知道每个人都知道的东西，而这正是我们所说的“共同”背景知识。人类在大约 20 万年的进化过程中发展起来的这种天才优化过程效果很好，也正是因为我们都知道我们都知道什么。但这就是 NLU 的问题所在：机器不知道我们遗漏了什么，因为它们不知道我们都知道什么。净结果？ NLU 非常非常困难，因为如果软件程序不能以某种方式“发现”人类在语言交流中遗漏和隐含地假设的所有内容，则它们无法完全理解我们语言表达背后的思想。这真的是 NLU 挑战（而不是解析、词干提取、词性标注、命名实体识别等）。以下是 NLU 中的一些众所周知的挑战——这些问题通常在计算语言学中给出。图 2 中显示的是（仅部分）以红色突出显示的缺失文本。在上面的图 2 中，显示了 NLU 中的一组众所周知的挑战。这些例子表明，NLU 中的挑战是发现（或揭示）那些缺失的信息，并隐含地假定为共享和共同的背景知识。下面的图 3 显示了“缺失文本现象”的进一步示例，因为它们与转喻的概念以及发现隐含在所谓的名词复合词中的隐藏关系的挑战有关。在此背景下，我们现在提供三个原因，说明为什么机器学习和数据驱动的方法不会为自然语言理解挑战提供解决方案。</p><p>ML 方法甚至与 NLU 无关：ML 是压缩，语言理解需要解压缩 上述讨论（希望）是一个令人信服的论点，即机器理解自然语言是困难的，因为 MTP——也就是说，因为我们日常话语中的普通口语被高度（如果不是最佳）压缩，因此“理解”的挑战在于解压缩（或发现）丢失的文本——而对于我们人类来说，这是有效沟通的天才发明，机器理解语言是困难的，因为机器可以不知道我们都知道什么。但是 MTP 现象正是为什么数据驱动和机器学习方法虽然在某些 NLP 任务中可能有用，但甚至与 NLU 无关。在这里，我们为这个（无可否认的）强有力的主张提供了正式的证明：（机器）可学习性（ML）和可压缩性（COMP）之间的等价性已经在数学上建立起来。也就是说，已经确定只有当数据高度可压缩（即它有很多冗余）时，数据集的可学习性才能发生，反之亦然（参见这篇文章和出现的重要文章“可学习性可以是不可判定的”） 2019 年发表在《自然》杂志上）。虽然可压缩性和可学习性之间的证明在技术上相当复杂，但直观上很容易理解为什么：学习是关于消化大量数据并在多维空间中找到一个“覆盖”整个数据集（以及看不见的）的函数具有相同模式/分布的数据）。因此，当所有数据点都可以压缩成一个流形时，就会发生可学习性。但是 MTP 告诉我们 NLU 是关于解压缩的。因此，我们现在拥有的是以下内容： 上面所说的是以下内容：机器学习是关于发现将大量数据泛化为单个函数。另一方面，由于 MTP，自然语言理解需要智能“解压缩”技术，以揭示所有缺失和隐含假设的文本。因此，机器学习和语言理解是不相容的——事实上，它们是矛盾的。 ML 本质上是一种基于在数据中找到一些模式（相关性）的范式。因此，该范式的希望是在捕捉自然语言中的各种现象时存在统计上的显着差异。但是，请考虑以下事项（有关此示例的讨论，请参阅此和此，因为它与 Winograd 模式挑战相关）：请注意，反义词/反义词，例如“小”和“大”（或“打开”和“关闭”，等）以相同的概率出现在相同的上下文中。因此，(1 a) 和 (1 b) 在统计上是等价的，但即使对于一个 4 岁的孩子来说， (1 a) 和 (1 b) 也有很大不同： (1 a) 中的“it”指的是“手提箱”而在（1 b）中它指的是“奖杯”。基本上，在简单的语言中，（1 a）和（1 b）在统计上是等价的，尽管在语义上相去甚远。因此，统计分析不能建模（甚至不能近似）语义——就这么简单！有人可能会争辩说，只要有足够多的例子，系统就可以建立统计显着性。但是需要多少个例子来“学习”如何解析（1）中的结构中的引用？</p><p>在机器学习/数据驱动的方法中，没有类型层次结构，我们可以在其中对“包”、“手提箱”、“公文包”等进行泛化声明，所有这些都被视为通用类型“容器”的子类型。因此，在纯粹的数据驱动范式中，上述每一项都是不同的，必须在数据中分别“看到”。如果我们将上述模式的所有细微句法差异添加到语义差异中（比如将“因为”更改为“尽管”——这也会更改对“它”的正确指称），那么粗略的计算会告诉我们一个 ML/数据驱动系统需要查看上述内容的 40,000,000 种变体，以了解如何解析诸如 (2) 之类的句子中的引用。如果有的话，这在计算上是不可信的。正如 Fodor 和 Pylyshyn 曾引用著名认知科学家 George Miller 的名言，为了捕捉 NLU 系统所需的所有句法和语义变化，神经网络可能需要的特征数量超过宇宙中的原子数量！这里的寓意是：统计无法捕捉（甚至不能近似）语义。逻辑学家长期以来一直在研究一种称为“内涵”（带有“s”）的语义概念。为了解释什么是“内涵”，让我们从所谓的意义三角形开始，下面举一个例子：因此，每个“事物”（或每个认知对象）都由三部分组成：指代概念的符号，以及这个概念有（有时）实际实例。我有时会说，因为“独角兽”这个概念没有“实际”实例，至少在我们生活的世界里！概念本身是其所有潜在实例的理想化模板（因此它接近于理想化的柏拉图形式！）您可以想象几个世纪以来哲学家、逻辑学家和认知科学家可能会如何争论概念的本质以及它们是如何定义的.不管那场辩论如何，我们可以就一件事达成一致：一个概念（通常由某个符号/标签所指）由一组属性和属性定义，也许还有额外的公理和既定事实等。 然而，一个概念与实际（不完美）实例不同。在完美的数学世界中也是如此。因此，例如，虽然下面的算术表达式都具有相同的扩展，但它们具有不同的内涵：因此，虽然所有表达式的计算结果都是 16，因此在一种意义上（它们的 VALUE）是相等的，但这只是它们的属性之一.实际上，上面的表达式还有其他几个属性，例如它们的句法结构（这就是 (a) 和 (d) 不同的原因）、运算符的数量、操作数的数量等。 VALUE（只是一个属性）是称为外延，而所有属性的集合就是内涵。在应用科学（工程、经济学等）中，如果这些对象仅在 VALUE 属性中相等，我们可以安全地认为它们相等，但在认知（尤其是语言理解）中，这种相等性失败了！这是一个简单的例子：假设 (1) 是真的——也就是说，假设 (1) 确实发生了，并且我们看到了它/见证了它。尽管如此，这并不意味着我们可以假设 (2) 为真，尽管我们所做的只是将 (1) 中的 &#39;16&#39; 替换为一个（假设）等于它的值。所以发生了什么事？我们用一个假定等于它的对象替换了真陈述中的一个对象，并且我们从真实的事物推断出不真实的事物！好吧，发生的事情是这样的：虽然在物理科学中，我们可以很容易地用一个具有一个属性的对象来替换它，但这在认知中是行不通的！这是另一个可能与语言更相关的例子：我们通过简单地将“亚历山大大帝的导师”替换为与其相等的值，即亚里士多德，得到了 (2)，这显然是荒谬的。同样，虽然“亚历山大大帝的导师”和“亚里士多德”在某种意义上是平等的（它们都作为所指对象具有相同的价值），但这两个思想对象在许多其他属性上是不同的。那么，这个关于“内涵”的讨论有什么意义呢？自然语言充斥着内涵现象，因为思想的对象——语言所传达的——具有不可忽视的内涵方面。但是机器学习/数据驱动方法的所有变体都是纯粹的外延——它们对对象的数字（向量/张量）表示进行操作，而不是它们的符号和结构属性，因此在这种范式中，我们无法用自然语言对各种内涵现象进行建模。顺便说一下，神经网络纯粹是外延的，因此不能表示内涵这一事实是它们总是容易受到对抗性攻击的真正原因，尽管这个问题超出了本文的范围。我在本文中讨论了三个证明机器学习和数据驱动方法甚至与 NLU 无关的原因（尽管这些方法可能用于一些本质上是压缩任务的文本处理任务）。以上三个原因中的每一个都足以结束这趟失控的火车，我们的建议是停止尝试记忆语言的徒劳努力。在传达我们的想法时，我们传递高度压缩的语言表达，需要用大脑来解释和“揭示”所有缺失但隐含假设的背景信息。</p><p>语言是我们用来编码我们可能拥有的无限数量思想的外部人工制品。在很多方面，那么，在......</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://thegradient.pub/machine-learning-wont-solve-the-natural-language-understanding-challenge/">https://thegradient.pub/machine-learning-wont-solve-the-natural-language-understanding-challenge/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/方法/">#方法</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1071595.html"><img src="http://img2.diglog.com/img/2021/8/thumb_6bf5887c1877f4aec6acdfed74853dfe.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1071595.html">Yann LeCun 开始从事人工智能和最近的自我监督学习研究</a></div><span class="my_story_list_date">2021-8-6 2:43</span></div><div class="col-sm"><div><a target="_blank" href="/story/1071312.html"><img src="http://img2.diglog.com/img/2021/8/thumb_d25e971a608ecc2145b06859f5f7e2c4.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1071312.html">逐行学习 Julia</a></div><span class="my_story_list_date">2021-8-5 20:47</span></div><div class="col-sm"><div><a target="_blank" href="/story/1070852.html"><img src="http://img2.diglog.com/img/2021/7/thumb_54569667c5a255b970a1b6b4751dcd49.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1070852.html">深度学习的未来是光子</a></div><span class="my_story_list_date">2021-7-31 23:32</span></div><div class="col-sm"><div><a target="_blank" href="/story/1070680.html"><img src="http://img2.diglog.com/img/2021/7/thumb_ea000cc86049b842b12c6bead90a1097.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1070680.html">平台教非专家使用机器学习</a></div><span class="my_story_list_date">2021-7-31 1:27</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>