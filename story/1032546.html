<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么皮肤损伤是花生，而脑瘤是坚硬的坚果</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">为什么皮肤损伤是花生，而脑瘤是坚硬的坚果</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-01 05:42:28</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/d5e8e075af2e9e7b24f2df6f0866e641.png"><img src="http://img2.diglog.com/img/2020/11/d5e8e075af2e9e7b24f2df6f0866e641.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>为什么医学图像分析中的一些问题对AI来说比其他问题更难，我们能做些什么呢？</p><p>在一篇里程碑式的论文[1]中，艾伦·图灵提出了一项评估计算机智能的测试。这项测试后来被恰当地命名为图灵测试，描述了一个人通过书面笔记与计算机或另一个人进行互动。如果这个人分不清谁是谁，计算机就会通过测试，我们可以得出结论，它显示出人类水平的智能行为。</p><p>在十多年后的另一篇里程碑式的论文[2]中，远远领先于他那个时代的肌肉骨骼放射科医生格威姆·洛德威克(Gwilym Lodwick)提出使用计算机检查医学图像，并设想。</p><p>&#34；[...]。有效使用的诊断程序库可以使任何称职的诊断放射科医生成为国际专家的等价物[...]。</p><p>他创造了计算机辅助诊断(CAD)这个术语，指的是帮助放射科医生解释和量化医学图像中的异常情况的计算机系统。这导致了计算机科学和放射学相结合的一个全新的研究领域。</p><p>随着高效深度学习算法的出现，人们变得更加雄心勃勃，计算机辅助诊断领域正慢慢转向计算机化诊断。杰弗里·辛顿甚至声称，我们应该立即停止培训放射科医生，因为从现在开始的五年或十年后，深层神经网络很可能在所有领域都会比人类表现得更好。</p><p>然而，要让计算机被接受为训练有素的医学专业人员的可行替代品，要求它超过人类，或者至少与人类平起平坐，似乎是公平的。这个医学图像分析的图灵测试，就像它经常被提到的那样，对于一些医学问题即将得到解决，但许多问题仍然悬而未决。如何解释这种差异呢？是什么让一个问题比另一个更难？</p><p>本文对医学成像问题的可学习性进行了非正式分析，并将其分解为三个主要部分：</p><p>我们随后将分析一组论文，所有这些论文都声称在一些医学图像分析任务中具有人类水平的性能，看看它们有什么共同之处。之后，我们将总结真知灼见，并提出展望。</p><p>机器学习算法本质上是从一组训练样本中提取和压缩信息来解决某一特定任务。在监督学习的情况下，这些信息可以来自不同的来源：标签和样本。粗略地说，数据越多，问题就越容易学习。数据污染越少，问题就越容易学习。标签越干净，问题就越容易学。这一点将在下面详细阐述。</p><p>训练数据是机器学习应用成功的最重要的元素之一。对于许多计算机视觉问题，图像可以从社交媒体中挖掘出来，并由非专家进行注释。对于医疗数据来说，情况并非如此。通常很难获得，并且需要专业知识来生成标签。因此，大型的公开可用的和精心策划的数据集仍然很稀缺。</p><p>尽管如此，一些(半)公开的数据集已经出现，例如Kaggle糖尿病视网膜病变挑战[3]、LIDC-IDRI胸部CT数据集[4]、NIH胸部X射线数据集[5]、斯坦福的胸部X射线数据集[6]、Radboud的数字病理数据集[7]、HAM10000皮肤病变数据集[8]和OPTIMAM乳房X光照相数据集[9]。这些数据集的示例图像如图1所示。这些公共数据集的可用性加速了医学图像分析深度学习解决方案的研究和开发。</p><p>虽然在许多机器学习应用程序中，可用数据量非常重要，而且往往是一个限制因素，但它不是唯一的变量。无论数据的大小如何，向统计模型提供随机噪声不会导致模型学习区分猫和狗。我们将数据复杂性分为三种类型：图像质量、维数和可变性。</p><p>从医学图像分析开始，图像质量就一直是一个重要的研究课题，它有几个方面的定义[11]。信噪比通常被用来表征质量，尽管其他方面，如伪影的存在、运动模糊和设备的正确处理也决定了质量[12]。例如，在乳房X光照相的情况下，乳房可能会被错放在机器中，导致重要结构的可见性很差。图像可能曝光不足或过度曝光，起搏器、植入物或化疗端口等伪影可能会使图像难以阅读。这些示例如图2所示。</p><p>信噪比的定义有很多种。从ML的角度来看，最简单的方法之一是与任务相关的信息占图像中总信息的比例。</p><p>维数灾难是机器学习理论中的一个众所周知的问题，它描述了这样的观察结果，即随着每个特征维数的增加，需要指数级增加的数据点才能获得合理的空间填充。通过引入卷积形式的权重分担和使用非常深入的神经网络，部分诅咒已经解除。然而，医学图像通常具有比自然图像高得多的维度形式(例如，ImageNet仅具有224x224的图像)。</p><p>具有复杂结构的2D图像，来自MRI和CT的3D图像，甚至4D图像(例如，体积随着时间的演变)都是标准。复杂的数据结构通常意味着您必须手工制作一个能够很好地解决问题的体系结构，并利用您知道的数据中的结构来使模型工作，例如，请参见[13，14]。</p><p>不同制造商的扫描仪使用不同的材料、采集参数和后处理算法[15，16]。不同的病理实验室使用不同的染色方法[17]。有时，由于处理采集的人不同，即使是用同一扫描仪从同一实验室采集的数据看起来也会有所不同。这些都增加了学习问题的复杂性，因为模型需要变得不变，或者工程师必须通过手工制作预处理算法来弥补这一点，这些预处理算法对数据进行标准化，使用大量数据增强，或者改变网络体系结构，使它们对每个不相关的源都是不变的。</p><p>尽管无监督的方法越来越受欢迎，但大多数问题仍然是通过监督来解决的，在这种情况下，我们还有另一个复杂性的来源：标签。我们再次分离了三个因素：(1)描述提供标签的细节的标签粒度，(2)涉及输出空间复杂性的类别复杂度，以及(3)与标签噪声相关的标注质量。</p><p>数据可以在不同的粒度级别进行注释。通常，在训练期间使用的标签的粒度不必与在推理期间生成的粒度相同。例如，当仅在图像级标签上训练(例如，多实例学习)时可以生成分割图(图像中每个像素的标签)，当仅在类标签上训练时可以生成报告，当在像素级标签上训练时可以生成类级标签，等等。图4中给出了概述，下面详细说明配置。</p><p>像素级标签-分段：在此设置中，算法为案例中的每个像素提供轮廓注释和/或输出标签。在早期的医学成像系统中，分割通常被用作检测算法的一种预处理形式，但在2012年ImageNet革命之后，这种情况变得不那么常见了。分割算法也被用来量化异常，这可以再次用于在临床试验中获得一致的读数，或者作为放射治疗等治疗的准备。对于自然图像，通常在基于实例的分割和语义分割之间进行区分。在第一种情况下，算法应该为类的每个实例提供不同的标签，在第二种情况下，只为每个类单独提供不同的标签。这种区别在生物医学成像中不太常见，在生物医学成像中，图像(例如肿瘤)中通常只有一个类别的实例，和/或检测一个实例就足以将图像分类为阳性。</p><p>边界框-检测：注释器也可以只在感兴趣的区域周围画一个框，而不是为图像中的每个像素提供/生成标签。这可以节省注释过程中的时间，但提供的信息可能较少：长方体有四个自由度，轮廓通常更多。很难说这对算法有什么影响。</p><p>研究级别标签-分类：另一种更简单的注释方式是只标记具有特定异常的病例。这对于整个研究已经是围绕相关病理的作物的问题很有意义，例如皮肤病变分类和彩色眼底成像中的异常分类。对于其他问题来说，这要困难得多，比如乳腺癌的检测，在这种情况下，一项研究通常是由四张大图像组成的时间序列，而病理只占总研究规模的一小部分。该算法首先要学习图像的哪些部分是相关的，然后才能学习实际的特征。</p><p>报告-报告生成：同样，更简单的注解方式是使用直接从放射学报告中提取的标签，而不是注释器。这种类型的标签有时被称为银标注释[18]。从这些数据中学习可能很困难，因为需要首先使用自然语言处理来从报告中提取标签，这可能会增加标签噪声。在这类标签中，我们可以再做一个区分：结构化报告或非结构化报告。在第一种情况下，医生被限制在一个特定的标准，在后一种情况下，医生可以自由地写他们想要的东西。无论哪一个最好，都是一个有趣的争论：非结构化报告有可能包含更多信息，但更难解析。</p><p>将图像分类为两个不同的类别或1000个不同的类别之间存在差异。第一种情况意味着将空间分成两个可分离的区域，后者分成1000个，这可能更难。</p><p>在许多应用中，例如，检测胸部X光中的异常，一个样本可以同时具有多个标签。这如图6所示，它显示了胸部X光8数据集中的类的共现关系的圆形图。</p><p>除了标注的粒度外，还有一个因素可以影响人工智能算法的可学习性和最终性能：标注的准确性。对于大多数医学成像问题，基本上有两种方法来处理注释：使用人类参考或其他已知比人类更准确的测试。</p><p>如果标签是由医学专业人员生成的，则通常存在大量的读取器间的可变性(不同的读取器对同一样本给出不同的标签)和读取器内的可变性(同一读取器在不同的时间对同一样本进行注释时给出不同的标签)。这种不一致导致了标签噪声，这使得训练好的模型变得更加困难。</p><p>这其中一个重要的推论是，使用这些标签来评估模型，很难证明你实际上比人类表现得更好。因此，使用这类标签的问题是受标签限制的：即使我们拥有最好的可能架构、无限数量的训练数据和计算能力，我们也永远无法大幅超越人类，因为他们定义了基本事实。</p><p>成像通常被用作第一次检查，因为它通常是廉价和非侵入性的。如果发现可疑的东西，患者会接受更多的、准确度更高的检查。例如，在胸部X光图像中，我们有时会进行微生物培养，以确认患者是否患有结核病。对于肿瘤的检测，可以采用活组织检查和组织病理学结果。这些类型的标签提供了更少的噪音和更准确的地面事实。</p><p>使用这些标签的问题是受数据限制的：如果我们只给一个算法足够的数据，对它进行良好的训练，并使用正确的架构，它应该能够在某个时候超越人类，前提是第二次测试比阅读图像的人更好。</p><p>为了更好地了解最新的技术和未来的发展，我们将分析几篇最近的论文。这绝不是一个详尽的清单，我们也不认为论文中的结果是准确的。在未来的临床试验中，当数据和源代码公之于众时，或者当方法在挑战中进行评估时，所有关于(超级)人类表现的说法都变得更加可信。</p><p>下表提供了每篇论文的摘要、模型训练所依据的数据以及用于问题的标签。有关更严格的、面向临床的概述，请参考[19]和[20]。</p><p>Rajpurkar，Pranav等人。Chexnet：基于深度学习的胸部x光片的放射科医师级肺炎检测。arxiv预印本：1711.05225(2017年)。</p><p>一种自动检测肺炎(的放射证据)的模型。通过将其与几位放射科医生的放射标签进行比较来断言人类的表现。</p><p>该数据集包含14种病理的标签，这些标签是使用NLP提取的，因此，使用了银标准标签。不过，只有肺炎的征兆才用作标签，问题只限於标签，该系统会与放射科医生所订的标签作比较。</p><p>Putha，Preetham等人。人工智能可以可靠地报告胸部X光吗？：放射科医生对训练了230万次X光的算法进行了验证。&#34；arxiv预印本：1807.07455(2018年)。</p><p>使用为每个发现训练的不同模型检测到九种不同的发现。大约有四种病理显示了人类的表现。</p><p>大约120K正视X光。2D单视图像缩小到224x222。总共使用了120万张胸部X光，缩小到未指明的大小。应用归一化技术来减少变异。</p><p>这些标签是使用NLP从放射学报告中提取的，因此是银标准标签。多类标签，将其视为二进制分类问题。</p><p>欧文、杰里米等人。ChExpert：具有不确定性标签和专家比较的大型胸部X光照片数据集。AAAI人工智能会议论文集。第33卷。2019年。</p><p>一个模型被训练来区分14个不同的发现。这些异常中的五个都显示了人类的表现。</p><p>采用NLP(银色标准标签)提取标签，标签受限，为每个病理组织训练一个模型，并使用基于不确定性的标签。</p><p>Hwang，Eui jin，等人。基于深度学习的胸片上主要胸部疾病自动检测算法的开发和验证。&#34；JAMA NETWORK OPEN 2.3(2019年)：e191095-e191095。</p><p>提出了一种检测四种胸片异常的算法，并与15名医生进行了比较，结果表明该算法具有较高的检测性能。</p><p>约有55K张X线片发现正常，35K张发现异常，用来制作模型。</p><p>二分类，四种异常合并为一种。采用组织病理学/外部试验和放射学标签相结合的方法。很大程度上是标签限制的问题。</p><p>南、鞠刚等人。基于深度学习的胸片恶性肺结节自动检测算法的开发与验证。放射学290.1(2019年)：第218-228页。</p><p>一个模型被训练来检测胸片上的恶性肺结节，该模型与18个人类读取器进行了比较，显示出优于大多数读取器的性能。</p><p>像素级和图像级标签都用于训练。恶性肿瘤是经活检证实的，因此这是一个非常有限的问题。这个问题被表述为二进制分类。</p><p>阿迪拉、迭戈等人。基于低剂量胸部CT的三维深度学习的端到端肺癌筛查。自然医学25.6(2019)：954-961。</p><p>深度神经网络在低剂量CT肺癌分类中的表现优于6位人类阅读器。</p><p>42K低剂量CT扫描，裁剪并调整大小为240x240x240。(3D数据)。</p><p>模型的不同部分在区域水平或病例水平的标签上进行训练(结果由活检决定)。使用的是经生物学证实的恶性肿瘤(数据有限)。二进制分类。</p><p>麦金尼、斯科特·迈耶等人。乳腺癌筛查人工智能系统的国际评估，“自然”杂志577.7788(2020年)：89-94页。</p><p>DNN被训练来在筛查乳房X光检查中检测乳腺癌。该模型与英国和美国的数据集进行了比较，并显示出在这两个数据集上的表现都优于放射科医生。</p><p>这位模特接受了大约130K的乳房X光检查培训。乳房X光照片由四个视图组成，每个乳房两个视图。</p><p>二进制分类。基本事实是根据组织病理学报告确定的。这是一个数据受限的问题。</p><p>Esteva，A.，Kuprel，B.，Novoa，R.A.，Ko，J.，Swetter，S.M.，Blau，H.M.和Thrun，S.，2017。用深度神经网络对皮肤癌进行皮肤科医生级分类。“自然”，542(7639)，第115-118页。</p><p>提出了一种将皮肤病图像分为恶性或良性的模型，并与21名认证皮肤科医生进行了比较。</p><p>该模型是基于大约130K的图像开发的。数据集包括来自普通相机的彩色图像和皮肤镜图像。图像缩放到299x299。</p><p>该模型在活检证实和放射科医生标记的病例的组合上进行了训练。因此，这是一个标签受限的问题。该模型使用许多不同的类别进行训练，与人类在二进制分类问题上所做的比较。</p><p>韩秀生，Kim，M.S.，Lim，W.，Park，G.H.，Park，I.，Chang，S.E.，2018。基于深度学习算法的皮肤良恶性肿瘤临床图像分类。皮肤病研究杂志，138(7)页，1529-1538页。</p><p>提出了一种可以对12种不同皮肤肿瘤进行分类的模型。该算法的性能与一组16名皮肤科医生进行了比较，这些皮肤科医生只使用经活检证实的病例。该模型的表现与皮肤科医生小组相当。</p><p>该网络是在大约17K图像上开发的。这组图像包括来自不同大小的普通相机的彩色图像(CNN使用的大小未指明)。</p><p>大多数病理检查都被活检证实，因此这是一个数据有限的问题。最终的模型采用12类分类。</p><p>Bejnordi，B.E.，Veta，M.，Van Diest，P.J.，Van Ginneken，B.，Karssemeijer，N.，Litjens，G.，Van Der Laak，J.A.，Hermsen，M.，Manson，Q.F.，Balkenholh，M.和Geessink，O.。深度学习算法检测女性乳腺癌淋巴结转移的诊断评估。“美国医学会杂志”，318(22)，第2199-2210页。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://thegradient.pub/why-skin-lesions-are-peanuts-and-brain-tumors-harder-nuts/">https://thegradient.pub/why-skin-lesions-are-peanuts-and-brain-tumors-harder-nuts/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/损伤/">#损伤</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/lesions/">#lesions</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/标签/">#标签</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>