<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>为什么Uber Engineering从Postgres切换到MySQL Why Uber Engineering Switched from Postgres to MySQL</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Why Uber Engineering Switched from Postgres to MySQL<br/>为什么Uber Engineering从Postgres切换到MySQL </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-02-27 18:34:17</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/2/b9ae6585dd0a046ce56de21615ea2c3c.png"><img src="http://img2.diglog.com/img/2021/2/b9ae6585dd0a046ce56de21615ea2c3c.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>The early architecture of Uber consisted of a monolithic backend application written in Python that used   Postgres  for data persistence. Since that time, the architecture of Uber has changed significantly, to a model of   microservices  and new data platforms. Specifically, in many of the cases where we previously used Postgres, we now use   Schemaless , a novel database sharding layer built on top of MySQL. In this article, we’ll explore some of the drawbacks we found with Postgres and explain the decision to build Schemaless and other backend services on top of MySQL.</p><p>Uber的早期架构包括一个用Python编写的整体式后端应用程序，该应用程序使用Postgres进行数据持久化。自那时以来，Uber的架构已发生了巨大变化，已成为微服务和新数据平台的模型。具体来说，在以前使用Postgres的许多情况下，我们现在使用Schemaless，这是一种基于MySQL的新型数据库分片层。在本文中，我们将探讨Postgres所发现的一些缺点，并解释在MySQL之上构建Schemaless和其他后端服务的决定。</p><p>    We’ll look at all of these limitations through an analysis of Postgres’s representation of table and index data on disk, especially when compared to the way MySQL represents the same data with its   InnoDB storage engine . Note that the analysis that we present here is primarily based on our experience with the somewhat old Postgres 9.2 release series. To our knowledge, the internal architecture that we discuss in this article has not changed significantly in newer Postgres releases, and the basic design of the on-disk representation in 9.2 hasn’t changed significantly since at least the Postgres 8.3 release (now nearly 10 years old).</p><p>    我们将通过分析Postgres在磁盘上的表和索引数据的表示法来研究所有这些限制，尤其是与MySQL用其InnoDB存储引擎表示相同数据的方式进行比较时。请注意，我们在此处进行的分析主要是基于我们对较旧的Postgres 9.2版本系列的经验。据我们所知，我们在本文中讨论的内部体系结构在较新的Postgres发行版中并未发生显着变化，并且至少自Postgres 8.3发行版以来，9.2中的磁盘上表示形式的基本设计没有发生显着变化（现在已接近10个）。岁）。</p><p>   Implement a   multiversion concurrency control  (MVCC) mechanism so that different connections have a transactional view of the data they work with</p><p>   实施多版本并发控制（MVCC）机制，以便不同的连接对其使用的数据具有事务性视图</p><p> Considering how all of these features will work together is an essential part of designing how a database represents data on disk.</p><p> 考虑所有这些功能如何协同工作是设计数据库如何表示磁盘上数据的重要部分。</p><p> One of the core design aspects of Postgres is immutable row data. These immutable rows are called “tuples” in Postgres parlance. These tuples are uniquely identified by what Postgres calls a   ctid . A  ctid  conceptually represents the on-disk location (i.e., physical disk offset) for a tuple. Multiple  ctids can potentially describe a single row (e.g., when multiple versions of the row exist for MVCC purposes, or when old versions of a row have not yet been reclaimed by the   autovacuum  process). A collection of organized tuples form a table. Tables themselves have indexes, which are organized as data structures (typically B-trees) that map index fields to a  ctid  payload.</p><p> Postgres的核心设计方面之一是不可变的行数据。这些不变的行在Postgres中被称为“元组”。这些元组由Postgres所谓的ctid唯一标识。 ctid从概念上表示元组的磁盘上位置（即物理磁盘偏移）。多个ctid可以潜在地描述单个行（例如，当出于MVCC目的而存在该行的多个版本时，或者当自动真空处理尚未回收该行的旧版本时）。有组织的元组的集合构成一个表。表本身具有索引，这些索引被组织为将索引字段映射到ctid有效负载的数据结构（通常是B树）。</p><p> Typically, these  ctids  are transparent to users, but knowing how they work helps you understand the on-disk structure of Postgres tables. To see the current  ctid  for a row, you can add “ ctid ” to the column list in a  WHERE  clause:</p><p> 通常，这些ctid对用户是透明的，但是了解它们的工作原理有助于您了解Postgres表的磁盘结构。要查看行的当前ctid，可以在WHERE子句的列列表中添加“ ctid”：</p><p> uber@[local] uber=&gt; SELECT ctid, * FROM my_table LIMIT 1; -[ RECORD 1 ]--------+------------------------------ ctid                 | (0,1) ...other fields here...</p><p> uber @ [local] uber =＆gt; SELECT ctid，*从my_table LIMIT 1开始； -[记录1] -------- ++ ------------------------------ ctid | （0,1） ...这里的其他领域... </p><p> To explain the details of the layout, let’s consider an example of a simple users table. For each user, we have an auto-incrementing user ID primary key, the user’s first and last name, and the user’s birth year. We also define a compound secondary index on the user’s full name (first and last name) and another secondary index on the user’s birth year. The   DDL  to create such a table might be like this:</p><p>为了说明布局的详细信息，让我们考虑一个简单的用户表的示例。对于每个用户，我们都有一个自动递增的用户ID主键，用户的名字和姓氏以及用户的出生年份。我们还在用户的全名（名字和姓氏）上定义了复合二级索引，并在用户的出生年份定义了另一个二级索引。创建这样的表的DDL可能是这样的：</p><p> CREATE TABLE users (     id SERIAL,     first TEXT,     last TEXT,     birth_year INTEGER,     PRIMARY KEY (id) );  CREATE INDEX ix_users_first_last ON users (first, last);  CREATE INDEX ix_users_birth_year ON users (birth_year);</p><p> CREATE TABLE用户（  ID SERIAL，  第一个TEXT，  最后一个TEXT，  birth_year INTEGER，  主键（id） ）;  创建索引ix_users_first_last ON用户（第一个，最后一个）；  CREATE INDEX ix_users_birth_year开用户（birth_year）；</p><p> Note the three indexes in this definition: the primary key index plus the two secondary indexes we defined.</p><p> 请注意此定义中的三个索引：主键索引加上我们定义的两个辅助索引。</p><p> For the examples in this article we’ll start with the following data in our table, which consists of a selection of influential historical mathematicians:</p><p> 对于本文中的示例，我们将从表中的以下数据开始，该数据由一些有影响力的历史数学家组成：</p><p>  As described earlier, each of these rows implicitly has a unique, opaque  ctid. Therefore, we can think of the internal representation of the table like this:</p><p>  如前所述，这些行中的每一个都隐式具有唯一的，不透明的ctid。因此，我们可以这样考虑表的内部表示形式：</p><p>    The B-tree is defined on the  id  field, and each node in the B-tree holds the  ctid  value. Note that in this case, the order of the fields in the B-tree happens to be the same as the order in the table due to the use of an auto-incrementing  id , but this doesn’t necessarily need to be the case.</p><p>    B树在id字段上定义，并且B树中的每个节点都保存ctid值。请注意，在这种情况下，由于使用了自动递增的ID，因此B树中字段的顺序恰好与表中的顺序相同，但不一定是这种情况。</p><p> The secondary indexes look similar; the main difference is that the fields are stored in a different order, as the B-tree must be organized lexicographically. The ( first ,  last ) index starts with first names toward the top of the alphabet:</p><p> 二级索引看起来很相似。主要区别在于字段的存储顺序不同，因为B树必须按字典顺序组织。 （first，last）索引以名字开头，朝向字母表的顶部： </p><p>    As you can see, in both of these cases the  ctid  field in the respective secondary index is not increasing lexicographically, unlike in the case of an auto-incrementing primary key.</p><p>如您所见，在这两种情况下，二级索引中的ctid字段在字典上都没有增加，这与自动递增主键的情况不同。</p><p> Suppose we need to update a record in this table. For instance, let’s say we’re updating the birth year field for another estimate of al-Khwārizmī’s year of birth, 770 CE. As we mentioned earlier, row tuples are immutable. Therefore, to update the record, we add a new tuple to the table. This new tuple has a new opaque  ctid , which we’ll call   I  . Postgres needs to be able to distinguish the new, active tuple at   I  from the old tuple at   D . Internally, Postgres stores within each tuple a version field and pointer to the previous tuple (if there is one). Accordingly, the new structure of the table looks like this:</p><p> 假设我们需要更新该表中的一条记录。举例来说，假设我们要更新出生年份字段，以估算赫卡里兹米（Khwārizmī）的出生年份770 CE。如前所述，行元组是不可变的。因此，为了更新记录，我们向表中添加了一个新的元组。这个新的元组具有一个新的不透明ctid，我们将其称为I。 Postgres需要能够将I处的新活动元组与D处的旧元组区分开。在内部，Postgres在每个元组中存储一个版本字段和一个指向前一个元组的指针（如果有的话）。因此，表的新结构如下所示：</p><p>  As long as two versions of the al-Khwārizmī row exist, the indexes must hold entries for both rows. For brevity, we omit the primary key index and show only the secondary indexes here, which look like this:</p><p>  只要存在al-Khwārizmī行的两个版本，索引就必须同时包含两个行的条目。为简便起见，我们省略了主键索引，而在此处仅显示了二级索引，如下所示：</p><p>    We’ve represented the old version in red and the new row version in green. Under the hood, Postgres uses  another field holding the row version to determine which tuple is most recent. This added field lets the database determine which row tuple to serve to a transaction that may not be allowed to see the latest row version.</p><p>    我们用红色表示旧版本，用绿色表示新行。在幕后，Postgres使用另一个保存行版本的字段来确定哪个元组是最新的。通过此添加的字段，数据库可以确定哪个行元组可服务于可能不允许查看最新行版本的事务。</p><p>    When we insert a new row into a table, Postgres needs to replicate it if streaming replication is enabled. For crash recovery purposes, the database already maintains a   write-ahead log  (WAL) and uses it to implement   two-phase commit . The database must maintain this WAL even when streaming replication is not enabled because the WAL allows the atomicity and durability aspects of   ACID .</p><p>    当我们在表中插入新行时，如果启用了流复制，则Postgres需要对其进行复制。为了崩溃恢复，数据库已经维护了一个预写日志（WAL），并使用它来实现两阶段提交。即使未启用流复制，数据库也必须维护此WAL，因为WAL允许ACID的原子性和持久性。</p><p> We can understand the WAL by considering what happens if the database crashes unexpectedly, like during a sudden power loss. The WAL represents a ledger of the changes the database plans to make to the on-disk contents of tables and indexes. When the Postgres daemon first starts up, the process compares the data in this ledger with the actual data on disk. If the ledger contains data that isn’t reflected on disk, the database corrects any tuple or index data to reflect the data indicated by the WAL. It then rolls back any data that appears in the WAL but is from a partially applied transaction (meaning that the transaction was never committed).</p><p> 我们可以通过考虑如果数据库意外崩溃（例如突然断电期间）会发生什么情况来了解WAL。 WAL代表数据库计划对表和索引的磁盘上内容进行的更改的分类帐。当Postgres守护程序首次启动时，该过程会将此分类帐中的数据与磁盘上的实际数据进行比较。如果分类帐中包含未反映在磁盘上的数据，则数据库会更正任何元组或索引数据以反映WAL指示的数据。然后，它回滚出现在WAL中但来自部分应用的事务的任何数据（这意味着该事务从未提交）。</p><p> Postgres implements streaming replication by sending the WAL on the master database to replicas. Each replica database effectively acts as if it’s in crash recovery, constantly applying WAL updates just as it would if it were starting up after a crash. The only difference between streaming replication and actual crash recovery is that replicas in “hot standby” mode serve read queries while applying the streaming WAL, whereas a Postgres database that’s actually in crash recovery mode typically refuses to serve any queries until the database instance finishes the crash recovery process.</p><p> Postgres通过将主数据库上的WAL发送到副本来实现流复制。每个副本数据库都有效地像在崩溃恢复中一样，不断地应用WAL更新，就像崩溃后启动一样。流复制和实际崩溃恢复之间的唯一区别是，处于“热备用”模式的副本在应用流WAL时会提供读取查询，而实际上处于崩溃恢复模式的Postgres数据库通常会拒绝提供任何查询，直到数据库实例完成崩溃恢复过程。 </p><p> Because the WAL is actually designed for crash recovery purposes, it contains low-level information about the on-disk updates. The content of the WAL is at the level of the actual on-disk representation of row tuples and their disk offsets (i.e., the row  ctids ). If you pause a Postgres master and replica when the replica is fully caught up, the actual on-disk content on the replica exactly matches what’s on the master byte for byte. Therefore, tools like   rsync  can fix a corrupted replica if it gets out of date with the master.</p><p>因为WAL实际上是为崩溃恢复目的而设计的，所以它包含有关磁盘更新的低级信息。 WAL的内容在行元组及其磁盘偏移量（即行ctids）的实际磁盘上表示形式上。如果在副本完全被追上时暂停Postgres主副本，则副本上的实际磁盘内容与主字节上的内容完全匹配。因此，如果rsync之类的副本与主副本过期，则rsync之类的工具可以修复该副本。</p><p>    The first problem with Postgres’s design is known in other contexts as   write amplification . Typically, write amplification refers to a problem with writing data to SSD disks: a small logical update (say, writing a few bytes) becomes a much larger, costlier update when translated to the physical layer. The same issue arises in Postgres. In our previous example when we made the small logical update to the birth year for al-Khwārizmī, we had to issue at least four physical updates:</p><p>    Postgres设计的第一个问题在其他情况下称为写入放大。通常，写放大是指将数据写到SSD磁盘时遇到的问题：小的逻辑更新（例如，写入几个字节）在转换到物理层时会变得更大，更昂贵。在Postgres中也会出现同样的问题。在前面的示例中，当我们对哈瓦里兹米（Khwārizmī）的出生年份进行了较小的逻辑更新时，我们必须发布至少四个物理更新：</p><p>  In fact, these four updates only reflect the writes made to the main tablespace; each of these writes needs to be reflected in the WAL as well, so the total number of writes on disk is even larger.</p><p>  实际上，这四个更新仅反映对主表空间的写操作。这些写操作中的每一个也需要反映在WAL中，因此磁盘上的写操作总数甚至更大。</p><p> What’s noteworthy here are updates 2 and 3. When we updated the birth year for al-Khwārizmī, we didn’t actually change his primary key, nor did we change his first and last name. However, these indexes still must be updated with the creation of a new row tuple in the database for the row record. For tables with a large number of secondary indexes, these superfluous steps can cause enormous inefficiencies. For instance, if we have a table with a dozen indexes defined on it, an update to a field that is only covered by a single index must be propagated into all 12 indexes to reflect the  ctid  for the new row.</p><p> 这里值得注意的是更新2和3。当我们更新al-Khwārizmī的出生年份时，我们实际上没有更改他的主键，也没有更改他的名字和姓氏。但是，仍然必须通过在数据库中为行记录创建新的行元组来更新这些索引。对于具有大量二级索引的表，这些多余的步骤可能会导致极大的效率低下。例如，如果我们在一个表上定义了十二个索引，则必须仅将一个索引覆盖的字段更新传播到所有12个索引中，以反映新行的ctid。</p><p>  This write amplification issue naturally translates into the replication layer as well because replication occurs at the level of on-disk changes. Instead of replicating a small logical record, such as “Change the birth year for  ctid   D  to now be 770,” the database instead writes out WAL entries for all four of the writes we just described, and all four of these WAL entries propagate over the network. Thus, the write amplification problem also translates into a replication amplification problem, and the Postgres replication data stream quickly becomes extremely verbose, potentially occupying a large amount of bandwidth.</p><p>  由于复制发生在磁盘更改级别，因此该写放大问题自然也转化为复制层。数据库没有复制一个小的逻辑记录，如“将ctid D的出生年份更改为现在的770”，而是为我们刚才描述的所有四次写写了WAL条目，并且所有这四个WAL条目都传播了过来。网络。因此，写放大问题也转化为复制放大问题，并且Postgres复制数据流很快变得非常冗长，可能占用大量带宽。</p><p> In cases where Postgres replication happens purely within a single data center, the replication bandwidth may not be a problem. Modern networking equipment and switches can handle a large amount of bandwidth, and many hosting providers offer free or cheap intra–data center bandwidth. However, when replication must happen between data centers, issues can quickly escalate. For instance, Uber originally used physical servers in a colocation space on the West Coast. For disaster recovery purposes, we added servers in a second East Coast colocation space. In this design we had a master Postgres instance (plus replicas) in our western data center and a set of replicas in the eastern one.</p><p> 如果Postgres复制仅发生在单个数据中心内，则复制带宽可能不是问题。现代网络设备和交换机可以处理大量带宽，许多托管服务提供商提供免费或廉价的内部数据中心带宽。但是，当必须在数据中心之间进行复制时，问题可能会迅速升级。例如，Uber最初在西海岸的托管空间中使用物理服务器。为了灾难恢复，我们在第二个东海岸托管空间中添加了服务器。在这种设计中，我们在西部数据中心有一个主Postgres实例（加上副本），在东部有一个副本集。</p><p> Cascading replication  limits the inter–data center bandwidth requirements to the amount of replication required between just the master and a single replica, even if there are many replicas in the second data center. However, the verbosity of the Postgres replication protocol can still cause an overwhelming amount of data for a database that uses a lot of indexes. Purchasing very high bandwidth cross-country links is expensive, and even in cases where money is not an issue it’s simply not possible to get a cross-country networking link with the same bandwidth as a local interconnect. This bandwidth problem also caused issues for us with WAL archival. In addition to sending all of the WAL updates from West Coast to East Coast, we archived all WALs to a file storage web service, both for extra assurance that we could restore data in the event of a disaster and so that archived WALs could bring up new replicas from database snapshots. During peak traffic early on, our bandwidth to the storage web service simply wasn’t fast enough to keep up with the rate at which WALs were being written to it.</p><p> 级联复制将数据中心间的带宽要求限制为仅在主副本和单个副本之间所需的复制数量，即使第二个数据中心中有很多副本也是如此。但是，Postgres复制协议的详细信息仍然可能导致使用大量索引的数据库的数据量巨大。购买非常高带宽的越野链接非常昂贵，即使在钱不成问题的情况下，也根本不可能获得具有与本地互连相同带宽的越野网络链接。这个带宽问题也给我们的WAL归档带来了麻烦。除了将所有WAL更新从西海岸发送到东海岸之外，我们还将所有WAL都存档到文件存储Web服务中，以确保在发生灾难时我们可以恢复数据，并确保存档的WAL可以启动。数据库快照中的新副本。在早期的高峰流量期间，存储Web服务的带宽根本不够快，无法跟上WAL写入速率。 </p><p>  During a routine master database promotion to increase database capacity, we ran into a Postgres 9.2 bug. Replicas followed   timeline switches   incorrectly , causing some of them to misapply some WAL records. Because of this bug, some records that should have been marked as inactive by the versioning mechanism weren’t actually marked inactive.</p><p>在例行升级master数据库以增加数据库容量的过程中，我们遇到了Postgres 9.2错误。副本跟随时间轴开关不正确，导致其中一些错误地应用了一些WAL记录。由于存在此错误，某些本应由版本控制机制标记为无效的记录实际上并未被标记为无效。</p><p>   This query would return two records: the original al-Khwārizmī row with the 780 CE birth year, plus the new al-Khwārizmī row with the 770 CE birth year. If we were to add  ctid  to the  WHERE  list, we would see different  ctid  values for the two returned records, as one would expect for two distinct row tuples.</p><p>   该查询将返回两条记录：原始的al-Khwārizmī行与780 CE出生年份，以及新的al-Khwārizmī行与770 CE出生年份。如果我们将ctid添加到WHERE列表中，那么对于两个返回的记录，我们将看到不同的ctid值，就像人们期望的是两个不同的行元组一样。</p><p> This problem was extremely vexing for a few reasons. To start, we couldn’t easily tell how many rows this problem affected. The duplicated results returned from the database caused application logic to fail in a number of cases. We ended up adding defensive programming statements to detect the situation for tables known to have this problem. Because the bug affected all of the servers, the corrupted rows were different on different replica instances, meaning that on one replica row   X  might be bad and row   Y  would be good, but on another replica row   X  might be good and row   Y  might be bad. In fact, we were unsure about the number of replicas with corrupted data and about whether the problem had affected the master.</p><p> 由于几个原因，这个问题非常棘手。首先，我们无法轻易得知此问题影响了多少行。从数据库返回的重复结果在许多情况下导致应用程序逻辑失败。我们最终添加了防御性编程语句，以检测已知有此问题的表的情况。因为该错误影响了所有服务器，所以在不同的副本实例上损坏的行是不同的，这意味着在一个副本上，行X可能是坏的，行Y可能是好的，但是在另一副本上，行X可能是好的，行Y可能是好的坏的。实际上，我们不确定数据损坏的副本数量以及问题是否影响了主服务器。</p><p> From what we could tell, the problem only manifested on a few rows per database, but we were extremely worried that, because replication happens at the physical level, we could end up completely corrupting our database indexes. An essential aspect of B-trees are that they must be periodically   rebalanced , and these rebalancing operations can completely change the structure of the tree as sub-trees are moved to new on-disk locations. If the wrong data is moved, this can cause large parts of the tree to become completely invalid.</p><p> 据我们所知，该问题仅出现在每个数据库的几行上，但我们非常担心，由于复制发生在物理级别，因此我们最终可能会完全破坏我们的数据库索引。 B树的一个重要方面是必须定期重新平衡它们，并且当子树移动到新的磁盘位置时，这些重新平衡操作可以完全改变树的结构。如果移动了错误的数据，则可能导致树的大部分变为完全无效。</p><p> In the end, we were able to track down the actual bug and use it to determine that the newly promoted master did not have any corrupted rows. We fixed the corruption issue on the replicas by resyncing all of them from a new snapshot of the master, a laborious process; we only had enough capacity to take a few replicas out of the load balancing pool at a time.</p><p> 最后，我们能够找到实际的错误并使用它来确定新提升的master没有任何损坏的行。我们通过从主服务器的新快照重新同步所有副本（这是一个费力的过程）来修复副本上的损坏问题。我们只有足够的容量来一次从负载平衡池中取出几个副本。</p><p> The bug we ran into only affected certain releases of Postgres 9.2 and has been fixed for a long time now. However, we still find it worrisome that this class of bug can happen at all. A new version of Postgres could be released at any time that has a bug of this nature, and because of the way replication works, this issue has the potential to spread into all of the databases in a replication hierarchy.</p><p> 我们遇到的错误仅影响了Postgres 9.2的某些版本，并且已经修复了很长时间。但是，我们仍然发现，此类错误根本不会发生。可能会随时发布具有这种错误的新版本的Postgres，并且由于复制的工作方式，此问题有可能传播到复制层次结构中的所有数据库中。</p><p>  Postgres does not have true replica MVCC support. The fact that replicas apply WAL updates   results in them having a copy of on-disk data identical to the master  at any given point in time. This design poses a problem for Uber.</p><p>  Postgres没有真正的副本MVCC支持。副本应用WAL更新的事实导致它们在任何给定时间点都具有与主数据库相同的磁盘数据副本。这种设计给Uber带来了问题。 </p><p> Postgres needs to maintain a copy of old row versions for MVCC. If a streaming replica has an open transaction, updates to the database are blocked if they affect rows held open by the transaction. In this situation, Postgres pauses the WAL application thread until the transaction has ended. This is problematic if the transaction takes a long amount of time, since the replica can severely lag behind the master. Therefore, Postgres applies a timeout in such situations: if a transaction blocks the WAL application for a   set amount of time , Postgres kills that transaction.</p><p>Postgres需要维护MVCC的旧行版本的副本。如果流复制副本具有打开的事务，则如果数据库更新影响事务保持打开的行，则会阻止对数据库的更新。在这种情况下，Postgres暂停WAL应用程序线程，直到事务结束。如果事务处理要花费很长时间，则这是有问题的，因为副本可能严重滞后于主服务器。因此，Postgres在这种情况下应用超时：如果事务在指定的时间内阻止了WAL应用程序，则Postgres将终止该事务。</p><p> This design means that replicas can routinely lag seconds behind master, and therefore it is easy to write code that results in killed transactions. This problem might not be apparent to application developers writing code that obscures where transactions start and end. For instance, say a developer has some code that has to email a receipt to a user. Depending on how it’s written, the code may implicitly have a database transaction that’s held open until after the email finishes sending. While it’s always bad form to let your code hold open database transactions while performing unrelated blocking I/O, the reality is that most engineers are not database experts and may not always understand this problem, especially when using an ORM that obscures low-level details like open transactions.</p><p> 这种设计意味着副本通常会比主副本落后几秒钟，因此很容易编写导致交易终止的代码。对于编写模糊事务开始和结束位置的代码的应用程序开发人员来说，此问题可能并不明显。例如，假设开发人员有一些代码必须通过电子邮件将收据发送给用户。根据编写方式的不同，代码可能会隐式地保持数据库事务处于打开状态，直到电子邮件完成发送为止。尽管在执行不相关的阻塞I / O时让代码保持开放的数据库事务总是很糟糕的形式，但现实是大多数工程师不是数据库专家，并且可能并不总是了解此问题，尤其是在使用掩盖了底层细节的ORM时像公开交易。</p><p>  Because replication records work at the physical level, it’s not possible to replicate data between different general availability releases of Postgres. A master database running Postgres 9.3 cannot replicate to a replica running Postgres 9.2, nor can a master running 9.2 replicate to a replica running Postgres 9.3.</p><p>  由于复制记录在物理级别上起作用，因此无法在Postgres的不同常规可用性版本之间复制数据。运行Postgres 9.3的主数据库不能复制到运行Postgres 9.2的副本，运行9.2的主数据库也不能复制到运行Postgres 9.3的副本。</p><p>  Run a command called  pg_upgrade  on the master, which updates the master data in place. This can easily take many hours for a large database, and no traffic can be served from the master while this process takes place.</p><p>  在主数据库上运行一个名为pg_upgrade的命令，该命令将就地更新主数据库数据。对于大型数据库，这可能很容易花费多个小时，并且在此过程发生时，无法从主服务器提供任何流量。</p><p>  Create a new snapshot of the master. This step completely copies all data from the master, so it also takes many hours for a large database.</p><p>  创建主服务器的新快照。此步骤完全复制了主数据库中的所有数据，因此大型数据库也要花费许多时间。</p><p>  Bring each replica back into the replication hierarchy. Wait for the replica to fully catch up to all updates applied by the master while the replica was being restored.</p><p>  将每个副本带回到复制层次结构中。等待副本完全恢复到副本还原时由主服务器应用的所有更新。</p><p> We started out with Postgres 9.1 and successfully completed the upgrade process to move to Postgres 9.2. However, the process took so many hours that we couldn’t afford to do the process again. By the time Postgres 9.3 came out, Uber’s growth increased our dataset substantially, so the upgrade would have been even lengthier. For this reason, our legacy Postgres instances run Postgres 9.2 to this day, even though the current Postgres GA release is 9.5.</p><p> 我们从Postgres 9.1开始，并成功完成了升级过程，以迁移到Postgres 9.2。但是，该过程花费了很多小时，因此我们无力承担再次执行该过程的费用。到Postgres 9.3发布时，Uber的增长大大增加了我们的数据集，因此升级本来就更长。因此，即使当前的Postgres GA版本为9.5，我们的传统Postgres实例也可以运行Postgres 9.2。 </p><p> If you are running Postgres 9.4 or later, you could use something like   pglogical,  which implements a logical replication layer for Postgres. Using pglogical, you can replicate data among different Postgres releases, meaning that it’s possible to do an upgrade such as 9.4 to 9.5 without incurring significant downtime. This capability is still problematic because it’s not integrated into the Postgres mainline tree, and pglogical is still not an option for people running on older Postgres releases.</p><p>如果您运行的是Postgres 9.4或更高版本，则可以使用pgologic之类的东西，它为Postgres实现了一个逻辑复制层。使用pgologic，您可以在不同的Postgres版本之间复制数据，这意味着可以将9.4升级到9.5，而不会造成大量停机。该功能仍然存在问题，因为它尚未集成到Postgres主线树中，并且对于在较旧的Postgres版本上运行的用户而言，pgologic仍然不是一种选择。</p><p>  In addition to explaining some of Postgres’s limitations, we also explain why MySQL is an important tool for newer Uber Engineering storage projects, such as Schemaless. In many cases, we found MySQL more favorable for our uses. To understand the differences, we examine MySQL’s architecture and how it contrasts with that of Postgres. We specifically analyze how MySQL works with the   InnoDB storage engine . Not only do we use InnoDB at Uber; it’s perhaps the most popular MySQL storage engine.</p><p>  除了说明Postgres的一些局限性之外，我们还说明了为什么MySQL是更新的Uber Engineering存储项目（例如Schemaless）的重要工具。在许多情况下，我们发现MySQL更适合我们的使用。为了理解这些差异，我们研究了MySQL的体系结构及其与Postgres的对比。我们专门分析MySQL如何与InnoDB存储引擎一起使用。我们不仅在Uber使用InnoDB；它可能是最受欢迎的MySQL存储引擎。</p><p>  Like Postgres, InnoDB supports advanced features like MVCC and mutable data. An exhaustive discussion of InnoDB’s on-disk format is outside the scope of this article; instead, we’ll focus on its core differences from Postgres.</p><p>  与Postgres一样，InnoDB支持MVCC和可变数据等高级功能。关于InnoDB磁盘格式的详尽讨论不在本文讨论范围之内。相反，我们将重点介绍其与Postgres的核心区别。</p><p> The most important architectural difference is that while Postgres directly maps index records to on-disk locations, InnoDB maintains a secondary structure. Instead of holding a pointer to the on-disk row location (like the  ctid  does in Postgres), InnoDB secondary index records hold a pointer to the primary key value. Thus, a secondary index in MySQL associates index keys with primary keys:</p><p> 最重要的架构差异是，尽管Postgres将索引记录直接映射到磁盘上的位置，但InnoDB维护二级结构。 InnoDB二级索引记录拥有一个指向主键值的指针，而不是持有一个指向磁盘上行位置的指针（就像ctid在Postgres中一样）。因此，MySQL中的辅助索引将索引键与主键相关联：</p><p>  In order to perform an index lookup on the (first, last) index, we actually need to do two lookups. The first lookup searches the table and finds the primary key for a record. Once the primary key is found, a second lookup searches the primary key index to find the on-disk location for the row.</p><p>  为了对（第一个，最后一个）索引执行索引查找，我们实际上需要执行两次查找。第一次查找将搜索表并找到记录的主键。找到主键后，第二次查找将搜索主键索引，以找到该行的磁盘位置。</p><p> This design means that InnoDB is at a slight disadvantage to Postgres when doing a secondary key lookup, since two indexes must be searched with InnoDB compared to just one for Postgres. However, because the data is normalized, row updates only need to update index records that are actually changed by the row update. Additionally, InnoDB typically does row updates in place. If old transactions need to reference a row for the purposes of MVCC MySQL copies the old row into a special area called the   rollback segment .</p><p> 这种设计意味着在执行辅助键查找时，InnoDB相对于Postgres略有不利，因为与IngreDB相比，必须使用InnoDB搜索两个索引。但是，由于数据已标准化，所以行更新仅需要更新由行更新实际更改的索引记录。另外，InnoDB通常会进行行更新。如果出于MVCC的目的，旧事务需要引用一行，则MySQL将旧行复制到称为回滚段的特殊区域中。</p><p> Let’s follow what happens when we update al-Khwārizmī’s birth year. If there is space, the birth year field in the row with  id  4 is updated in place (in fact, this update always happens in place, as the birth year is an integer that occupies a fixed amount of space). The birth year index is also updated in place to reflect the new date. The old row data is copied to the rollback seg</p><p> 让我们来看看更新al-Khwārizmī的出生年月会发生什么。如果有空间，则ID为4的行中的出生年份字段将被适当地更新（实际上，此更新总是在适当位置进行，因为出生年份是一个占用固定空间量的整数）。出生年份指数也已更新，以反映新日期。将旧行数据复制到回滚段 </p><p>......</p><p>...... </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://eng.uber.com/postgres-to-mysql-migration/">https://eng.uber.com/postgres-to-mysql-migration/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/mysql/">#mysql</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/postgres/">#postgres</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>