<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>机器学习：停滞不前 </title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">机器学习：停滞不前 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-01-15 20:22:38</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/1/d1998c7879f04b35452c7a84d64ba939.jpeg"><img src="http://img2.diglog.com/img/2021/1/d1998c7879f04b35452c7a84d64ba939.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>任何基本发现都涉及很大程度的风险。如果一个想法可以保证可行，那么它将从研究领域转向工程领域。不幸的是，这也意味着，至少如果通过“客观”指标（例如引文）来衡量失败，大多数研究事业将总是失败。</p><p> 学术界的建设旨在为研究人员提供一个下行绿篱或安全网。在这里，他们可以追求雄心勃勃的想法，在这种想法中，成功的可能性仅次于愿景的勇气。</p><p> 学者为了交换知识自由而牺牲了物质机会成本。社会钦佩冒险者，因为社会只有通过他们英勇的自我牺牲才能前进。</p><p> 不幸的是，我们对学者的钦佩和声望大部分来自过去的时代。经济学家是第一个提出如何在不承担金钱或知识风险的情况下保持学术声誉的人。他们将出现在CNBC财务上，并谈论“更正”或“非理性的恐惧/繁荣”。无论他们的预测有多正确，他们的媒体个性都会随着YouTube推荐算法的反馈循环而增长。</p><p> 很难将责任归咎于任何个人研究人员，毕竟，尽管风险对集体有利，但对个人几乎必然有害。但是，这种无风险的方法越来越流行，并且特别渗透到我的“机器学习”领域。 FAANG具有学术任命的薪水是当今世界上最好的工作。</p><p> 借助最先进的技术（SOTA），我们对作为创新者的增量研究人员给予了奖励和称赞，他们增加了预算，以便他们可以进行更多增量研究，与向其报告的员工或研究生人数平行。</p><p>     机器学习博士学位的学生是新的投资银行分析师，他们都在职业选择上寻求选择，但是在肤浅的方式上有所不同，例如更喜欢冥想而不是聚会，大麻和大麻。 Adderall超过酒精和可卡因。 </p><p>大型实验室的整个数据科学面试过程已经成为琐事和声望的混合体。查看投资组合可能需要很长时间，但是要检查您是从斯坦福大学毕业还是与Google Brain共同撰写了一篇论文，现在这是一个很好的过滤器！</p><p> 我们对流程进行了游戏化和标准化，以至于开始类似于咨询访谈中的案例研究。</p><p>       刚接触机器学习的年轻学生经常问我，深度学习需要了解哪些数学知识，而我的答案是矩阵乘法和平方函数的导数。所有这些神经元类比在解释机器学习的实际工作方式上弊大于利。</p><p> LSTM是一堆矩阵乘法，Transformers是一堆矩阵乘法，CNN使用卷积，卷积是矩阵乘法的概括。</p><p> 深度神经网络由矩阵乘法组成，矩阵乘法之间偶尔存在非线性</p><p>  矩阵乘法是雅克·菲利普·玛丽·贝纳特（Jacques Philippe Marie Benet）于1812年发明的，但您可以认为向前传播的发明要晚得多。</p><p> 使用自动微分，向后遍历基本上是免费的，并且可以像50位数字长除法一样进行计算。推导复杂的长梯度是一种严格的伪造，在我们拥有计算机之前是有用的。 </p><p>当我在圣地亚哥加州大学圣地亚哥分校读研究生时，我记得回避了深度学习，因为它不被认为是认真的机器学习，因为没有很好的证据证明这些模型为何有效。</p><p>  我已经了解到“深度学习”是一个经验领域，这是“艰难的方式”，所以为什么某件事或某件事如何起作用通常是轶事，而不是理论上的。</p><p> 在经验领域，最优秀的人才通常是那些积累了最大经验的人才，并且基本上有两种方式可以做到这一点。</p><p>  年龄是经验的代表，但是有效的实验方法可以让您压缩获得更多经验所需的时间。</p><p> 如果您有数据中心可供使用，这将进一步提高您的学习能力。如果您和您的所有同龄人都可以访问数据中心，那么这又是一个乘法反馈循环，因为你们都可以互相学习。</p><p> 这有助于解释为什么仅在少数几个实验室（例如Google Brain，DeepMind和Open AI）中发布了机器学习中最具影响力的研究。到处都有反馈循环。</p><p>  特别是在过去的三年中，不断增加的工作量不断增加，纸质标题看起来像小报头条： </p><p>从根本上减少了对所有数据集的“有用”机器学习研究，从而使Transformers更快，更小并且可以扩展到更长的序列长度。</p><p>  这是一个让人想起NP完整性的问题-期刊充斥着证明另一个问题是NP完整性的证据。</p><p>     通过梯度下降学习神经网络权重，通过研究生下降学习神经网络体系结构。</p><p>   研究生下降是当今机器学习中获得最先进性能的最可靠方法之一，它也是您实验室拥有的众多研究生或员工的完全并行化方法。有了研究生的血统，比从事不确定的项目，您更有可能被出版或晋升。</p><p> 研究生后裔的流行源于货运文化配置，其中某些损失函数，深度和结构通常被认为是良好的。</p><p> 很难从第一原理中得出真正的理由，因为机器学习算法是一个复杂的系统，其参数具有很大的可变性，并且相互作用是非线性且不可预测的。烧蚀有帮助，但即使如此，仍无法在如此广泛的参数范围内得出结论。</p><p> 试图直观地说明某种特定技术如何运作或为何起作用的技术论文，看起来往往比占星术更像科学，而不是科学，其句子为“鼓励网络对其预测充满信心”。 </p><p>我有时会给人一种印象，即学者认为过渡到大型模型的过程类似于：</p><p>  考虑运行大型模型很简单，但实际进行操作当然并不容易。这种极受欢迎的模因可以很好地说明这种误解。</p><p>  如果像堆叠更多层这样简单的事情比统计学习更好，那么您必须怀疑谁才是真正的小丑。</p><p> 要“堆叠更多的层”，您需要担心模型和数据并行性，流水线化，调整超参数，硬件加速器，网络与计算，存储与IO瓶颈，提早停止，可伸缩的体系结构，精简，修剪等。</p><p> 您可以使用Chernoff Bounds，Markov不等式在假定为高斯的某个参数上完成所有收敛边界，但是如果您提出的算法比“ STACK MORE LAYERS”更糟糕，那么您提出的算法就不是很好。</p><p> 每篇论文都是SOTA，具有强大的理论保证，直观的解释，可解释且公平，但几乎没有相互一致的</p><p>   我对因果推理持谨慎乐观的态度，我希望看到的是它从冥想工具逐渐过渡到人们每天实际使用的图书馆。 </p><p>这并不意味着我反对数学形式主义，如果我喜欢数学，并且希望在“深度学习”中看到更多的话–我只警告您不要假装严谨。</p><p> 假设某些有关数据的“很好的属性”以使定理得以解决，那么梯度求导将占用附录的多个页面，而不仅仅是使用自动微分。</p><p>  最好的方法是将复杂的数学思想简单地组合到神经网络中，因为您可以利用读者的审美敏感性，并讨论为什么傅立叶是计算的核心。优化社区常常对此感到愧where，因为他们提出诸如swish的激活功能，然后花大量篇幅讨论损失形势的好特性。</p><p> 传播新想法最可靠的方法是创建一个基准，使现有的SOTA方法失败，然后说明您的技术如何更好。这在设计上是很难的，应该很难将未经验证的想法替换为经过验证的想法。这种技术的优点是不需要任何Twitter参数。</p><p> 必须避免成为加里·马库斯（Gary Marcus），并批评现有的技术，而不提出其他更有效的方法，这一点很重要。</p><p> 如果您无法做到这一点，那么也许您没有偶然发现有用的东西，也许只是美丽或优雅。寻找温暖和模糊的环境仍然是一个值得追求的目标。</p><p>  尽管我的介绍性哀叹使您相信机器学习方面没有创新，但我们已将货运培养定为单一文化-事实并非没有。 </p><p>机器学习中仍然有大量创新发生，而不仅仅是数据科学家或机器学习研究人员。</p><p> 我相信这些项目代表着对抗机器学习停滞的一线希望。</p><p>   编程语言是一种思维工具，需要以与其他任何消费产品相同的敏感性进行设计。</p><p> 当我第一次对Keras感兴趣时，我的一些同伴向我提到，在Tensorflow中做“真正的ML工作”并不那么认真，这让我默默地想知道为什么他们不使用FORTRAN进行编程。</p><p> Keras向我介绍了将机器学习视为语言设计问题的想法。我现在不再像思考矩阵或神经元那样思考分层，就像我阅读论文时一样。</p><p> ＃创建layerslayer1 = layers.Dense（2，激活=＆＃34; relu＆＃34 ;，名称=＆＃34; layer1＆＃34;）layer2 = layers.Dense（3，激活=＆＃34; relu＆＃34; ，name =＆＃34; layer2＆＃34;）layer3 = layers.Dense（4，name =＆＃34; layer3＆＃34;）＃在测试输入上调用图层x =​​ tf.ones（（3，3））y = layer3（layer2（layer1（x）））</p><p>  矩阵是线性映射，但是线性映射比矩阵考虑的直观得多 </p><p>然后，您可以构建具有多个输入和输出的网络，以构建更多有趣的网络。</p><p>  Keras是一个以用户为中心的库，而Tensorflow特别是Tensorflow 1.0是一个以机器为中心的库。 ML研究人员从层的角度考虑，自动微分引擎从计算图的角度考虑。</p><p> 就我而言，我的时间比一台机器的周期更有价值，所以我宁愿使用Keras之类的工具。</p><p> 这并不意味着我对缓慢的代码感到满意，这就是为什么构建能够快速运行用户友好代码的优秀编译器和XLA之类的中间表示形式至关重要的原因。</p><p> 性能与抽象是错误的二分法-计算的历史证明了这一点</p><p> 把用户放在第一位是Fast.ai在构建其深度学习库时所采用的方法。我认为Jeremy Howard是机器学习的Don Norman。</p><p>   Fast.ai不仅专注于模型构建部分，还围绕以下所有内容构建工具。 </p><p>工具并不是指黑匣子服务，而是指机器学习特有的软件设计模式。代替抽象工厂的是抽象方法，例如流水线以链接预处理步骤，回调以尽早停止到变形金刚的通用且简单的实现。设计模式比黑盒子有用，因为您可以了解它们的工作原理，对其进行修改并为自己和他人改进。</p><p> 敬上nbdev的荣誉称号，其目的是消除所有阻碍以代码库形式传递代码的障碍，这些障碍包括消除人类使用git表示，持续集成和自动PyPi软件包提交等所有使用笔记本的烦恼。</p><p>  深度学习的承诺是差异化计算，在这里，您无需编写程序来完成某些任务，而是提供模型输入/输出对，并要求它为您生成程序。</p><p>  一旦开始将神经网络视为程序，就可以将程序视为神经网络并区分程序。</p><p> 不幸的是，Python是不可区分的，这是Google和Facebook都使用可自动区分的Python绑定在C ++中构建了自己的语言。</p><p> 另一方面，Julia是一种用于科学计算的语言，默认情况下所有内容都可以自动区分。因此，如果您构建ODE求解器，则可以免费获得神经ODE求解器。</p><p> 普通和偏微分方程式是我们规范化科学中从天文学到药理学的大多数关系的方式，因此能够将这些模拟速度提高几个数量级，这意味着我们正处于科学计算的黄金时代。 </p><p>我还特别发现用于机器人仿真的神经ODE方法非常令人兴奋。强化学习的主要问题是模型通常很大且难以训练，但是如果您可以将模型像可区分的白盒ODE求解器一样对待，而不是将模拟视为黑盒，则可以使模型更小，更容易训练。</p><p>   如果您在2018年问了这个问题，答案可能是Open AI。他们用漂亮的代理商演示打败了世界，使代理打败了专业的视频游戏玩家。他们提醒我为什么我首先要学习机器学习。他们的声誉掩盖了一切，但随着时间的流逝，它成为他们的核心产品。</p><p>  查看带有漂亮字体的漂亮博客文章，付费使用GPT-3。 Open AI不是一家平台公司。</p><p>  我想不出NLP团队没有尝试过HuggingFace的一家大公司。他们在论文发表后的几天内添加了新的Transformer模型，维护了标记生成器，数据集，数据加载器，NLP应用程序。 HuggingFace创建了多层平台，每个平台本身都可以成为一家引人注目的公司。</p><p> HuggingFace将为解决比AGI少投机性的问题创造数十亿美元的价值。 HuggingFace避免了通常的ML启动陷阱，后者变成了咨询公司或引文库。</p><p>  散客（Haskeller）通常类似于邪教，一切都是功能，或者是Monad或Lens。通常很难理解Bi-类别是什么，或者为什么要关注它。</p><p> 现实世界中的Haskell尝试将IO建模为Monad或将Web服务器建模为具有状态的函数，但我相信Haskellers的应用程序还不够完善。 </p><p>Haskell是世界上最好的函数式编程语言，而神经网络就是函数</p><p> 这是Hasktorch背后的主要动机，它使您可以通过组合功能运算符来发现新型的神经网络体系结构。贾斯汀·勒（Justin Le）在他的《纯函数类型模型》系列中对这一想法做了很多解释</p><p>       诚然，RNN代码示例虽然有点复杂，但考虑到它的代码量要比典型神经网络库中的代码量级低几个数量级，而不是通过将Logistic回归与几个额外的运算符结合使用而无法获得。我希望我能说服贾斯汀有一天为我们这个凡人创造一个演讲，以发明我们自己的神经网络体系结构。</p><p>  开放式AI健身房是创建平台的伟大尝试，该平台将各种游戏用作强化学习研究的基准。目标是最终推动强化学习，以解决更复杂的问题并真正突破界限。</p><p> 不幸的是，追逐SOTA意味着基准成为了目标，并且整个研究人员群体都对该数据集采用了诸如“世界模型”之类的过拟合技术。因此，总的来说，有一种趋向于在简单基准上越来越复杂的算法的趋势，而最复杂的基准（如Dota）似乎正在使用具有令人印象深刻的基础架构扩展的最简单算法。</p><p>  Unity是当今最用户友好的游戏引擎，我喜欢它并将其用于我的所有附带项目。 Unity ML代理是您将视频游戏转变为强化学习环境的一种方式。</p><p> 强化学习环境本质上是自定义数据集，我相信它将成为复杂机器人应用程序的实际模拟器。使用虚张声势的游戏来创建复杂的多主体协商，在这种协商中，主体需要掌握物理知识并理解错觉。疯了！ </p><p>*完全公开：我在Graphcore工作，但以下只是我个人的想法，并不代表Graphcore的正式职位*</p><p> GPU在机器学习中普及的一个重要原因是它们本质上是快速矩阵乘法机器。深度学习本质上是矩阵乘法。</p><p> 但是，深度学习之所以流行的一个重要原因是存在用于矩阵乘法的快速硬件。</p><p> 这使得很难推理深度学习是否是解决机器学习问题的最佳方法，因为深度学习取决于当时的可用硬件类型。 Sara Hooker的问题称为“硬件彩票”。</p><p>    严格更强大的编程模型是多指令多数据机，其中任意指令可以在任意数据点上彼此完全并行运行，并以所有方式相互通信。 MIMD硬件可以做任何事情，SIMD硬件可以做任何事情（特别是使用Tensorflow进行快速矩阵乘法），以及做更多的事情（尤其是概率建模和图形算法）。</p><p>  基于矩阵乘法的算法的回报越来越小，模型的尺寸和成本正成倍增加。</p><p> 特别是像GPT-3这样的语言模型开始变得像“大型强子对撞机”一样。 </p><p>当今市场上最好的MIMD机器是Graphcore的IPU（智能处理单元），我希望看到更多的研究人员走出他们的舒适区域，从MIMD而不是SIMD的角度看待世界，并有信心 停止从事Google所做的货物崇拜活动。  我很 悲伤地看到 在 机器学习 是 最令人兴奋的 工作是 从 机器学习 的 外部的 - 我已经 在这个领域 工作了10年 ，今天 我 学习更多 在Twitter上 疯狂的 外地人 然后 我 从 同行评议 的论文 做 。  我想听到更多可能不可行的建议，我想要更多的想法，我希望机器学习再次变得有趣。  保持开放的态度，最重要的是不要成为这个人。 如果您喜欢此操作，请订阅以下内容，让我知道！  谢谢sudomaze，thecedarprince，krishnanpc和19_Rafael在我直播自己在twitch.tv/marksaroufim上撰写本文时提供了有用的反馈 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation">https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/learning/">#learning</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1043540.html"><img src="http://img2.diglog.com/img/2021/1/thumb_eca3d009aa50e1c05be0f9f75c178957.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1043540.html">带有代码的ML，深度学习，CV和NLP项目 </a></div><span class="my_story_list_date">2021-1-9 2:43</span></div><div class="col-sm"><div><a target="_blank" href="/story/1042938.html"><img src="http://img2.diglog.com/img/2021/1/thumb_99194f67e5542018f2c1c325832cade0.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1042938.html">半小时学习Rust </a></div><span class="my_story_list_date">2021-1-2 15:5</span></div><div class="col-sm"><div><a target="_blank" href="/story/1042737.html"><img src="http://img2.diglog.com/img/2021/1/thumb_21e735e5c447d0e212be6c0fde13073a.jpeg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1042737.html">机器学习需要大量精力 </a></div><span class="my_story_list_date">2021-1-1 8:48</span></div><div class="col-sm"><div><a target="_blank" href="/story/1041349.html"><img src="http://img2.diglog.com/img/2020/12/thumb_a1e03f13b93caa82816da7f2f4e8b479.gif" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1041349.html">为了赢得大流行后的发展，edtech需要开始大胆思考 </a></div><span class="my_story_list_date">2020-12-23 4:28</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>