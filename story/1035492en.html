<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>ZFS简介An Introduction to ZFS</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">An Introduction to ZFS<br/>ZFS简介</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-11-21 12:13:30</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/11/0d66977d13db5c45bbb3662f88ac1b1e.png"><img src="http://img2.diglog.com/img/2020/11/0d66977d13db5c45bbb3662f88ac1b1e.png" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>ZFS has become increasingly popular in recent years. ZFS on Linux (ZoL) has pushed the envelope and exposed many newcomers to the ZFS fold. iXsystems has adopted the newer codebase, now called  OpenZFS, into its codebase for TrueNAS CORE. The purpose of this article is to help those of you who have heard about ZFS but have not yet had the opportunity to research it.</p><p>近年来，ZFS变得越来越流行。 Linux（ZoL）上的ZFS推动了极限，并使许多新手接触到ZFS折叠。 iXsystems已将更新的代码库（现在称为OpenZFS）纳入TrueNAS CORE的代码库。本文的目的是帮助那些了解ZFS但尚未有机会对其进行研究的人们。</p><p> Our hope is that we leave you with a better understanding of how and why it works the way it does. Knowledge is key to the decision-making process, and we feel that ZFS is something worth considering for most organizations.</p><p>我们的希望是让您更好地了解它的工作方式以及工作方式。知识是决策过程的关键，我们认为ZFS对于大多数组织来说都是值得考虑的事情。</p><p>  ZFS is a  filesystem, but unlike most other file systems it is also the  logical volume manager or LVM. What that means is ZFS directly controls not only how the bits and blocks of your files are stored on your hard drives, but it also controls how your hard drives are logically arranged for the purposes of RAID and redundancy. ZFS is also classified as a  copy-on-write or  COW filesystem. This means that ZFS can do some cool things like  snapshots that a normal filesystem like NTFS could not. A snapshot can be thought of like it sounds, a photograph of how something was at a point in time. How a COW filesystem works, however, has some important implications that we need to discuss.</p><p>ZFS是一个文件系统，但与大多数其他文件系统不同，它也是逻辑卷管理器或LVM。这意味着ZFS不仅直接控制文件的位和块在硬盘上的存储方式，而且还控制为RAID和冗余目的在逻辑上排列硬盘的方式。 ZFS也被归类为写时复制或COW文件系统。这意味着ZFS可以做一些很酷的事情，例如快照，而NTFS这样的普通文件系统则做不到。快照可以听起来像是声音，是某个时间点的照片。但是，COW文件系统的工作方式具有一些重要的含义，我们需要讨论。</p><p>  Hard Drives work such that the pieces of your data are stored in  Logical Block Addresses, or LBAs. ZFS is aware of what LBAs a specific file is stored in. Let us say we need to write a file that is big enough to fit into 3 blocks. We are going to store that file in LBA 1000, 1001, and 1002. This is considered a sequential write, as all of these blocks are stored directly next to each other. For spinning hard drives, this is ideal, as the write head does not have to move off of the track it is on.</p><p>硬盘的工作方式是将数据片段存储在逻辑块地址或LBA中。 ZFS知道特定文件存储在哪些LBA中。假设我们需要编写一个足以容纳3个块的文件。我们将把该文件存储在LBA 1000、1001和1002中。这被视为顺序写入，因为所有这些块都直接相邻存储。对于旋转硬盘驱动器，这是理想的选择，因为写入头不必移出其所在的轨道。</p><p>  Now, let us say we make a change to the file and the part that was stored at LBA 1001 needs to be modified. When we write that change, ZFS does not over-write the part of the file that was stored in 1001. Instead, it will write that block to LBA 2001. LBA 1001 will be kept as-is until the snapshot keeping it there expires. This allows us to have both the current version of the file, and the previous one, while  only storing the difference. However, the next time we go to read the file back, the read head of our spinning hard drive needs to read LBA 1000, go to the track where LBA 2001 is stored, read that, and then go back to the track where LBA 1002 is stored. This phenomenon is called  fragmentation.</p><p>现在，假设我们对文件进行了更改，并且需要修改LBA 1001中存储的零件。当我们写入更改时，ZFS不会覆盖存储在1001中的文件部分。相反，它将把该块写入LBA2001。LBA1001将保持原样，直到将其保留在快照上为止。这样，我们既可以存储文件的当前版本，也可以拥有前一个版本，同时仅存储差异。但是，下次我们回读该文件时，旋转硬盘驱动器的读取头需要读取LBA 1000，转到存储LBA 2001的轨道，然后再回到LBA 1002的轨道。被储存了。这种现象称为碎片。</p><p>  To make ZFS pools easier to understand, we are going to focus on using small storage containers as you may have around the house or shop. Before we continue, it is worth defining some terms. A  VDEV, or virtual device, is a logical grouping of one or more storage devices. A  pool is then a logically defined group built from 1 or more VDEVs. ZFS is very customizable, and therefore, there are many different types of configurations for VDEVs. You can think of the construction of a ZFS pool by visualizing the following graphic:</p><p>为了使ZFS池更易于理解，我们将着重于使用小型存储容器，因为您可能在家中或商店附近都有。在继续之前，值得定义一些术语。 VDEV或虚拟设备是一个或多个存储设备的逻辑分组。因此，池是由1个或多个VDEV构建的逻辑定义的组。 ZFS是非常可定制的，因此，VDEV有许多不同类型的配置。您可以通过可视化以下图形来思考ZFS池的构造：</p><p>  Starting from the smallest container size, we have our drives. We can see that in this visualization we have two drives in each larger container. These two larger containers are our VDEVs. The single largest container, then, is our pool. In this configuration, we would have each pair of drives in a  mirror. This means that one drive can fail in either (or both!) VDEV and the pool would continue to function in a  degraded state.</p><p>从最小的容器大小开始，我们有驱动器。我们可以看到，在此可视化中，每个较大的容器中都有两个驱动器。这两个较大的容器是我们的VDEV。那么，最大的容器就是我们的游泳池。在这种配置中，我们将每对驱动器放在一个镜像中。这意味着一个驱动器可能在一个（或两个！）VDEV中发生故障，并且该池将继续在降级状态下运行。</p><p>  However, if 2 drives in a  single VDEV, all of the data in our entire pool is lost. There is no redundancy of the pool itself, all redundancy in ZFS is in the VDEV layer. If one  VDEV fails, there is not enough information to rebuild the missing data.</p><p>但是，如果单个VDEV中有2个驱动器，则整个池中的所有数据都会丢失。池本身没有冗余，ZFS中的所有冗余都在VDEV层中。如果一个VDEV发生故障，则没有足够的信息来重建丢失的数据。</p><p>  Next, we need to define what  RAID-Z is and what the various levels of RAID-Z are. RAID-Z is a way of putting multiple drives together into a VDEV and storing  parity, or fault tolerance. In ZFS, there is no dedicated “parity drive” like in  Unraid, but it instead stores parity across all of the drives in the VDEV.  The amount of parity that is spread across the drives determines the level of RAID-Z. It is in this way more similar to traditional hardware RAID.</p><p>接下来，我们需要定义什么是RAID-Z以及什么级别的RAID-Z。 RAID-Z是一种将多个驱动器放到VDEV中并存储奇偶校验或容错能力的方法。在ZFS中，没有像Unraid中一样的专用``奇偶校验驱动器''，而是在VDEV中的所有驱动器之间存储了奇偶校验。分布在驱动器上的奇偶校验量决定了RAID-Z的级别。这样更类似于传统的硬件RAID。</p><p> What can make RAID-Z a better approach than a mirrored configuration is that it does not matter  what drive fails in a RAID-Z. Each drive is an equal partner, whereas, in a mirrored configuration, each mirrored VDEV is a separate entity. This benefit of RAID-Z comes at the cost of performance, however, and a mirrored pool will almost always be  faster than RAID Z.</p><p>与镜像配置相比，可以使RAID-Z更好的方法是，RAID-Z中的哪个驱动器发生故障都没有关系。每个驱动器都是平等的伙伴，而在镜像配置中，每个镜像的VDEV是一个单独的实体。 RAID-Z的这一优势是以性能为代价的，但是镜像池几乎总是比RAID Z更快。</p><p> RAID-Z is similar to a traditional  RAID 5. In RAID-Z you have one drive worth of parity. In other words, if you lose one drive, your pool will continue to function. For RAID-Z you need a minimum of 3 drives per VDEV. You can have 3, 7, or even 12 drives in a RAID-Z VDEV. The more drives which you add, however, the longer it will take to  resilver, or rebuild.</p><p>RAID-Z与传统RAID 5相似。在RAID-Z中，您有一个值得奇偶校验的驱动器。换句话说，如果丢失一个驱动器，则池将继续运行。对于RAID-Z，每个VDEV至少需要3个驱动器。 RAID-Z VDEV中可以有3、7甚至12个驱动器。但是，您添加的驱动器越多，重新进行银色或重建所需的时间就越长。</p><p> This increased time increases the risk of your data, as a second drive failure during this process would destroy your pool. ZFS will resilver while the data is still in use, it is a live recovery. The implication of this is that our disks are working harder than usual during this process, and this can increase the chances of a second drive failure. Your data is still accessible and in production, while it is reading all of the parity data from the existing members of your VDEV and then writing it to the new disk.</p><p>时间的增加会增加数据的风险，因为在此过程中第二次驱动器故障会破坏您的池。当数据仍在使用时，ZFS将重新同步，这是实时恢复。这意味着在此过程中，我们的磁盘工作得比平时更努力，这会增加第二个驱动器发生故障的机会。当您从VDEV的现有成员读取所有奇偶校验数据并将其写入新磁盘时，您的数据仍可访问并处于生产状态。</p><p>  A RAID-Z2 VDEV is more akin to a RAID 6. In this configuration, 2 drives worth of parity is stored across all of your devices. You can lose up to two drives per VDEV and your pool will still function. Adding more parity drives increases calculations required which means you need more processing performance to operate the array.</p><p>RAID-Z2 VDEV更类似于RAID6。在此配置中，所有设备上均存储了2个值得奇偶校验的驱动器。每个VDEV最多可能丢失两个驱动器，并且池仍将运行。添加更多的奇偶校验驱动器会增加所需的计算量，这意味着您需要更高的处理性能才能操作阵列。</p><p>  Finally, a RAID-Z3 VDEV provides three drives worth of parity, so you can lose up to three drives per VDEV and your pool will still function.  The more drives of parity you add, however, the slower your performance ends up being. You need a minimum of four but should use at least five drives to build a RAID-Z3 VDEV.</p><p>最后，RAID-Z3 VDEV提供了三个值得奇偶校验的驱动器，因此每个VDEV最多可以丢失三个驱动器，并且池仍将运行。但是，添加的奇偶校验驱动器越多，最终性能就越慢。您至少需要四个驱动器，但至少应使用五个驱动器来构建RAID-Z3 VDEV。</p><p>  There are two ways in which we measure speed or  fastness,  IOPS, and  Throughput. In RAIDZ, more drives will give you more throughput, or the actual read and write speed you see when transferring files. However, if you have ever tried to run multiple file copies in Windows simultaneously, you may have noticed the more you do, the slower it gets. It does not always get slower at a constant rate, the more you try to do disks will get exponentially slower. This is because your disk can only do so many Input/Output Operations per Second, or IOPS.</p><p>我们可以通过两种方式来测量速度或牢固度：IOPS和吞吐量。在RAIDZ中，更多的驱动器将为您提供更大的吞吐量，或您在传输文件时看到的实际读写速度。但是，如果您曾经尝试在Windows中同时运行多个文件副本，则可能会注意到执行的次数越多，获取的速度就越慢。它并不总是以恒定的速度变慢，您尝试做的磁盘越多，速度将成指数地变慢。这是因为您的磁盘每秒只能执行这么多的输入/输出操作，即IOPS。</p><p> RAIDZ will scale in  throughput with the more disks you add, but it does not scale with  IOPS. What that generally means is, RAIDZ is not traditionally the best choice for I/O intensive workloads, as the amount of IOPS is roughly limited to the slowest member of our VDEV if we exclude all of the caching ZFS has. Virtualization, as we are discussing here, is highly dependent on I/O.</p><p>RAIDZ将随着您添加的更多磁盘来扩展吞吐量，但不会与IOPS一起扩展。通常，这意味着RAIDZ并不是传统上用于I / O密集型工作负载的最佳选择，因为如果我们排除所有ZFS缓存，IOPS的数量将大致限于VDEV中最慢的成员。正如我们在此处讨论的，虚拟化高度依赖于I / O。</p><p> Earlier, we discussed that ZFS is a COW filesystem, and because of that it suffers from data fragmentation. There are direct performance implications that stem from that fact.  The more “full” your pool is, the slower it will ultimately get. Write speeds in ZFS are directly tied to the amount of  adjacent free blocks there are to write to. As your pool fills up, and as data fragments, there are fewer and fewer blocks that are directly adjacent to one another. A single large file may span blocks scattered all over the surface of your hard drive. Even though you would expect that file to be a sequential write, it no longer can be if your drive is full.</p><p>之前，我们讨论了ZFS是COW文件系统，因此它遭受数据碎片的困扰。这个事实直接影响性能。您的池越“满”，它将最终变得越慢。 ZFS中的写入速度直接与要写入的相邻空闲块的数量有关。随着池的填充和数据片段的增加，彼此相邻的块越来越少。单个大文件可能会跨越硬盘表面散布的块。即使您希望该文件是顺序写入，但如果驱动器已满，它就不再可能。</p><p>  In the above graphic, we can see a Seagate 1TB mobile drive that I tested in CrystalDiskMark. It can do about 130 MB/s of sequential read and writes. We can also see that when we start doing random 4k I/O, the speed falls about  100x. This is meant to illustrate the performance impact of data fragmentation. Additionally, we can see that the latency for these lookups can take about  half of a second, and we are limited to about 350 IOPS. In order to be fast, virtualization workloads on traditional hard drives need to have many disks in order to compensate for this slowness. It would not be uncommon to see a pool constructed of 10 or more VDEVs of mirrored drives.</p><p>在上图中，我们可以看到我在CrystalDiskMark中测试的Seagate 1TB移动驱动器。它可以执行大约130 MB / s的顺序读取和写入。我们还可以看到，当我们开始执行随机4k I / O时，速度下降了大约100倍。这旨在说明数据分段对性能的影响。此外，我们可以看到这些查找的延迟可能需要大约半秒，并且我们限制为大约350 IOPS。为了提高速度，传统硬盘驱动器上的虚拟化工作负载需要具有多个磁盘，以弥补这种缓慢性。看到一个由10个或更多镜像驱动器的VDEV构成的池的情况并不少见。</p><p> Additionally, there is some wisdom we can  borrow from the ZFS community. As your pool fills up, and sequential writes become increasingly difficult to accomplish due to fragmentation, it will slow down in a non-linear way. As a general rule of thumb, at about 50% capacity your pool will be noticeably slower than it was when it was 10% capacity. At about 80%-96% capacity, your pool starts to become very slow, and ZFS will actually change its write algorithm to ensure data integrity, further slowing you down.</p><p>此外，我们可以从ZFS社区中借鉴一些智慧。随着您的池已满，并且由于碎片而导致连续写入变得越来越困难，它将以非线性方式减慢速度。根据一般经验，大约50％的容量将比10％的容量慢。在大约80％-96％的容量下，您的池开始变得非常缓慢，ZFS实际上将更改其写入算法以确保数据完整性，从而进一步降低速度。</p><p> This is where SSDs come in. They radically change the game because they work very differently at the physical layer. They do not have read and write heads that are flying around a spinning disk back and forth trying to find your data. With the physical limitations of disk-based drives out of the way, SSDs can read and write non-sequential data  much faster. They do not suffer the penalties of these rules nearly as severely, fragmentation does not hurt their performance to the same degree.</p><p>这就是SSD的用武之地。它们从根本上改变了游戏，因为它们在物理层上的工作方式大不相同。他们没有读写磁头在旋转的磁盘上来回移动来查找数据。借助基于磁盘的驱动器的物理限制，SSD可以更快地读取和写入非顺序数据。他们没有遭受这些规则的惩罚那么严重，分散不会在相同程度上损害他们的表现。</p><p>   Hard drives have increased in capacity by leaps-and-bounds over the past couple of decades. We have seen hard drives grow from a single gigabyte in capacity and just last year  Western Digital announced that 18 and 20 terabyte drives are coming in 2020. What has not changed, is their ability to do I/O. Hard drives of old and new alike are still bound by physical limitations. Even those new monsters will only really be able to do about 400 or so random IOPS, only about four times what they once did all those years ago.  The Samsung 970 EVO plus pictured above, however, can do  over 14,000.</p><p>在过去的几十年中，硬盘的容量得到了突飞猛进的增长。我们已经看到硬盘驱动器的容量已经从1 GB增长了，就在去年Western Digital宣布2020年将有18 TB和20 TB的驱动器出现。不变的是它们的I / O能力。新旧硬盘仍然受到物理限制。即使是那些新怪物，也只能真正执行约400次左右的随机IOPS，仅是这些年前的四倍。上图所示的Samsung 970 EVO plus可以完成超过14,000个操作。</p><p>  If we receive enough feedback, in another piece we can talk about further tuning performance in ZFS.</p><p>如果我们收到足够的反馈，则可以在另一篇文章中讨论ZFS的进一步调优性能。</p><p>  Finally, we need to briefly explore a few additional topics about our underlying storage configuration. In a Windows computer, if you plug in a new hard drive or flash drive you need to format it and assign it a drive letter before you can use it. Similarly, when you finish creating your Pool in ZFS, you need to create a dataset in order to actually start using it. When you format your flash drive, Windows asks you to specify an  Allocation Unit Size.</p><p>最后，我们需要简要探讨一些有关基础存储配置的主题。在Windows计算机中，如果插入新的硬盘驱动器或闪存驱动器，则需要对其进行格式化并为其分配驱动器号，然后才能使用它。同样，在ZFS中完成创建Pool时，需要创建一个数据集才能真正开始使用它。格式化闪存驱动器时，Windows要求您指定分配单位大小。</p><p>  In ZFS the term for this is called the  Record Size. This value represents the maximum size of a  block. A block assembles the pieces of your data into logical groupings. In TrueNAS Core, you can define the record size on the pool level. Its child datasets will inherit the record size you set in the Pool, or you can also specify a different record size when you create it. Additionally, you can modify the record size at any point. However, doing so will only affect new data as it is written to your pool and not any existing data.</p><p>在ZFS中，此术语称为“记录大小”。该值表示块的最大大小。块将数据片段组装成逻辑分组。在TrueNAS Core中，您可以在池级别上定义记录大小。其子数据集将继承您在“池”中设置的记录大小，或者在创建它时也可以指定其他记录大小。此外，您可以随时修改记录大小。但是，这样做只会影响新数据，因为它会写入池中，而不会影响任何现有数据。</p><p> TrueNAS Core creates datasets with 128k record sizes by default. This is meant to be a well-rounded decision. Depending on your workflow, you may wish to increase or decrease this value. If you were  running a database server, as an example, it would make more sense to set the value to a smaller number. A 4k record size in that example would allow each transaction in the database to be written directly to disk, rather than waiting to fill the entire 128k record in the default configuration. As a general rule of thumb, Smaller record sizes offer lower latency, whereas larger ones offer higher overall throughput.</p><p>TrueNAS Core默认创建记录大小为128k的数据集。这是一个全面的决定。根据您的工作流程，您可能希望增加或减少此值。例如，如果您正在运行数据库服务器，则将值设置为较小的数字会更有意义。在该示例中，记录大小为4k，可以将数据库中的每个事务直接写入磁盘，而不必等待以默认配置填充整个128k记录。一般而言，较小的记录提供较低的延迟，而较大的记录提供较高的总体吞吐量。</p><p>  A 128k recordsize spans 32  sectors on a 4k native hard drive. A sector is the lowest-level piece of the storage puzzle. It is the closest thing to the  physical  embodiment of your data inside of the storage medium that we are going to go over.</p><p>一个128k的记录大小跨越4k本地硬盘驱动器上的32个扇区。扇区是存储难题中最底层的部分。这是我们要遍历的存储介质内部数据的物理实施方式中最接近的内容。</p><p> ZFS needs to be aware of this information in order to make intelligent decisions on how to read and write data to the disks. You can tell it what the sector size is by providing the  ashift value. TrueNAS Core does a pretty good job of doing this for you automatically. Most modern disks have a 4k sector size. For these drives, ZFS needs to have an ashift value of 12. For older 512b drives, you would use an ashift value of 9.</p><p>ZFS需要了解此信息，以便就如何向磁盘读取和写入数据做出明智的决定。您可以通过提供偏移值来告诉它扇区大小是多少。 TrueNAS Core在为您自动执行此操作方面做得很好。大多数现代磁盘的扇区大小为4k。对于这些驱动器，ZFS的偏移值为12。对于较旧的512b驱动器，偏移值为9。</p><p> For some SSDs, the story muddies. While they report to the OS that they are 4k drives, in reality, they function internally as 8k drives. These devices would require you to manually assign the ashift value, and you should use 13. If you are unsure, it is better to be too high than to be too low. Too small of an ashift value will cripple performance.</p><p>对于某些固态硬盘，故事变得混乱。他们向操作系统报告它们是4k驱动器时，实际上，它们在内部用作8k驱动器。这些设备将要求您手动分配ashift值，并且应使用13。如果不确定，最好将其设置为过高而不是过低。偏差值太小会削弱性能。</p><p>  The ZFS Adaptive Replacement Cache, or  ARC, is an algorithm that caches your files in system memory. This type of cache is a read cache and has no  direct impact on write performance. In a traditional file system, an LRU or Least Recently Used cache is used. The way a cache works is if you open a file on your computer it will then put that file in the cache. If you then close and reopen it, the file will load from the cache rather than your hard drive.</p><p>ZFS自适应替换缓存或ARC是一种将文件缓存在系统内存中的算法。这种类型的缓存是读缓存，对写性能没有直接影响。在传统的文件系统中，使用LRU或最近最少使用的缓存。缓存的工作方式是，如果您在计算机上打开文件，它将把该文件放入缓存中。如果然后关闭然后重新打开，文件将从缓存而不是硬盘驱动器加载。</p><p> An  LRU cache will evict the least recently used items from the cache first. Let us say the file we are talking about is an Excel spreadsheet. Assume you have opened that file and got it in your cache. This Excel file is something you access frequently throughout your workday. You make your changes, then close it to go work on a PowerPoint and write some emails, the LRU cache will potentially run out of space and evict the Excel file from the cache. So when you open it again later in your day, rather than reading it from the cache, it has to load it from disk. Caches are usually much larger than Office documents, but we are using this as a conceptual example.</p><p>LRU缓存将首先从缓存中逐出最近最少使用的项目。我们说的文件是一个Excel电子表格。假设您已打开该文件并将其保存在缓存中。这个Excel文件是您在整个工作日中经常访问的文件。您进行更改，然后将其关闭以在PowerPoint上工作并写一些电子邮件，LRU缓存可能会用完空间并从缓存中逐出Excel文件。因此，当您在当天晚些时候再次打开它时，必须从磁盘加载它，而不是从缓存中读取它。缓存通常比Office文档大得多，但是我们将其用作概念性示例。</p><p>  The ARC differs from this in that it takes the recent eviction history into account. Each time a file is evicted from the ARC the occurrence is logged. The algorithm will give weight to these logs, and files which have been evicted before but in cache again with a lower priority for them to be evicted again.</p><p>ARC与此不同之处在于，它考虑了最近的迁离历史。每次从ARC撤出文件时，都会记录该事件。该算法将权衡这些日志以及之前已撤出但又在缓存中的文件，但优先级较低，因此可以再次撤出它们。</p><p> The level 2 ARC, or  L2ARC is an extension of the ZFS ARC. If a file is evicted from the ARC, it is moved to the L2ARC rather than just being removed. The L2ARC resides on a disk (or disks), rather than in system memory. Since RAM is expensive, this feature is a useful way to expand your caching capabilities. With the advent of NVMe and Optane, relatively high speed and large caching can be achieved. It comes with a cost, however. Since the ARC will need to know that these files are stored in the L2ARC, it has to store that information in RAM.</p><p>级别2 ARC或L2ARC是ZFS ARC的扩展。如果将文件从ARC逐出，则将其移到L2ARC而不是被删除。 L2ARC驻留在一个或多个磁盘上，而不是系统内存中。由于RAM昂贵，因此此功能是扩展缓存功能的有用方法。随着NVMe和Optane的出现，可以实现相对较高的速度和较大的缓存。但是，这是有代价的。由于ARC需要知道这些文件存储在L2ARC中，因此它必须将该信息存储在RAM中。</p><p> According to  this Reddit post by an Oracle employee, the formula for calculating the ARC header mappings is:</p><p>根据Oracle员工在Reddit上的帖子，计算ARC标头映射的公式为：</p><p> (L2ARC size in bytes) / (ZFS recordsize in bytes) * 70 bytes = ARC header size in bytes</p><p>（L2ARC大小以字节为单位）/（ZFS记录大小以字节为单位）* 70字节= ARC标头大小以字节为单位</p><p> So let us make some sense of that for our purposes. We will use the TrueNAS default of 128Kb blocks and a 256GB NVME SSD as an L2ARC.</p><p>因此，让我们对这一点有所了解。我们将使用TrueNAS默认的128Kb块和256GB NVME SSD作为L2ARC。</p><p>  This would be a pretty common configuration choice for a lower-end VM storage box. If you only had 16GB or of RAM in your system, all of your ARC space would be wasted with L2ARC mappings and you would only have 2GB of the entire rest of your system. For a 256GB L2ARC you would want a  minimum of 32GB of ram. 64GB would be recommended.</p><p>对于低端VM存储盒来说，这将是一个非常常见的配置选择。如果您的系统中只有16GB或RAM，则所有L2ARC映射都将浪费ARC空间，而整个系统其余部分将只有2GB。对于256GB的L2ARC，您至少需要32GB的内存。建议使用64GB。</p><p>  We want to keep data cached in  both the ARC  and the L2ARC, it is a tiered approach. The above comparison attempts to illustrate this fact. Even using a software RAMDisk driver, my PC’s  relatively slow DDR4 2400 is about 3x faster than my 1TB Samsung PM981. The  old wisdom was always to have no more than  about 5x the L2ARC than your RAM.</p><p>我们希望将数据缓存在ARC和L2ARC中，这是一种分层方法。以上比较试图说明这一事实。即使使用软件RAMDisk驱动程序，我的PC相对较慢的DDR4 2400也比1TB三星PM981快3倍。过去的经验是，L2ARC的内存通常不超过RAM的5倍。</p><p> The ultimate goal here is to prevent our pool from having to do as many reads as we can. While the ARC does not directly cache writes, it can speed up your write performance by freeing your drives from having to constantly read from disk.</p><p>这里的最终目标是防止我们的池必须尽我们所能进行多次读取。尽管ARC不直接缓存写入，但它可以使驱动器不必从磁盘中不断读取数据，从而可以提高写入性能。</p><p>  Before we can explain what the ZIL is or what a SLOG does, we must first explain what and how data is written to a ZFS pool. When a write to disk occurs, it must first pass through system memory where a  transaction group or  TXG is created. ZFS will then  asynchronously commit this data to the pool, meaning there are no checks/ balances to ensure that the data got to the pool successfully. In the event of a crash or a power failure, corruption will have occurred and you will have lost the data being written.</p><p>在解释ZIL是什么或SLOG做什么之前，我们必须首先解释什么以及如何将数据写入ZFS池。写入磁盘时，它必须首先通过创建事务组或TXG的系统内存。然后，ZFS会将这些数据异步提交到池中，这意味着没有检查/余额来确保数据成功到达池中。如果发生崩溃或电源故障，将发生损坏，并且您将丢失正在写入的数据。</p><p> ZFS can also write data blocks   synchronously. You can force ZFS to do this by setting the “sync=always” flag in your pool. In addition to the above, a  sync write commits the writes to the  ZIL, or  ZFS Intent Log, in parallel with system memory. If the normal TXG is written to your pool successfully, the data in the ZIL will be erased. If the system crashes or its power interrupted, the data will remain in the ZIL. Since system memory is volatile, and our ZIL is not, this can be considered an insurance policy for our write commits. The speed of your writes, however, is now tied to the speed of your ZIL.</p><p>ZFS还可以同步写入数据块。您可以通过在池中设置“ sync = always”标志来强制ZFS执行此操作。除上述内容外，同步写入还会将写入与系统内存并行地提交到ZIL或ZFS Intent Log。如果正常的TXG成功写入您的池，则ZIL中的数据将被删除。如果系统崩溃或电源中断，数据将保留在ZIL中。由于系统内存是易失性的，而我们的ZIL则不是，因此可以将其视为写提交的保险政策。但是，您的写入速度现在与ZIL的速度有关。</p><p> By default, the ZIL lives in your pool, but in a logically separate place. It is only ever read from in one scenario. If there was a crash or a power failure. Every time your system is restarted it has to re-import your ZFS pool before proceeding. When the pool is reimported, ZFS will look to see if there was any remaining data written to the ZIL. If there was, that means there were writes that had not yet been committed to disk. It will then read that data and commit it as a new TXG to your pool.</p><p>默认情况下，ZIL位于您的池中，但位于逻辑上分开的位置。它只能在一种情况下读取。如果发生崩溃或电源故障。每次重新启动系统时，都必须先重新导入ZFS池，然后再继续。重新导入池后，ZFS将查看是否有任何剩余数据写入ZIL。如果存在，那意味着存在尚未提交到磁盘的写操作。然后它将读取该数据并将其作为新的TXG提交到您的池中。</p><p> What this all means is that with a sync write, your data is written to the disks in your pool  twice. This is called a  write amplification, and it will slow any write commits to your pool to a crawl. Sync writes have a high cost, cutting your write performance in  half or more.</p><p>这意味着通过同步写入，您的数据将两次写入池中的磁盘。这称为写放大，它将减慢对池的所有写提交到爬网的速度。同步写入的成本很高，将写入性能降低一半或更多。</p><p>  This is where the  SLOG comes in, a SLOG is a separate piece of hardware that acts as a dedicated place for the ZIL to live. Having separate hardware prevents the write amplification effect on your pool. It also allows you to put the ZIL on a much faster device. That is important because your writes will still be limited by the speed of your SLOG. These transactions are especially sensitive to disk latency, which is more important than throughput or IOPS for a SLOG device.  It is for that reason that we recommend Intel Optane for use when sync writes are a requirement.</p><p>这是SLOG的来源，SLOG是一个单独的硬件，充当ZIL的专用场所。具有单独的硬件可防止对池的写放大作用。它还允许您将ZIL放在速度更快的设备上。这很重要，因为您的写入仍将受到SLOG速度的限制。这些事务对磁盘延迟特别敏感，磁盘延迟比SLOG设备的吞吐量或IOPS更重要。出于这个原因，当需要同步写入时，我们建议使用Intel Optane。</p><p>  It is worth noting that the ZIL is  not a ZFS write cache! Your pool’s write speed will  always be faster with asynchronous writes, even if you are using a SLOG when using sync writes. If you feel your data is sensitive enough to require sync writes, buy a SLOG.</p><p>值得注意的是ZIL不是ZFS写缓存！即使使用同步写入时，即使使用SLOG，异步写入也会使池的写入速度始终更快。如果您认为自己的数据足够敏感，需要进行同步写入，请购买SLOG。</p><p>  We have spent some time in this article discussing the key concepts surrounding ZFS. We hope that we have helped provide to provide the necessary knowledge and references to get you started in the world of ZFS. When you go to build a lab, or when you go out to bid for a new storage solution, the Open Source is a tremendous resource that should be considered.</p><p>在本文中，我们花了一些时间讨论有关ZFS的关键概念。我们希望我们已经帮助提供了必要的知识和参考，以帮助您入门ZFS。当您去建立一个实验室，或者当您去竞购一个新的存储解决方案时，开源是一个巨大的资源，应该考虑。</p><p> We have not covered everything in the piece. Special Allocation Classes are an OpenZFS feature, and they allow you to accelerate metadata on your spinning drives with flash storage. Additionally, you can use them to get a better-performing deduplication. This is still a new feature in OpenZFS we have not yet tested or vetted for their viability or value.</p><p>我们没有涵盖所有内容。特殊分配类是OpenZFS的功能，它们使您可以使用闪存在旋转驱动器上加速元数据。此外，您可以使用它们来获得性能更好的重复数据删除。这仍然是OpenZFS的一项新功能，我们尚未对其可行性或价值进行过测试或审查。</p><p> Additionally, ZFS does on-the-fly compression, has native encryption support, and a whole-host of new features are actively being developed. We hope to follow this introduction to ZFS piece up with more content in the future around ZFS as new things come about.</p><p>此外，ZFS可以进行即时压缩，具有本机加密支持，并且正在积极开发大量新功能。我们希望随着对ZFS的介绍，随着新事物的到来，在将来围绕ZFS提供更多的内容。</p><p> If you want to put this into action in a lab, check out our series starting with  Building a Lab Part 1 Planning with TrueNAS and VMWare ESXi.</p><p>如果您想在实验室中付诸实践，请查看我们的系列文章，从构建实验室第1部分，使用TrueNAS和VMWare ESXi计划开始。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://www.servethehome.com/an-introduction-to-zfs-a-place-to-start/">https://www.servethehome.com/an-introduction-to-zfs-a-place-to-start/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/zfs/">#zfs</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/分享/">#分享</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy; 2020 diglog.com </div></div></body></html>