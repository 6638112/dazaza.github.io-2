<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>昨天的兔子CDN中断细节 Details of Yesterday's Bunny CDN Outage</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">Details of Yesterday's Bunny CDN Outage<br/>昨天的兔子CDN中断细节 </h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2021-06-23 17:25:47</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2021/6/cffa18a3084632ad934f14338ddab6c6.jpg"><img src="http://img2.diglog.com/img/2021/6/cffa18a3084632ad934f14338ddab6c6.jpg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>If there was one metric at bunny.net that we obsess about more than performance, that would be reliability. We have redundant monitoring, auto-healing at multiple different levels, three redundant DNS networks and a system designed to tie all of this together and assure your services stay online.</p><p>如果Bunny.net有一个指标，我们痴迷于比性能多，那将是可靠性。我们具有多余的监控，在多个不同级别的自动愈合，三个冗余DNS网络和一个系统设计用于将所有这一切联系在一起并确保您的服务在线停留。</p><p> That being said, this gets so much harder. After an almost stellar 2 year uptime, on 22nd of June, bunny.net experienced a 2+ hour near system-wide outage caused by DNS failure. In a blink of an eye, we lost over 60% of traffic, and wiped out hundreds of Gbits of throughput. Despite all of these systems being in place, a very simple update brought it all crumbling down, affecting over 750.000 websites.</p><p> 话虽如此，这变得更加困难。经过几乎恒星2年的正常运行时间，于6月22日，Bunny.net经历了2个以上的时间靠近System-宽的中断，由DNS失败引起。在眨眼间，我们损失了超过60％的流量，并淘汰了数百个吞吐量。尽管所有这些系统到位，但一个非常简单的更新将其崩溃，影响超过750.000个网站。</p><p> To say we are disappointed would be an understatement, but we want to take this opportunity to learn, improve and build an even more robust platform. In the spirit of transparency, we also want to share what happened and what we&#39;re doing to resolve this going into future. Perhaps even help other companies learn from our mistakes.</p><p> 要说我们感到失望会是轻描淡写的，但我们希望借此机会学习，改进和建立更强大的平台。本着透明度的精神，我们也希望分享发生的事情以及我们＆＃39;重新做出解决这一目标。也许甚至帮助其他公司从我们的错误中吸取教训。</p><p>  I will say this is somehow probably the usual story. It all started with a routine update. We are currently in the process of massive reliability and performance improvements throughout the platform and a part of that was improving the performance of our SmartEdge routing system. SmartEdge leverages a large amount of data that is periodically synced to our DNS nodes. To do this, we take advantage of our Edge Storage platform that is responsible for distributing the large database files around the world through Bunny CDN.</p><p>  我会说这可能是一个常见的故事。这一切都始于常规更新。我们目前正在整个平台的大规模可靠性和性能改进过程，其中一部分正在提高我们的SmartEdge路由系统的性能。 SmartEdge利用了定期同步到我们DNS节点的大量数据。为此，我们利用我们的边缘存储平台，负责通过Bunny CDN分发世界各地的大型数据库文件。</p><p> In an effort to reduce memory, traffic usage, and Garbage Collector allocations, we recently switched from using JSON to a binary serialization library called  BinaryPack. For a few weeks, life was great, memory usage was down, GC wait time was down, CPU usage was down, until it all went down.</p><p> 为了减少内存，流量使用和垃圾收集器分配，我们最近将JSON转换为名为BinaryPack的二进制序列化库。几周后，生活很大，内存用法下来，GC等待时间下降，CPU使用率下降，直到它一切都倒了。</p><p> On June 22nd at 8:25 AM UTC, we released a new update designed to reduce the download size of the optimization database. Unfortunately, this managed to upload a corrupted file to the Edge Storage. Not a problem by itself, the DNS was designed to work either with data or without data and was designed to graciously ignore any exceptions. Or so we thought.</p><p> 6月22日在UTC上午8:25，我们发布了一个新的更新，旨在减少优化数据库的下载大小。不幸的是，这可以将损坏的文件上传到边缘存储。没有问题本身，DNS旨在使用数据或没有数据工作，并且旨在慷慨地忽略任何异常。或者我们想到了。</p><p> Turns out, the corrupted file caused the BinaryPack serialization library to immediately execute itself with a stack overflow exception, bypassing any exception handling and just exiting the process. Within minutes, our global DNS server fleet of close to a 100 servers was practically dead.</p><p> 拒绝，损坏的文件导致BinaryPack序列化库立即使用堆栈溢出异常执行自己，绕过任何异常处理并仅退出该过程。几分钟之内，我们的全球DNS服务器队靠近100台服务器实际上已经死亡。 </p><p>   It took us some time to actually realize what was going on. After 10 minutes, we realized the DNS servers were restarting and dying and there was actually no way to bring them back up in this state.</p><p>我们花了一些时间实际上意识到正在发生的事情。 10分钟后，我们意识到DNS服务器正在重新启动和死亡，实际上没有办法将它们带回这种状态。</p><p> We thought we were ready for this. We have the ability to immediately roll back any deployments within a click of a button. And this is when we realized, things were much more complicated than they seemed. We immediately rolled back all updates for the SmartEdge system, but it was already too late.</p><p> 我们以为我们已经为此做好了准备。我们有能力立即回滚按钮中的任何部署。这是我们意识到的时候，事情比他们似乎更复杂。我们立即回滚了SmartEdge系统的所有更新，但它已经太晚了。</p><p> Both SmartEdge and the deployment systems we use rely on Edge Storage and Bunny CDN to distribute data to the actual DNS servers. On the other hand, we just wiped out most of our global CDN capacity.</p><p> SmartEdge和Deployment Systems都使用依赖Edge Storage和Bunny CDN来将数据分发到实际的DNS服务器。另一方面，我们只是擦除了我们的大部分全球CDN容量。</p><p> While the DNS is auto-healing by itself, every time it attempted to come back, it would try to load the broken deployment and simply crash again. As you can imagine, this essentially prevented the DNS servers from reaching the CDN to download the update and continued in a loop of crashes.</p><p> 虽然DNS自身自动治疗，但每次尝试回来时，它都会尝试加载破损的部署并再次崩溃。正如您可以想象的那样，这基本上阻止了DNS服务器到达CDN下载更新并继续在崩溃中。</p><p> As you can see at 8:35 (15:35), a few servers were still struggling to keep up with requests, but it wasn&#39;t with much effect and we dropped the majority of traffic, down to 100Gbit.</p><p> 正如您可以在8:35（15:35）所看到的那样，一些服务器仍在努力跟上请求，但它没有效果，我们将大部分交通放到100Gbit。</p><p>   At 8:45 we came up with a plan. We manually deployed an update that disabled the SmartEdge system to the DNS nodes. Things finally seemed like they were working. Turns out we were very, very wrong. Due to the CDN failure, the DNS servers also ended up downloading corrupted versions of the GeoDNS databases and suddenly, all requests were going into Madrid. As one of our smallest PoPs, it quickly got obliterated.</p><p>   8:45我们想出了一个计划。我们手动部署了一个将SmartEdge系统禁用到DNS节点的更新。事情终于似乎正在工作。事实证明我们非常非常错。由于CDN故障，DNS服务器还结束了下载损坏的地理数据库版本，突然，所有请求都进入马德里。作为我们最小的流行音乐之一，它很快被删除了。</p><p> To make things worse, now 100 servers were restarting in a loop, which started crashing our central API, and even the servers we were able to bring back were now failing to start properly.</p><p> 要使事情更糟糕，现在100个服务器在循环中重新启动，这开始崩溃我们的中央API，甚至我们能够带回的服务器现在无法正常启动。 </p><p> It took us some time to realize what was actually going on and after multiple attempts to re-establish the networking, we gave up on the idea.</p><p>我们花了一些时间来实现实际在多次尝试重新建立网络后的实际发生的事情，我们放弃了这个想法。</p><p> We were stuck. We desperately needed to get things back online as soon as possible, but we practically managed to kill the whole platform with one simple corrupted file.</p><p> 我们被困了。我们迫切需要尽快在线获取在线，但我们实际上设法用一个简单的损坏文件杀死整个平台。</p><p>  Since all of our internal distribution was now corrupted and served through the CDN, we had to find an alternative. As a temporary measure, at around 9:40 we decided that if we&#39;re sending all requests to one region, we might as well send those to our biggest region. We prepared a routing update that routed all requests through Frankfurt instead.</p><p>  由于我们的所有内部分配现在遭到损坏并通过CDN服务，我们必须找到替代方案。作为临时措施，在9:40左右，我们决定，如果我们向一个地区发送所有请求，我们也可以向我们最大的地区发送那些。我们准备了一个路由更新，通过法兰克福路由所有请求。</p><p> This was our first success, and a decent portion of traffic was coming back online. But it wasn&#39;t a solution. We manually deployed this to a couple of DNS servers, but the rest of the fleet was still sending everything to Madrid, so we needed to act fast.</p><p> 这是我们的第一个成功，而且交通的体面部分正在网上回来。但它不是一个解决方案。我们手动部署到几个DNS服务器，但船队的其余部分仍然向马德里发送一切，所以我们需要快速行动。</p><p> We decided we screwed up big time, and the only way to get out of this was to stop using our own systems entirely. To do that, we went to work and painstakingly migrated all of our deployment systems and files over to a third party cloud storage service.</p><p> 我们决定搞砸了大时间，唯一的途径是为了完全停止使用自己的系统。为此，我们去上班，煞费苦心地将所有部署系统和文件迁移到第三方云存储服务。</p><p> At 10:15, we were finally ready. We rewired our deployment system and DNS software to connect through to the new storage and hit Deploy. Traffic was slowly but surely coming back, and at 10:30 we were back in the game. Or so we thought.</p><p> 在10:15，我们终于准备好了。我们重新提供了我们的部署系统和DNS软件，以连接到新的存储和命中部署。交通缓慢但肯定会回来，在10:30，我们回到了比赛中。或者我们想到了。</p><p> Of course, everything was on fire and while we were doing our best to rush this, while also dealing with hundreds of support tickets and keeping everyone properly informed, we made a bunch of typos and mistakes. We knew it&#39;s important to stay calm in these situations, but this is easier said than done.</p><p> 当然，一切都在火上，当我们尽最大努力赶上这一点，同时也处理数百个支持门票，并使每个人都妥善了解，我们造成了一堆错字和错误。我们知道它在这些情况下保持冷静，但这比完成这更容易。 </p><p> Turns out during our rush to get this fixed, we deployed an incorrect version of the GeoDNS database, so while we re-established the DNS clusters, they were still sending requests to Madrid. We were getting more and more frustrated, but it was time to calm down, double-check everything and make the final deployment.</p><p>在我们急于获得此修复过程中，我们部署了一个错误的Geodns数据库版本，因此在我们重新建立DNS集群时，它们仍然向马德里发送请求。我们越来越沮丧，但是是时候冷静下来了，仔细检查一切并进行最终部署。</p><p> At 10:45, we did just that. Now connecting everything to a third-party service, we managed to sync up the databases, deploy the newest file sets and get things back online.</p><p> 在10:45，我们做到了。现在将所有内容连接到第三方服务，我们设法同步数据库，部署最新文件集并在线获取内容。</p><p> We painstakingly watched traffic pick back up for 30 minutes, while making sure things were back online. Our Storage was being pushed to its limits as without the SmartEdge system, we were serving a lot of uncached data. Things finally started stabilizing at 11:00, and bunny.net was back online in recovery mode.</p><p> 我们煞费苦心地看着交通挑选30分钟，同时确保事情在线回来。我们的存储被推到了它的限制，因为没有SmartEdge系统，我们正在为许多未加工的数据提供服务。终于在11:00开始稳定的东西，Bunny.net在恢复模式下返回。</p><p>  We designed all of our systems to work together and rely on each other, including the critical pieces of our internal infrastructure. If you build a bunch of cool infrastructure, you&#39;re of course lured into implementing this into as many systems as you can.</p><p>  我们设计了我们所有的系统，共同努力，依赖于彼此，包括我们内部基础架构的关键作品。如果你建造一堆很酷的基础设施，你当地诱惑到尽可能多的系统中。</p><p> Unfortunately, that allowed something as simple as a corrupted file to crash down multiple layers of redundancy with no real way of bringing things back up. It crashed our DNS, it crashed the CDN, it crashed the storage and finally, it crashed the optimizer service.</p><p> 不幸的是，它允许一些像腐败的文件一样简单，以崩溃多层冗余，没有真正的方式让东西备份。它崩溃了我们的DNS，它崩溃了CDN，它崩溃了存储，最后，它崩溃了优化的服务。</p><p> In fact, the ripple effect even crashed our API and our dashboard as hundreds of servers were being brought back up, which in turn finally also crashed the logging service.</p><p> 事实上，涟漪效应甚至崩溃了我们的API，我们的仪表板作为数百个服务器被带回了，这反过来又会崩溃了伐木服务。</p><p>  While we believe this should never have happened in the first place, we are taking it as a valuable lesson learned. We are definitely not perfect, but we are doing our best to get as close as possible. Going forward, the only way to get there is to learn and improve on our mistakes.</p><p>  虽然我们相信这一点永远不会发生在第一个地方，但我们将其作为一种有价值的课程。我们绝对不完美，但我们正在尽力尽可能接近。展望未来，获得的唯一方法就是学习和改善我们的错误。 </p><p> First of all, we want to apologize to anyone affected and reassure everyone that we are treating this with the utmost urgency. We had a good run of multiple years without an extensive system-wide failure, and we are determined to make sure this does not happen again anytime soon.</p><p>首先，我们想向任何受影响的人道歉，并安抚大家，我们以最大的紧迫性对待这一点。我们在没有广泛的系统范围内失败的情况下有很多多年的频率，我们决心确保不再发生这种情况。</p><p> To do this, the first and smallest step will be to phase out the BinaryPack library and make sure we run a more extensive testing on any third-party libraries we work with in the future.</p><p> 为此，第一个和最小的步骤将逐步逐步淘汰BinaryPack库，并确保我们在未来工作的任何第三方图书馆运行更广泛的测试。</p><p> The bigger problem also became apparent. Building your own infrastructure inside of its own ecosystem can have dire consequences and can fall down like a set of dominos. Amazon proved this in the past, and back then we thought this won&#39;t happen to us, and oh how wrong we were.</p><p> 更大的问题也变得明显。在自己的生态系统内建立自己的基础设施可以有可怕的后果，可以像一组多米诺骨一样摔倒。亚马逊在过去证明了这一点，然后我们认为这是我们的妻子，哦，我们有多错了。</p><p> We are currently planning a complete migration of our internal APIs to a third-party independent service. This means if their system goes down, we lose the ability to do updates, but if our system goes down, we will have the ability to react quickly and reliably without being caught in a loop of collapsing infrastructure.</p><p> 我们目前正计划将内部API的完整迁移到第三方独立服务。这意味着如果他们的系统下降，我们就会失去更新的能力，但如果我们的系统下降，我们将有能力快速且可靠地反应而不会陷入倒塌的基础设施。</p><p> Finally, we are making the DNS system itself run a local copy of all backup data with automatic failure detection. This way we can add yet another layer of redundancy and make sure that no matter what happens, systems within bunny.net remain as independent from each other as possible and prevent a ripple effect when something goes wrong.</p><p> 最后，我们正在使DNS系统本身运行具有自动故障检测的所有备份数据的本地副本。这样我们就可以添加另一层冗余，并确保无论发生什么事，Bunny.net中的系统仍然可以尽可能彼此独立，并在出现问题时防止纹波效果。</p><p> I would like to share my thanks to the support team that was working tirelessly to keep everyone in the loop and all of our users for bearing with us while we battled through this.</p><p> 我想对我的支持团队分享，因为我们努力让我们在循环中的所有人身上和我们所有的用户都在与我们联系到我们的态度。</p><p> We understand this has been a very stressful situation not only for ourselves, but especially for all of you who rely on us to stay online, so we are making sure we learn and improve from these events and come out more reliable than ever.</p><p> 我们理解这一直是一个非常紧张的局面，不仅对自己，而且特别是对我们所有人依靠我们留在网上的人，所以我们确保我们从这些活动中学习和改进，并比以往任何时候都更加可靠。 </p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://bunny.net/blog/the-stack-overflow-of-death-dns-collapse/">https://bunny.net/blog/the-stack-overflow-of-death-dns-collapse/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/兔子/">#兔子</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/yesterday/">#yesterday</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/dns/">#dns</a></button></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>