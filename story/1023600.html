<!doctype html><html lang="zh-hans"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>变压器是图形神经网络</title><link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous"><link rel="stylesheet" href="/img/css.css?random="><link data-rh="true" rel="icon" href="/img/favicon.ico"/><script data-ad-client="ca-pub-6067137220025946" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script type="text/javascript" src="https://platform-api.sharethis.com/js/sharethis.js#property=5effb96910009800120b8d4d&product=inline-share-buttons" async="async"></script>
<script>var _hmt = _hmt || [];(function() {var hm = document.createElement("script");hm.src = "https://hm.baidu.com/hm.js?03c1a0f31299b4a2fbb83c34d6beaac9";var s = document.getElementsByTagName("script")[0]; s.parentNode.insertBefore(hm, s);})();</script></head><body><div id="my_header"><div class="container"><nav class="navbar navbar-expand-lg"><a class="navbar-brand" href="/"><img alt="diglog" src="/img/logo.v1.gif" class="rounded-sm"></a><button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button><div class="collapse navbar-collapse" id="navbarNavAltMarkup"><div class="navbar-nav"></div></div></nav></div></div><div class="container"><div id="my_content"><h1 class="page_narrow">变压器是图形神经网络</h1><div class="row"><div class="col-lg-12 col-12"><div class="my_story_list_item shadow p-3 mb-5 bg-white rounded"><div class="story_page_pub_time page_narrow">2020-09-13 03:07:21</div><div class="story_img_container"><a href="http://img2.diglog.com/img/2020/9/53f4115cd014386bd7de9c4496ec9370.jpeg"><img src="http://img2.diglog.com/img/2020/9/53f4115cd014386bd7de9c4496ec9370.jpeg" class="img-fluid my_story_img" onerror="this.style.display='none'"></a></div><div class="page_narrow text-break page_content"><p>我的工程师朋友经常问我：深入学习图形听起来很棒，但是有真正的应用吗？</p><p>虽然Pinterest、阿里巴巴和Twitter的推荐系统都使用了图形神经网络，但一个更微妙的成功故事是Transformer架构，它在NLP世界掀起了一场风暴。通过这篇文章，我想在图形神经网络(GNNs)和变形金刚之间建立一种联系。我将讨论NLP和GNN社区中模型体系结构背后的直觉，使用等式和数字建立联系，并讨论我们如何共同努力来推动未来的发展。让我们先来谈谈模型架构的目的--表示学习。</p><p>在更高的层次上，所有的神经网络结构都将输入数据的表示构建为向量/嵌入，其编码关于数据的有用的统计和语义信息。然后，这些潜在的或隐藏的表示可以用于执行一些有用的事情，例如对图像进行分类或翻译句子。神经网络通过接收反馈(通常通过误差/损失函数)来学习建立越来越好的表示。</p><p>对于自然语言处理(NLP)，传统上，递归神经网络(RNNs)以顺序方式(即，一次一个单词)构建句子中每个单词的表示。直观地说，我们可以将RNN层想象为传送带，其上的文字从左到右进行自回归处理。最后，我们为句子中的每个单词获得一个隐藏特征，并将其传递给下一个RNN层或用于我们选择的NLP任务。</p><p>我强烈推荐克里斯·奥拉(Chris Olah)的传奇博客，上面有关于RNN的概述和NLP的表示学习。</p><p>最初用于机器翻译的Transformers已经逐渐取代了主流NLP中的RNN。该体系结构采用了一种新的表示学习方法：完全取消重复，Transformers使用注意力机制来构建每个单词的特征，以确定句子中的所有其他单词有多重要。前面提到的那个词。知道了这一点，单词的更新特征只是所有单词特征的线性变换的总和，按它们的重要性加权。</p><p>早在2017年，这个想法听起来非常激进，因为NLP社区非常习惯于使用RNN处理文本的顺序-一次一个词的风格。这篇论文的标题可能是火上浇油！作为回顾，Yannic Kilcher做了一个出色的视频概述。</p><p>让我们通过将上一段翻译成数学符号和向量的语言来发展对体系结构的直觉。我们将句子$\mathcal{S}$中第$i$&$h；个单词的隐藏特征$h$从$\ell$层更新到$\ell+1$层，如下所示：</p><p>$$h_{i}^{\ell+1}=\text{注意}\Left(Q^{\ell}h_{i}^{\ell}\，K^{\ell}h_{j}^{\ell}\，V^{\ell}h_{j}^{\ell}\right)$$即，$$\h_{i}^{\ell+1}=\sum_{j\in\mathcal{S}w_{ij}\Left(V^{\ell}h_{j}^{\ell}\right)$$。</p><p>其中$j\in\mathcal{S}$表示句子中的词集，$Q^{\ell}，K^{\ell}，V^{\ell}$是可学习的线性权重(分别表示用于注意力计算的查询、键和值)。注意机制是对句子中的每个单词并行执行的，以在一个镜头中获得它们的更新特征-这是Transformers over RNNs的另一个加点，它逐个单词地更新特征。</p><p>要让这种直截了当的点积注意机制发挥作用被证明是棘手的。可学习权重的不良随机初始化会破坏训练过程的稳定性。我们可以通过并行执行多个注意力头部并连接结果(每个头部现在都有单独的可学习权重)来克服这一点：</p><p>$$\text{head}_k=\text{注意}\Left(q^{k，\ell}h_i^{\ell}\，K^{k，\ell}h_j^{\ell}\，V^{k，\ell}h_j^{\ell}\right)，$$。</p><p>其中，$Q^{k，\ell}、K^{k，\ell}、V^{k，\ell}$是$k$&#k注意标题的可学习权重，$O^{\ell}$是向下投影，以匹配层间$h_i^{\ell+1}$和$h_i^{\ell}$的尺寸。</p><p>多个头部允许注意力机制本质上对冲它的赌注，观察与前一层不同的隐藏特征的变换或方面。我们稍后再详细讨论这个问题。</p><p>激发最终变形金刚架构的一个关键问题是，注意机制之后的单词功能可能具有不同的规模或大小。这可能是由于某些单词在对其他单词的特征求和时具有非常尖锐或非常分散的注意力权重$w_{ij}$。此外，在单个要素/矢量条目级别，跨多个注意头连接-每个注意头可能以不同的比例输出值-可能会导致最终矢量$h_{i}^{\ell+1}$的条目具有很大范围的值。遵循传统的ML智慧，在管道中添加一个规格化层似乎是合理的。</p><p>Transformers使用LayerNorm克服了问题(2)，LayerNorm在特征级别规格化并学习仿射变换。此外，按特征维度的平方根缩放点积关注度有助于抵消问题(1)。</p><p>最后，作者提出了另一个控制规模问题的诀窍：具有特殊结构的位置式两层MLP。在多头注意之后，他们通过可学习的权重将$h_i^{\ell+1}$投射到(荒谬的)更高维度，在那里经历RELU非线性，然后被投射回其原始维度，随后进行另一次归一化：</p><p>老实说，我不确定过度参数化的前馈子层背后的确切直觉是什么。我认为LayerNorm和Scaled点积并没有完全解决突出的问题，所以大的MLP是一种重新缩放彼此独立的特征向量的技巧。根据Jannes Muenchmeyer的说法，前馈子层确保了变压器是一个通用的逼近器。因此，与在隐藏层上保持相同维度相比，投影到非常高的维度空间、应用非线性并重新投影到原始维度允许模型表示更多的功能。</p><p>Transformer架构还极易适应非常深的网络，使NLP社区能够在模型参数和数据方面进行扩展。每个多头注意子层和前馈子层的输入和输出之间的剩余连接是堆叠变压器层的关键(但为清楚起见在图中省略)。</p><p>图形神经网络(GNNs)或图形卷积网络(GCNS)构建图形数据中节点和边的表示。它们通过邻域聚合(或消息传递)来实现这一点，在邻域聚合(或消息传递)中，每个节点从其邻居那里收集特征，以更新其周围的本地图结构的表示。堆叠几个GNN层使模型能够在整个图中传播每个节点的特征--从它的邻居到邻居，以此类推。</p><p>在其最基本的形式中，GNN通过对添加到来自每个相邻节点$j\in\Mathcal{N}(I)$的特征聚合$h_j^{\ell}$的节点自身特征$h_i^{\ell}$进行非线性变换，来更新位于$\ell$层的节点$i$(例如，😆)的隐藏特征$h$。</p><p>$$h_{i}^{\ell+1}=\sigma\Big(U^{\ell}h_{i}^{\ell}+\sum_{j\in\mathcal{N}(I)}\Left(V^{\ell}h_{j}^{\ell}\right)\Big)，$$。</p><p>其中$U^{\ell}，V^{\ell}$是GNN层的可学习权重矩阵，$\sigma$是非线性函数，如REU。在本例中，$\Mathcal{N}$(😆)$=${😘，😎，😜，🤩}。</p><p>邻域节点$j\in\mathcal{N}(I)$上的求和可以由其他输入大小不变的聚集函数(例如简单的均值/最大值)或诸如经由注意力机制的加权和之类的更强大的东西来代替。</p><p>如果我们做多个平行的邻居头聚合，并用注意力机制(即加权和)代替邻居$j$上的求和，我们就会得到图注意力网络(GAT)。加上规格化和前馈MLP，瞧，我们就有了一个图形转换器！</p><p>要使这种联系更明确，可以将一个句子看作一个完全连通的图，其中每个单词都与其他单词相连。现在，我们可以使用GNN为图(句子)中的每个节点(单词)构建功能，然后可以使用这些功能执行NLP任务。</p><p>概括地说，这就是变形金刚正在做的事情：它们是具有多头注意力的GNN，作为邻里聚合功能。标准GNN从其局部邻域节点$j\in\mathcal{N}(I)$聚合特征，而用于NLP的Transformers for NLP将整个句子$\mathcal{S}$视为局部邻域，在每一层聚合来自每个单词$j\in\mathcal{S}$的特征。</p><p>重要的是，各种针对特定问题的技巧-如位置编码、因果/掩蔽聚合、学习速率计划和广泛的预培训-对于变形金刚的成功至关重要，但在GNN社区中很少出现。同时，从GNN的角度看变形金刚可以激励我们去掉架构中的许多花哨。</p><p>现在我们已经在变形金刚和GNN之间建立了联系，让我来说说一些想法。首先，全连通图表是NLP的最佳输入格式吗？</p><p>在统计NLP和ML之前，诺姆·乔姆斯基(Noam Chomsky)等语言学家专注于发展语言结构的形式理论，如句法树/图。TreeLSTM已经尝试过了，但是也许Transformers/GNN是将语言理论和统计NLP这两个世界结合在一起的更好的架构？例如，Mila和Stanford最近的一项工作探索了用语法树增强预先训练的变形金刚，如Bert[Sachan等人，2020]。</p><p>完全连通图的另一个问题是，它们使学习单词之间的长期依赖关系变得非常困难。这很简单，因为图中的边数与节点数成二次函数关系，也就是说，在$n$word语句中，Transformer/GNN将对$n^2$对单词进行计算。花了很大一笔钱，事情就变得一发不可收拾了。</p><p>NLP社区对长序列和依赖关系问题的观点很有趣：使注意力机制在输入大小方面稀疏或自适应，在每一层中添加递归或压缩，以及使用位置敏感散列来有效关注，这些都是更好的转换器的有前途的新想法。有关更多细节，请参阅麦迪逊·梅关于“变形金刚”长期背景的出色调查。</p><p>看到GNN社区的想法加入进来会很有趣，例如，句子图稀疏的二进制划分似乎是另一种令人兴奋的方法。BP-Transformers递归地将句子细分为两个，直到它们可以从句子标记构建层次二叉树。这种结构性归纳偏向帮助模型以内存高效的方式处理较长的文本序列。</p><p>NLP社区有几篇关于“变形金刚”可能学到什么的有趣论文。基本前提是注意句子中的所有单词对-目的是确定哪些对是最有趣的-使Transformers能够学习一些类似于特定任务的语法。在多中心注意中，不同的中心也可能在关注不同的句法属性。</p><p>在图的术语中，通过在完整图上使用GNN，我们能否从GNN如何在每一层执行邻域聚合中恢复最重要的边-以及它们可能带来的影响？我还不太相信这个观点。</p><p>我更赞同多头机制的最优化观点--拥有多个注意力头可以改善学习，克服糟糕的随机初始化。例如，这些论文表明，变压器磁头可以在培训后修剪或移除，而不会对性能产生重大影响。</p><p>多头邻域聚合机制在GNNs中也被证明是有效的，例如，GAT使用相同的多头关注，而MONET使用多个高斯核来聚合特征。虽然发明多头技巧是为了稳定注意力机制，但多头技巧会成为挤出额外模型性能的标准吗？</p><p>相反，具有简单聚合函数(例如SUM或MAX)的GNN不需要用于稳定训练的多个聚合头。如果我们不必计算句子中每个单词对之间的配对兼容性，对变形金刚来说不是很好吗？</p><p>变形金刚能从完全摆脱关注中获益吗？Yann Dauphin和合作者最近的工作提出了一种替代的ConvNet架构。变形金刚也可能最终会做一些类似于ConvNets的事情！</p><p>阅读新的变形金刚论文让我觉得，在确定最佳的学习速度计划、热身策略和衰变设置时，训练这些模型需要一些类似于黑魔法的东西。这可能很简单，因为模型太大了，而研究的NLP任务又是如此具有挑战性。</p><p>但最近的结果表明，这也可能是由于体系结构内的标准化和残余连接的特定排列造成的。</p><p>我很喜欢阅读新的@DeepMind Transformer论文，但为什么训练这些模型如此黑暗？对于基于单词的LM，我们使用了16,000个热身步骤和500,000个腐烂步骤，并牺牲了9,000只山羊。&#34；https://t.co/dP49GTa4ze pic.twitter.com/1K3Fx4s3M8。</p><p>-Chaitanya K.Joshi(@chaitjo)2020年2月17日。</p><p>我知道我在大喊大叫，但这让我产生了怀疑：我们真的需要昂贵的结对关注、过度参数化的MLP子层和复杂的学习时间表的多个头部吗？我们真的需要拥有大量碳足迹的大型车型吗？对于手头的任务具有良好的归纳偏差的架构应该更容易训练吗？</p><p>要从NLP的角度深入研究Transformer架构，请查看这些令人惊叹的博客文章：插图变压器和注释变压器。</p><p>此外，这个博客并不是第一个将GNN和变形金刚联系起来的博客。下面是Arthur Szlam关于注意力/记忆网络、GNN和变形金刚之间的历史和联系的精彩演讲。同样，DeepMind的明星云集的职位论文介绍了Graph Networks框架，统一了所有这些想法。对于代码演练，DGL团队有一个关于seq2seq作为图问题和将Transformers作为GNN构建的很好的教程。</p><p>这篇帖子最初出现在台大Graph深度学习实验室的网站和媒体上，现在也被翻译成了中文和俄文。一定要在Twitter、Reddit或HackerNews上加入讨论！</p><p>变压器是图神经网络的特例。这对一些人来说可能是显而易见的，但下面的博客帖子很好地解释了这些重要的概念。Https://t.co/H8LT2F7LqC</p><p>-Oriol Vinyals(@oriolVinyalsML)2020年2月29日。</p><p>Chaitanya K.Joshi是新加坡A*STAR的研究工程师，致力于图形神经网络及其在加速科学发现方面的应用。他于2019年获得新加坡国立大学计算机科学学士学位，此前曾在Xavier Bresson博士手下担任研究助理。他的工作已经在顶级机器学习场所展示，包括NeurIPS，ICLR和INFORMS。</p><p>如果你喜欢这篇文章，并想听更多，请订阅梯度版，并在Twitter上关注我们。</p></div><div id="story_share_this"><div class="sharethis-inline-share-buttons"></div></div><div class="text-break sotry_link page_narrow"><a target="_blank" href="https://thegradient.pub/transformers-are-graph-neural-networks/">https://thegradient.pub/transformers-are-graph-neural-networks/</a></div><div class="story_tags page_narrow"><button type="button" class="btn btn-light my_tag"><a href="/tag/神经网络/">#神经网络</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/图形/">#图形</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/graph/">#graph</a></button><button type="button" class="btn btn-light my_tag"><a href="/tag/学习/">#学习</a></button></div></div><div class="shadow p-3 mb-5 bg-white rounded clearfix"><div class="container"><div class="row"><div class="col-sm"><div><a target="_blank" href="/story/1023373.html"><img src="http://img2.diglog.com/img/2020/9/thumb_a6915f007908a107dc669edbb47b027c.jpg" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1023373.html">整个宇宙可能是一个神经网络</a></div><span class="my_story_list_date">2020-9-12 0:43</span></div><div class="col-sm"><div><a target="_blank" href="/story/1022068.html"><img src="http://img2.diglog.com/img/2020/9/thumb_ecb7ebc1f5a6156b0fd9e36e140460b2.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1022068.html">Learning@home hivemind-在互联网上训练大型神经网络</a></div><span class="my_story_list_date">2020-9-4 15:7</span></div><div class="col-sm"><div><a target="_blank" href="/story/1021761.html"><img src="http://img2.diglog.com/img/2020/9/thumb_f96a3eeaa75d092d8ffc5cc98339ecd4.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1021761.html">基于神经网络的超新星快速探测</a></div><span class="my_story_list_date">2020-9-2 21:42</span></div><div class="col-sm"><div><a target="_blank" href="/story/1018049.html"><img src="http://img.diglog.com/img/2020/8/thumb_a1915e6aa6b37ba0df2f12e8f5106eb0.png" class="img-fluid" onerror="this.style.display='none'"></a></div><div class="item_title"><a target="_blank" href="/story/1018049.html">InvoiceNet：从发票单据中提取信息的神经网络</a></div><span class="my_story_list_date">2020-8-14 23:6</span></div></div></div></div><div class="my_movie_list_item shadow p-3 mb-5 bg-white rounded"><button type="button" class="btn btn-link my_tag"><a href="/tag/web2.0/">#web2.0</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/google/">#google</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/设计/">#设计</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/创意/">#创意</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/摄影/">#摄影</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/游戏/">#游戏</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/图片/">#图片</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/软件/">#软件</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/视频/">#视频</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/手机/">#手机</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/广告/">#广告</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/apple/">#apple</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/iphone/">#iphone</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/网站/">#网站</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/免费/">#免费</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/下载/">#下载</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/windows/">#windows</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/微软/">#微软</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/firefox/">#firefox</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/苹果/">#苹果</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/blog/">#blog</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/音乐/">#音乐</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/博客/">#博客</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/wordpress/">#wordpress</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/恶搞/">#恶搞</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/艺术/">#艺术</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/qq/">#qq</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/web/">#web</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/谷歌/">#谷歌</a></button><button type="button" class="btn btn-link my_tag"><a href="/tag/工具/">#工具</a></button></div></div></div><div id="my_footer"><div class=""><a href="/tags/">tags</a> <a href="/users/">users</a></div>&copy;2012-2021 diglog.com </div></div></body></html>